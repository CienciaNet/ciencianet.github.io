<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on CienciaNet</title>
    <link>https://ciencianet.com.ar/post/</link>
    <description>Recent content in Posts on CienciaNet</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Copyright © 2008–2018, Steve Francia and the Hugo Authors; all rights reserved.</copyright>
    <lastBuildDate>Sat, 27 Aug 2022 12:36:17 -0300</lastBuildDate><atom:link href="https://ciencianet.com.ar/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Los plásticos pueden ser tan valiosos como el diamante: sus aplicaciones especiales y espaciales</title>
      <link>https://ciencianet.com.ar/post/los-plasticos-pueden-ser-tan-valiosos-como-el-diamante/</link>
      <pubDate>Sat, 27 Aug 2022 12:36:17 -0300</pubDate>
      
      <guid>https://ciencianet.com.ar/post/los-plasticos-pueden-ser-tan-valiosos-como-el-diamante/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Javier C. Quagliano Amado.&lt;/strong&gt; Departamento de Química Aplicada, Instituto de Investigaciones Científicas y Técnicas para la Defensa (CITEDEF).&lt;/p&gt;
&lt;p&gt;⊣ Foto de &lt;a href=&#34;https://unsplash.com/@sen7?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&#34;&gt;Sen&lt;/a&gt; en &lt;a href=&#34;https://unsplash.com/es/s/fotos/plastic?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&#34;&gt;Unsplash&lt;/a&gt; ⊢&lt;/p&gt;
&lt;p&gt;Los plásticos son considerados como materiales muy útiles e indispensables para la vida actual. Sin embargo se les da un menor valor que a otros materiales debido a que sus principales usos son en aplicaciones en los cuales se los desecha, como por ejemplo envases y embalajes para alimentos. Sin embargo, la familia de los plásticos es muy grande. Los plásticos más utilizados son el polietileno de baja densidad (LDPE) y de alta densidad (HDPE), polipropileno (PP), polietilentereftalato (PET), policloruro de vinilo (PVC), poliestireno (PS), y composiciones de polímeros incluídos en “la categoría 7”, como puede verse en muchos productos plásticos. Estos a veces son conocidos como los “Siete grandes”, dada la masividad de su uso.&lt;/p&gt;
&lt;p&gt;Sin embargo, existen otros miembros de la familia de los plásticos a pesar de ser menos conocidos, son muy valiosos y únicos por su funcionalidad excepcional. Es por ejemplo el caso del Kevlar, nombre comercial de una poliaramida (poliamida aromática), que fue el único material posible para su uso en los paracaídas de las naves espaciales que aterrizaron en Marte, como las misiones &lt;a href=&#34;https://es.wikipedia.org/wiki/Programa_Viking&#34;&gt;Viking&lt;/a&gt; y &lt;a href=&#34;https://es.wikipedia.org/wiki/Mars_Pathfinder&#34;&gt;Pathfinder&lt;/a&gt;. En este sentido, su valor aquí fue mayor que el diamante más caro. Aún así, el Kevlar es utilizado en numerosas aplicaciones de alta demanda (siendo la más conocida la tela de los chalecos antibalas para protección personal), por ese motivo su precio fue reduciéndose para que su uso se masifique.&lt;/p&gt;
&lt;h4 id=&#34;otro-ejemplo&#34;&gt;Otro ejemplo&lt;/h4&gt;
&lt;p&gt;Elastómeros especiales son utilizados en propulsantes compuestos para cohetes de combustible sólido. Los propulsantes sólidos se usan para impulsar los llamados vectores, es decir, los vehículos aeroespaciales, ya sean o no guiados de alguna manera, generalmente no tripulados. En los propulsantes sólidos el combustible es un sal oxidante englobada en una matriz de elastómero, que cual es a la vez otro combustible. Este elastómero es un poliuretano derivado de un tipo especial de polibutadieno llamado “hidroxi-terminal”.&lt;/p&gt;
&lt;p&gt;Esta es una variante del mismo polibutadieno utilizado en algunos tipos de neumáticos. Este polibutadieno hidroxi-terminal, o PBHT, se hace reaccionar con isocianatos para formar un poliuretano elastomérico. Este poliuretano es una matriz elástica, formada por un red de enlaces químicos que contiene una carga alta de oxidante (perclorato de amonio), carga que llega a ser hasta del 80% (aunque generalmente va de 60 a 65% en peso). El PBHT es uno de los pocos polioles que admiten esa alta cantidad de carga de relleno. Prácticamente es el material de elección para este fin desde su invención a fines de los años 60.&lt;/p&gt;
&lt;p&gt;En el Departamento de Química Aplicada del Instituto de &lt;a href=&#34;https://www.argentina.gob.ar/defensa/citedef&#34;&gt;Investigaciones Científicas y Técnicas para la Defensa&lt;/a&gt; (CITEDEF), dependiente del Ministerio de Defensa, se investiga este tipo de polímeros, y se utilizan en la preparación de las formulaciones propulsantes. Hasta el momento estos materiales son de origen extranjero, sin embargo es posible adquirirlos y adecuarlos a las necesidades. Aparte de su rol como ligante (“&lt;em&gt;binder&lt;/em&gt;”) en el propulsante, el PBHT es formulado convenientemente con cargas minerales para obtener pinturas especiales (llamadas “inhibiciones”) las cuales se aplican en la cara interna de los tubos motor cohete. El objetivo es brindar una protección a la carcaza de muy altas temperaturas que se generan por la combustión del propulsante (del orden de 2000 a 3000°C).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2022/08/cohete.png&#34; alt=&#34;Maqueta del proyecto del cohete &amp;quot;Gradicom III&amp;quot;, exhibido en el predio de CITEDEF.&#34;&gt;&lt;/p&gt;
&lt;p&gt;En el área de cohetería existe un considerable desarrollo en nuestro país. Las actividades se remontan a la década del sesenta y setenta, cuando la Fuerza Aérea Argentina coordinó el lanzamiento de varios cohetes, incluso con animales para experimentación (el famoso mono &lt;a href=&#34;https://es.wikipedia.org/wiki/Juan_(mono)&#34;&gt;Juan&lt;/a&gt;). Luego de un período de abandono se volvieron a realizar lanzamientos de cohetes con propulsante sólido en base a PBHT entre 2009 (Gradicom I) y 2011 (cohete Gradicom II) orientados a investigaciones de la atmósfera. Desde 2020 se está tratando de volver a recuperar estas capacidades. Una aplicación específica activa es el sistema de lanzadores múltiples CP-30, desarrollado primeramente en CITEDEF y operacional en el Ejército Argentino.
Otra área diferente de avance en el tema de vectores se centraliza en el uso de combustibles líquidos, como los usados por el cohete Tronador, desarrollado por la Comisión Nacional de Actividades Aeroespaciales y orientado a desarrollos satelitales. El desarrollo de estas actividades es fundamental para que nuestro país se mantenga como una nación aspirante a adoptar las principales tecnologías de los países desarrollados.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Una mirada física al juego del fútbol: ¡que el borracho no te saque la pelota!</title>
      <link>https://ciencianet.com.ar/post/una_mirada_fisica_al_juego_del_futbol/</link>
      <pubDate>Tue, 15 Jun 2021 18:36:17 -0300</pubDate>
      
      <guid>https://ciencianet.com.ar/post/una_mirada_fisica_al_juego_del_futbol/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Juan Pablo Álvarez y Emilio A. Winograd.&lt;/strong&gt;
⊣ Foto de &lt;a href=&#34;https://unsplash.com/@jaenix?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&#34;&gt;Jannik Skorna&lt;/a&gt; en &lt;a href=&#34;https://unsplash.com/?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&#34;&gt;Unsplash&lt;/a&gt; ⊢&lt;/p&gt;
&lt;p&gt;La toma de decisiones basada en el análisis de grandes volúmenes de datos es una actividad cada vez más frecuente en todas las disciplinas y el deporte, en particular, sigue esa misma tendencia. El béisbol, básquet y fútbol americano son ejemplos pioneros y paradigmáticos de ello. La forma en que se analizan los jugadores y las acciones del juego en estos deportes han modificado notablemente el espectáculo. En nuestro fútbol, también las decisiones basadas en datos son cada vez más habituales y su uso está bastante extendido en lo que concierne al fichaje de jugadores. Sin embargo, su aplicación a los partidos en sí está más retrasada que en otros deportes, principalmente por tratarse de un juego con dinámica compleja, con situaciones con baja repetitividad, con baja cantidad de recompensas (goles) y sujeto a acciones imprevisibles.&lt;/p&gt;
&lt;p&gt;Sin embargo, por más complejo que sea, el fútbol no responde a la dinámica de lo impensado en lo que al juego se refiere. En la ciencia, sabemos que la dinámica de procesos con variables aleatorias puede ser predicha. Ejemplos abundan en todas las disciplinas, desde el movimiento browniano explicado en uno de los célebres trabajos de Einstein en 1905, pasando por las predicciones climáticas, aplicaciones a la economía, entre tantas otras. Esto también se expresa en el trabajo publicado por Chacoma, Almeira, Perotti, y Billoni, del Instituto de Física Enrique Gaviola, en Physical Review E, &amp;quot;Modeling ball possession dynamics in the game of Football&amp;quot; [&lt;a href=&#34;#1&#34;&gt;1&lt;/a&gt;], donde logran mostrar que, a pesar de su complejidad, en el fútbol es posible identificar y modelar patrones del juego de forma relativamente sencilla.&lt;/p&gt;
&lt;p&gt;Utilizando la reciente recopilación de eventos que ocurrieron durante 1941 partidos de las cinco mayores ligas de Europa, de la Copa Mundial 2018 y de la Eurocopa 2016 [&lt;a href=&#34;#2&#34;&gt;2&lt;/a&gt;], Chacoma y colaboradores analizan, principalmente, cómo es la dinámica del juego durante la tenencia del balón. Allí logran identificar que, en la mayoría de los casos, hay solo dos o tres jugadores involucrados, y que el evento más habitual es el pase, seguido por las acciones uno contra uno (duelos). Estos últimos son los mayores responsables de las pérdidas de balón. Las observaciones las utilizan para alimentar un modelo de tres agentes (jugadores), donde dos compañeros de equipo se enfrentan a un rival, quien intenta hacerse con el balón. Este modelo logran reducirlo a un sistema unidimensional de marcha aleatoria, donde la marcha (la jugada) finaliza cuando el agente (el defensor) se aproxima a la barrera absorbente (el balón). A pesar de su simplicidad, este modelo captura adecuadamente los comportamientos estadísticos de las longitudes de los pases, los tiempos de posesión y del número de pases realizados y sirve de base para el diseño de entrenamientos específicos en la búsqueda de mejorar aspectos relacionados con la posesión del balón.&lt;/p&gt;
&lt;p&gt;Sería interesante analizar cuáles de estos hallazgos son aplicables al fútbol argentino y sudamericano. En el juego de posesión, el pase es un elemento central y es parte esencial del modelo descrito. Las características de nuestro fútbol, presumiblemente más friccionado, con mayor cantidad de interrupciones, de pases largos a disputar con el rival y más propenso a las acciones individuales, ¿responde al mismo comportamiento? Andrés Chacoma, primer autor del estudio, intuye que no deberían existir diferencias significativas, aunque destaca la necesidad de obtener y analizar datos de esas ligas para responder esta inquietud.&lt;/p&gt;
&lt;h2 id=&#34;referencias&#34;&gt;Referencias:&lt;/h2&gt;
&lt;p&gt;&lt;a id=&#34;1&#34;&gt; 1. &lt;/a&gt; Chacoma, A., N. Almeira, J. I. Perotti, and O. V. Billoni. &amp;quot;Modeling ball possession dynamics in the game of football.&amp;quot; &lt;a href=&#34;https://doi.org/10.1103/PhysRevE.102.042120&#34;&gt;Physical Review E 102, no. 4 (2020): 042120&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;2&#34;&gt; 2. &lt;/a&gt; Pappalardo, Luca, Paolo Cintia, Alessio Rossi, Emanuele Massucco, Paolo Ferragina, Dino Pedreschi, and Fosca Giannotti. &amp;quot;A public data set of spatio-temporal match events in soccer competitions.&amp;quot; &lt;a href=&#34;https://doi.org/10.1038/s41597-019-0247-7&#34;&gt;Scientific data 6, no. 1 (2019): 1-15&lt;/a&gt;.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Desarrollo matemático permite mejorar la interpretación de imágenes médicas</title>
      <link>https://ciencianet.com.ar/post/desarrollo-matematico-mejora-imagenes-medicas/</link>
      <pubDate>Thu, 20 May 2021 19:56:13 -0300</pubDate>
      
      <guid>https://ciencianet.com.ar/post/desarrollo-matematico-mejora-imagenes-medicas/</guid>
      <description>
        
          &lt;h4 id=&#34;paula-bergero-instituto-de-investigaciones-fisicoquímicas-teóricas-y-aplicadas-inifta-y-universidad-nacional-de-la-plata&#34;&gt;&lt;strong&gt;Paula Bergero&lt;/strong&gt;. Instituto de Investigaciones Fisicoquímicas Teóricas y Aplicadas (INIFTA) y Universidad Nacional de La Plata.&lt;/h4&gt;
&lt;p&gt;⊣ Foto de &lt;a href=&#34;https://unsplash.com/@bacila_vlad?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&#34;&gt;Bacila Vlad&lt;/a&gt; en &lt;a href=&#34;https://unsplash.com/?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&#34;&gt;Unsplash&lt;/a&gt; ⊢&lt;/p&gt;
&lt;p&gt;El aporte hace posible la corrección de errores de proyección y de baja resolución en imágenes de ecografías, radiografías y otros estudios y así perfeccionar los diagnósticos que se basan en el análisis fractal, como el caso de enfermedades vasculares del ojo.&lt;/p&gt;
&lt;p&gt;Los diagnósticos a partir de imágenes de estudios médicos podrán mejorar gracias a un desarrollo matemático realizado por investigadores de la Universidad Nacional de La Plata y el CONICET. Los investigadores encontraron que la proyección de una estructura curva e irregular se distorsiona al ser tomada demasiado “de cerca”, arrojando valores erróneos. Según el trabajo publicado en la revista Physical Review E, las distorsiones causan diferencias en la dimensión fractal -el parámetro que caracteriza la irregularidad y ramificación de la imagen- que pueden llegar hasta el 6%.&lt;/p&gt;
&lt;p&gt;“Lo que nosotros encontramos es cómo recuperar la dimensión fractal verdadera a partir de la imagen distorsionada”, precisó la física Isabel Irurzun, directora del grupo de Modelado y Experimentación en Sistemas Complejos y coautora del trabajo junto a Juan Tenti y Sabrina Hernández Guiance. En particular, este desarrollo permitirá mejorar el diagnóstico de enfermedades vasculares del ojo, en las que las imágenes son tomadas a muy corta distancia. La dimensión fractal de la imagen refleja el estado vascular del ojo, y un cambio del 6% debido a una deformación podría interpretarse erróneamente como una patología. La mejora en el diagnóstico podrá beneficiar a las personas afectadas por la retinopatía diabética, quienes representan aproximadamente el 25% de los diabéticos, según la encuesta realizada en 2019 por el Consejo Oftalmológico Argentino.&lt;/p&gt;
&lt;p&gt;El aporte realizado por los investigadores permite corregir dos tipos de deformaciones que pueden afectar el análisis de las imágenes con fines diagnósticos. Por un lado, las distorsiones que se deben a la proyección que realiza el equipo médico para generar la representación plana del órgano, y por el otro, las que son causadas por una insuficiente resolución de la imagen. “El análisis fractal es una técnica matemática incorporada en diversos equipos de imágenes biomédicas. Sin embargo presenta algunas limitaciones, ya que incluso cuando el diseño del equipo es adecuado, el análisis puede estar afectado por las condiciones del estudio”, explicó Irurzun. “Las superficies corporales son curvas, así que tanto el problema de la proyección como el de la resolución afectarán en mayor o menor medida a todas las imágenes médicas: tomografías, ecografías, radiografías, entre otras”, agregó.&lt;/p&gt;
&lt;p&gt;El avance permitirá perfeccionar los diagnósticos basados en el análisis de la irregularidad y ramificación de las estructuras, como la presencia de placas ateromatosas en la aorta y la clasificación de tumores por su malignidad. En referencia a las enfermedades vasculares del ojo, Elizabeth Santiago-Cortés, investigadora de la Corporación Universitaria del Cauca en Colombia y autora de varias publicaciones sobre análisis fractal en estudios de la retina, viene reclamando junto a sus colaboradores que a menos que se estandarice, la utilidad del análisis como herramienta de diagnóstico es dudosa. Esta deuda podrá ser saldada gracias al desarrollo de Irurzun y su equipo.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2021/05/ojo-fractal.png&#34; alt=&#34;Izquierda: Imagen de la retina humana y del árbol circulatorio. Derecha: estructura fractal creada por computadora por Irurzun y colaboradores.&#34;&gt;&lt;/p&gt;
&lt;p&gt;Para estudiar las distorsiones, los investigadores crearon estructuras 3D por computadora que imitan la red de vasos sanguíneos del ojo y generaron imágenes como las obtenidas por estudios médicos. Así, encontraron la resolución de las imágenes que minimiza la distorsión en la dimensión fractal debido a la relación de tamaño entre el grosor de las ramas y la curvatura. Según Irurzun, la solución del problema es técnicamente sencilla, aunque incrementaría los costos. Actualmente están trabajando en desarrollar algoritmos para equipos médicos que capturen la dimensión fractal correcta a pesar de las deformaciones. “Esperamos tener resultados pronto”, se entusiasmó.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; &lt;a href=&#34;https://doi.org/10.1103/PhysRevE.103.012138&#34;&gt;Fractal dimension of diffusion-limited aggregation clusters grown on spherical surfaces&lt;/a&gt;. J. M. Tenti , S. N. Hernández Guiance, and I. M. Irurzun PHYSICAL REVIEW E 103, 012138 (2021).&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Sobre la medición reciente del muón</title>
      <link>https://ciencianet.com.ar/post/sobre-la-medicion-reciente-del-muon/</link>
      <pubDate>Wed, 14 Apr 2021 15:13:35 -0300</pubDate>
      
      <guid>https://ciencianet.com.ar/post/sobre-la-medicion-reciente-del-muon/</guid>
      <description>
        
          &lt;p&gt;Foto de Reidar Hahn. Imagen con permiso de Fermilab (en alta resolución &lt;a href=&#34;https://mod.fnal.gov/mod/stillphotos/2013/0200/13-0243-05D.hr.jpg&#34;&gt;aquí&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Gastón Giribet.&lt;/strong&gt; Facultad de Ciencias Exactas  Naturales, Universidad de Buenos Aires y CONICET.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Los muones son partículas fundamentales. Son, en efecto, partículas muy comunes en nuestro entorno. Acaso alcance con decir que un muón atraviesa tu corazón cada segundo. Y al igual que éste, parecen encerrar secretos.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;el-espín-del-muón&#34;&gt;El espín del muón&lt;/h3&gt;
&lt;p&gt;En la rapsódica clasificación de partículas elementales que damos en llamar Modelo Estándar de la física de partículas, los muones pertenecen a la subfamilia cuyos miembros se denominan “leptones”, entre los que también encontramos al electrón. De hecho, el muón puede ser considerado un hermano mayor del electrón, ya que comparte con éste todas sus propiedades, excepto su masa: Los muones tienen una masa de aproximadamente 105 MeV, lo que los hace unas doscientas veces más masivos que el electrón; pero, más allá de esa diferencia, son exactamente iguales: tienen la misma carga eléctrica que el electrón (&lt;em&gt;e&lt;/em&gt; = -1) y tienen el mismo espín que el electrón (&lt;em&gt;s&lt;/em&gt; = ½).&lt;/p&gt;
&lt;p&gt;El espín, como sabemos, es un grado de libertad que las partículas fundamentales exhiben. El fotón tiene espín &lt;em&gt;s&lt;/em&gt; = 1, los leptones tienen espín &lt;em&gt;s&lt;/em&gt; = ½, al igual que los quarks, y las otras partículas tienen otros valores del espín, éstos siempre dados por números semi-enteros: los fotones y los gluones, por ejemplo, tienen espín &lt;em&gt;s&lt;/em&gt; = 1, al igual que las partículas W y las Z, mientras que la el bosón de Higgs es la única partícula de las 37 que conforman el Modelo Estándar que no presenta espín, i.e. cuyo espín es &lt;em&gt;s&lt;/em&gt; = 0.&lt;/p&gt;
&lt;p&gt;El espín es, decíamos, un grado de libertad que las partículas presentan. Así como tienen masa, carga eléctrica u otros tipos de carga, las partículas también tienen espín. El espín puede considerarse como una unidad elemental de giro. Al ser las partículas fundamentales puntos en el espacio, el espín de éstas no puede, &lt;em&gt;stricto sensu&lt;/em&gt;, ser pensado como el giro de la misma, ya que no hay en ellas un volumen que gire. Cabe recalcar que las partículas elementales no son pequeñas, ni muy pequeñas, ni muy muy pequeñas, sino que son infinitamente pequeñas; son, hasta donde sabemos, puntos en el espacio. Debido a esto, el espín debe ser pensado como una “unidad de giro”, un giro elemental, un giro inmanente de esos puntos en el espacio a los que llamamos partículas, un giro que está antes que aquello que gira. Esto, lo sé, puede resultar conceptualmente escurridizo, pero eso sólo se debe a que nuestro entendimiento, perezoso, insiste con pensar a todo girar como el girar de algo. No ocurre así con las partículas puntuales: El espín es giro, un giro inmanente, no es el girar de algo.&lt;/p&gt;
&lt;p&gt;Pero, ¿por qué hablamos tanto del espín? Pues bien, … es que el espín es importante para entender la cuestión que nos convoca. Expliquemos el porqué.&lt;/p&gt;
&lt;p&gt;Al ser partículas cargadas eléctricamente y al tener espín, los muones, así como los electrones, presentan un momento magnético. Es decir, podemos pensarlos como pequeños dipolos magnéticos que, en tanto tales, se acoplan al campo magnético como si fueran pequeños imanes. Esto ofrece una manera sencilla de medir el denominado “momento magnético” del electrón; es decir, el factor que da cuenta de cuánto se acoplan esos pequeños imanes puntuales que llamamos electrones a un campo magnético externo. La manera de medirlo es la siguiente: si uno dispone un electrón en presencia de un campo magnético externo, el influjo de este último hará que el electrón, en tanto pequeño dipolo magnético, adquiera un movimiento de precesión en torno a la dirección en la que el campo apunta. La velocidad de ese giro resulta ser proporcional al acoplamiento con el campo magnético externo. Así, la medición de esa frecuencia de precesión permite obtener de manera directa el momento magnético, que suele denotarse factor-&lt;em&gt;g&lt;/em&gt;, o simplemente &lt;em&gt;g&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;El factor-&lt;em&gt;g&lt;/em&gt; del electrón, decíamos, no es sino la propensión que esta partícula tiene a interactuar con el campo magnético en el que se encontrara embebida. Este campo externo puede estar dado por el núcleo atómico en torno al cual el electrón se encuentra orbitando, o dado por otro electrón compañero, o por un arreglo experimental que se hubiere montado. En todos esos casos, el acoplamiento entre el electrón y el campo magnético en el que éste se encuentra inmerso está dado por el mismo factor-&lt;em&gt;g&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Fue muy al comienzo de la física de partículas cuando se advirtió que el valor del factor-&lt;em&gt;g&lt;/em&gt; para un electrón parecía ser &lt;em&gt;g&lt;/em&gt; = 2; un valor peculiar ya que si se pensaba al electrón clásicamente era esperable un factor diferente. No obstante, ya a comienzos de la década del 30 se entendía la razón del valor 2 para el factor-&lt;em&gt;g&lt;/em&gt; electrónico; la ecuación de Dirac, que es la ecuación que describe el campo cuyas excitaciones llamamos “electrón”, predice exactamente ese valor, &lt;em&gt;g&lt;/em&gt; = 2. En efecto, la explicación de ese valor es uno de los grandes primeros logros de la teoría cuántica relativista.&lt;/p&gt;
&lt;h3 id=&#34;el-vacío-cuántico&#34;&gt;El vacío cuántico&lt;/h3&gt;
&lt;p&gt;Ahora bien, los datos experimentales de la medición del factor-&lt;em&gt;g&lt;/em&gt; llegarían unos años más tarde: En 1948, Kusch y Foley midieron por primera vez el factor-&lt;em&gt;g&lt;/em&gt;, obteniendo como resultado el valor &lt;em&gt;g&lt;/em&gt; = 2,00232, ligeramente superior al límpido 2 predicho por la teoría. Esta discrepancia con el valor &lt;em&gt;g&lt;/em&gt; = 2 es lo que se conoce como “momento magnético anómalo”, y es la razón por la cual uno refiere a la cantidad &lt;em&gt;g&lt;/em&gt;-2 como la verdaderamente relevante.&lt;/p&gt;
&lt;p&gt;Esta discrepancia entre el valor &lt;em&gt;g&lt;/em&gt; = 2,00232 y el valor &lt;em&gt;g&lt;/em&gt; = 2, sin embargo, se entendió al poco tiempo. Fue Schwinger quien propuso la explicación correcta de por qué el valor experimental obtenido para &lt;em&gt;g&lt;/em&gt; no era 2 sino un valor ligeramente mayor. La razón es la mecánica cuántica: según la teoría cuántica, lo que conocemos como vacío es algo muy diferente al concepto clásico que tenemos de él. El vacío no es la ausencia total de materia y energía en una región del espacio. La teoría cuántica de campos nos enseña que, de hecho, el vacío no es la nada sino algo muy diferente: Es un constante fluctuar de partículas y antipartículas que se crean y aniquilan incesantemente, dando origen a una estructura efervescente que, por consiguiente, denominamos “vacío cuántico”. Así, la mera presencia de una partícula en el espacio, por caso un electrón o un muón, genera en torno a sí una polarización de ese vacío cuántico, y la partícula termina vistiéndose de un halo fluctuante de partículas y antipartículas virtuales arrancadas a la nada.&lt;/p&gt;
&lt;p&gt;Podemos entender esta poiesis cuántica de partículas virtuales y cómo ésta afecta al comportamiento de un electrón en presencia de un campo externo de la siguiente manera: en la teoría cuántica de campos, la interacción de un electrón y el campo magnético externo se describe en términos del intercambio de partículas. La interacción electromagnética está mediada por un fotón, y ese fotón colisiona con el electrón afectando su comportamiento. Mientras que la visión macroscópica que tenemos del fenómeno es la de un electrón exhibiendo un movimiento de precesión debido al influjo de un campo magnético externo, la visión microscópica del mismo fenómeno nos devela que es la colisión entre el electrón y uno de los fotones que conforman el campo magnético lo que acaece (ver Figura 1-A). Al interactuar con el fotón, el electrón reacciona de la manera observada. Ahora bien, debido a la fluctuante naturaleza del vacío cuántico, el electrón tiene permitido efectuar muchas más peripecias que el mero interactuar con el fotón. Por ejemplo, antes de hacer esto último, el electrón pudo haber elegido emitir su propio fotón virtual, luego impactar con aquel que le trae la información del campo magnético externo, y finalmente reabsorber el fotón virtual originalmente emitido (Figura 1-B). Pero hay más: como si se tratara de un malabarista que encuentra desafiante poner más bolas en el aire, el electrón puede generar, no sólo uno, sino dos, tres o más fotones virtuales y, luego de impactar con aquel que le trae la información del campo magnético externo, reabsorberlos (Figura 1-C). Puede hacer todo eso y muchas cosas más, muchas otras piruetas cuánticas, cada una de ellas con cierta probabilidad de realización. El resultado, entonces, se obtiene luego de sumar sobre todas esas peripecias, sobre todas esas historias; mientras que el resultado clásico &lt;em&gt;g&lt;/em&gt; = 2 sólo corresponde a tener en cuenta el impacto directo entre electrón y el fotón (Figura 1-A). En otras palabras y tal como explicaba Schwinger, el resultado experimental para &lt;em&gt;g&lt;/em&gt; será un poco mayor que 2 debido a que el electrón tiene a su disposición la creación de pares de partículas y antipartículas virtuales que terminan vistiendo el valor clásico &lt;em&gt;g&lt;/em&gt; = 2 de otras posibles historias en ese interactuar, modificando así el valor del momento magnético ligeramente.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2021/04/muon-figura-1.png&#34; alt=&#34;Las líneas sólidas en color negro representan electrones (o muones), mientras que las líneas punteadas en color rojo representan fotones.&#34;&gt;&lt;/p&gt;
&lt;p&gt;Los cálculos teóricos sobre todas las posibles contribuciones de pares de partículas y antipartículas en la interacción entre el electrón y el campo magnético externo arrojan una predicción al valor de g extremadamente precisa, con más de una decena de cifras significativas de exactitud. El hecho de que los resultados experimentales para el factor-&lt;em&gt;g&lt;/em&gt; del electrón concuerden con los cálculos teóricos a semejante orden de precisión se yergue, hoy, como el resultado más preciso de toda disciplina científica.&lt;/p&gt;
&lt;h3 id=&#34;el-momento-magnético-del-muón&#34;&gt;El momento magnético del muón&lt;/h3&gt;
&lt;p&gt;Ahora bien, ¿ocurre lo mismo con el muón? Siendo que el muón y el electrón son dos partículas que, junto al tauón, coinciden en todas sus propiedades con excepción de la masa, ¿no debería, acaso, el resultado del momento magnético del muón arrojar resultados con precisión semejante a la obtenida para el caso del electrón? Pues, la historia no es tan sencilla: al ser el muón unas doscientos veces más masivo que el electrón, ése es mucho más propenso que este último a interactuar con partículas de gran masa. Típicamente, el acoplamiento de una partícula es proporcional al cuadrado de su masa, de manera tal que, al ser 200 veces más masivo que el electrón, el muón resulta ser unas 40.000 veces más susceptible a la interacción con las partículas virtuales. Esto requiere, en particular, mucho más detalle en el cálculo de las distintas contribuciones al acoplamiento entre el muón y el fotón. Al disponer un muón en un campo magnético externo, el muón, al igual que un electrón lo haría, comenzará a experimentar una precesión en torno a su eje de giro. Esa interacción entre el campo magnético externo y el muón se describirá, también como en el caso del electrón, en términos de intercambios de partículas, incluyendo las peripecias cuánticas que involucran partículas virtuales: al interactuar con el fotón portador del campo magnético externo, el muón puede colisionar directamente, pero puede también generar partículas virtuales antes de hacerlo y reabsorberlas luego de ello; puede generar fotones virtuales, partículas Z virtuales, e incluso generar partículas de Higgs virtuales, entre muchas otras cosas. El resultado teórico correspondería, pues, a sumar sobre todas esas posibilidades.&lt;/p&gt;
&lt;p&gt;Ese cálculo, aunque la teoría para llevarlo a cabo es bien conocida y, por ello, no presupone mayor desarrollo teórico, sí requiere de paciencia y de una notable pericia a la hora de emplear las técnicas de cálculo. Es un cómputo extremadamente sensible a errores y a omisiones. Una tarea minuciosa es necesaria a la hora de tener “todas” las configuraciones intermedias en cuenta y con su adecuado “peso”. La comunidad especializada ha llegado a un “resultado de consenso” acerca del cálculo teórico para el factor-&lt;em&gt;g&lt;/em&gt; del muón; a saber: &lt;em&gt;g&lt;/em&gt; = 2,0023318319.&lt;/p&gt;
&lt;p&gt;Ahora sólo quedaría cotejar con el experimento.&lt;/p&gt;
&lt;h3 id=&#34;los-experimentos&#34;&gt;Los experimentos&lt;/h3&gt;
&lt;p&gt;El momento magnético del muón se midió por primera vez a comienzos de la década del 50 en la Universidad de Columbia, y una década más tarde el CERN arrojaba resultados en concordancia. El advenimiento de mejoras tecnológicas y nuevas técnicas experimentales permitieron un aumento significativo en la precisión, y hacia la década de 1990 los nuevos experimentos con los que hoy contamos estaban ya siendo diseñados. En 2001, el Laboratorio Nacional de Brookhaven, en los Estados Unidos, daba a conocer el valor medido experimentalmente para el factor-&lt;em&gt;g&lt;/em&gt; del muón; éste era: &lt;em&gt;g&lt;/em&gt; = 2,0023318404.&lt;/p&gt;
&lt;p&gt;Como vemos, el valor experimental para &lt;em&gt;g&lt;/em&gt;-2 difiere del teórico en la novena cifra significativa, lo que podría hacernos creer que se trata de un éxito de la teoría. Pero lo cierto es que no es así: se trata de una medición muy precisa, por lo que los errores en la medición son extremadamente pequeños, lo suficiente como para considerar que se trata de un desacuerdo.&lt;/p&gt;
&lt;p&gt;A partir de 2013, partes del experimento de Brookhaven se trasladaron al Fermilab, en Chicago, donde se montó un arreglo similar pero con notables mejoras. Funciona allí la colaboración Muon &lt;em&gt;g&lt;/em&gt;-2, que realiza el experimento más preciso hasta el momento para medir el factor-&lt;em&gt;g&lt;/em&gt; del muón. El experimento consiste en un anillo de unos 15 metros de perímetro en el que se aceleran muones a muy altas velocidades. Luego de ser inyectados en el anillo, los muones giran mientras se los somete a un campo magnético externo de intensidad muy controlada. Debido al campo magnético, los muones experimentan el mentado movimiento de precesión cuya frecuencia es medida con la precisión requerida.&lt;/p&gt;
&lt;p&gt;El pasado 7 de abril, en conferencia de prensa, la colaboración trabajando en el experimento Muon &lt;em&gt;g&lt;/em&gt;-2 dio a conocer sus resultados, que vienen a confirmar la discrepancia entre el valor para el factor-&lt;em&gt;g&lt;/em&gt; medido experimentalmente y la predicción teórica basada en el Modelo Estándar. En otras palabras, los resultados de Muon &lt;em&gt;g&lt;/em&gt;-2 resultan ser consistentes con los experimentos de Brookhaven, exhibiendo, cuando se los considera en conjunto, una tensión de unas 4,2 sigmas de desviación estándar con respecto al valor teórico.&lt;/p&gt;
&lt;p&gt;Esto nos enfrenta a una discrepancia entre teoría y experimento, ahora corroborada y medida con mayor precisión. Surge así la pregunta: ¿Estamos ante el descubrimiento de una grieta en el edificio teórico que llamamos Modelo Estándar de partículas fundamentales? ¿Estamos ante lo que los físicos de partículas llaman “nueva física”? La verdad es que no lo sabemos con certeza.&lt;/p&gt;
&lt;p&gt;Hay, al menos, tres posibles explicaciones para la discrepancia observada: una posibilidad es que ésta se deba a una fluctuación estadística. Que los datos de las corridas del experimento analizadas hasta la fecha arrojen como resultado una discrepancia con el valor teórico de consenso a 4,2 sigmas hace que este sea un caso serio contra la teoría que tenemos; pero en física de partículas, dada la enorme exactitud que se ha alcanzado en los experimentos y dada la gran dependencia de los resultados con la estadística, la convención es que un descubrimiento se considera tal sólo cuando se han alcanzado los 5 sigmas de desviación, y para ello hace falta seguir analizando los resultados que la colaboración Muon &lt;em&gt;g&lt;/em&gt;-2 ha tomado en los últimos años.&lt;/p&gt;
&lt;p&gt;La segunda posible explicación para la discrepancia es que a la hora de efectuar los cálculos teóricos se hayan subestimado las contribuciones de partículas virtuales a la hora de pensar todas las formas en las que el muón puede interactuar con el campo magnético externo. Al tener una masa considerable, el muón es también propenso a emitir partículas virtuales que no son solamente partículas elementales, sino también hadrones. Los hadrones son estados ligados que están, a su vez, formados de muchas otras partículas que interactúan mediante la fuerza nuclear fuerte. Esos estados están descriptos por la teoría llamada cromodinámica cuántica en un régimen en la que la misma es difícil de domeñar. Ejemplos de hadrones son el pión y el kaón, formados éstos por un par quark-antiquark embebido en un orgiástico menjunje de gluones y acompañado con una fluctuante maraña de pares de quarks-antiquarks virtuales que se crean y aniquilan incesantemente. ¡Algo muy complejo! La consideración de la posibilidad de que el muón, antes de interactuar con el fotón del campo magnético externo, emita y luego reabsorba uno de estos hadrones complica mucho el cálculo teórico, ya que los hadrones tienen, per se, una complicada estructura interna. Y aunque la física de hadrones es en alguna medida conocida y contamos con datos experimentales para extrapolar resultados, ponderar su contribución es, sin lugar a dudas, la parte más abstrusa del cálculo. En el caso del electrón esto no representaba un inconveniente, ya que el electrón, debido a su baja masa (0,51 MeV), no tiene alta propensión a este tipo de interacciones; pero sí es menester tenerlas en cuenta en el caso del muón.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2021/04/muon-figura-2.png&#34; alt=&#34;Representación del acoplamiento de los muones (µ) y los fotones (̉γ) que constituyen el campo magnético externo, lo que incluye las correcciones cuánticas debido al intercambio de distintas partículas virtuales.&#34;&gt;&lt;/p&gt;
&lt;p&gt;La tercera posible explicación, y la que se anuncia con más acento debido a las posibles implicancias que ésta tendría para la física, es que el Modelo Estándar no sea un modelo completo; es decir, que deba el Modelo ser completado con la adición de partículas que aún no conocemos. Podría estar ocurriendo que el muón, en su interactuar con el fotón del campo magnético externo, estuviera generando otras partículas virtuales cuya existencia desconocemos pero de las que el muón sí se entera. La posibilidad de que el muón pueda emitir y luego reabsorber una partícula tal, además de hacerlo con las que ya conocemos, aumentaría el valor de &lt;em&gt;g&lt;/em&gt; acercándolo al valor experimental observado. Así, sugieren algunos, podríamos estar observando indirectamente la presencia de esas partículas nuevas, desconocidas, nuevas partículas fundamentales que deberíamos agregar a nuestro Modelo; en breve, podríamos estar frente a lo que los físicos de partículas insisten con llamar “nueva física”.&lt;/p&gt;
&lt;p&gt;Tenemos por delante una década de refinamiento de los cálculos teóricos, de mejoras en las técnicas de observación, y de recopilación de más datos estadísticos. El tiempo nos sabrá decir cuál es la explicación a la sutil discrepancia entre el valor teorizado y el valor observado para el factor-&lt;em&gt;g&lt;/em&gt; del muón. Pero, insisto, seamos cautelosos; no sería la primera vez que el Modelo Estándar saliese victorioso luego de haber sido desafiado, y aunque la precisión alcanzada en el experimento Muon &lt;em&gt;g&lt;/em&gt;-2 es asombrosa, es necesario esperar.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Comunicar la ciencia: ¿Resultados o procesos?</title>
      <link>https://ciencianet.com.ar/post/comunicar-la-ciencia/</link>
      <pubDate>Sat, 30 Jan 2021 10:52:14 -0300</pubDate>
      
      <guid>https://ciencianet.com.ar/post/comunicar-la-ciencia/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero.&lt;/strong&gt; Instituto de Investigaciones Fisicoquímicas Teóricas y Aplicadas (INIFTA) y Universidad Nacional de La Plata.&lt;/p&gt;
&lt;p&gt;(Foto &lt;a href=&#34;https://unsplash.com/@romankraft?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&#34;&gt;Roman Kraft&lt;/a&gt; en &lt;a href=&#34;https://unsplash.com/?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&#34;&gt;Unsplash&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;En esta reseña comentamos el artículo La noticia sobre ciencia: Sesgo hacia la comunicación de los resultados sobre los procesos de la investigación científica. En el trabajo, publicado en diciembre de 2020 en una revista chilena especializada en comunicación de la ciencia, el investigador en biología y comunicador Antonio Mangione (Universidad Nacional de San Luis e Instituto Multidisciplinario de Biología IMIBIO CONICET) presenta una propuesta de análisis de noticias periodísticas de ciencia en grandes medios que permite detectar la presencia de sesgos.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Muchas cosas cambiaron desde la llegada de la pandemia. Por ejemplo, la forma en que nos relacionamos, la forma en que aprendemos, y también la forma en que recibimos, analizamos e interpretamos las noticias sobre ciencia. La imagen de ciencia poseedora de una verdad absoluta, unánime  e inmutable siempre fue una visión deformada, y ahora queda claro más que nunca. La ciudadanía debe lidiar en estos tiempos con aquello que casi nunca formaba parte de la “ciencia comunicada”: los procesos de creación de conocimiento. En este nuevo protagonismo de la parte oculta de la ciencia, queda a la vista que la supuesta línea recta que nos lleva hacia el progreso es en realidad más parecida a un revoltijo de ovillos de lana escalando una montaña. Quedan en evidencia los errores, los retrocesos, las rectificaciones, las –a veces enormes- limitaciones de los métodos, las distintas posibles interpretaciones de un mismo hecho, las diversas escuelas, las tensiones económicas y políticas que atraviesan la creación de conocimiento científico.&lt;/p&gt;
&lt;p&gt;Otro proceso que actualmente queda a la vista es la relación compleja de la ciencia con la tecnología, con límites difusos entre una y otra (¿dónde termina la ciencia y empieza la tecnología cuando se trata de vacunas?), y retroalimentaciones tanto positivas como negativas. Así, por ejemplo, el mayor conocimiento sobre el patógeno (ciencia) permite la creación de nuevos test diagnósticos (tecnología), que a su vez posibilitan extensos estudios epidemiológicos (ciencia) que nos permiten, nuevamente, saber más y generar estrategias de control y manejo (que también, en tanto herramientas, son tecnología). Pero también la interferencia entre algunos tipos de test diagnósticos de dengue y de COVID-19 (tecnología) puede resultar una limitación para el estudio de un fenómeno como la coexistencia de epidemias (ciencia).&lt;/p&gt;
&lt;p&gt;Sumado a esta súbita aparición en escena del “backstage” de la ciencia, tenemos de repente cientos de protagonistas que toman el papel de interlocutores: personas que se dedican a investigar (en casi cualquier tema), personal de salud, estudiantes de doctorado. Estamos lejos del estereotipo de sabiduría encarnada en grandes -y pocas- figuras del mundo del conocimiento. No nos hablan hoy Luis Leloir ni Cecilia Grierson, sino que nos llegan numerosos discursos por día de decenas de personas con diploma de universidad o portando la insignia del CONICET, que además son divergentes: incluso a veces se pelean en público (por televisión o redes sociales). Las referencias se vuelven difusas. Una infectóloga que atiende en un hospital, ¿es una científica? Además, en algunos casos estos nuevos interlocutores manifiestan más o menos explícitamente sus posiciones políticas, algo fuera de todo contrato tácito respecto de lo que una persona de ciencia “debe ser” según la imagen dominante (estereotipada en personas de anteojos y guardapolvos, recluidas en laboratorios y sin más pasiones que el conocimiento).&lt;/p&gt;
&lt;p&gt;Este crudo baño de realidad, esta pluralidad de voces, lejos de tranquilizar a la población, la saca de la “zona de confort”, aquella en que la noticia de ciencia es ese cuadradito del diario que nos promete cargadores inalámbricos o drogas que retrasen el envejecimiento, o nos informa sobre un nuevo agujero negro en una lejana galaxia. Podemos usar aquí una analogía. La realidad recién descubierta es que los Reyes Magos son los padres, y a veces ni siquiera tienen plata. Eso genera la sensación de ser inocentes víctimas de un engaño, desazón, decepción, falta de confianza... hasta que llega la fase de negociación y aceptación, porque después de todo, los regalos que (la ciencia) nos trae son enormes. Sin embargo, aquí la analogía falla, pues una parte de la población queda detenida en el descrédito y la desconfianza. Una parte de la población, entonces, será proclive a posturas anticientíficas como el terraplanismo, el movimiento antivacunas y las teorías conspirativas.&lt;/p&gt;
&lt;p&gt;Desde la perspectiva de la comunicación pública de la ciencia, la transición desde una imagen de ciencia estereotipada e ingenua hacia otra epistemológicamente más adecuada y más real no puede ser sino beneficiosa. Que nos toque transitarla de sopetón en un escenario de pandemia, es toda otra cuestión en sí misma.&lt;/p&gt;
&lt;h3 id=&#34;las-noticias-prepandémicas&#34;&gt;Las noticias prepandémicas&lt;/h3&gt;
&lt;p&gt;¿Por qué estas controversias, tensiones políticas y problemas metodológicos nos parecen nuevos? ¿De qué trataban las noticias de ciencia prepandémicas de los grandes medios de comunicación? De la complejidad de la actividad científica real y de su –deformado- reflejo en la noticia nos habla el artículo de Mangione.&lt;/p&gt;
&lt;p&gt;La conclusión es que los procesos de investigación científica están escasamente desarrollados en las noticias sobre ciencia, mientras que son los resultados los que se llevan las palmas de la representación. ¿Pero por qué se plantea la distinción entre proceso y resultado?&lt;/p&gt;
&lt;p&gt;Como Mangione plantea al inicio del artículo, históricamente la comunicación pública ha pasado desde un poco exitoso modelo de déficit cognitivo, en el que el interés del público por la ciencia se incrementa aportando información respecto a hechos científicos, hacia otro modelo algo más sutil en el que se plantea que la clave es acercar científicos y públicos. Pero considerando que la brecha entre ellos no está ya asentada en la cantidad de información a la que accede cada grupo sino a otra dimensión: tener o no tener experiencia en la propia creación de conocimiento. Es decir, aportar información sobre las prácticas científicas, es decir, sobre los procesos, resulta más eficiente en la creación de una cultura científica que el mero aporte de datos o resultados.&lt;/p&gt;
&lt;p&gt;Ahora bien, basado el Latour y otros, nos cuenta Mangione que si miramos una publicación científica en una revista revisada por pares, es decir, una pieza de comunicación de ciencia destinada a la misma comunidad científica, veremos que aún allí la mayor parte de los procesos de producción del conocimiento están ausentes. Porque en virtud de la legibilidad y de los límites a la extensión de los textos y de la construcción de un hilo argumental resistente a embates, para llegar al &lt;em&gt;paper&lt;/em&gt; fue previamente operada sobre la investigación un recorte y una simplificación de las condiciones en que ese conocimiento fue generado.&lt;/p&gt;
&lt;p&gt;Y esto representa un gran desafío para quienes producen noticias científicas para el público, puesto que si la comunicación científica “se aborda restando elementos de juicio y comprensión sobre la génesis, contexto y alcance de la investigación científica entonces es esperable que la distancia entre  conocimiento  científico y los públicos no solo se amplíe, sino que además se caricaturice la actividad científica”.&lt;/p&gt;
&lt;p&gt;Por todo esto es que resulta deseable una comunicación pública de ciencia que esté balanceada respecto de la presentación de resultados de la investigación científica de y las condiciones en que los mismos fueron construidos.&lt;/p&gt;
&lt;h3 id=&#34;lo-que-las-noticias-de-ciencia-dicen-y-lo-que-no&#34;&gt;Lo que las noticias de ciencia dicen (y lo que no)&lt;/h3&gt;
&lt;p&gt;¿Cómo detectar esta diferencia entre resultados y procesos? Para cuantificar esta subrepresentación, Mangione analizó 35 noticias y 27 artículos con notas de opinión o entrevistas, de uno de los dos portales que concentran la mayor parte del flujo informativo en Argentina: el portal Clarín. En ellas, estudió la frecuencia de aparición de distintos elementos en los textos en formato digital entre los días  01-01-2016 al 1-07-2017. Para detectar el relato de procesos relacionados con la producción científica, las referencias buscadas fueron aquellas relacionadas con un abordaje epistemológico (es decir, reflexiones sobre el quehacer científico), ético, político e histórico. Por ejemplo, menciones a fuentes de financiamiento, a controversias, a polémicas, a errores y a problemas metodológicos. Para detectar el informe de resultados, se buscó la presencia de optimismo epistemológico, utilidad, progreso y espectacularidad. Ejemplos de estas categorías son las menciones a posibles extensiones a humanos de investigaciones en animales, mejoras casi infinitas en capacidades tecnológicas, valoraciones de los resultados en términos de futuras curas a enfermedades.&lt;/p&gt;
&lt;p&gt;Entonces, para cada nota y tanto para las dimensiones indicadoras de resultados como de procesos, se asignó un valor de 0, si no se hallaba ninguna categoría; 1 si se encontraban menos de la mitad; 2 si se detectaban la mitad; 3 si se hallaban más de la mitad.&lt;/p&gt;
&lt;p&gt;Restando al valor dado para procesos el valor dado para resultados se obtiene el parámetro que el artículo propone para analizar el sesgo: si es positivo, la noticia está orientada a procesos, mientras que si es negativo está orientada hacia resultados. El 0 es un indicador de que la nota periodística está balanceada en contenidos.&lt;/p&gt;
&lt;p&gt;Con esta metodología se determinó que de las 35 noticias analizadas, el 82% tiene sesgo hacia resultados. Sólo 3 de las 35 equilibraban las referencias a resultados con las referencias a procesos. En cambio, se encontró que el 44% de las 27 notas de opinión y entrevistas estudiadas está sesgada a favor de los procesos.&lt;/p&gt;
&lt;p&gt;Otra pregunta interesante en esta búsqueda de sesgo en las noticias hacia el informe de resultados o hacia el relato de procesos es si existía diferencia entre las notas anónimas o aquellas firmadas por periodistas. Del material analizado, el 83% no está firmado (son producciones de agencias o redacción). Curiosamente, el 85% de las notas basadas en procesos están redactadas por otros medios o firmadas.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2021/01/comciencia.png&#34; alt=&#34;Sesgo en 35 noticias científicas (izquierda) y en 27 notas de opinión/entrevistas (derecha). Las barras muestran el número total de notas en cada grupo.  Los números indican la cantidad de notas según tengan autoría conocida, discriminando entre paréntesis la cantidad de textos firmados.&#34;&gt;&lt;/p&gt;
&lt;p&gt;Esta cuantificación resulta relevante, puesto que según la mirada expuesta por Mangione, la incorporación de conceptos vinculados a los procesos de la investigación científica en las noticias periodísticas especializadas favorece el diálogo, la discusión y el debate entre expertos científicos y públicos. Dado que las noticias firmadas, las notas de opinión y entrevistas están sesgadas hacia el relato de procesos, y por ende a una mejor representación de la complejidad de la investigación científica, que a su vez promueve un acercamiento entre personas de ciencia y público no experto, esta investigación no solo plantea que una mejor comunicación pública de la ciencia en medios es posible, sino que claramente nos propone un camino. La observación de que el material periodístico firmado es mejor (más complejo y reflexivo) muestra además que el giro hacia una mejor comunicación de ciencia (que la actual pandemia ha revelado como indispensable) es mayormente responsabilidad de la política editorial de los medios.&lt;/p&gt;
&lt;p&gt;¿Qué reflejará la metodología de análisis propuesta por Mangione cuando se analicen las noticias científicas de 2020? ¿Y las noticias pospandémicas? Es probable que el giro ya esté en marcha, pero depende de los medios, del periodismo especializado y también de quienes se dedican a la ciencia y a la educación sostener la presencia de los procesos de la investigación científica en la imagen pública de la ciencia para que el cambio se consolide.&lt;/p&gt;
&lt;h3 id=&#34;trabajo-completo&#34;&gt;Trabajo completo:&lt;/h3&gt;
&lt;p&gt;Mangione, AM. &lt;a href=&#34;https://doi.org/10.32457/scr.v1i1.660&#34;&gt;La noticia sobre ciencia: Sesgo hacia la comunicación de los resultados sobre los procesos de la investigación científica. (2020) SciComm 1 (1).&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; &lt;a href=&#34;mailto:mangione.antonio@gmail.com&#34;&gt;mangione.antonio@gmail.com&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Neumonía de Wuhan. ¿Qué dicen los modelos matemáticos de propagación del coronavirus 2019-nCoV?</title>
      <link>https://ciencianet.com.ar/post/neumonia-de-wuhan-que-dicen-los-modelos-matematicos/</link>
      <pubDate>Fri, 31 Jan 2020 20:35:28 -0300</pubDate>
      
      <guid>https://ciencianet.com.ar/post/neumonia-de-wuhan-que-dicen-los-modelos-matematicos/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero&lt;/strong&gt;. Grupo de Modelado y Simulaciones de la Propagación de Enfermedades Infecciosas (INIFTA - UNLP).&lt;/p&gt;
&lt;p&gt;Este artículo  trata sobre los modelos matemáticos para enfermedades infecciosas y su uso para estudiar el nuevo virus 2019-nCoV, cuya propagación ha llegado a 18 países, motivando que la OMS declare estado de emergencia internacional.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2020/01/23312_lores.jpeg&#34; alt=&#34;Morfología exhibida por coronavirus. Fuente: CDC/ Alissa Eckert, MSMI; Dan Higgins, MAMS.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;En diciembre de 2019 una nueva enfermedad, de alta contagiosidad y mortalidad, apareció en un mercado de la ciudad china de Wuhan. El alerta lo dio un número llamativo de casos de neumonía atípica en personas que habían concurrido al mercado. Muy rápidamente se identificó al responsable: un virus del tipo coronavirus, pariente de los virus responsables de brotes de SARS (Síndrome respiratorio agudo grave) y MERS (Síndrome respiratorio de Oriente Medio), que fue bautizado como 2019-nCoV. También muy velozmente, apenas días después, se obtuvo el genoma del virus y se generó el protocolo para su identificación por la técnica de biología molecular llamada TR-PCR (Reacción en cadena de la polimerasa con transcriptasa inversa). Luego de sospecharse de las serpientes como origen del brote, se apunta ahora a los murciélagos como probable reservorio. Para el 28 de enero solo en China se contaban casi 1500 casos confirmados y más de 80 fallecimientos, mientras que una docena de países ya habían reportando la llegada del virus. Primeramente, la Organización Mundial de la Salud estableció recomendaciones para la población son de tipo generales: las habituales para prevenir enfermedades respiratorias y para evitar la contaminación por medio de alimentos animales.  Pueden consultarse aquí &lt;a href=&#34;https://www.who.int/es/emergencies/diseases/novel-coronavirus-2019/advice-for-public&#34;&gt;https://www.who.int/es/emergencies/diseases/novel-coronavirus-2019/advice-for-public&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Sin embargo, debido a la extensión del coronavirus a 18 países, con más de 8000 casos y 120 decesos para el 30 de enero de 2020,  la OMS ya ha declarado el estado de emergencia internacional.&lt;/p&gt;
&lt;h3 id=&#34;primeros-trabajos&#34;&gt;Primeros trabajos&lt;/h3&gt;
&lt;p&gt;El 24 de enero de 2020 fue publicado de modo online en la revista The Lancet el primer trabajo científico sobre la nueva enfermedad, describiendo el desarrollo de la misma en 41 pacientes [&lt;a href=&#34;#1&#34;&gt;1&lt;/a&gt;]. También hay disponibles mapas para seguir la aparición de nuevos casos en &lt;a href=&#34;https://gisanddata.maps.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6&#34;&gt;tiempo real&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;También para esa fecha habían aparecido tres informes del MRC Centre for Global Infectious Disease Analysis del Imperial College London, dando cuenta de los primeros resultados teóricos. Mediante análisis de datos realizaron estimaciones del número real de infectados. Los resultados aportados por el modelado matemático de la nueva enfermedad sugieren la existencia de transmisibilidad entre personas y también estiman su contagiosidad promedio.&lt;/p&gt;
&lt;h3 id=&#34;modelado-matemático-de-enfermedades-infecciosas&#34;&gt;Modelado matemático de enfermedades infecciosas&lt;/h3&gt;
&lt;p&gt;El modelado matemático de una enfermedad infecciosa es una representación simplificada de la enfermedad y de las poblaciones involucradas que sirve principalmente para estimar de modo cuantitativo lo que ocurrirá con la población bajo determinadas circunstancias.  Por ejemplo, cambios de gran efecto como la aparición de una nueva enfermedad en una población sin defensas o la introducción de una nueva vacuna, o situaciones más sutiles como cambio de calendarios vacunales, cambios poblacionales o un nuevo tratamiento antibiótico que reduzca los días en que una persona infectada pueda infectar a otras.&lt;/p&gt;
&lt;p&gt;Si bien hay modelos estándar que sirven como base, cada enfermedad debe ser modelada “a medida” incorporando toda la información y todos los mecanismos (el modo de contagio y la adquisición de inmunidad son dos de los más importantes) que se supongan relevantes. En esa suposición aparece el efecto del modelador, pues el peso y la forma en que se incluye cada mecanismo no es único, y por lo tanto existen diversos modelos para una misma enfermedad. También hay diversos tipos o estilos de modelos. Pueden ser también deterministas (que tienen una solución única para cada combinación de valores) o estocásticos, donde las soluciones son probabilísticas. Pero cualquiera sea la clase de modelo, para que resulte confiable se requiere que recupere como soluciones lo que sí se sabe de la enfermedad. Porque al predecir bien lo que ya sabemos, aumenta la confianza sobre las predicciones de lo que no sabemos. Los modelos más usados son los llamados “compartimentales”, en las cuales a las personas se las agrupa en compartimentos o casilleros según su estado epidemiológico: personas  susceptibles, personas infectadas, personas vacunadas (si existe formulación para la enfermedad modelada) personas totalmente inmunes y/o con distinto grado de inmunidad, etc. Los pasajes entre los diferentes casilleros están relacionados con distintos procesos. Uno fundamental es el contagio, que regula el pasaje de personas susceptibles de enfermarse a la categoría de infectadas, y que depende a su vez de factores como la virulencia, la cantidad de individuos y/o vectores infectadas y cómo se comportan.&lt;/p&gt;
&lt;p&gt;Modelos más complejos, de acuerdo a la enfermedad de que se trate, pueden incluir muchos detalles: la edad de las personas, su género y qué tanto se frecuentan socialmente, la efectividad de la vacunación, la pérdida de la inmunidad, la transmisión madre-hijo de la patología y otros procesos similares. Los valores que elegimos para estas propiedades de la enfermedad son los llamados parámetros (por ejemplo, decidimos que la efectividad de la vacuna será del 97% y que la inmunidad se pierde a los 5 años de ser vacunado). En el caso de los modelos deterministas, la información se escribe matemáticamente en términos de ecuaciones diferenciales acopladas (ya que los cambios de las poblaciones de cada tipo, en cada instante, dependen unas de otras), y luego el sistema de ecuaciones se resuelve numéricamente, haciendo evolucionar el sistema en el tiempo, mediante computadoras. La solución del sistema de ecuaciones, que es única para cada conjunto de parámetros, nos permite conocer el estado de la población en cada instante, y en particular cuántos son los casos de enfermedad producidos. Si se desea evaluar el impacto relativo de distintos efectos para ver cuáles son más relevantes para el desarrollo de la enfermedad, se modifican de a uno los  parámetros en el sistema de ecuaciones y se vuelve a resolver, para luego comparar las soluciones. Así puede estimarse, por ejemplo, si un aumento en la contagiosidad persona-persona sería más relevante que la dispersión del animal vector.&lt;/p&gt;
&lt;p&gt;En el caso de los modelos estocásticos, los eventos entre individuos de cada clase epidemiológica del modelo (contagio, vacunación, recuperación, pérdida de inmunidad, muerte, etc.) ocurren o no mientras el tiempo transcurre según cierta probabilidad, y son sorteados en cada resolución del sistema de modo aleatorio. Por eso, cada vez que se resuelve el sistema en idénticas condiciones, los resultados varían: por ejemplo el número de infectados de cada epidemia simulada no es siempre el mismo. El procedimiento es, entonces, encontrar el resultado más probable –para esa combinación de parámetros- haciendo el promedio de miles de resultados equivalentes del sistema.&lt;/p&gt;
&lt;p&gt;Evidentemente, los modelos matemáticos son representaciones simplificadas y tienen limitaciones. No pueden tomarse sus soluciones como un pronóstico exacto a nivel numérico sino como indicadores de tendencias. Sin embargo, se los usa cada vez más frecuentemente para explorar distintas situaciones en la propagación de enfermedades (como la aparición de una nueva enfermedad) o medidas de control (como el agregado de una dosis extra o el cambio en el calendario vacunal). Luego de la epidemia de SARS ocurrida en 2003, las publicaciones de modelado matemático de enfermedades infecciosas aumentaron en el mundo; en China se cuadruplicaron [&lt;a href=&#34;#2&#34;&gt;2&lt;/a&gt;]. Otro ejemplo que ilustra el interés creciente en esta herramienta es que en 2015 la Organización Mundial de la Salud convocó a grupos de modelado matemático de todo el mundo a realizar un estudio de la introducción de una nueva formulación para inmunización contra el dengue, cuyos resultados  fueron tenidos en cuenta para las recomendaciones de uso de la vacuna. Actualmente la OMS tiene abierta una convocatoria para modelizar el efecto del agregado de refuerzos de vacuna DTP (difteria-tétanos-pertussis) en países de medios y bajos ingresos.&lt;/p&gt;
&lt;h3 id=&#34;modelado-matemático-de-la-neumonía-de-wuhan-por-coronavirus-2019-ncov&#34;&gt;Modelado matemático de la neumonía de Wuhan por coronavirus 2019-nCoV&lt;/h3&gt;
&lt;p&gt;El MRC Centre for Global Infectious Disease Analysis es un centro de investigación del Imperial College London que se dedica al modelado matemático de enfermedades infecciosas. Allí trabajan aproximadamente 175 investigadores de modo interdisciplinario, combinando áreas como estadística, matemática, epidemiología, genética, salud pública y economía. Además del análisis y la modelización de brotes de enfermedades infecciosas (tanto nuevas como endémicas), en este centro se estudian problemas relacionados con vacunas, resistencia a los antimicrobianos y eventos de salud considerados globales.&lt;/p&gt;
&lt;p&gt;Es uno de los centros de este tipo más grandes y experimentados, y por ello colabora estrechamente con la Organización Mundial de la Salud, gobiernos y organismos no gubernamentales de todo el mundo. Según su propia presentación, tienen una “capacidad única para responder a las amenazas emergentes como el Ébola y el Zika con análisis en tiempo real y modelos predictivos” (MRC)&lt;/p&gt;
&lt;p&gt;Los primeros dos informes del  MRC Centre for Global Infectious Disease Analysis (Imperial College London) no se refieren al modelado matemático sino a un análisis de la aparición y número de los casos reportados, en base a información poblacional. Este análisis sugiere que el brote de 2019-nCoV causó más casos de enfermedad respiratoria grave y moderada de los que se han detectado. El en primer informe, del 17 de enero, el MRC hace una estimación teórica el número total de casos infectados en la ciudad de Wuhan de 1.723. ¿Cómo obtienen este número? Usando el número de casos detectado fuera de China, y datos poblacionales y de vuelos internacionales. En particular, estimaron el número promedio de personas por día que volaron desde Wuhan hacia otros países en los dos últimos meses (3301 personas por día), el alcance efectivo de personas desde el aeropuerto de Wuhan (aproximadamente 19 millones) y que entre la infección y la detección transcurren 10 días. Según los cálculos, la probabilidad de que  una persona infectada en Wuhan viaje fuera de China antes de solicitar atención médica es de de 1 en 574 (~19.000.000/3301/10). Como para la fecha habían sido detectados 2 casos en Japón y otro en Tailandia, la estimación es que tuvo que haber del orden de 3x574 casos en la ciudad de Wuhan para que eso fuera posible. Informan también que debido a la poca información, la incerteza en el cálculo está entre 427 y 4471 casos. Pero sugieren que el contagio persona-persona no puede despreciarse. En el segundo reporte, con fecha 22 de enero, el MRC refina este cálculo, informando que el número estimado de casos en Wuhan con inicio de síntomas hasta el 18 de enero serían 4000, con un rango de incerteza entre 1000 y 9700.&lt;/p&gt;
&lt;p&gt;En el tercer informe del Imperial College London [&lt;a href=&#34;#3&#34;&gt;3&lt;/a&gt;] sobre el coronavirus Wuhan, el grupo de investigación informa resultados de analizar la transmisibilidad de la enfermedad mediante modelado. Ellos encuentra que, aunque el inicio de la epidemia se debió al pasaje del virus 2019-nCoV  desde una especie animal, la única explicación plausible para explicar la magnitud del brote de Wuhan es la transmisión del virus autosostenida por el contacto persona-persona y estimándose un número de reproducción (R0) de 2,6 (con un rango 1,5-3,5).&lt;/p&gt;
&lt;p&gt;¿Qué significa esto? ¿Cómo hicieron la estimación? A partir del modelado matemático previo de epidemias similares, construyeron un modelo de tipo estocástico para la infección por 2019-nCoV. Considerando la cantidad de infectados entre el inicio del brote el 1º de diciembre de 2019 y el 18 de enero de 2020, y suponiendo que el contagio inicial de tipo zoonótico afectó a 40 personas, el equipo de Londres estimó que en promedio cada uno de los infectados tuvo que haber infectado a más de 2 personas durante (2.6 personas es la estimación exacta, con un rango de incerteza entre 1.5 y 3.5.  Este valor es el llamado número de reproducción (R0),  que es el número de contagios secundarios que un individuo infectado genera durante la duración del período infectivo. La contagiosidad y la duración de la infección definen el R0  de cada enfermedad.  El concepto de R0 surge de analizar en un modelo simple (SIR, susceptible-infectado-recuperado) la condición umbral para que una  infección no se extinga sola luego de unos pocos casos sino que pueda causar una epidemia, afectando un número enorme de personas en una población enteramente susceptible.  Esto ocurre si el valor de  R0 es mayor que 1, de modo que con un R0 estimado de 2.6, la neumonía de Wuhan es capaz de desatar una epidemia.  A modo comparativo: el ébola tiene un R0  entre 2 y 3, el SARS entre 2 y 5 y el sarampión entre 12 y 18. ¿De dónde surge el rango 1.5 -3.5 en la estimación del R0 del 2019-nCoV? De la naturaleza estocástica del modelado y de la suposición de que el número de contagios zoonóticos que iniciaron la epidemia pudieran ser algo mayor o menor de  40 individuos. Estos valores estimados de R0  son los esperados al inicio del brote, cuando la población aún desconoce que se ha iniciado; afortunadamente hay evidencia –por ejemplo en el caso del SARS- de que la producción de contagios secundarios disminuye cuando la población ya está alertada y toma medidas de resguardo.&lt;/p&gt;
&lt;p&gt;Como resultado del estudio, en el MRC afirman que dado el valor de R0 para la infección por  2019-nCoV, las medidas de control que se implementen necesitarán bloquear más del 60% de los contagios para reducir R0 al punto de ser efectivas en el manejo de la epidemia.&lt;/p&gt;
&lt;p&gt;¿Cuáles son los supuestos establecidos por los investigadores para el modelado de la infección por 2019-nCoV?  Dada la poca evidencia disponible al momento y dada la similitud con otros coronavirus conocidos, supusieron que dos de las características fundamentales de la neumonía de Wuhan son similares a las de SARS. En primer lugar, la evidencia sugiere que  en el caso del 2019-nCoV también existen dos tipos de infectados bien diferentes. Se trata de  “superpropagadores”, es decir, personas que generaran una gran cantidad de contagios secundarios, mientras que otros infectados generarían muy pocos o ninguno.&lt;/p&gt;
&lt;p&gt;¿Cómo se modela esto? No hay una única manera. Podría, por ejemplo, imponerse en el modelo que unos infectados se recuperaran mucho más lento que otros, contagiando mucha más gente durante su tiempo de infección (archive). Pero una manera alternativa es que la infección les dure lo mismo pero aumentar para un grupo el parámetro que fija la contagiosidad, lo cual tendría sentido si algunos por ejemplo tosieran mucho más que otros. La elección de una u otra forma de modelar lo mismo depende del peso que le den a las distintas evidencias biológicas de que disponen y de las herramientas matemáticas que prefieran, es decir, de las personas que integran el grupo.&lt;/p&gt;
&lt;p&gt;La otra suposición que asumen en el MRC es que el tiempo medio de generación de nuevas infecciones es de 8.4 días, el mismo que fue estimado para  SARS.  Como parte de la metodología habitual, dada la incerteza en los parámetros, también exploraron otros escenarios donde la diferencia entre infectados “regulares” y superpropagadores fuese menor y también tuviese un tiempo menor de generación de nuevos infectados, encontrando que si bien los números obtenidos fueron algo diferentes, los comportamientos predichos para la enfermedad eran los mismos. Lo que se denomina, en el lenguaje del modelado, un resultado robusto.&lt;/p&gt;
&lt;p&gt;Sin embargo hay algunas diferencias de 2019-nCoV con SARS que ya son evidentes y que complican la situación: por ejemplo la existencia de casos de muy leve sintomatología pero sin embargo contagiosos, que no existen en SARS.  El número de casos de coronavirus 2019-nCoV ya ha superado, por otra parte, el número final de casos de SARS de la epidemia de 2003.&lt;/p&gt;
&lt;p&gt;Otros grupos de modelado también han publicado algunos resultados.  En el sitio MedRxiv, donde se publican preprints de trabajos en ciencias de la salud, un trabajo del 28 de enero estima el Ro en 3.11 (con rango entre  2.39 y 4.13), indicando que debe bloquearse un 58 a 76% de los contagios para detener la epidemia. &lt;a href=&#34;https://www.medrxiv.org/content/10.1101/2020.01.23.20018549v2&#34;&gt;https://www.medrxiv.org/content/10.1101/2020.01.23.20018549v2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Por eso, y a pesar de los tempranos resultados que pudieron tenerse gracias a los avances en modelado, es necesario un mayor conocimiento de la nueva enfermedad -en particular la relación entre la sintomatología y el grado de contagiosidad- para poder ofrecer una mejor estimación de la magnitud del problema de esta nueva enfermedad y las medidas de control de interés para la salud pública.&lt;/p&gt;
&lt;h3 id=&#34;referencias&#34;&gt;Referencias&lt;/h3&gt;
&lt;p&gt;&lt;a id=&#34;1&#34;&gt; 1. &lt;/a&gt; Lancet Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China
Chaolin Huang et al. &lt;a href=&#34;https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(20)30183-5/fulltext&#34;&gt;https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(20)30183-5/fulltext&lt;/a&gt; DOI:https://doi.org/10.1016/S0140-6736(20)30183-5 Publicado online el 24 de enero 2020&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;2&#34;&gt; 2. &lt;/a&gt; Trop Med Int Health. 2009 Nov;14 Suppl 1:92-100. doi: 10.1111/j.1365-3156.2009.02244.x. Epub 2009 Jun 5. Mathematical modelling of SARS and other infectious diseases in China: a review.
Han XN1, de Vlas SJ, Fang LQ, Feng D, Cao WC, Habbema JD.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;3&#34;&gt; 3. &lt;/a&gt; MRC Centre for Global Infectious Disease Analysis &lt;a href=&#34;https://www.imperial.ac.uk/medicine/departments/school-public-health/infectious-disease-epidemiology/mrc-global-infectious-disease-analysis/about-us/&#34;&gt;https://www.imperial.ac.uk/medicine/departments/school-public-health/infectious-disease-epidemiology/mrc-global-infectious-disease-analysis/about-us/&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Primer Reporte: &lt;a href=&#34;https://www.imperial.ac.uk/media/imperial-college/medicine/sph/ide/gida-fellowships/2019-nCoV-outbreak-report-17-01-2020.pdf&#34;&gt;https://www.imperial.ac.uk/media/imperial-college/medicine/sph/ide/gida-fellowships/2019-nCoV-outbreak-report-17-01-2020.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Segundo Reporte: &lt;a href=&#34;https://www.imperial.ac.uk/media/imperial-college/medicine/sph/ide/gida-fellowships/2019-nCoV-outbreak-report-22-01-2020.pdf&#34;&gt;https://www.imperial.ac.uk/media/imperial-college/medicine/sph/ide/gida-fellowships/2019-nCoV-outbreak-report-22-01-2020.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Tercer Reporte: &lt;a href=&#34;https://www.imperial.ac.uk/mrc-global-infectious-disease-analysis/news--wuhan-coronavirus/&#34;&gt;https://www.imperial.ac.uk/mrc-global-infectious-disease-analysis/news--wuhan-coronavirus/&lt;/a&gt;
SARS Modeling Super-spreading Events for Infectious Diseases: Case Study SARS Thembinkosi Mkhatshwa  Anna Mummert &lt;a href=&#34;https://arxiv.org/pdf/1007.0908.pdf&#34;&gt;https://arxiv.org/pdf/1007.0908.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

        
      </description>
    </item>
    
    <item>
      <title>&#34;¡Felicitaciones! Usted ha ganado un premio&#34; - Una mirada de género sobre las distinciones en ciencia</title>
      <link>https://ciencianet.com.ar/post/felicitaciones-ud-ha-ganado-un-premio/</link>
      <pubDate>Sun, 09 Sep 2018 22:13:05 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/felicitaciones-ud-ha-ganado-un-premio/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero.&lt;/strong&gt; Investigadora de CONICET en el INIFTA (Instituto de Investigaciones Fisicoquímicas Teóricas y Aplicadas). Docente en la UNLP.&lt;/p&gt;
&lt;p&gt;En el área de la Ciencia y la Tecnología existen diferentes instituciones que conceden premios como estímulo, recompensa o reconocimiento a la labor de investigación de alguna persona. Estas distinciones suelen llevar el nombre de una persona destacada del área que promueven, ya fallecida. En general, el galardón entregado al ganador consiste de un incentivo económico, y un objeto simbólico como un diploma de honor o una medalla.  En esta entrada desarrollamos una mirada de género sobre estos reconocimientos.&lt;/p&gt;
&lt;h3 id=&#34;evaluación-entre-pares&#34;&gt;&lt;strong&gt;Evaluación entre pares&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;La ciencia es un ámbito androcéntrico. Existen abundantes estudios que muestran la existencia de sesgo de género en detrimento de las mujeres. Uno de los efectos de este sesgo es el denominado “Techo de cristal”, un límite invisible pero infranqueable que aparece bloqueando el acceso de las investigadoras a las posiciones de más reconocimiento y/o poder. Los factores que confluyen para generar esta desigualdad son diversos y de diferente índole, pero uno de ellos es la evaluación de antecedentes.&lt;/p&gt;
&lt;p&gt;Como se documentó ampliamente en un estudio reciente &lt;a href=&#34;#1&#34; title=&#34;1&#34;&gt;[1]&lt;/a&gt;, en las evaluaciones que se realizan en ciencia existe un sesgo de género,  a favor de los hombres, que se hace presente a la hora de analizar candidatos para una posición de investigación en una universidad. Se mostró que este sesgo es no intencional, indistinto a la edad, el status académico y el propio género del evaluador, pero sí tiene que ver con el estereotipo cultural dominante.&lt;/p&gt;
&lt;p&gt;Este sesgo de género existente en el mundo de las ciencias en detrimento de la mujer opera también en otros ámbitos donde son evaluados los antecedentes académicos de las personas, como  ocurre con postulantes para ser invitados a dar una conferencia en ciertos congresos o para recibir alguna distinción a la labor científica realizada. En este contexto, no resulta sorprendente que la inmensa mayoría de los ganadores de dichos premios o distinciones sean hombres.&lt;/p&gt;
&lt;p&gt;Esta situación de inequidad resulta muy perniciosa ya que refuerza la idea de los científicos hombres como más competentes que sus colegas mujeres e instala fuertemente la idea de que los mejores científicos de la categoría son hombres. La gran visibilidad adicional que reciben estas distinciones, cuyos resultados son recibidos por toda la comunidad e incluso por la prensa, hace que estas ideas sean extendidas a la sociedad entera.&lt;/p&gt;
&lt;p&gt;Sin embargo, este no es el único factor que discrimina a las mujeres. Estos premios, salvo contadísimas excepciones, tienen nombre de hombre. Desgraciadamente, no está disponible la información de las personas postuladas a cada premio, sin embargo sí puede accederse a los ganadores, y en ocasiones, al jurado. Así, resulta que los jurados de estos premios con nombre de hombre son también hombres, y los ganadores, por supuesto, son mayoritariamente hombres. Las mujeres parecemos estar excluidas de este circuito de reconocimiento.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2018/09/glass-ceiling.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;decime-cuál-cuál-cual-es-tu-nombre&#34;&gt;&lt;strong&gt;Decime cuál, cuál, cual, es tu nombre&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Aquí, para ilustrar, se listan los nombres de algunos premios que concede la Academia Nacional de Ciencias Exactas, Físicas y Naturales: &lt;em&gt;&lt;strong&gt;Premio “Félix Lilli” en Ingeniería Vial&lt;/strong&gt;&lt;/em&gt;, &lt;em&gt;&lt;strong&gt;Premio “Pedro Cattaneo” en Tecnología de Alimentos&lt;/strong&gt;&lt;/em&gt;, &lt;em&gt;&lt;strong&gt;Premio “Angel L. Cabrera” en Ciencias Biológicas&lt;/strong&gt;&lt;/em&gt;, &lt;em&gt;&lt;strong&gt;Premio “Juan Olsacher” en Ciencias de la Tierra&lt;/strong&gt;&lt;/em&gt;, &lt;em&gt;&lt;strong&gt;Premio “Patricio Laura” en Ingeniería Mecánica&lt;/strong&gt;&lt;/em&gt;, &lt;em&gt;&lt;strong&gt;Premio “Venancio Deulofeu” en Química&lt;/strong&gt;&lt;/em&gt;, &lt;em&gt;&lt;strong&gt;Premio “Pedro E. Zadunaisky” en Matemática&lt;/strong&gt;&lt;/em&gt;, &lt;em&gt;&lt;strong&gt;Premio “Carlos C. Bollini” en Física&lt;/strong&gt;&lt;/em&gt;, &lt;em&gt;&lt;strong&gt;Premio “Ramón Enrique Gaviola” en Astronomía&lt;/strong&gt;&lt;/em&gt;, &lt;em&gt;&lt;strong&gt;Premios Estímulo en Física &amp;quot;Guido Beck&amp;quot;&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Esta elección de nombres masculinos no representa un caso aislado. Premio &amp;quot;Dr. José A. Catoggio&amp;quot; de la  Asociación Argentina de Químicos Analíticos, Premio “Rey Pastor” de la Unión Matemática Argentina, premios “Luis Masperi” y “J. J. Giambiagi” de la Asociación  de Física de Argentina, y la lista podría seguir. Una singularidad para celebrar es la distinción que otorga la Asociación Argentina de Investigaciones Fisicoquímicas cada dos años a científicos talentosos de la Argentina, de hasta 47 años de edad, que se hayan destacado por su labor en la investigación y el desarrollo de la fisicoquímica. Este premio lleva el nombre “María Cristina Giordano”. También otorga otros dos premios con los nombres “Hans  Schumacher” y “Pedro J. Aymonino”.&lt;/p&gt;
&lt;h3 id=&#34;and-the-winner-is&#34;&gt;And the winner is...&lt;/h3&gt;
&lt;p&gt;El Ministerio de Ciencia, Tecnología e Innovación Productiva –recientemente degradado a Secretaría por el Gobierno actual- inició el 2003 la entrega de los Premios “Houssay”, “Sábato” y fugazmente (entre 2010 y 2011) entregó también el premio “Rebeca Gerschman” a investigadoras de más de 60 años. Desde 2008, se agregó la ”Distinción Investigador/a de la Nación”.&lt;/p&gt;
&lt;p&gt;Los premios Houssay tienen varias categorías. En la categoría Trayectoria, entregada 25 veces, las mujeres representan un 24%. En las otras, que han sido otorgadas en menos oportunidades, la representación femenina ronda el 30%. En esta línea de premios que no llevan nombre propio hay otros, como el &amp;quot;Premio a la labor científica, tecnológica y artística de la Universidad Nacional de La Plata&amp;quot;, el premio que otorga la Sociedad Argentina de Biofísica a la mejor tesis doctoral, y los Premios Trayectoria y Estímulo de la Fundación Bunge y Born. El Premio Bunge y Born a la Trayectoria se otorga desde 1964 “a destacados científicos argentinos” y sólo fue recibido por 3 mujeres, es decir, un 5,5%. En cambio, en el Premio Estímulo, destinado a jóvenes investigadores, iniciado en 2001, el porcentaje de mujeres es mayor: 30%.&lt;/p&gt;
&lt;h3 id=&#34;en-el-nombre-del-premio&#34;&gt;En el nombre del premio&lt;/h3&gt;
&lt;p&gt;¿Por qué insistir en que el nombre del premio es relevante? Porque es el lenguaje el que establece nuestras categorías de pensamiento y construye la cosmovisión desde la cual percibimos el mundo. El lenguaje opera como principio de clausura: su límite es el límite de los significados que constituyen el mundo. En palabras de Wittgenstein, «los límites de mi lenguaje significan los límites de mi mundo». Según su análisis de la violencia sobre las mujeres, Sánchez y Femenias resaltan “la importancia del lenguaje, no sólo como opción de clausura sino, fundamentalmente, como factor de apertura a la resignificación, el reconocimiento, el sentido y la toma de la palabra de las mujeres; un modo de ruptura con los pactos de silencio, implícitos y  efectivos, que suelen encubrir la violencia contra las mujeres” &lt;a href=&#34;#2&#34; title=&#34;2&#34;&gt;[2]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Y la invisibilización es considerada una forma de violencia. Simbólica, pero violencia al fin. Como expresan Estrada y Cormik en su carta, “La falta de acciones tendientes a combatir las desigualdades de género constituye implícitamente una aceptación y prolongación de las condiciones actuales.” En este sentido, el uso de lenguaje inclusivo en la comunicación oficial, el establecimiento de premios con nombre de mujeres destacadas de la ciencia y la vigilancia de adecuadas representaciones de género entre quienes otorguen y entre quiennes obtengan los galardones se presenta como una interesante vía de apertura para resignificar el rol de la mujer en la ciencia, desdibujar el androcentrismo y acercarnos a la igualdad de oportunidades.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;La falta de acciones tendientes a combatir las desigualdades de género constituye implícitamente una aceptación y prolongación de las condiciones actuales. La ausencia de mujeres entre los conferencistas invitados resulta grave no sólo como falta de reconocimiento a la labor femenina en el área, sino también como mensaje para las estudiantes y becarias, en la medida en que se les muestra la física como una ciencia eminentemente masculina.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;Laura Cecilia Estrada y Cecilia Cormick (Carta abierta a la Asociación Física Argentina, año 2017)&lt;/em&gt;  &lt;/p&gt;
&lt;h3 id=&#34;referencias&#34;&gt;Referencias&lt;/h3&gt;
&lt;p&gt;&lt;a id=&#34;1&#34;&gt;1.&lt;/a&gt; Moss-Racusin, C.A., et al., &lt;a href=&#34;https://doi.org/10.1073/pnas.1211286109&#34;&gt;Science faculty’s subtle gender biases favor male students&lt;/a&gt;. PNAS, 2012. &lt;strong&gt;109&lt;/strong&gt;(41): p. 5.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;2&#34;&gt;2.&lt;/a&gt;  Sánchez, E.A. and M.L. Femenías, Articulaciones sobre la violencia contra las mujeres. 2008, La Plata: Edulp.&lt;/p&gt;
&lt;h2 id=&#34;comentarios&#34;&gt;Comentarios&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;mailto:cyabalos@yahoo.com.ar&#34;&gt;Cecilia&lt;/a&gt; - &lt;time datetime=&#34;2018-09-09 20:10:12&#34;&gt;Sep 0, 2018&lt;/time&gt;&lt;/p&gt;
&lt;p&gt;Excelente! Espero que sea leído por los varones que visitan este sitio.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href=&#34;mailto:sbusting@gmail.com&#34;&gt;Sebastian&lt;/a&gt; - &lt;time datetime=&#34;2018-09-09 21:34:43&#34;&gt;Sep 0, 2018&lt;/time&gt;&lt;/p&gt;
&lt;p&gt;Muy bueno! El mensaje es muy claro.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href=&#34;mailto:merinolina39@gmail.com&#34;&gt;Lina&lt;/a&gt; - &lt;time datetime=&#34;2018-09-10 19:42:47&#34;&gt;Sep 1, 2018&lt;/time&gt;&lt;/p&gt;
&lt;p&gt;Muy buena nota!&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href=&#34;mailto:patemai@yahoo.com.ar&#34;&gt;Maite&lt;/a&gt; - &lt;time datetime=&#34;2018-09-24 16:30:31&#34;&gt;Sep 1, 2018&lt;/time&gt;&lt;/p&gt;
&lt;p&gt;Muy buena nota. Así como en la ciencia lo mismo sucede en el arte, está instituido que los premios y los museos lleven nombre de hombres. Hay que continuar señalando las desigualdades.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href=&#34;mailto:glvillate@hotmail.com&#34;&gt;Guillermo Luis&lt;/a&gt; - &lt;time datetime=&#34;2019-03-15 08:48:59&#34;&gt;Mar 5, 2019&lt;/time&gt;&lt;/p&gt;
&lt;p&gt;Muy esclarecedor texto , me parece que es la punta del ovillo.&lt;/p&gt;
&lt;hr /&gt;

        
      </description>
    </item>
    
    <item>
      <title>La teoría de la relatividad y el centro galáctico</title>
      <link>https://ciencianet.com.ar/post/la-teoria-de-la-relatividad-y-el-centro-galactico/</link>
      <pubDate>Sun, 19 Aug 2018 23:49:23 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/la-teoria-de-la-relatividad-y-el-centro-galactico/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Gaston Giribet.&lt;/strong&gt;  Center for Cosmology and Particle Physics de New York University, Universidad de Buenos Aires y CONICET.&lt;/p&gt;
&lt;p&gt;La vía láctea, galáctica espiral de la que nuestro sistema solar forma modesta parte, alberga en su centro un enorme agujero negro cuya masa es cuatro millones de veces superior a la de nuestro sol. Sus coordenadas ubican al astro central en la región del cielo que los antiguos convinieron en llamar la constelación de Sagitario, hecho que explica en parte su nombre: Sagittarius A.&lt;/p&gt;
&lt;p&gt;Cientos de miles de millones de estrellas forman las aspas de la galaxia. Cientos de miles de millones de soles arremolinados en torno a un eje común. Algunas de esas estrellas, las más cercanas al centro, orbitan en torno a Sagittarius A zamarreadas por el intenso campo gravitatorio que éste genera. Una de esas estrellas, llamada S2, es la protagonista de esta historia.&lt;/p&gt;
&lt;p&gt;S2 tiene la masa equivalente a la de diez soles como el nuestro. Sisífica, circunda al agujero negro del centro galáctico trazando una órbita cuasi-elíptica que completa y repite al cabo de dieciséis años. Los astrónomos vienen siguiendo el movimiento de S2 con perseverancia desde 1992; una tarea que al día de hoy, luego de veintiséis años de observación, los ha dotado de un registro detallado de su trayectoria. Esto, junto con la proximidad que S2 alcanza con Sagittarius A, hace de esta estrella la candidata perfecta para poner a prueba nuestras teorías acerca de qué ocurre en el centro galáctico, esa región distante en la que nacen estrellas y anida un gigante.&lt;/p&gt;
&lt;p&gt;Esto nos permite, en particular, testar varias de las predicciones de la teoría de la relatividad. Según la teoría de la relatividad general, formulada por Einstein en 1915, el tiempo transcurre más lento en las regiones en las cuales el campo gravitatorio es más intenso. Y en la proximidad de un agujero negro como Sagittarius A el campo gravitatorio alcanza una intensidad tal que el tiempo, allí, se enlentece apreciablemente. La forma en la que dicha dilatación temporal puede ser medida es mediante la observación del color de la luz proveniente de esas regiones de intensa gravedad: La luz es una onda, y el color es la forma en la que las ondas de luz expresan el paso del tiempo. La luz de color violeta, por ejemplo, corresponde a una vibración de setecientos billones de veces por segundo, mientras que la luz de color rojo corresponde a una vibración de cuatrocientos billones de veces por segundo, poco más que la mitad.&lt;/p&gt;
&lt;p&gt;Debido a esto, la luz que nos llega de los procesos físicos que acaecen en regiones donde la gravedad es más intensa y, por lo tanto, donde el tiempo fluye más lento, ha de ser de un tinte más rojizo que el esperado. En otras palabras, el tiempo transcurrido entre dos oscilaciones de una onda de luz generada por un dado proceso físico es mayor donde la gravedad es más intensa, lo que se traduce en que recibimos de allí menos oscilaciones por cada segundo de los que nuestros relojes terrestres marcan. Este efecto se denomina “corrimiento al rojo gravitatorio”.&lt;/p&gt;
&lt;p&gt;Al corrimiento al rojo se le suma un segundo efecto, también predicho por Einstein. Éste recibe el nombre de “efecto Doppler transversal”, aunque poco tiene que ver con el efecto Doppler usual que conocemos para las ondas acústicas emitidas por fuentes que se acercan o alejan. Según la teoría de la relatividad especial, de 1905, el tiempo también transcurre más lento para los cuerpos que llevan grandes velocidades, velocidades tales como los vertiginosos veinticinco millones de kilómetros por hora que S2 alcanza en su orbitar cerca de Sagittarius A. Así, según la teoría, en el caso de S2 el efecto de corrimiento al rojo gravitatorio y el efecto Doppler transversal deberían potenciarse y dotar a la estrella de un sospechoso tinte rojizo al encontrarse ésta en el periastro de su órbita.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2018/08/S2-768x489.jpg&#34; alt=&#34;Representación artística de la órbita de S2 en torno a Sagittarius A. Crédito de la imagen: ESO/M. Kornmesser.&#34;&gt;&lt;/p&gt;
&lt;p&gt;Fue precisamente ese ligero corrimiento al rojo en la luz emitida por S2 lo que los astrónomos del Observatorio Europeo en el Sur (ESO) se dispusieron a observar en mayo de 2018, momento en que la estrella se encontró en su punto más cercano al gigante Sagittarius A. Una observación astronómica de esta naturaleza requiere una precisión inédita. Si bien el radio de la órbita de la estrella en su punto de acercamiento máximo al agujero negro es de unos veinte mil millones de kilómetros, esa distancia empalidece ante los veintiséis mil años luz que separan el centro galáctico de la Tierra.&lt;/p&gt;
&lt;p&gt;Esto hace que pretender observar la órbita de S2 con suficiente detalle demande el abuso de cuatro grandes telescopios operando en tándem para formar, así, un gran telescopio resultante. Las señales de los cuatro telescopios se combinan en un sofisticado sistema óptico, denominado GRAVITY, especialmente diseñado para la empresa -y se emplean otros sistemas, como el denominado SINFONI-. Esta tecnología le permitió al grupo del Instituto Max Planck de Alemania liderado por Reinhard Genzel obtener una sensibilidad única y, gracias a esto, un resultado contundente: Al pasar cerca del agujero negro del centro galáctico la estrella S2 se mostró sonrojada, en perfecta concordancia con las predicciones de la teoría de Einstein.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referencias:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://doi.org/10.1051/0004-6361/201833718&#34;&gt;&lt;em&gt;&amp;quot;Detection of the gravitational redshift in the orbit of the star S2 near the Galactic centre massive black hole&amp;quot;&lt;/em&gt;&lt;/a&gt;, R. Abuter et al., GRAVITY Collaboration, Astronomy and Astrophysics 615 (2018) L15.  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.sciencemag.org/news/2018/07/star-s-black-hole-encounter-puts-einstein-s-theory-gravity-test&#34;&gt;&lt;em&gt;&amp;quot;Milky Way’s black hole provides long-sought test of Einstein’s general relativity&amp;quot;&lt;/em&gt;&lt;/a&gt;, en Nature, July 26th 2018.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

        
      </description>
    </item>
    
    <item>
      <title>Einstein, Tucumán y el infinito</title>
      <link>https://ciencianet.com.ar/post/einstein-tucuman-y-el-infinito/</link>
      <pubDate>Wed, 04 Jul 2018 03:53:12 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/einstein-tucuman-y-el-infinito/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Gaston Giribet.&lt;/strong&gt; Investigador del Center for Cosmology and Particle Physics de New York University. Profesor de la UBA e Investigador Principal del CONICET.&lt;/p&gt;
&lt;p&gt;Hace 75 años, Albert Einstein y Wolfgang Pauli escribían en coautoría un artículo en el que estudiaban las divergencias de la teoría de Kaluza y Klein, la teoría que sostiene que nuestro universo podría tener una quinta dimensión. Al poco de comenzar a leer ese artículo, publicado en &lt;em&gt;Annals of Mathematics&lt;/em&gt; en 1943, uno inevitablemente atiende a una curiosa cita bibliográfica que aparece al pie de la primera página. Se trata de la cita a una ignota precuela de 1941 publicada por Albert Einstein en la Revista de la Universidad Nacional de Tucumán. En su artículo tucumano, Einstein daba una demostración ingeniosa de la inexistencia de soluciones masivas y finitas en su teoría de la Relatividad General.&lt;/p&gt;
&lt;p&gt;El artículo de Tucumán fue encargado a Einstein por el matemático italiano Alessandro Terracini interpósita persona. Fue Guido Fubini quien, a mediados de 1941, intercedió para pedirle a Einstein que considerara la posibilidad de enviar una contribución para la Revista de la Universidad Nacional de Tucumán, revista que por aquel entonces estaba siendo fundada por Terracini y colaboradores (Terracini, 1941; 1944). Einstein aceptó amablemente la invitación y cumplió con enviar un artículo breve, al que se refirió como “&lt;em&gt;ein hübscher beweis&lt;/em&gt;”, una donosa demostración (Einstein, 1941c).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2018/07/BetoII-238x300.jpg&#34; alt=&#34;Albert Einstein, en Princeton NJ.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;En su artículo, Einstein presentaba una demostración sucinta de la inexistencia de soluciones esféricamente simétricas y regulares en la teoría de la Relatividad General. Más precisamente, Einstein demostraba en su trabajo que sus ecuaciones para el campo gravitatorio no admitían soluciones que cumplieran con los siguientes cuatro requisitos simultáneamente:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;a)&lt;/em&gt; exhibir simetría esférica,&lt;/li&gt;
&lt;li&gt;&lt;em&gt;b)&lt;/em&gt; no depender explícitamente del tiempo,&lt;/li&gt;
&lt;li&gt;&lt;em&gt;c)&lt;/em&gt; aproximar a largas distancias las soluciones de las ecuaciones newtonianas,&lt;/li&gt;
&lt;li&gt;&lt;em&gt;d)&lt;/em&gt; no presentar divergencias del campo.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Obedecer los cuatro requisitos implicaría un absurdo, concluía el trabajo. Estas cuatro propiedades son, desde el punto de vista de la física, requerimientos muy naturales. En efecto, se trata de las propiedades que uno esperaría del campo gravitatorio generado, por ejemplo, por una partícula: Si uno piensa en una partícula u otro objeto masivo, compacto, esférico y en reposo, entonces las condiciones &lt;em&gt;a)&lt;/em&gt; y &lt;em&gt;b)&lt;/em&gt; no son sino la exigencia de que el campo generado por el objeto respete las simetrías del objeto mismo –esférico y en reposo–. Por su parte, pedir la validez de la condición &lt;em&gt;c)&lt;/em&gt; es simplemente pedir que lejos del objeto, donde el campo gravitatorio generado por éste es desdeñable, dicho campo sea bien aproximado por la teoría gravitatoria de Newton, que se sabe cierta precisamente en el régimen de campo débil. Es la requisito &lt;em&gt;d)&lt;/em&gt;, acaso, el que demanda más justificación. La razón fundamental para pedir, o al menos desear, que el campo generado por un objeto no sea divergente sino finito es que las singularidades –los infinitos– que con frecuencia aparecen en las soluciones a las ecuaciones de campo introducen un alto grado de arbitrariedad en la teoría, algo que se considera inaceptable para una teoría fundamental. Cuando las ecuaciones arrojan soluciones que en algún punto o región del espacio o del tiempo resultan infinitas, entonces las ecuaciones pierden predictibilidad, ya que el infinito no es “mucho” sino una indeterminación del valor calculado.&lt;/p&gt;
&lt;p&gt;Este problema se acentúa cuando hablamos del campo gravitatorio, ya que, según la teoría de Einstein, el campo gravitatorio es inextirpable de la geometría del espacio-tiempo mismo. Debido a esto, un resultado infinito de las ecuaciones del campo gravitatorio implica una fisura en el entramado espacio-temporal. Asimismo, Einstein encontraba un segundo inconveniente en el infinito: Era su íntima convicción que la materia debía ser parte inescindible de la teoría del campo, y que los objetos no eran los que generaban campos –gravitatorios o eléctricos– sino que éstos eran, per se, campo. Es decir, para Einstein la materia, las partículas, debía poder ser explicada como acumulación finita de campo; como si las partículas que gravitan no fueran otra cosa que la mera concentración del mismo campo gravitatorio que parecen generar. Su conclusión de que el campo en el centro del objeto que lo generaba debía ineluctablemente ser infinito parecía atentar contra esta bella idea.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2018/07/PaperCopy.png&#34; alt=&#34;El artículo de Einstein en la Revista UNT 1941&#34;&gt;&lt;/p&gt;
&lt;p&gt;El trabajo de Einstein (Einstein, 1941a) venía demostrar de una manera ingeniosa que las soluciones de las ecuaciones del campo gravitatorio eran inevitablemente divergentes, inevitablemente infinitas en algún punto del espacio. El título original del borrador de Einstein en alemán era “&lt;em&gt;Beweis der Nichtexistenz von Singularitätsfreien Gravitationsfeldern mit nicht Verschwindender Gesamtmasse&lt;/em&gt;”, traducción literal de los títulos con los que después se publicaría (Einstein, 1941a; 1941d): “Demostración de la no existencia de campos gravitacionales sin singularidades de masa total no nula”.&lt;/p&gt;
&lt;p&gt;El manuscrito se encuentra en posesión de la Biblioteca Matemática de la Universidad de Turín. Se trata de un trabajo no muy conocido, aunque aparece citado en algunas fuentes, e.g. (Föolsing, 1993; Earman y Eisenstaedt, 1999; van Dongen, 2002). Fue Abraham Taub, un reconocido relativista, quien reseñó el artículo de Einstein para Mathematical Review (Taub, 1942). La traducción del original en alemán fue supervisada por el mismo Terracini, quien en una carta dirigida a Einstein le informó de que se iban a preparar dos traducciones, una al castellano y otra al inglés (Terracini, 1941). Ambas versiones (Einstein, 1941d; 1941a) se publicaron en el mismo volumen de la serie A de la Revista de la Universidad Nacional de Tucumán en diciembre de 1941.&lt;/p&gt;
&lt;p&gt;En las ediciones siguientes la Revista continuaría publicando artículos en el tema; en especial, trabajos relacionados con teorías de campo unificado, e.g. (Santaló, 1954; 1959). La demostración de Einstein de la inexistencia de soluciones no-divergentes a las ecuaciones de campo adquiere importancia, no por tratarse de un resultado novedoso en el contexto de la Relatividad General, sino debido a su cualidad de ser fácilmente generalizable a otras teorías de campos. Una de esas generalizaciones es la que presentaron Einstein y Pauli en su artículo (Einstein y Pauli, 1943), en el que investigaban la existencia de soluciones regulares en la teoría de Kaluza-Klein.&lt;/p&gt;
&lt;p&gt;Otra generalización de la demostración de Einstein fue la adaptación que, en 1948, Papapetrou hizo al caso de la teoría no-simétrica del campo unificado en la que en ese momento Einstein y su soldadesca se encontraban trabajando (Einstein, 1945; Einstein and Straus, 1946; Einstein, 1949; Einstein and Kaufman, 1952; 1953; 1954; 1955). Papapetrou argüía en su trabajo (Papapetrou, 1948) que tampoco la generalización no-simétrica de la Relatividad General estaba a salvo de los temibles infinitos.&lt;/p&gt;
&lt;p&gt;El criterio de existencia de soluciones no-divergentes a las ecuaciones de campo llegó a convertirse en un regente severo en la búsqueda de Einstein de una teoría del campo unificado. Su firme creencia en que en una teoría final los constituyentes de la materia deberían poder ser entendidos como acumulaciones finitas de campo puro lo llevó, en varias ocasiones, a abandonar líneas de investigación que al comienzo se mostraban promisorias. Algunos incluso afirman que, para Einstein, el hecho de que la materia y los campos fueran entidades de naturaleza distinta era tan poco satisfactorio como el hecho de que las distintas fuerzas de la naturaleza no nacieran de una única e irreducible ecuación.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Agradecimientos:&lt;/strong&gt; El autor le agradece a Mariano Galvagno por la colaboración en la redacción de este texto, con las traducciones, la bibliografía, y por las discusiones sobre temas relacionados.&lt;/p&gt;
&lt;h3 id=&#34;referencias&#34;&gt;Referencias&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Earman J. and Eisenstaedt J., Einstein and singularities, Stud. Hist. Mod. Phys. 30, 2 (1999), 185.&lt;/li&gt;
&lt;li&gt;Einstein A. and Rosen N., The particle problem in the general theory of relativity, Phys. Rev. 48 (1935), 73. - Einstein A., On a stationary system with spherical symmetry consisting of many gravitating masses, Ann. of Math. 40, 2 (1939), 922.&lt;/li&gt;
&lt;li&gt;Einstein A., Demonstration of the non-existence of gravitational fields with a non-vanishing total mass free of singularities, Revista de la Universidad Nacional de Tucum´an, A2, (1941), 11.&lt;/li&gt;
&lt;li&gt;Einstein A., letter to G. Fubini, dated on August 5 of 1941. Italian translation of the letter addressed to G. Fubini, dated on August 5 of 1941.&lt;/li&gt;
&lt;li&gt;Einstein A., letter to G. Fubini, dated on August 5 of 1941; in German. Copy at The Albert Einstein Archives, The Hebrew University of Jerusalem, Israel.&lt;/li&gt;
&lt;li&gt;Einstein A., Demostración de la no existencia de campos gravitacionales sin singularidades de masa total no nula, Revista de la Universidad Nacional de Tucumán, A2, (1941), 5.&lt;/li&gt;
&lt;li&gt;Einstein A. and Pauli W., On the non-existence of regular stationary solutions of relativistic field equations, Ann. of Math. 44, 2 (1943), 131.&lt;/li&gt;
&lt;li&gt;Einstein A., A generalization of the relativistic theory of gravitation, Ann. of Math. 46, 4 (1945), 578.&lt;/li&gt;
&lt;li&gt;Einstein A. and Straus E., A generalization of the relativistic theory of gravitation. II, Ann. of Math. 47, 4 (1946), 731.&lt;/li&gt;
&lt;li&gt;Einstein A., Autobiographical Notes, Open Court Publishing Company, La Salle, Illinois (1949).&lt;/li&gt;
&lt;li&gt;Einstein A. and Kaufman B., Volume in honor to Louis de Broglie, Paris, (1952), 321.&lt;/li&gt;
&lt;li&gt;Einstein A., The meaning of relativity, Princeton University Press, 1953.&lt;/li&gt;
&lt;li&gt;Einstein A. and Kaufman B., Algebraic properties of the field in the relativistic theory of the asymmetric field, Ann. of Math. 59, 2 (1954).&lt;/li&gt;
&lt;li&gt;Einstein A. and Kaufman B., A new form of the general relativistic field equations, Ann. of Math. 52, 1 (1955).&lt;/li&gt;
&lt;li&gt;Einstein A., Über die spezielle und die allgemeine Relativistätsheorie, Vieweg, 1969.&lt;/li&gt;
&lt;li&gt;Einstein A. and Besso M., Correspondance (1903-1955), Spanish translation, Tusquets Editores, Barcelona, 1994.&lt;/li&gt;
&lt;li&gt;Fölsing A., Einstein A., Biographie, Suhrkamp Taschenbuch 3087, Suhrkamp Verlag Frankfurt am Main, Ffm 1993A (1993).&lt;/li&gt;
&lt;li&gt;Galvagno M., Giribet G., The particle problem in classical gravity: A historical note on 1941, Eur. J. Phys. 26 (2005), 97.&lt;/li&gt;
&lt;li&gt;Hlavaty V., Geometry of Einstein’s unified field theory, P. Noordhoff-Gromingen-Holland, 1957.&lt;/li&gt;
&lt;li&gt;Lichnerowicz A., Théories relativistes de la gravitation et de l’electromagnétisme, Masson, 1955.&lt;/li&gt;
&lt;li&gt;Pauli W., Zur Theorie der Gravitation und der Elektrizität von Hermann Weyl, Physikalische Zeitschrift, 20 (1919), 10.&lt;/li&gt;
&lt;li&gt;Pauli W., Writtings on Physics and Phylosophy, Springer-Verlag Berlin Heidelberg, 1994.&lt;/li&gt;
&lt;li&gt;Papapetrou A., The question of non-singular solutions in the generalized theory of gravitation, Phys. Rev. 73, 9 (1948), 1105.&lt;/li&gt;
&lt;li&gt;Santaló L.A., Revista de la Universidad Nacional de Tucumán A1 10 (1954), 19.&lt;/li&gt;
&lt;li&gt;Santaló L.A., Revista de la Universidad Nacional de Tucumán A1 XII (1959), 31.&lt;/li&gt;
&lt;li&gt;Schrödinger E., Proc. R. Ir. Acad. 49A, 43 (1943), 225.&lt;/li&gt;
&lt;li&gt;Schrödinger E., Proc. R. Ir. Acad. 52A (1948), 1.&lt;/li&gt;
&lt;li&gt;Schrödinger E., Space-Time Structure, Cambridge University Press, 1950.&lt;/li&gt;
&lt;li&gt;Sen D.K., Particles and/or fields, Academic Press Inc. (London), 1968.&lt;/li&gt;
&lt;li&gt;Taub A.H., Review of Demonstration of the non-existence of gravitational fields with a non-vanishing total mass free of singularities by A. Einstein; Mathematical Review, AMS 2004, MR0006877.&lt;/li&gt;
&lt;li&gt;Terracini A., letter to A. Einstein, dated on October 21 of 1941; in German. The Albert Einstein Archives, The Hebrew University of Jerusalem, Israel.&lt;/li&gt;
&lt;li&gt;Terracini A., letter to A. Einstein, dated on August 6 of 1944; in Spanish. The Albert Einstein Archives, The Hebrew University of Jerusalem, Israel.&lt;/li&gt;
&lt;li&gt;Tonnelat M.A., La Théorie du champ unifié d’ Einstein et quelques-uns de ses développements, Gauthier-Villars, 1955. Vizgin V.P., Unified field theory in the first third of the 20th century, Historical Studies, Birkhäuser Verlag (1994).&lt;/li&gt;
&lt;/ul&gt;

        
      </description>
    </item>
    
    <item>
      <title>La torre donde la luz gravita</title>
      <link>https://ciencianet.com.ar/post/la-torre-donde-la-luz-gravita/</link>
      <pubDate>Thu, 07 Jun 2018 18:08:52 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/la-torre-donde-la-luz-gravita/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Gaston Giribet.&lt;/strong&gt; Center for Cosmology and Particle Physics de New York University, Universidad de Buenos Aires y CONICET.&lt;/p&gt;
&lt;p&gt;Unas semanas atrás visité a dos colegas en la universidad de Harvard. Mientras trabajábamos los tres en el altillo del ala oeste del Laboratorio Jefferson, en un espacio luminoso rodeado de pizarrones, recordé el dato –anecdótico, reconozco– de que fue precisamente allí, en la torre que ese altillo corona, donde en 1959 se realizó uno de los experimentos que vendrían a convalidar las predicciones de la teoría de la relatividad general de Einstein. En aquella torre, hace casi sesenta años, se medía por primera vez que la luz también cae.&lt;/p&gt;
&lt;h3 id=&#34;en-la-torre&#34;&gt;En la torre&lt;/h3&gt;
&lt;p&gt;En efecto, la luz gravita, y fue en esa torre que se midió por primera vez su peso. Según la teoría de Einstein, y mal que le pese a Machado, por más sutil que un cuerpo sea jamás llegará a ser ingrávido. Todas las formas de energía y materia, incluso la gravedad misma, están a merced del influjo gravitatorio. La luz no escapa a tal suerte; si bien carece de masa, igual gravita.&lt;/p&gt;
&lt;p&gt;Esto se debe a que los efectos de la gravedad no son sino la deformación misma del espacio y el tiempo, la curvatura del escenario en el que ocurren todos los procesos físicos. La materia, la energía, y la luz en particular se ciñen a la forma que el continuo espaciotiemporal adopta alrededor. Así, desde Einstein, las órbitas planetarias, las parábolas balísticas y todos los fenómenos que asociamos a la fuerza gravitatoria se entienden, no como el influjo de la fuerza gravitatoria sobre los astros y otros objetos, sino como la curvatura del mismísimo espaciotiempo en el que esos entes existen, y a esas formas curvas del espacio han estos entes de amoldar su surcarlo.  &lt;/p&gt;
&lt;h3 id=&#34;la-curvatura-del-espacio-la-dilatación-del-tiempo&#34;&gt;La curvatura del espacio, la dilatación del tiempo&lt;/h3&gt;
&lt;p&gt;Ya en 1919, cuatro años después de la formulación definitiva de la teoría de la relatividad general, Arthur Eddington observaba la deflexión de la luz producida por el campo gravitatorio del sol. Porque nos convoca otro asunto, no ahondaremos aquí en esa historia, nada desprovista de anécdotas y detalles curiosos. Nos alcanzará con decir que la campaña de Eddington para observar el eclipse solar del 29 de mayo de 1919, en la que se observaría por primera vez la deflexión de la luz producida por la gravedad del gran astro, terminaría por dotar a Einstein de una fama mundial que, hasta el momento, se limitaba a dextros académicos.&lt;/p&gt;
&lt;p&gt;Eddington comprobaba con su telescopio ya en 1919 que la gravedad curvaba la trayectoria de la luz y que lo hacía en la cantidad precisa que Einstein había predicho en 1915. En otras palabras, se verificaba que la luz ceñía su trayectoria a la curvatura del espacio. Ahora bien, ¿qué del tiempo? Está en las bases de la teoría de la relatividad la naturaleza inescindible del espacio y el tiempo, que desde Einstein pasan a formar parte de un entramado único e inseparable, el espaciotiempo.&lt;/p&gt;
&lt;p&gt;Así, si la teoría predice que la gravedad genera una deformación del espacio entonces ha el tiempo de sufrir lo suyo. El tiempo también debe deformarse en presencia del campo gravitatorio. Si en el caso del espacio es ya complicado aprehender la noción de curvatura, qué esperar del tiempo, concepto tanto más escurridizo. ¿Qué significa que el tiempo se deforme? ¿Cómo se expresaría tal deformación en el fluir de los procesos?&lt;/p&gt;
&lt;p&gt;La relatividad nos lo explica: Desde la formulación de la teoría especial de la relatividad, en 1905, sabemos que el transcurrir del tiempo no es un fenómeno absoluto. Para diferentes observadores el ínterin entre dos sucesos físicos puede ser distinto. Por ejemplo, puede darse que, según uno de ellos, hayan transcurrido unos pocos minutos entre un proceso A y un proceso B, mientras que para el otro haya ese lapso sido de varios años. Para que esto se dé, alcanza con que la velocidad relativa entre un observador y el otro sea lo suficientemente alta. Cuanto más alta la velocidad con la que un observador se aleja del (o acerque al) otro, más grande será la discrepancia entre el lapso que cada uno medirá entre los mismos eventos.&lt;/p&gt;
&lt;p&gt;Y cabe aclarar que no se debe esta discrepancia a efectos de la percepción, sino a que el tiempo verdaderamente transcurre de manera diferente para ambos. Este efecto, conocido como dilatación temporal, permite explicar muchos fenómenos físicos que incluso podemos comprobar a escalas terrestres. En particular, explica por qué, cada segundo que pasa, nuestro cuerpo es atravesado por una decena de muones (partículas subatómicas similares a los electrones pero doscientas veces más pesadas).&lt;/p&gt;
&lt;p&gt;Esos muones, generados en la alta atmósfera, tienen una vida media muy corta. Debido a ello, según nuestro terrícola parecer no debería ese efímero existir alcanzarles para llegar desde la alta atmósfera hasta nosotros. Aun así, lo logran. ¿Cómo? Debido al efecto de la dilatación temporal que acabamos de explicar: Al moverse los muones a gran velocidad respecto de nosotros, la discrepancia entre el tiempo de su existencia que nosotros esperaríamos y el tiempo que ellos mismos experimentan es muy grande. Debido a esto, mientras nosotros esperaríamos que se extinguieran antes de llegar al suelo, el reloj interno de los muones atrasa con respecto a nuestra expectativa, y esto les permite sobrevivir al largo –no tan largo para ellos– viaje y alcanzarnos.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2018/06/RebkaFoto-228x300.jpg&#34; alt=&#34;Robert Pound, Harvard University.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Pero esto no es todo; ni siquiera lo más sorprendente: Hay, además del que acabamos de explicar, un segundo efecto relativista que también produce la dilatación temporal. Este fue propuesto por Einstein un poco más tarde, en 1911, a medio camino entre su teoría especial de 1905 y su teoría general de 1915. A diferencia del efecto de dilatación que describimos arriba, que tiene que ver con la discrepancia entre los lapsos observados por dos experimentalistas que se mueven a gran velocidad uno respecto del otro, existe un efecto de dilatación más desconcertante, por cuanto no tiene que ver con el movimiento relativo entre observadores sino con el lugar en el que ellos se encuentran.&lt;/p&gt;
&lt;p&gt;Según predijo Einstein en 1911, en las regiones del espacio en las cuales el campo gravitatorio es más intenso, el tiempo transcurre más lento &lt;a href=&#34;#1&#34;&gt;[1]&lt;/a&gt;. Tan cierto cuanto extraño pueda sonar. En particular, en el piso de nuestra habitación, al encontrarse éste más cerca del centro gravitatorio de la tierra, el tiempo transcurre más lento que en el techo. Estrictamente hablando, uno envejece más rápido si duerme en la litera de arriba que en la de abajo. Esto no se debe, aclaremos, a que los relojes o aparatos de medición que empleáremos para medir el paso del tiempo se estropeen. Se trata de una verdadera deformación en el tiempo producida por el campo gravitatorio. En el caso de nuestra habitación, al ser el campo gravitatorio del planeta tierra no muy fuerte y al ser el planeta tanto más grande que el tamaño la habitación misma, el efecto es despreciable y no debemos tenerlo en cuenta a la hora de elegir si dormir arriba o abajo.&lt;/p&gt;
&lt;p&gt;Para hacernos una idea de cuán sutil es el fenómeno a escalas terrestres, digamos que si decidiéramos hacer un experimento y dispusiéramos para ello un reloj en el techo de la habitación y otro en el suelo con la intención de medir cómo el segundo atrasa respecto al primero, deberíamos esperar cientos de millones de años para que tal retraso alcance a ser notable. Ahora bien, este efecto sí es apreciable en las cercanías de astros muy densos con campos gravitatorios fuertes. Por ejemplo, la radiación que nos llega de las cercanías de los agujeros negros presenta un corrimiento del espectro de la luz emitida que es muestra de esa dilatación temporal. En la superficie misma de los agujeros negros el efecto de letargo temporal debido a la intensidad del campo gravitatorio es tan fuerte que, literalmente, el tiempo allí deja de transcurrir.&lt;/p&gt;
&lt;p&gt;Resumiendo, entonces: La relatividad predice dos fenómenos por los cuales el transcurrir del tiempo no es un absoluto sino que pasa a ser relativo, en el sentido de que observadores diferentes discreparían en el lapso acaecido entre dos fenómenos. El primero de estos dos efectos es el de la dilatación temporal debida a la velocidad relativa entre dos observadores (Einstein 1905); el segundo, la dilatación temporal debida a que los dos observadores se encuentran en regiones donde el campo gravitatorio tiene diferentes intensidades (Einstein 1911; cf. Einstein 1915).&lt;/p&gt;
&lt;p&gt;Es el segundo de estos efectos el que se midió en aquella torre del Laboratorio Jefferson. Allí, en 1959, Robert Pound y su alumno Glen Rebka realizaron un ingenioso experimento que ellos mismos habían diseñado más temprano ese mismo año &lt;a href=&#34;#2&#34;&gt;[2]&lt;/a&gt;. Según sus propias palabras, ellos lograron medir el peso aparente de los fotones, dizque el peso de la luz. En aquella torre se encerraron durante diez días y llevaron a cabo sus mediciones con meticuloso empeño. Como ya mencionamos, el efecto de la dilatación temporal debido a la altura es extremadamente pequeño como para ser apreciables a escalas cotidianas. —Si no fuera así, uno debería haber visto a sus vecinos del piso de arriba envejecer más rápido que los del piso de abajo—. Debido a esto, medir tal efecto requería una precisión experimental extrema, una exactitud que es difícil de imaginar con la tecnología rudimentaria de aquellos años.&lt;/p&gt;
&lt;p&gt;Hoy, en épocas en la que los observatorios LIGO y VIRGO miden ondas gravitacionales con una precisión inaudita, en la que los parámetros de la física de partículas se miden en el acelerador LHC en perfecto ajuste con la predicción teórica, en vísperas de la observación de astros distantes con una resolución angular de los pocos microsegundos de arco, es pertinente recordar aquel experimento pionero de Pound y Rebka, en el que con un arreglo experimental casero estos dos físicos lograban medir el peso de la luz.  &lt;/p&gt;
&lt;h3 id=&#34;el-aparente-peso-de-los-fotones&#34;&gt;El aparente peso de los fotones&lt;/h3&gt;
&lt;p&gt;El experimento fue propuesto por Pound y su estudiante en 1959, y ese mismo año lo llevaron a cabo &lt;a href=&#34;#3&#34;&gt;[3]&lt;/a&gt;. Los resultados fueron publicados a comienzo de 1960 en la revista &lt;em&gt;Physical Review Letters&lt;/em&gt; bajo el título de “&lt;em&gt;Apparent weight of photons&lt;/em&gt;”; esto es, “El aparente peso de los fotones” (los fotones son las partículas de las que la luz se compone).&lt;/p&gt;
&lt;p&gt;La primera vez que supe del experimento de Pound y Rebka quedé atónito. Fue en una clase de relatividad general en la Universidad de Buenos Aires, en 1997. El profesor, Rafael Ferraro, nos contaba de los detalles, detalles que están resumidos con su incomparable claridad y concisión en su libro “El espacio-tiempo de Einstein”. ¿Cómo no sorprenderse? Ferraro nos contaba que ya a fines de los años 50s los físicos habían verificado experimentalmente la predicción de Einstein de que la luz pesaba, que la luz caía, gravitaba, que el tiempo se detenía junto a su frecuencia.&lt;/p&gt;
&lt;p&gt;La forma de verificar esto experimentalmente fue la siguiente: Pound y Rebka dispusieron dos fuentes de radiación, una a cada extremo de la torre. Una en aquel altillo (aunque el sitio ha sido refaccionado hace quince años y es difícil vislumbrar hoy su forma original) y otra en el subsuelo del edificio. Esas fuentes de radiación estaban compuestas de un isótopo de hierro, el 57Fe, un átomo que emite o absorbe un rayo gama (i.e. luz de una frecuencia alta muy específica) cuando sus electrones cambian su estado de configuración. La fuente en lo alto de la torre funcionaba como emisor, emitiendo un rayo gama; la fuente en el subsuelo, por el contrario, oficiaba de receptor y venía a absorber el rayo gama emitido por su compañera a más de 22 metros de altura.&lt;/p&gt;
&lt;p&gt;El empleo de 57Fe les permitió a los experimentalistas lograr una precisión fina de las energías emitidas y absorbidas. Habían tratado ya con otros tipos de fuente, de otros materiales, como el isótopo 67Zn, pero los resultados no alcanzaban la precisión requerida.&lt;/p&gt;
&lt;p&gt;El principio físico básico que está detrás del experimento es el siguiente: Por un lado, la energía del rayo gama emitido o absorbido por la fuente es directamente proporcional a la frecuencia de la radiación electromagnética (i.e. al color de la luz). Por otro lado, la frecuencia de la luz (es decir, su color) es la forma en la que la luz expresa el paso del tiempo. La luz se vuelve más azulada cuando el tiempo transcurre más rápido, y se vuelve más rojiza cuando el tiempo transcurre más lento.&lt;/p&gt;
&lt;p&gt;Así, si era cierto que en lo alto de la torre, donde el campo gravitatorio de la tierra es más tenue, el tiempo habría de transcurrir más rápido que en las profundidades del subsuelo, donde el campo gravitatorio de la tierra es más fuerte, entonces la fuente emisora en el altillo emitiría un fotón con una frecuencia que cambiaría ligeramente en su trayecto hasta el sótano. Por lo tanto, abajo, el receptor debería medir un fotón más azulado que aquél que habría dejado la fuente.&lt;/p&gt;
&lt;p&gt;El color de la luz (i.e. la frecuencia) debería cambiar, lo que se entendería, por un lado, como la diferencia en el transcurrir del tiempo en ambos extremos de la torre, y, por el otro, como la ganancia de energía (i.e. de frecuencia) que la luz acumularía durante su caída. El resultado fue contundente: La luz cambió de color. El tiempo transcurría más lento en aquel sótano. Los fotones, aunque siempre viajando a la misma velocidad, igual ganaban energía en su caída, viraban al tono azul, caían. En breve, la luz gravita.&lt;/p&gt;
&lt;h3 id=&#34;el-experimento-de-pound-y-rebka&#34;&gt;El experimento de Pound y Rebka&lt;/h3&gt;
&lt;p&gt;Hubo muchos detalles a tener en cuenta. El experimento pretendía medir un efecto casi inapreciable, de una parte en mil billones; una precisión como aquella de la que aún hoy se enorgullecen los físicos de partículas que la logran en sus aceleradores modernos. El primero de los detalles a tener en cuenta en el arreglo experimental fue el del control de los efectos externos que pudieran disturbar las mediciones. Era imprescindible controlar muy bien el ruido que pudiera provenir de fuentes exógenas. Al tratarse de un experimento que involucraba la emisión y absorción de radiación, era menester blindar el dispositivo.&lt;/p&gt;
&lt;p&gt;También era necesario minimizar las colisiones que los fotones de la radiación gama podrían llegar a sufrir con las moléculas de aire a lo largo de los 22 metros de caída en la torre. Para minimizar los efectos espurios, el experimento fue enfundado por bolsas de un material de aspecto metalizado, similar al que se emplea para empacar algunos componentes electrónicos comerciales.&lt;/p&gt;
&lt;p&gt;Tratándose de un experimento tan sensible, era necesario repetir la emisión y absorción de los fotones muchas veces y durante un tiempo prolongado, a efectos de tener una estadística confiable y minimizar errores de medición. El experimento se llevó a cabo durante diez días en condiciones controladas y con un protocolo preciso. La forma ingeniosa en la que se controló que lo que se estaba observando era, en efecto, la dilatación temporal debido al campo gravitatorio fue la siguiente: Los experimentalistas combinaron ese efecto con el otro efecto de dilatación temporal relativista que mencionamos arriba. Esto es, movieron la fuente a medida que ésta emitía los fotones. Lo hicieron empleando un cilindro hidráulico que les permitía controlar la velocidad con precisión.&lt;/p&gt;
&lt;p&gt;Así, movieron la fuente hacia arriba, alejándola del subsuelo, para que el efecto de dilatación temporal y el efecto Doppler produjeran un corrimiento al rojo que compensara el corrimiento al azul que el fotón acumulaba en su caída. Luego, revirtieron la velocidad, moviendo la fuente hacia abajo, acercándola hacia el emisor, para que el efecto de dilatación temporal y el efecto Doppler ahora se sumaran al del campo gravitatorio y lo reforzaran. La alternancia de ese movimiento permitió evidenciar el efecto más claramente.&lt;/p&gt;
&lt;p&gt;Otro de los problemas más importantes que realizar un experimento de esta naturaleza implicaba era el control de la temperatura de la fuente. La temperatura no es sino la agitación térmica de los átomos que componen un material; en este caso, el movimiento de los átomos que constituían las fuentes. Tal movimiento producía per se un ruido que enmascaraba los efectos que se deseaba medir. Este problema, además, se acentuaba si la temperatura de la fuente emisora y la de la fuente receptora eran diferentes, lo que no era difícil de imaginar tratándose de un subsuelo versus un altillo. Así, era importante no sólo mantener las fuentes lo suficientemente frías sino también asegurarse de que la diferencia de temperatura entre ellas no alcanzara la décima de grado centígrado. Dispusieron, por esto, de una termocupla para controlar la diferencia de temperaturas. No atender a este detalle habría sido fatal para la exactitud de las mediciones. De hecho, intentos por medir este mismo efecto, llevados a cabo por Cranshaw, Schiffer y Whitehead de unos meses antes, habían fracasado por este motivo.&lt;/p&gt;
&lt;p&gt;Por último, algo que lejos de ser un detalle fue crucial para el experimento, llevó a Pound y Rebka a considerar lo que en ese momento era un reciente descubrimiento de la física teórica, el efecto Mössbauer &lt;a href=&#34;#4&#34;&gt;[4]&lt;/a&gt;. Pound era un reconocido físico nuclear y un experto en la interacción entre la materia y la radiación gama. A él se le atribuyen muchos aportes en el área y, al respecto, cabe agregar que su condición de no-Nobel no está libre de controversias. Por lo tanto, él estaba al tanto del efecto descubierto un año atrás, en 1958, por Rudolf Mössbauer; efecto que luego daría origen a la técnica de espectroscopía que lleva el mismo nombre.&lt;/p&gt;
&lt;p&gt;Lo que Mössbauer había mostrado era que el brutal retroceso que un átomo aislado sufre al ser alcanzado por un fotón gama, sería despreciable si tal átomo, en lugar de estar aislado, formara parte de una red cristalina de muchos átomos. El impulso propinado en ese choque se repartiría entre los otros átomos que lo acompañan en la red, de manera tal que el culatazo se tornaría desdeñable. Esto fue muy importante para el experimento de la torre ya que, así, no era necesario preocuparse por la perturbación que tal choque produciría sobre los efectos sutiles que se pretendía medir.&lt;/p&gt;
&lt;p&gt;Todo esto da cuenta del desafío tecnológico que un experimento así implicaba en los años 50s – 60s, un desafío que demandó el ingenio infinito y la atención incorruptible de esos dos grandes físicos. Al cabo de diez días de operación, el experimento cesó. El cociente relativo medido entre la frecuencia de la luz emitida por la fuente y la frecuencia absorbida por el receptor fue de 5,13 10&lt;sup&gt;-15&lt;/sup&gt;, con un error del 10%, en plena concordancia con la predicción de Einstein: 4,9 10&lt;sup&gt;-15&lt;/sup&gt;. Esto no sólo nos da una idea de la increíble precisión del experimento de Pound y Rebka, sino también de la sutil pequeñez del efecto relativista a escalas terrestres.&lt;/p&gt;
&lt;p&gt;Aun así, aunque ligera, la luz llegó a pesar.    &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referencias&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;1&#34;&gt; 1. &lt;/a&gt; A. Einstein, &lt;em&gt;Ann. Physik 35 (1911) 898.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;2&#34;&gt; 2. &lt;/a&gt; R.V. Pound and G.A. Rebka, &lt;em&gt;Phys. Rev. Lett. 3 (1959) 439.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;3&#34;&gt; 3. &lt;/a&gt; R.V. Pound and G.A. Rebka, &lt;em&gt;Phys. Rev. Lett. 4 (1960) 337.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;4&#34;&gt; 4. &lt;/a&gt; R.L. Mossbauer, &lt;em&gt;Z. Physik 151 (1958) 124.&lt;/em&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>¿Cómo hace una bacteria para sobrevivir en condiciones extremas? Nuevos avances acerca de cómo un motor molecular regula la dirección de transporte del cromosoma.</title>
      <link>https://ciencianet.com.ar/post/como-hace-una-bacteria-para-sobrevivir-en-condiciones-extremas-nuevos-avances-acerca-de-como-un-motor-molecular-regula-la-direccion-de-transporte-del-cromosoma/</link>
      <pubDate>Tue, 01 May 2018 23:15:32 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/como-hace-una-bacteria-para-sobrevivir-en-condiciones-extremas-nuevos-avances-acerca-de-como-un-motor-molecular-regula-la-direccion-de-transporte-del-cromosoma/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Augusto Borges&lt;/strong&gt; y &lt;strong&gt;Osvaldo Chara&lt;/strong&gt;. &lt;a href=&#34;https://sysbioiflysib.wordpress.com/&#34;&gt;SysBio&lt;/a&gt;, Instituto de Física de Líquidos y Sistemas Biológicos (CONICET, UNLP), La Plata, Argentina.&lt;/p&gt;
&lt;p&gt;Las bacterias poseen la capacidad de intercambiar &lt;a href=&#34;https://es.wikipedia.org/wiki/%C3%81cido_desoxirribonucleico&#34;&gt;ADN&lt;/a&gt; como un mecanismo de adaptación y supervivencia. Para ello, se pueden valer de motores moleculares capaces de transportar ADN a través de membranas celulares a gran velocidad y con una dirección definida (&lt;em&gt;i.e&lt;/em&gt;. la bacteria que posee el ADN debe asegurarse que la bacteria receptora reciba una copia completa de ADN una vez que el proceso se ha iniciado). &lt;em&gt;Bacillus subtilis&lt;/em&gt; es una bacteria que, si las condiciones ambientales no fueran favorables, posee la asombrosa capacidad de crear una nueva versión de sí misma, capaz de sobrevivir durante decenas de años en un estado ‘durmiente’. Para lograr esto, &lt;em&gt;B. subtilis&lt;/em&gt; lleva adelante un proceso conocido como &lt;a href=&#34;https://es.wikipedia.org/wiki/Esporulaci%C3%B3n&#34;&gt;esporulación&lt;/a&gt; donde una copia completa del &lt;a href=&#34;https://es.wikipedia.org/wiki/Cromosoma&#34;&gt;cromosoma&lt;/a&gt; bacteriano se transporta rápidamente hacia una célula hija. Esta célula hija (conocida como espora) posee un tamaño mucho menor que la célula madre y su contenido y membrana estarán adaptados para conferir resistencia a numerosas perturbaciones externas (tales como cambios de temperatura, deshidratación o radiaciones). Para garantizar que este proceso se lleve a cabo de manera exitosa, la bacteria emplea un motor molecular, la proteína SpoIIIE, capaz de transportar ADN usando la energía provista por la hidrólisis de &lt;a href=&#34;https://es.wikipedia.org/wiki/Adenos%C3%ADn_trifosfato&#34;&gt;ATP&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/cUlbXnchQ_M&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Descripción del video:&lt;/strong&gt; Interacción del motor SpoIIIE con una molécula única de ADN  que contiene secuencias específicas (SRS) en presencia de ATP. Se muestra la dinámica de interacción y translocación de ADN predicha por un modelo matemático que asume que el motor SpoIIIE se une/desune del ADN y es capaz de  deslizarse mediante difusión y translocación a lo largo del ADN. La región coloreada en azul indica la posición de las secuencias específicas. En rojo y verde se observan eventos de translocación y unión-difusión. En blanco se muestran sitios del ADN no ocupados por el motor.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;El mecanismo por el cual este motor logra unirse al cromosoma y desplazarlo en la dirección adecuada aún no había sido completamente elucidado. Pocos días atrás, publicamos un trabajo en &lt;a href=&#34;https://www.nature.com/srep/&#34;&gt;&lt;strong&gt;Scientific Reports&lt;/strong&gt;&lt;/a&gt;, en el que descubrimos lo que creemos es un paso clave en el mecanismo subyacente al accionar del motor molecular SpoIIIE. Este trabajo es el fruto de una colaboración entre nuestro grupo de trabajo, &lt;a href=&#34;https://sysbioiflysib.wordpress.com/&#34;&gt;SysBio&lt;/a&gt;, en el &lt;a href=&#34;http://iflysib.unlp.edu.ar/&#34;&gt;Instituto de Física de Líquidos y Sistemas Biológicos&lt;/a&gt; (CONICET, UNLP) en La Plata y el grupo de Marcelo Nöllmann, en el &lt;a href=&#34;http://marcnol.weebly.com/&#34;&gt;Centre de Biochimie Structurale&lt;/a&gt; (CNRS/INSERM/UM) Montpellier, Francia.&lt;/p&gt;
&lt;p&gt;En un &lt;a href=&#34;http://embor.embopress.org/content/14/5/473&#34;&gt;artículo previo&lt;/a&gt; publicado en &lt;strong&gt;EMBO Reports&lt;/strong&gt;, esta colaboración nos permitió determinar que, en ausencia de ATP, el motor molecular SpoIIIE es capaz de explorar rápidamente largas regiones de ADN por un mecanismo de difusión lineal para encontrar secuencias específicas (llamadas SRS) que informan la dirección de transporte de ADN (&lt;em&gt;i.e.&lt;/em&gt; secuencias que actúan como señales de tráfico e indican en qué dirección avanzar). Una vez que el motor se une a estas secuencias, el mecanismo que estabiliza esta interacción es regulado por la diferencia en la velocidad de disociación con respecto al ADN no específico.&lt;/p&gt;
&lt;p&gt;En el nuevo trabajo publicado en &lt;strong&gt;Scientific Reports&lt;/strong&gt;, investigamos si las secuencias específicas SRS regulan la dirección  de transporte del ADN al reclutar y orientar SpoIIIE o si simplemente catalizan su actividad de translocación. Para ello, empleamos técnicas de microscopía de fuerza atómica y ensayos de translocación de cinética rápida para determinar la localización y la dinámica de la difusión y la translocación de complejos SpoIIIE en el ADN, con o sin secuencias específicas SRS. Luego, combinamos nuestros resultados con modelos matemáticos para encontrar que SpoIIIE determina la dirección de transporte del ADN, a través de la regulación catalítica de su actividad motora. De este modo, SpoIIIE reconoce las secuencias específicas SRS que determinan la dirección de translocación y en ese caso su probabilidad de activación se ve incrementada varios órdenes de magnitud.&lt;/p&gt;
&lt;p&gt;Este simple mecanismo, combinado un mecanismo complementario basado en la exploración activa de ADN por SpoIIIE, sería suficiente para garantizar la direccionalidad de transporte de ADN &lt;em&gt;in vivo&lt;/em&gt; durante la esporulación.&lt;/p&gt;
&lt;p&gt;Entender cómo funcionan este tipo de motores moleculares puede tener repercusiones en distintas áreas de la biología, ya que estos motores pertenecen a una gran familia de proteínas que incluye translocasas y helicasas. Estas proteínas tienen papeles importantes, no solo en el transporte de ADN, sino también en la replicación, recombinación y reparación de ADN, regulación de expresión génica, maduración y transporte de ARN mensajero y muchos otros.&lt;/p&gt;
&lt;hr&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/c6BNcfPHF4Y&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Descripción del video:&lt;/strong&gt; Interacción del motor SpoIIIE con un ADN teniendo regiones específicas (SRS) en ausencia de ATP predichas por el modelo matemático mencionado en el video anterior. En el panel superior y medio, se muestran los motores uniéndose o deslizándose en el ADN en tiempo real y su acumulación, respectivamente. Se utilizan cinco ADNs representativos. El panel inferior muestra la frecuencia de motores unidos a la secuencia específica del ADN a medida que avanza el tiempo.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; &lt;a href=&#34;https://www.nature.com/articles/s41598-018-23400-8&#34;&gt;Sequence-dependent catalytic regulation of the SpoIIIE motor activity ensures directionality of DNA translocation&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Patricia Massolo - La mujer que nunca conocí</title>
      <link>https://ciencianet.com.ar/post/patricia-massolo/</link>
      <pubDate>Sat, 28 Apr 2018 14:59:04 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/patricia-massolo/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero.&lt;/strong&gt; Investigadora de CONICET en el INIFTA (Instituto de Investigaciones Fisicoquímicas Teóricas y Aplicadas). Docente en la UNLP.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;La mujer que nunca conociste&lt;/strong&gt;&lt;/em&gt; es el nombre de una campaña de Wikipedia destinada a visibilizar el trabajo de las mujeres en la cual se invita a crear entradas sobre mujeres destacadas en alguna de las áreas del desarrollo humano. La ciencia es una de las áreas en la que los aportes de las mujeres no han sido suficientemente difundidos o valorados. De modo que, aunque no como entrada de Wikipedia sino como un escrito de tipo más personal, acepto el desafío de escribir sobre una mujer destacada de la Física, a quien -lamentablemente- nunca conocí. Claudia Patricia Massolo -Patricia, como la llamaba su entorno- fue una física argentina que se especializó en el área de Materia Condensada y Física Nuclear. Nacida en Monte Grande, se recibió de Licenciada en Física en la Facultad de Ciencias Exactas de la UNLP, donde se doctoró en 1977. Fue investigadora de CONICET en La Plata y se desempeñó como docente del Departamento de Física, alcanzando el cargo de profesora en 1986. A lo largo de su carrera realizó contribuciones disciplinares pero también se destacó por su compromiso con el medio ambiente y la salud, con los derechos humanos y con la docencia. Patricia Massolo falleció tempranamente, en Junio de 1993, a la edad de 43 años, de un cáncer de estómago.  Fue compañera del físico platense Fidel Schaposnik y los hijos de ambos también se dedican a la ciencia.&lt;/p&gt;
&lt;h3 id=&#34;compromiso-con-el-medio-ambiente&#34;&gt;&lt;strong&gt;Compromiso con el medio ambiente&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2018/04/a10-634x1024.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Patricia  tuvo varias participaciones en la determinación de contaminación radiactiva en el ambiente y alimentos. Respecto de este compromiso que asumía al poner sus saberes en función de aportes concretos a la sociedad, decía &amp;quot;es indispensable que la comunidad sepa que este es uno de los roles fundamentales de la Universidad&amp;quot;. Junto a su colega Judith Desimoni, también física de La Plata, Patricia participó de la identificación de componentes radioactivas en residuos de origen industrial que fueron encontrados en el año 1989 por vecinos  en una cantera cercana al Aeropuerto de la ciudad de La Plata &lt;a href=&#34;#1&#34;&gt;[1]&lt;/a&gt;. Tal vez por este antecedente, en marzo de 1992 la Fundación Greenpeace  de América Latina contactó a Patricia Massolo y le hizo llegar muestras de leche en polvo marca Jorgiano que el Estado había comprado a proveedores afines al gobierno, destinada a ser entregada a niños y embarazadas en planes sociales.&lt;/p&gt;
&lt;p&gt;Esta leche presentaba características macroscópicas que evidenciaban que no estaba en buen estado. Se sospechaba que podría ser mezcla de leches adquiridas por los empresarios a muy bajo precio a países afectados por desechos de la &lt;a href=&#34;https://es.wikipedia.org/wiki/Accidente_de_Chern%C3%B3bil&#34;&gt;explosión de Chernobyl&lt;/a&gt;.  Según relata Guillermo Bibiloni en su artículo &lt;em&gt;De la Espectroscopía Óptica a la Nuclear. Dos Mujeres de la Física Preocupadas por La Salud&lt;/em&gt; &lt;a href=&#34;#2&#34;&gt;[2]&lt;/a&gt;, Massolo preparó muestras de leche marca Jorgiano y marca Cotar (de Santa Fe, que podía tomarse como estándar) y midió tanto la presencia de &lt;a href=&#34;https://es.wikipedia.org/wiki/Is%C3%B3topo&#34;&gt;isótopos&lt;/a&gt; radiactivos de &lt;a href=&#34;https://es.wikipedia.org/wiki/Potasio&#34;&gt;potasio&lt;/a&gt; (compatibles con la radiactividad natural de las pasturas de la zona) como también de isótopos radiactivos de &lt;a href=&#34;https://es.wikipedia.org/wiki/Cesio&#34;&gt;cesio&lt;/a&gt;, que son subproductos de la actividad nuclear.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2018/04/espectros-768x664.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;El detector semiconductor para bajo conteo de germanio hiperpuro que tenía disponible en su laboratorio no era la mejor herramienta para la tarea, pero eso no la detuvo. Con ayuda a una colega de la Comisión Nacional de Energía Atómica, Patricia Massolo obtuvo espectros de radiación gamma de las diferentes muestras de leche y pudo determinar que en la leche Cotar no se detectó la presencia de cesio radioactivo pero en la leche Jorgiano detectó los isótopos 137Cs y 134Cs en proporciones tales que coincidían con la que debía esperarse si se hubieran producido en la Central de Chernobyl y en una fecha coincidente con la explosión. Para poder afirmar esto Massolo utilizó el coeficiente de &lt;a href=&#34;https://es.wikipedia.org/wiki/Abundancia_natural&#34;&gt;abundancia&lt;/a&gt; relativa de los isótopos de Cesio en muestras de leche de marca Frisolac, recogidas en Holanda poco después del accidente de Chernobyl, y le aplicó un factor de corrección que contempla la diferente vida media de los isótopos. Por otro lado, la menor concentración del isótopo 40K en la leche Jorgiano respecto a los valores esperables para Argentina sugería que podría tratarse de una mezcla de leche argentina e importada de países afectados.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2018/04/a16-502x1024.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Las serias implicaciones de las conclusiones que arrojaban estos estudios, realizados entre junio y octubre de 1992, motivaron a que Patricia Massolo pidiera ayuda a sus colegas europeos para una verificación de los resultados. De este modo, Patricia Massolo hizo llegar muestras de leche a Holanda, al Kernfysisch Versneller Instituut de la Universidad de Groninger, donde los hallazgos fueron confirmados poco después.  A raíz de este caso, se presentó en la Cámara de Senadores de la Provincia de Buenos Aires un proyecto para controles de contaminantes y substancias radioactivas que incluía explícitamente al Laboratorio de Física Nuclear del Departamento de Física de la UNLP como ente de control. Además de tener sustancias radiactivas la leche estaba contaminada también con escherichia coli. La situación tomó amplio estado público como uno de los mayores casos de corrupción de Argentina, ya que  los dueños de la empresa proveedora de la leche, &lt;a href=&#34;https://es.wikipedia.org/wiki/Miguel_%C3%81ngel_Vicco&#34;&gt;Miguel Angel Vicco&lt;/a&gt; y &lt;a href=&#34;https://es.wikipedia.org/wiki/Carlos_Spadone&#34;&gt;Carlos Spadone&lt;/a&gt;, fueron secretarios privados de Carlos Menem, el entonces presidente.  Luego de idas y vueltas legales, los empresarios responsables,  Vicco y Carlos Spadone, fueron sobreseídos y la causa fue cerrada en 2005, al considerarla proscripta por cuestiones técnicas. Poco antes de su fallecimiento fue nuevamente consultada por Greenpeace cuando en el gobierno de Menem se reflotó un proyecto para la instalación de un repositorio nuclear o &lt;strong&gt;&lt;em&gt;&amp;quot;basurero nuclear&amp;quot;&lt;/em&gt;&lt;/strong&gt; en Gastre (Chubut) que recibiría desechos de combustibles agotados provenientes de las plantas argentinas y de otros países.&lt;/p&gt;
&lt;h3 id=&#34;docencia&#34;&gt;&lt;strong&gt;Docencia&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Patricia Massolo desempeñó sus tareas como docente en el Departamento de Física de la Facultad de Ciencias Exactas de la UNLP. Durante los años en que ejerció la docencia ocurrió la transición entre el viejo plan de estudios  y el nuevo diseño del año 1989, de modo que se crearon nuevas asignaturas, como Experimentos Cuánticos I y II y se modificaron algunas metodologías de trabajo. En ese contexto Patricia fue partícipe de varias experiencias innovadoras, en algunas junto a su compañero. La publicación de los pequeños trabajos de investigación que hacían los jóvenes estudiantes de la Licenciatura en Física para graduarse (Tesinas o Trabajos de Diploma) fue un aspecto en el que también fue pionera en el Departamento. También tuvo a su cargo, junto a Guillermo Bibiloni, la formación de tesistas de doctorado: Mario Rentería, Sergio Moreno, Félix Requejo y Jorge Shitu. Patricia animaba fuertemente a sus tesistas a viajar a otros países como parte de su formación como científicos, aún en temas que no fueran estrictamente los de su plan de trabajo.&lt;/p&gt;
&lt;h3 id=&#34;derechos-humanos&#34;&gt;&lt;strong&gt;Derechos humanos&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Otro de los aspectos en que Patricia Massolo tuvo un rol activo es el de los Derechos Humanos. Patricia presentó junto a su compañero en 1986 una solicitud a la Universidad Nacional de La Plata, avalada por más de 100 docentes, estudiantes e investigadores del Departamento de Física de la Facultad de Ciencias Exactas, para realizar un homenaje a las víctimas que había generado en la comunidad académica la dictadura cívico-militar de 1976 &lt;a href=&#34;#3&#34;&gt;[3]&lt;/a&gt;. Esta iniciativa, que se considera pionera en la UNLP, se concretó con la colocación de una placa en el acceso al edificio del Departamento de Física donde se lee:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Son varios los miembros de este Departamento que han sufrido persecución, tortura y muerte. Jorge Bonafini, Federico Ludden y María de los Ángeles Valeriani aún permanecen desaparecidos. Esta placa representa nuestro reclamo permanente a su aparición con vida y el compromiso indestructible de exigir justicia e impedir que se repita lo sucedido&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Me animo a afirmar que Patricia se sentiría orgullosa al saber que su discípulo, el físico platense Mario Rentería, es el director &lt;em&gt;ad honorem&lt;/em&gt; de  DD. HH. de la Facultad de Ciencias Exactas, y gracias a su iniciativa uno de los edificios de la institución recibió el nombre de &lt;em&gt;Edificio Abuelas de Plaza de Mayo&lt;/em&gt;. Su temprano fallecimiento dejó una profunda ausencia que se percibe entre quienes trabajaron con ella. Sus colegas escribieron una sentida dedicatoria para homenajearla, en la cual la describen como una persona cálida, inteligente, entusiasta y plena de alegría &lt;a href=&#34;#4&#34;&gt;[4]&lt;/a&gt;. En el año 2003 se realizó una muestra para recordar a Patricia en el &lt;a href=&#34;http://museo.fisica.unlp.edu.ar/&#34;&gt;Museo del Departamento de Física&lt;/a&gt; de la Facultad de Ciencias Exactas de la UNLP, con participación de la comunidad académica. Para la ocasión, el periodista Horacio Verbitsky escribió una síntesis sobre el caso de la leche contaminada y el rol que tuvo Patricia en él:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;quot;Me impresiona advertir cuántos años han pasado. Recordé los hechos en septiembre de 2002, cuando un juez que jugaba al tenis con Menem cerró la causa y absolvió a los últimos procesados en la causa que se inició con las notas de Página/12. Por si alguien aún no lo supiera, el juez Ballestero recordó en su sentencia que ninguno de los procesados había pasado un solo día privado de su libertad, ni el secretario presidencial Miguel Angel Vicco, ni los hermanos Carlos y Lorenzo Spadone, ni los menos conocidos responsables de la venta al Estado para los planes de asistencia a los pobres más pobres de leche contaminada con bacterias y sustancias radiactivas, tal vez provenientes de Chernobyl. El inicuo fallo consigna que la causa no puso avanzar porque el ministerio de Salud, que los mismos delincuentes controlaban, celebró un acuerdo con las empresas proveedoras y desistió de la acción. Ahora que nuestro maltratado país intenta revalorizar lo público, recordamos la actitud de Claudia Patricia Massolo. Ella expresa a quienes en aquellos años sórdidos no se apartaron de los principios éticos más elementales y pusieron su saber al servicio de la sociedad. Ya que no cárcel, vergüenza eterna para quienes son su contracara y usaron sus cargos en beneficio personal con desprecio absoluto por la suerte de los demás, incluyendo la salud y la vida de los más indefensos. Horacio Verbitsky&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Volviendo a la época en que supe de la existencia de Patricia, recuerdo que su compañero Fidel Schaposnik solía buscar la motivación de los alumnos en las clases de cuántica diciéndonos, por ejemplo, que &lt;em&gt;&amp;quot;Dirac, a la edad que tienen ustedes, ya había hecho todo esto…&amp;quot;&lt;/em&gt; Su intención creo que era buena, pero a algunos Dirac no nos entusiasmaba particularmente. Algunos queríamos -algún día- parecernos un poquito a Patricia.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Agradezco a Mario Rentería y a Jorge Shitu por las charlas y el material que generosamente compartieron conmigo. Un especial agradecimiento para Fidel Schaposnik por su tiempo, sus relatos, y sobre todo, por la recomendación que me dio -previa  a la lectura del texto- de hacer públicas estas palabras sobre Patricia aún si él no estuviera de acuerdo... alentándome a hacer justamente lo que hacía ella.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;referencias&#34;&gt;Referencias&lt;/h4&gt;
&lt;p&gt;&lt;a name=&#34;1&#34;&gt;&lt;/a&gt;1. C.P. Massolo y J. Desirnoni, &amp;quot;Estudio sobre componentes radioactivas presentes en residuos encontrados en la cantera situada en 6bis y 622, Zona Aeropuerto, La Plata&amp;quot;, Informe Final, 1989.&lt;/p&gt;
&lt;p&gt;&lt;a name=&#34;2&#34;&gt;&lt;/a&gt;2. A.G. BIBILONI. &lt;em&gt;De la Espectroscopía Óptica a la Nuclear. Dos Mujeres de la Física Preocupadas por La Salud.&lt;/em&gt; Acta Farm. Bonaerense 18 (3): 231-5 (1999).&lt;/p&gt;
&lt;p&gt;&lt;a name=&#34;3&#34;&gt;&lt;/a&gt;3. Materia Pendiente Nº 25. Revista de la Facultad de Ciencias Exactas de la UNLP. &lt;a href=&#34;http://www.exactas.unlp.edu.ar/uploads/docs/materia&#34;&gt;http://www.exactas.unlp.edu.ar/uploads/docs/materia&lt;/a&gt;_pendiente__25.pdf.&lt;/p&gt;
&lt;p&gt;&lt;a name=&#34;4&#34;&gt;&lt;/a&gt;4. Gary Crawley and Sydney Gales. Nuclear Physics A Volume 569, Issues 1–2, 7 March 1994, Page vii  &lt;a href=&#34;https://doi.org/10.1016/0375-9474(94)90089-2&#34;&gt;https://doi.org/10.1016/0375-9474(94)90089-2&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;comentarios&#34;&gt;Comentarios&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;renteria@fisica.unlp.edu.ar&#34;&gt;Mario Rentería&lt;/a&gt; - &lt;time datetime=&#34;2018-04-30 05:23:26&#34;&gt;Apr 1, 2018&lt;/time&gt;&lt;/p&gt;
&lt;p&gt;Bueno, siendo las primeras horas del 30 de Abril de 2018, a 41 años de la primera ronda de las Madres de Plaza de Mayo, me alegra constatar que las semillas que se siembran broten en el momento menos esperado, y más oportuno, en estos días nuevamente aciagos en nuestro país. Leo esto con gran orgullo, con la vista más que nublada por lágrimas, que ya no son de tristeza, pero tuvieron que pasar muchos años... Aprovecho este espacio para hacer algunos comentarios. Me sorprendió ver el texto de la primera placa que pusimos en 1986 en homenaje a los compañeros detenidos-desaparecidos del Departamento de Física durante la última dictadura cívico-militar-clerical (sólo se decía &amp;quot;militar&amp;quot; en esos años) . Homenaje que en realidad comprendía, y sigue siendo, el reclamo de aparición con vida al Estado y la reivindicación de los ideales por los que lucharon los 30.000, que nos permitieron recobrar la democracia, como se puede leer en parte del expediente que le dio origen. Tardé en darme cuenta que ese texto provenía de la nota de la referencia 3, en la cual es errónea la mención a María &amp;quot;de los Angeles&amp;quot; Valeriani. Solo el apellido de María de los Milagros Baleriani estaba mal escrito en esa primera placa de vidrio, debido a dificultades en el proceso de reconstrucción de Memoria. Horacio Verbitsky fue el único periodista que respondió rápida y positivamente a una serie de mails que envié en 2003 a las plumas más conocidas de los periódicos en esos años. Sus palabras, que se leían en una gigantografia en papel que colgaba desde los balcones del Museo de Física, aún hoy resuenan muy actuales... Los dos colegas que describen a Patricia en la referencia 4 son dos enormes físicos nucleres experimentales, el primero de la Michigan State University (USA) y el segundo del Institut de Physique Nucleaire d´ Orsay (Francia), además de relevante gestor de la ciencia francesa. Finalmente, me quedo con las palabras de Fidel, que nos alienta a hacer lo que justamente hacía Patricia, dar testimonio de lo que creemos verdadero aún cuando no todos puedan estar de acuerdo. Gracias Paula por este trabajo de memoria y por un nuevo, y para mí siempre insuficiente, homenaje a Patricia.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;admedus@gmail.com&#34;&gt;Andrés&lt;/a&gt; - &lt;time datetime=&#34;2018-05-02 10:59:40&#34;&gt;May 3, 2018&lt;/time&gt;&lt;/p&gt;
&lt;p&gt;Brillante! Muy lindo texto. Gracias Paula!&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Materia oscura, ondas gravitacionales y agujeros negros primordiales</title>
      <link>https://ciencianet.com.ar/post/materia-oscura-ondas-gravitacionales-y-agujeros-negros-primordiales/</link>
      <pubDate>Thu, 19 Apr 2018 15:31:31 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/materia-oscura-ondas-gravitacionales-y-agujeros-negros-primordiales/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Gastón Giribet.&lt;/strong&gt; Investigador del Center for Cosmology and Particle Physics de New York University. Profesor de la UBA e Investigador Principal del CONICET.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Trabajos recientes sugieren que una parte importante (se estima que hasta un 1%) de la materia oscura del universo podría estar formada por agujeros negros primordiales; es decir, por agujeros negros que no se formaron a partir de la muerte de estrellas sino que existen desde los orígenes del universo. Esta es una idea que tomó fuerza en los últimos dos años y que intentamos resumir aquí.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Dos años atrás, luego de que la colaboración LIGO &lt;a href=&#34;#1&#34; title=&#34;1&#34;&gt;[1]&lt;/a&gt; detectara ondas gravitacionales producidas por lo que, según entendemos, es la coalescencia de dos agujeros negros de aproximadamente 30 masas solares, una vieja pero renovada idea comenzó a ganar terreno: Los agujeros negros con masas dentro del rango de 20 a 100 masas solares podrían explicar una porción -acaso substancial- de la materia oscura del universo.&lt;/p&gt;
&lt;p&gt;La materia oscura, sabemos, es la materia responsable de mantener las galaxias unidas, impidiendo que éstas se disgreguen debido a la fuerza centrífuga. En efecto, la velocidad de rotación observada en las galaxias es mucho mayor que la que uno podría esperar si ellas sólo contuviesen la materia que en ellas vemos brillar. A tales velocidades de rotación la materia que “vemos” no sería suficiente como para contrarrestar la fuerza centrífuga y las galaxias terminarían por desmembrarse y esparcirse en el medio intergaláctico.&lt;/p&gt;
&lt;p&gt;Se infiere de esto –y de otras observaciones de diferente naturaleza– que hay en el universo mucha más materia que la que nosotros vemos. La gravedad que genera esa “materia oscura” termina por delatarla. La materia que “vemos” es la poca que tiene la suerte de interactuar con la luz que nos permite verla. La gran mayoría de la materia es oscura o, mejor dicho, perfectamente transparente; es sólo a través de su influencia gravitatoria que sabemos de ella.&lt;/p&gt;
&lt;p&gt;Se impone, así, la pregunta: ¿Qué es la materia oscura? ¿Qué la compone? ¿Está formada acaso de partículas que, como los neutrinos, no interactúan con la luz y sólo lo hacen débilmente con la materia nuclear? ¿Está formada la materia oscura de partículas que ni siquiera con la fuerza nuclear interactúan dándole la exclusividad a la fuerza gravitatoria? ¿Es la materia oscura simplemente un gas de objetos estelares oscuros, compactos, opacos?, ¿o se trata de un montón de partículas frías débilmente interactuantes que inundan el espacio y nos atraviesan imperceptibles por aquello de que percibir es también ver? En definitiva, ¿qué tipo de materia es la materia oscura?&lt;/p&gt;
&lt;p&gt;Una vieja idea que, aunque se creyó descartada hace tiempo, hoy reaparece con renovado ímpetu propone que parte de la materia oscura se compone de objetos astronómicos masivos y compactos; más precisamente, de agujeros negros con masas que rondan la decena de veces la masa de nuestro sol. No toda la materia oscura puede deberse a ellos, pero sí es posible que alguna porción de ella sea simplemente eso, agujeros negros.&lt;/p&gt;
&lt;p&gt;Dos artículos sobre esta idea que llamaron mi atención en su momento, allá por marzo de 2016. Éstos son &lt;a href=&#34;#2&#34; title=&#34;2&#34;&gt;[2]&lt;/a&gt; y &lt;a href=&#34;#3&#34; title=&#34;3&#34;&gt;[3]&lt;/a&gt;, ambos publicados en &lt;em&gt;Physical Review Letters&lt;/em&gt;. En esos trabajos se especula que la abundancia de sistemas binarios de agujeros negros podría dar cuenta de parte de la materia oscura que sabemos que existe en el universo. Un sistema binario –en este caso de agujeros negros– es un par de astros que rota uno en torno al otro para entrar, al cabo de tiempos prolongados, en un movimiento espiral que culmina en la coalescencia de ambos y la consecuente producción de ondas gravitacionales.&lt;/p&gt;
&lt;p&gt;Es eso lo que LIGO observa, esas ondas. Esto permite estimar la tasa de ocurrencia de esos eventos de coalescencia: del orden de las decenas de eventos por cada Giga-Pársec cubo por cada año. Teniendo en cuenta esta tasa, los trabajos de marzo de 2016 afirman que los sistemas binarios de agujeros negros con masas de alrededor de algunas docenas de masas solares podrían ser, en efecto, la huella de la materia oscura &lt;a href=&#34;#2&#34; title=&#34;2&#34;&gt;[2]&lt;/a&gt;. Estos agujeros negros tienen que ser, por supuesto, agujeros negros primordiales; es decir, agujeros negros que no se formaron por el colapso gravitacional tras la muerte de estrellas sino que nacieron mucho antes de que ninguna estrella haya existido, durante las primeras fracciones de segundo del universo, una etapa evolutiva del cosmos en la que la física de partículas fundamentales era la que dominaba la escena.&lt;/p&gt;
&lt;p&gt;Este origen “primordial” de los agujeros negros que formarían parte de la materia oscura es necesario debido a que sabemos que la materia oscura ha estado en el universo desde sus comienzos; por lo tanto, si no queremos entrar en conflicto con lo que sabemos sobre la cosmología del universo temprano, es mejor que estos objetos hayan estado allí desde el inicio, i.e. que sean primordiales.  &lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2018/04/agujeros.png&#34; alt=&#34;Los sistemas binarios de agujeros negros de origen primordial, con masas de alrededor de algunas docenas de masas solares, podrían ser responsables de una parte de la materia oscura.&#34;&gt;&lt;/p&gt;
&lt;p&gt;La idea de que la materia oscura podría estar compuesta de astros masivos no es nueva sino que data de varias décadas. Incluso durante mucho tiempo se la creyó una idea anticuada, ya descartada. Se pensaba hasta no hace más de uno o dos años que la cantidad de agujeros negros primordiales en el universo no podía ser apreciable debido a que, si lo fuera, no podríamos explicar los datos cosmológicos que observamos.&lt;/p&gt;
&lt;p&gt;Las restricciones observacionales sobre la abundancia de agujeros negros primordiales vienen principalmente de dos fenómenos: los efectos distorsivos de lentes gravitacionales y el efecto que este tipo de astros habría tenido sobre la radiación cósmica de fondo. No haber observado esos efectos hizo pensar a los especialistas por más dos décadas que, de existir, los agujeros negros primordiales debían ser muy pocos como para que su presencia en el cosmos fuera relevante.&lt;/p&gt;
&lt;p&gt;La historia cambió en 2016-2017, cuando nuevos estudios advirtieron que se habían sobreestimado las cotas para la abundancia de agujeros negros primordiales. Quedaba una posibilidad: Un análisis detallado muestra que existe una ventana de valores de masa, que va de entre 20 a 100 masas solares, para los cuales las cotas no son tan estrictas. Esto es, las observaciones cosmológicas no descartan una abundancia notable de agujeros negros primordiales siempre y cuando la masa de éstos sea de unas pocas decenas de masas solares.&lt;/p&gt;
&lt;p&gt;Lo interesante es que, justamente, ¡es ese el rango de masas detectado por LIGO! ¿Podemos decir, entonces, que lo que LIGO detectó no fueron sólo ondas gravitacionales sino también materia oscura? Fue esa la posibilidad que se sugería en &lt;a href=&#34;#2&#34; title=&#34;2&#34;&gt;[2]&lt;/a&gt;.  Esto reabrió la puerta a la idea de que la materia oscura o, mejor dicho, un porcentaje de ella, podría estar formada por agujeros negros, agujeros negros primordiales que forman sistemas binarios de algunas decenas de masas solares, sistemas binarios que hoy están en coalescencia y cuyo estallido final estamos comenzando a oír. Los autores de &lt;a href=&#34;#3&#34; title=&#34;3&#34;&gt;[3]&lt;/a&gt; hacen declaraciones concisas al respecto: Primero, el primer evento de ondas gravitatorias detectado por LIGO, denominado GW150914, se puede explicar por la coalescencia de los agujeros negros primordiales. Segundo, la tasa de fusión prevista para los agujeros negros primordiales estaría en concordancia con la tasa estimada por las colaboraciones de LIGO y Virgo si se asume que tales objetos constituyen cierta fracción de materia oscura. Una revisión más reciente del análisis, incorporando cálculos más detallados y teniendo en cuenta más efectos &lt;a href=&#34;#4&#34; title=&#34;4&#34;&gt;[4]&lt;/a&gt;, concluye que, según lo que entendemos hoy, hasta el 1% de la materia oscura podría estar constituida de agujeros negros. (Uno de los autores del trabajo &lt;a href=&#34;#4&#34; title=&#34;4&#34;&gt;[4]&lt;/a&gt; confirma que esta estimación es aceptada actualmente). Cabe decir también que esta idea, aunque compartida por varios expertos, ha sido recibida con escepticismo por otra parte de la comunidad especializada.&lt;/p&gt;
&lt;p&gt;Fuera como fuere, es una idea ciertamente interesante: ¿Está la materia oscura constituida de un conjunto de astros con unos pocos cientos de kilómetros de diámetro cada uno?  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referencias:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;1&#34;&gt; 1. &lt;/a&gt; LIGO and The Virgo Scientific collaborations, Phys. Rev. Lett. 116, 061102 (2016), &lt;a href=&#34;https://arxiv.org/abs/1602.03837&#34;&gt;arxiv:1602.03837&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;2&#34;&gt; 2. &lt;/a&gt;  S. Bird et al. Phys. Rev. Lett. 116, 201301 (2016), &lt;a href=&#34;https://arxiv.org/pdf/1603.00464.pdf&#34;&gt;arxiv:1603.00464&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;3&#34;&gt; 3. &lt;/a&gt;  M. Sasaki et al., Phys. Rev. Lett. 117, 061101 (2016), &lt;a href=&#34;https://arxiv.org/pdf/1603.08338.pdf&#34;&gt;arxiv:1603.08338&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;4&#34;&gt; 4. &lt;/a&gt; Y. Ali-Haïmoud et al., Phys. Rev. D 96, 123523 (2017), &lt;a href=&#34;https://arxiv.org/pdf/1709.06576.pdf&#34;&gt;arxiv:1709.06576&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Diagnóstico de estados psicóticos y teorías del sueño usando análisis automático del habla</title>
      <link>https://ciencianet.com.ar/post/diagnostico-de-estados-psicoticos-y-teorias-del-sueno-usando-analisis-automatico-del-habla/</link>
      <pubDate>Sun, 15 Apr 2018 00:32:21 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/diagnostico-de-estados-psicoticos-y-teorias-del-sueno-usando-analisis-automatico-del-habla/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Bruno Bianchi&lt;/strong&gt;. Universidad de Buenos Aires.&lt;/p&gt;
&lt;p&gt;La complejidad del lenguaje nos permite lograr expresar nuestros pensamientos y sentimientos de forma muy precisa. Sin embargo, esa misma complejidad podría dificultar su comprensión. Aquellos que hablamos una segunda lengua de forma no nativa (o sea, que aprendimos de grandes y no convivimos con esa lengua) lo sabemos muy bien. En cada conversación, en cada texto, se nos pasa por alto información que el emisor nos quiso dar. Inflexiones de voz, uso de determinadas palabras (y no de otras) para remarcar alguna característica particular (por ejemplo, no sería lo mismo decir “me caí” que “me desplomé”). En el &lt;a href=&#34;http://liaa.dc.uba.ar&#34;&gt;Laboratorio de Inteligencia Artificial Aplicada&lt;/a&gt; (LIAA) de la Facultad de Ciencias Exactas y Naturales de la UBA se trabaja con algoritmos que intentan, desde diferentes ángulos, manejar esta complejidad para comprender de forma profunda el contenido de un texto o un diálogo, ya sea a la hora de analizar la prevalencia de enfermedades psiquiátricas o analizar el contenido de los sueños de las personas, entre otras aplicaciones. En este artículo voy a hacer un rápido repaso de dos investigaciones llevadas a cabo en este laboratorio con algoritmos de Procesamiento del Lenguaje Natural (NLP, por sus siglas en inglés).&lt;/p&gt;
&lt;h3 id=&#34;diagnosis-temprana-de-enfermedades-psiquiátricas&#34;&gt;Diagnosis temprana de enfermedades psiquiátricas&lt;/h3&gt;
&lt;p&gt;Los estados psicóticos generan cambios en el lenguaje del paciente. En los estados psicóticos por esquizofrenia, en particular, aparece una disminución en la coherencia del discurso. Los psiquiatras usan esta característica como método de diagnóstico. Entrevistan a un paciente, le hacen preguntas, escuchan sus respuestas y deciden de alguna manera si las oraciones que arma tienen coherencia. Claro que no es la única variable que importa, pero es una más a la hora de diagnosticar. El principal problema es que medir esa coherencia no es sencillo para un psiquiatra. Mucho menos hacerlo de forma objetiva. Y muchísimo menos si esa falta de coherencia aparece gradualmente de modo de detectarla con antelación.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2018/04/fig3.webp&#34; alt=&#34;Cada punto del gráfico representa a un paciente según la medición de tres propiedades de su habla. Los puntos rojos corresponden a pacientes que más tarde fueron diagnosticados con psicosis. Puede observarse que la mayoría se encuentran en una región separada del gráfico de los pacientes sin diagnóstico. Reproducido de NPJ Schizophrenia bajo licencia CC by 4.0.&#34; title=&#34;http://dx.doi.org/10.1038/npjschz.2015.30&#34;&gt;&lt;/p&gt;
&lt;p&gt;Los investigadores del LIAA (en conjunto con otros investigadores distribuidos por el mundo) implementaron un algoritmo que toma las entrevistas de pacientes con antecedentes familiares de esquizofrenia (grupo de riesgo) y realiza una medición de la coherencia entre las oraciones. Este estudio cobró gran relevancia, ya que tuvo un gran éxito en detectar tempranamente a aquellos pacientes que finalmente fueron diagnosticados con esquizofrenia por los psiquiatras.&lt;/p&gt;
&lt;p&gt;El funcionamiento del algoritmo se basa en la técnica de Análisis Semántico Latente (LSA, por sus siglas en inglés), desarrollada inicialmente en los años 80. Esta se basa en asignar a cada palabra de un corpus de textos (por ejemplo, todos los artículos de wikipedia) a una posición en el espacio, de tal forma que aquellas que suelen aparecer en los mismos textos (y por lo tanto, semánticamente relacionadas) quedarán en posiciones cercanas en nuestro espacio de palabras. A partir de la generación de este espacio, la medición de la coherencia entre oraciones pasa a ser un simple cálculo geométrico de distancia entre las palabras de cada oración.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Artículo original:&lt;/strong&gt; Bedi, G., Carrillo, F., Cecchi, G. A., Slezak, D. F., Sigman, M., Mota, N. B., ... &amp;amp; Corcoran, C. M. &lt;a href=&#34;http://dx.doi.org/10.1038/npjschz.2015.30&#34;&gt;Automated analysis of free speech predicts psychosis onset in high-risk youths&lt;/a&gt;. NPJ Schizophrenia 1, 15030 &lt;strong&gt;(2015)&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;análisis-del-contenido-de-los-sueños&#34;&gt;Análisis del contenido de los sueños&lt;/h3&gt;
&lt;p&gt;El contenido de los sueños y su significado siempre fue una incógnita para la humanidad. Actualmente, una de las teorías que se propone para entender su rol evolutivo es la denominada “Teoría de Simulación de Peligro” (TST, por sus siglas en inglés), la cual propone que durante los sueños nuestro cerebro simula posibles situaciones de riesgo, a partir de las vivencias diarias. De esta forma, además de simular el riesgo, es capaz de simular posibles respuestas, las cuales podrían servir en caso de que esos riesgos fueran reales.&lt;/p&gt;
&lt;p&gt;Con el fin de validar las predicciones de esta teoría, investigadores del LIAA, nuevamente en colaboración con investigadores externos, se propusieron analizar computacionalmente los sueños de personas con algoritmos de NLP. Mientras que el análisis de los sueños se realiza comúnmente mediante el conteo de palabras asociadas a ciertos contextos (por ejemplo, en caso de querer analizar sueños relacionados a persecuciones se cuenta la aparición de verbos de la familia de “correr”), estos investigadores se propusieron analizar el contenido de los mismos de forma global, evitando así que la polisemia de los verbos pudiera interferir con la medición (verbos de la familia de “correr” se asocian con actividades deportivas, no solamente con persecuciones).&lt;/p&gt;
&lt;p&gt;Esta forma de análisis, complementaria al clásico conteo de palabras asociadas al contexto de estudio, permitiría una mejor comprensión de los sueños, abriendo una puerta hacia la comprensión de este proceso. Así, mediante el uso del ya mencionado LSA y de un algoritmo más complejo (pero no necesariamente más certero) denominado &amp;quot;word2vec&amp;quot;, estos investigadores lograron encontrar que en los relatos de sueños la palabra “correr” se asocia más a situaciones de riesgo que en relatos de la vida cotidiana, tal como lo propone la TST. Estas dos investigaciones son sólo dos ejemplos (de muchos) de cómo los algoritmos de &lt;a href=&#34;https://es.wikipedia.org/wiki/Aprendizaje_autom%C3%A1tico&#34;&gt;Machine Learning&lt;/a&gt; e &lt;a href=&#34;https://es.wikipedia.org/wiki/Inteligencia_artificial&#34;&gt;Inteligencia Artificial&lt;/a&gt; nos pueden ayudar a comprender procesos cognitivos, analizando grandes cantidades de texto de una forma compleja y holística.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Artículo original:&lt;/strong&gt; Altszyler, E., Ribeiro, S., Sigman, M., &amp;amp; Slezak, D. F. &lt;a href=&#34;http://dx.doi.org/10.1016/j.concog.2017.09.004&#34;&gt;The interpretation of dream meaning: Resolving ambiguity using Latent Semantic Analysis in a small corpus of text&lt;/a&gt;. Consciousness and Cognition 56, 178 (2017).&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>¿De qué signo sos? Viaje en el tiempo por algunas concepciones del mundo</title>
      <link>https://ciencianet.com.ar/post/de-que-signo-sos-viaje-en-el-tiempo-por-algunas-concepciones-del-mundo/</link>
      <pubDate>Fri, 06 Apr 2018 15:14:29 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/de-que-signo-sos-viaje-en-el-tiempo-por-algunas-concepciones-del-mundo/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero.&lt;/strong&gt; Investigadora de CONICET en el INIFTA (Instituto de Investigaciones Fisicoquímicas Teóricas y Aplicadas). Docente en la UNLP.&lt;/p&gt;
&lt;p&gt;En este artículo reseñamos el trabajo “Un recorrido histórico por algunas concepciones del mundo”. de Diego Petrucci, en el cual se efectúa un recorrido histórico que tiene como ejes las ideas que, desde la antigüedad, han influido sobre nuestras concepciones del mundo. El trabajo tiene el objetivo de fomentar la reflexión de los docentes de ciencia sobre la forma en que concebimos el mundo, ya que las diversas representaciones han definido tradiciones que impregnan actualmente nuestra cultura y que atraviesan las visiones científicas de cualquier disciplina.&lt;/p&gt;
&lt;h3 id=&#34;el-lógico-el-mago-y-el-ingeniero&#34;&gt;&lt;strong&gt;El lógico, el mago y el ingeniero&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;El artículo nos ofrece un repaso de las diversas concepciones de mundo reinantes en la antigüedad. Por ejemplo, para los egipcios la Tierra se concebía como una ostra y el cielo como una mujer o una vaca, y las fases de la Luna se originaban en la mordida de una cerda. Pero más allá de la descripción de estas representaciones, el recorrido del trabajo se extiende en la discusión de las tres cosmovisiones que resumen las ideas presentes en el Renacimiento: el Organicismo, el Neoplatonismo y el Mecanicismo, cuyas características se sintetizan en la Tabla 1.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2018/04/tabla1_petrucci-1-768x725.jpg&#34; alt=&#34;Tabla 1. Las tres cosmovisiones presentes en el Renacimiento: el Organicismo, el Neoplatonismo y el Mecanicismo.&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;cuatro-elementos&#34;&gt;&lt;strong&gt;Cuatro elementos&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Lo particular del enfoque de este trabajo es que el impacto de estas tradiciones y miradas de mundo está centralizado en la enseñanza de las ciencias naturales. ¿Y por qué este interés? Porque la concepción del mundo que tenga un ciudadano resulta determinante del rol que jugarán las ciencias, por ejemplo, en la toma de decisiones o en los intentos de comprensión de su entorno. El autor justifica en el artículo el interés en las concepciones de mundo a través de cuatro ítems. El primero está relacionado con el pasado: para conocer el modo en que filósofos naturales y científicos construían el conocimiento es necesario comprender las ideas que los regían. El segundo punto tiene que ver con el presente, ya que las tradiciones originadas en cada una de las concepciones sobreviven hasta nuestros tiempos y se filtran en la cultura contemporánea. En tercer lugar, el autor menciona que la visión personal de mundo que predomina tiene un impacto, más o menos explícitamente, en nuestro modo de vivir. Y como si todo esto no fuera suficientemente relevante, el último argumento sostiene que la visión que tengamos sobre del mundo nos habla de cómo podemos conocerlo, y viceversa.&lt;/p&gt;
&lt;p&gt;Por ejemplo, los métodos y actitudes que usaremos para investigar un dado fenómeno no serán iguales si entendemos el cosmos como un lugar mágico o si lo visualizamos como un complejísimo mecanismo de relojería. Así, no podemos siquiera mirar el cielo nocturno sin asumir -implícita o explícitamente- una postura epistemológica respecto del mundo (como ocurre en el ejemplo de la Figura 1). Y mucho menos, dice Petrucci, aprender ciencia:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“El conocimiento científico enseñado tiene que estar anclado en las observaciones que permitieron su construcción y enmarcado en un contexto social e históricamente situado”.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2018/04/epiciclo_petrucci.jpg&#34; alt=&#34;Figura 1: Los deferentes (esferas concéntricas) y los epiciclos (esferas menores introducidos en el s. II en Alejandría, como un intento de explicar las variaciones en el brillo y el tamaño aparente de los planetas sin abandonar la cosmología aristotélica.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;En el artículo se relatan los devenires de la evolución del mecanicismo a lo largo de los siglos y cómo fue que la ontología plasmó, fundamentalmente, con la mecánica relativista y la cuántica. En esta concepción de mundo, todos los fenómenos se explican a partir de alguna teoría mecánica (o de un conjunto de ellas) y a partir del orden causal, con leyes deterministas o probabilistas, reversibles o irreversibles, pero sin contemplar en la descripción de los fenómenos ninguna meta, propósito o finalidad.&lt;/p&gt;
&lt;h3 id=&#34;organicista-soy&#34;&gt;Organicista soy&lt;/h3&gt;
&lt;p&gt;Aunque el mecanicismo finalmente se impuso como la cosmovisión &amp;quot;oficial&amp;quot; de la ciencia, en el marco de la cual son financiadas las investigaciones científicas, puede identificarse en la cultura actual la convivencia pacífica de aspectos de cada una de las tres tradiciones. Encontramos la mirada organicista al hablar, por ejemplo, sobre una organización: “&lt;a href=&#34;https://www.infobae.com/2007/11/29/351728-aguilar-es-la-cabeza-la-organizacion-mafiosa/&#34;&gt;Aguilar es la cabeza de una organización mafiosa. Adrián es el brazo armado de Aguilar&lt;/a&gt;” Otros ejemplos del organicismo son las explicaciones teleológicas, es decir, las que suponen un propósito o fin subyacente al cual &amp;quot;la cosa&amp;quot; sirve y que justifica su existencia. Estas explicaciones abundan aun hoy en la enseñanza de las ciencias naturales: “&lt;a href=&#34;http://www.eltiempo.com/archivo/documento/MAM-488672&#34;&gt;Las glándulas y sus funciones.&lt;/a&gt; En el organismo existen glándulas que segregan y liberan en el organismo sustancias de vital importancia porque sirven al funcionamiento de los diferentes sistemas que intervienen en la vida del hombre.”&lt;/p&gt;
&lt;h3 id=&#34;el-presidente-neoplatónico&#34;&gt;El presidente neoplatónico&lt;/h3&gt;
&lt;p&gt;También es frecuente en la actualidad encontrar aspectos neoplatónicos. Esta mirada mágica está presente en creencias místicas como el empacho y el mal de ojos. O en los horóscopos. El autor se ocupa de comentar que una misma persona podía -puede- abrazar aspectos de diferentes tradiciones, sin un conflicto cognitivo. Por ejemplo, Copérnico es poseedor de características tanto aristotélicas como neoplatónicas: ubicaba al Sol en el centro del universo pero repudiaba la astrología. Otro ejemplo, que el artículo original no incluye, es el presidente actual de Argentina, que, a pesar de tener formación de ingeniero, como el gran referente del mecanicismo, también abraza aparentemente &lt;a href=&#34;http://www.ambito.com/800322-macri-confirmo-que-consulto-a-una-bruja-me-hizo-mucho-bien&#34;&gt;la videncia y la brujería&lt;/a&gt;. En este contexto, resulta interesante notar que se suele hablar de fracaso cuando la enseñanza de las ciencias no erradica las ideas místicas o esotéricas, o incluso cuando en las &lt;a href=&#34;https://elpais.com/elpais/2015/04/23/ciencia/1429792444_486485.html&#34;&gt;encuestas de percepción social de la ciencia&lt;/a&gt; se siguen manifestando rasgos aristotélicos.&lt;/p&gt;
&lt;p&gt;Sobre este punto, Petrucci reflexiona que, en vistas del peso de las representaciones antiguas en nuestra cultura, los resultados de la tarea de los docentes de ciencias deben ser evaluados de modo cuidadoso. La coexistencia de enfoques no se trata entonces de un fracaso, porque no parece ser un objetivo realista que el conocimiento científico enseñado reemplace a las concepciones previas de los estudiantes (como proponía el enfoque clásico de cambio conceptual), sino que desde la didáctica de las ciencias se busca que el conocimiento se incorpore como un nuevo marco explicativo, disponible para cuando se requiera su uso por parte del individuo.&lt;/p&gt;
&lt;p&gt;Puede decirse que el artículo objeto de esta reseña cumple el objetivo con que fue planteado, ya que propone un interesante recorrido para que el docente de ciencias naturales conozca un poco más y reflexione sobre las implicancias de las diferentes visiones de mundo y sobre las propuestas de acceso al conocimiento que se entretejen en su disciplina particular.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;“Un recorrido histórico por algunas concepciones del mundo”&lt;/strong&gt;. Petrucci, D. Revista de Enseñanza de Física. Rosario. Vol. 29, No. Extra, Nov. 2017, pp. 499–509. Enlace al &lt;a href=&#34;https://revistas.unc.edu.ar/index.php/revistaEF/article/view/18509&#34;&gt;trabajo original&lt;/a&gt;.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Memorias del horizonte</title>
      <link>https://ciencianet.com.ar/post/memorias-del-horizonte/</link>
      <pubDate>Thu, 22 Mar 2018 15:48:26 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/memorias-del-horizonte/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Gastón Giribet.&lt;/strong&gt; Investigador del Center for Cosmology and Particle Physics de New York University. Profesor de la UBA e Investigador Principal del CONICET.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;En 1976, Stephen Hawking publicó en The Physical Review un artículo inquietante en el que exponía su ya célebre “paradoja de la pérdida de información en los agujeros negros”, una observación que vendría a conturbar el ambiente de la física teórica por más de cuatro décadas. Durante este tiempo, la comunidad científica especializada ha visto retornar ese monstruo de la razón con formas renovadas, cada vez más temibles, una y otra vez; desde aquel artículo seminal &lt;a href=&#34;#1&#34; title=&#34;2&#34;&gt;[1]&lt;/a&gt; en los 1970s hasta las más recientes discusiones &lt;a href=&#34;#2&#34; title=&#34;2&#34;&gt;[2]&lt;/a&gt;, reavivadas con especial ahínco desde 2012. Aquí, un sesgo dialéctico nos lleva a recortar la enmarañada historia del problema y centrarnos en lo que fue el último intento de Hawking por resolver su célebre acertijo; lo que podríamos considerar su última idea.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;la-pérdida-de-información-en-los-agujeros-negros&#34;&gt;La pérdida de información en los agujeros negros&lt;/h3&gt;
&lt;p&gt;Los agujeros negros son los objetos más intrigantes del universo. Siendo la causa de los eventos más enérgicos y violentos en el cosmos, estos objetos astrofísicos desafían nuestra intuición como ningún otro fenómeno en la naturaleza, obligándonos a reconsiderar las leyes fundamentales de la física. Una de las propiedades fascinantes de los agujeros negros es su universalidad: Los agujeros negros son, en cierto sentido, únicos. Independientemente de las características de la estrella moribunda, que, después de su muerte, da lugar a la formación de un agujero negro, la forma de este último resulta ser la misma, indiferente a los detalles de la estrella original; el agujero negro sólo heredará la masa total y la velocidad de rotación del astro progenitor, y ningún otro rasgo.&lt;/p&gt;
&lt;p&gt;Esto significa que, independientemente de la composición química de la estrella, o su densidad, o cualquier otra característica, las propiedades del agujero negro resultante sólo dependerán de dos datos, la masa y la velocidad de rotación, sin tener en cuenta ninguna otra información. Cuando se observó por primera vez, este fenómeno de universalidad de los agujeros negros no representó más que una curiosidad; sin embargo, adquirió una dimensión drástica cuando, en 1974, Stephen Hawking descubrió que la mecánica cuántica predice que los agujeros negros emiten una forma débil pero persistente de radiación. Esta radiación, producida por la creación de partículas y antipartículas en las proximidades del agujero negro, es un proceso imparable que finalmente conduce a la evaporación total del cuerpo y, en consecuencia, a la desaparición de toda la información contenida en su interior.&lt;/p&gt;
&lt;p&gt;Por otra parte, de acuerdo con la teoría de la relatividad general, la naturaleza de la radiación emitida por el agujero negro no puede contener ninguna información sobre la materia que quedó atrapada en su interior. Esto da lugar a la siguiente pregunta: ¿Qué ocurrió con la información contenida en la estrella original antes de que se formara el agujero negro?&lt;/p&gt;
&lt;p&gt;Por un lado, los principios de la mecánica cuántica exigen que se preserve la información –esto se conoce como principio de unitariedad, que nos habla de la reversibilidad en la evolución hamiltoniana en la teoría cuántica–; por otro lado, la teoría de la relatividad general parece decirnos que los agujeros negros no tienen memoria y que toda la información contenida en el agujero negro se pierde para siempre. Esta es, precisamente, la paradoja de Hawking, y expresa mejor que cualquier otro resultado en la física la tensión existente entre la teoría de la relatividad y la teoría de los cuantos, los dos pilares sobre los que construimos todo el andamiaje teórico de la física.&lt;/p&gt;
&lt;h3 id=&#34;la-naturaleza-paradojal-y-la-física-fundamental&#34;&gt;La naturaleza paradojal y la física fundamental&lt;/h3&gt;
&lt;p&gt;La tensión existente entre la teoría de la relatividad y la teoría cuántica lleva en germen un verdadero cisma en nuestro corpus teórico. En el contexto que nos convoca, este cisma se expresa de la siguiente forma: Los agujeros negros están destinados a evaporarse mediante el proceso de radiación descubierto por Hawking, proceso que finalmente los llevará a desaparecer junto con toda la información que en ellos se encuentra encerrada. La mecánica cuántica, por su parte, prohíbe un acto de magia cósmico de estas características ya que dicta que la información habrá de ser preservada. ¿Cómo conciliar, pues, estas dos descripciones? ¿Cómo resolver la paradoja? ¿Cómo vivir tranquilos en un edificio teórico que presenta fisuras en su estructura basal?&lt;/p&gt;
&lt;p&gt;Una actitud frecuente para sobrellevar esta situación paradojal y apaciguar así la angustia del físico teórico es abusar del pragmatismo: Uno puede sentirse tentado a argüir que la radiación de los agujeros negros es, aunque persistente, extremadamente tenue; algo así como 10&lt;sup&gt;-28&lt;/sup&gt; Watts para un agujero negro de masa estelar; una potencia de radiación desdeñable, propia de un astro con una temperatura de tan sólo 10&lt;sup&gt;-7&lt;/sup&gt; Kelvin. Esto implica que el proceso de evaporación, aunque ineluctable, es inimaginablemente lento; tan lento que no sólo ninguno de los agujeros negros que existieron en la historia del universo ha tenido tiempo de evaporarse aún, sino que todos ellos tardarán en hacerlo un tiempo descomunal: 10&lt;sup&gt;67&lt;/sup&gt; años; es decir, cincuenta y siete órdenes de magnitud más que la edad actual del universo. Entonces, ¿por qué iría uno a preocuparse por un aspecto paradojal de la física que no se expresará en la naturaleza hasta dentro de un tiempo más largo que todo aquel que haya alguna vez transcurrido?&lt;/p&gt;
&lt;p&gt;La respuesta es simple: Cuando se trata de leyes fundamentales, una inconsistencia lógica, aunque se desarrolle ésta en el futuro remoto, es acuciante hoy. Estamos hablando aquí de teorías fundamentales de la física que presentan incompatibilidades inherentes, teorías sobre las que construimos toda la física que entendemos, toda nuestra concepción del cosmos. Aunque lenta y sutil, con persistencia neurálgica, la pérdida de información de los agujeros negros atenta contra la salud de nuestro saber más básico y es imprescindible el esfuerzo por resolverla.&lt;/p&gt;
&lt;h3 id=&#34;la-paradoja-y-sus-secuelas&#34;&gt;La paradoja y sus secuelas&lt;/h3&gt;
&lt;p&gt;Stephen Hawking formuló la primera versión de su paradoja en su artículo &lt;a href=&#34;#1&#34; title=&#34;1&#34;&gt;[1]&lt;/a&gt; de 1976. Y fue él quien advirtió antes que nadie las profundas implicancias conceptuales de su descubrimiento. Como consecuencia de esto, dedicó incansables esfuerzos a resolver el rompecabezas que él mismo supo plantear, esfuerzos que no cesaron hasta sus últimos días de actividad. Al comienzo, y durante los 1980s y 1990s, quizá debido a su sesgo inicial de físico relativista, Hawking interpretó su paradojal resultado como un indicio de que ciertos aspectos de la mecánica cuántica necesitaban ser reformulados para que se volviera esta teoría compatible con la de Einstein. Más tarde, llegado el siglo XXI, probablemente rendido ante la evidencia que la conjetura de Maldacena presentaba a favor de la mecánica cuántica de los agujeros negros &lt;a href=&#34;#3&#34; title=&#34;3&#34;&gt;[3]&lt;/a&gt;, Hawking anunció haber cambiado de parecer y adherir desde entonces a la idea de que es la teoría de Einstein la que debe sufrir modificaciones.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2018/03/stephen.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Desde el trabajo iniciático de 1976, muchos elaboraron versiones refinadas de la paradoja de la información, ensayaron diferentes respuestas a esas tantas versiones, y propusieron todo tipo de ideas especulativas sobre la estructura del espacio-tiempo para poder resolver el acertijo. Ideas de frontera en la física teórica tales como la teoría de las bolas difusas, la complementariedad de los agujeros negros, la pared de fuego, la correspondencia entre entrelazamiento cuántico y agujeros de gusano, nacen todas a partir del intento por resolver la paradoja de Hawking.&lt;/p&gt;
&lt;h3 id=&#34;la-última-idea-de-stephen-hawking&#34;&gt;La última idea de Stephen Hawking&lt;/h3&gt;
&lt;p&gt;En los últimos años, desde mediados de 2015, Hawking y sus colaboradores de la Universidad de Cambridge y de la Universidad de Harvard se vieron atraídos hacia una nueva idea; una idea promisoria a la que Hawking destinó sus exiguos últimos esfuerzos: Los agujeros negros podrían preservar la información de la estrella progenitora almacenándola cerca de su superficie –el denominado horizonte de eventos– para después devolverla al medio interestelar de una manera codificada en la radiación que ellos emiten.&lt;/p&gt;
&lt;p&gt;De alguna forma, esto contradice aquello que dijimos al comienzo acerca de que los agujeros negros sólo son capaces de recordar la masa y la velocidad angular de la estrella original. ¿Deja esto de ser cierto? En efecto, Hawking y sus colaboradores creyeron haber encontrado una manera de circunvalar esta limitación y entrevieron una forma en la que estos astros podrían guardar mucha más información de lo que se creía.&lt;/p&gt;
&lt;p&gt;En una conferencia &lt;a href=&#34;#4&#34; title=&#34;4&#34;&gt;[4]&lt;/a&gt; dictada en agosto de 2015 en el Instituto Real de Tecnología de Estocolmo, Hawking presentó su idea de manera proverbial: “La paradoja de la pérdida de información puede ser resuelta si uno considera [las perturbaciones del espacio-tiempo que] las partículas que entran al agujero negro causan en el horizonte. Así, la información puede ser recobrada”. En otras palabras, Hawking, basado en el trabajo previo de otros físicos entre los que descuella Andrew Strominger, propuso que, a diferencia de lo que se creía hasta el momento, la estructura del espacio-tiempo en las cercanías de los agujeros negros puede sufrir un tipo de deformación generada por la materia que cae dentro de ellos y usar esto como mecanismo de registro: Esta perturbación del espacio-tiempo podría servir para guardar de manera codificada la información de la materia en acreción.&lt;/p&gt;
&lt;p&gt;Esto se conoce como “efecto de memoria gravitacional” y significa que la superficie del astro se convertiría en algo así como un gran ábaco en el que se lleva la cuenta de las partículas que caen dentro de él, dejando cada una de ellas una huella en la estructura misma del espacio-tiempo que lo circunda. El tipo de perturbaciones que, según Hawking, las partículas producen en el horizonte de los agujeros negros al caer dentro de ellos puede ser pensada como un pequeño desplazamiento en la superficie del astro, algo así como un desarreglo minucioso de esa superficie que bien podemos pensar elástica; como si, al caer, las partículas produjeran una &lt;em&gt;traslación&lt;/em&gt; diferente en diferentes puntos del horizonte de eventos, arrugándolo.&lt;/p&gt;
&lt;p&gt;Es debido a esto que este mecanismo de almacenamiento de información mediante la deformación de la geometría del espacio-tiempo recibe el nombre de &lt;em&gt;super-traslación en el horizonte&lt;/em&gt;. La idea que Hawking anunció en su conferencia de Estocolmo era hasta ese momento muy vaga debido a que no dio allí ningún detalle técnico acerca de cómo calcular matemáticamente esas super-traslaciones del horizonte. Fue en un artículo posterior, escrito en colaboración con Malcom Perry y Andrew Strominger, que los detalles matemáticos fueron publicados &lt;a href=&#34;#5&#34; title=&#34;5&#34;&gt;[5]&lt;/a&gt;. La idea de Hawking, Perry y Strominger acerca de que las super-traslaciones en los horizontes de los agujeros negros podrían proveer un mecanismo mediante el cual estos astros almacenarían la información codificada del proceso que les dio origen es una idea tentadora que atrajo no poca atención.&lt;/p&gt;
&lt;p&gt;Desde hace poco más de dos años, esta idea no ha dejado de generar secuelas y generalizaciones, y ha suscitado también una aguerrida controversia; todo esto, muestra cabal de la manera en la que el genio de Stephen Hawking ha influenciado el quehacer en la física teórica hasta sus últimos días.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Agradecimientos&lt;/strong&gt; El autor le agradece a Laura Donnay por fragmentos del texto y discusiones sobre el contenido.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Artículos originales&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;1&#34;&gt; 1. &lt;/a&gt; S. Hawking, “&lt;em&gt;Breakdown of Predictability in Gravitational Collapse&lt;/em&gt;”, Phys. Rev. D 14 (1976) 2460. &lt;a href=&#34;https://journals.aps.org/prd/abstract/10.1103/PhysRevD.14.2460&#34;&gt;https://journals.aps.org/prd/abstract/10.1103/PhysRevD.14.2460&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;2&#34;&gt; 2. &lt;/a&gt; A. Almheiri, D. Marolf, J. Polchinski, J. Sully, &lt;em&gt;Black Holes: Complementarity or Firewalls?&lt;/em&gt;, JHEP 1302 (2013) 062. &lt;a href=&#34;https://arxiv.org/pdf/1207.3123.pdf&#34;&gt;https://arxiv.org/pdf/1207.3123.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;3&#34;&gt; 3. &lt;/a&gt; J. Maldacena, &lt;em&gt;Eternal Black Holes in AdS&lt;/em&gt;, JHEP 0304 (2003) 021. &lt;a href=&#34;https://arxiv.org/pdf/hep-th/0106112.pdf&#34;&gt;https://arxiv.org/pdf/hep-th/0106112.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;4&#34;&gt; 4. &lt;/a&gt; S. Hawking, &lt;em&gt;The Information Paradox for Black Holes&lt;/em&gt;, Talk given on 28 August 2015 at KTH Royal Institute of Technology, Stockholm. &lt;a href=&#34;https://arxiv.org/pdf/1509.01147.pdf&#34;&gt;https://arxiv.org/pdf/1509.01147.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;5&#34;&gt; 5. &lt;/a&gt; S. Hawking, M. Perry, A. Strominger, &lt;em&gt;Soft Hair on Black Holes&lt;/em&gt;, Phys. Rev. Lett. 116 (2016) 231301. &lt;a href=&#34;https://arxiv.org/pdf/1601.00921.pdf&#34;&gt;https://arxiv.org/pdf/1601.00921.pdf&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Una dieta demasiado pesada - Ingesta de plomo en niños</title>
      <link>https://ciencianet.com.ar/post/una-dieta-demasiado-pesada-ingesta-de-plomo-en-ninos/</link>
      <pubDate>Fri, 16 Feb 2018 16:53:26 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/una-dieta-demasiado-pesada-ingesta-de-plomo-en-ninos/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero.&lt;/strong&gt; Investigadora de CONICET en el INIFTA (Instituto de Investigaciones Fisicoquímicas Teóricas y Aplicadas). Docente en la UNLP.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;En esta entrada reseñamos el estudio publicado en la revista de la Sociedad Argentina de Pediatría “Contribución de la dieta a la exposición al plomo de niños de 1 a 7 años en La Plata, Buenos Aires” realizado de modo interdisciplinar por integrantes del Hospital de Niños “Sor María Ludovica” de La Plata, el Instituto Biológico provincial “Dr. Tomás Perón”y los Centros de Investigación CINDECA y CIMA de la Facultad de Ciencias Exactas de la UNLP.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;pesado-y-tóxico&#34;&gt;Pesado y tóxico&lt;/h3&gt;
&lt;p&gt;Según la &lt;a href=&#34;http://www.who.int/mediacentre/factsheets/fs379/es/&#34;&gt;Organización Mundial de la Salud&lt;/a&gt;, los grupos más vulnerables a los efectos tóxicos del plomo son los niños de corta edad y los pobres. En el caso de un infante con desnutrición los efectos que genera la presencia de este metal pesado en el organismo son aún peores ya que algunas deficiencias nutricionales aumentan la absorción. Las intoxicaciones agudas por plomo tienen efectos sobre el sistema nervioso central con síntomas muy visibles como convulsiones, coma y muerte. Pero con exposiciones menores, el plomo puede generar problemas más difusos y graduales como menor desarrollo intelectual, pobre capacidad de concentración (y por consiguiente,  menor rendimiento escolar), conductas antisociales. También puede provocar anemia y mal funcionamiento renal.&lt;/p&gt;
&lt;p&gt;Por este motivo, actualmente se considera que no existe un nivel seguro de plomo en sangre. El plomo ingresa al organismo mediante dos vías: inhalación de pequeñas partículas (lo que ocurría sobre todo antes de que se eliminaran las naftas con plomo y en caso de exposición laboral en actividades como minería o fabricación de pinturas) e ingestión de agua, polvo o alimentos que estuvieron previamente en contacto con este metal. Con el fin de poder identificar la ingesta diaria de plomo a través de la dieta, los investigadores encuestaron entre 2015 y 2016 a los padres de casi 100 niños y niñas entre 1 y 7 años de vida que llevaban dieta normal. Relevaron la cantidad, tipo y frecuencia de alimentos que ingerían habitualmente y también indagaron sobre los lugares y condiciones de compra de estos alimentos.&lt;/p&gt;
&lt;h3 id=&#34;marche-una-milanesa&#34;&gt;Marche una milanesa&lt;/h3&gt;
&lt;p&gt;Los autores, que son investigadores de distintas disciplinas que trabajan en instituciones de La Plata, realizaron un relevamiento del consumo de plomo en niños de la ciudad de La Plata que fueron atendidos en el Hospital de Niños local para controles de salud.&lt;/p&gt;
&lt;p&gt;Para medir la concentración de plomo presente en las diferentes preparaciones, los investigadores compraron en diferentes hipermercados y comercios barriales los ingredientes y prepararon las distintas comidas y bebidas que los padres referían ser consumidas por los niños. Entre estas preparaciones, cocinadas en el laboratorio siguiendo el modo más habitual, se incluyeron alimentos de todos los grupos: panificados, cereales, huevos, aceites y grasas, frutas, vegetales, carnes, Iácteos, bebidas, dulces, condimentos y agua.&lt;/p&gt;
&lt;p&gt;La “digestión” de los alimentos fue luego realizada por calor y presión en un horno microondas, y finalmente se tomaron pequeñas muestras a las que se midió la concentración de plomo mediante un aparato parecido en su aspecto a una impresora: el espectrómetro de emisión atómica por plasma de microondas (Figura 1).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2018/02/4100_mp-aes_lg.png&#34; alt=&#34;Figura 1: Espectrómetro de emisión atómica por plasma de microondas Agilent 4100 MP-AES Agilent, Santa Clara Ca, USA. En este equipo las muestras se calientan hasta ser atomizadas y luego se mide su emisión.:left&#34;&gt;
Conociendo la cantidad de porciones diarias que se consumen de un dado producto y el peso promedio de cada porción, se calcularon las ingestas de plomo en microgramos por día (mg/día) proporcionados por cada grupo de alimentos (Figura 2).&lt;/p&gt;
&lt;h3 id=&#34;mi-merienda-es-un-plomazo&#34;&gt;&lt;strong&gt;Mi merienda es un plomazo&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Como conclusión de las mediciones, los investigadores pudieron determinar que los alimentos que más plomo aportaron a la dieta fueron las carnes y derivados, con aproximadamente el 27%, los panificados (15%) y la leche (12%). Por otra parte, los huevos, dulces y cereales fueron los grupos con menores aportes. Una observación que resulta muy interesante y que resalta la importancia de realizar este tipo de estudios de modo local, es que los grupos de alimentos más involucrados en la ingesta de plomo difieren en diferentes regiones geográficas/culturas. Por ejemplo, en Estados Unidos los grandes culpables fueron los encurtidos de pepino, el chocolate con leche, las frutas y papas enlatadas y los camarones. Respecto del género, en España, fueron los cereales para los varones y el pescado y los mariscos para las mujeres.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2018/02/figura2.jpg&#34; alt=&#34;Figura 2. Aporte de plomo a la dieta estimada a partir de la encuesta de consumo. Martins E, et al. Contribución Arch Argent Pediatr 2018;116(1):14-20.&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;y-el-agua&#34;&gt;¿Y el agua?&lt;/h3&gt;
&lt;p&gt;Los investigadores encontraron dificultades para estimar el consumo de agua a partir de la encuesta, y por ello no aparece sumado en la Tabla. Para realizar una estimación, tomaron valores estándar de consumo reportados para las diferentes edades comprendidas en el estudio y restaron la ingesta informada por los padres de otras bebidas como jugos y gaseosas. Estimado de este modo, encontraron que el plomo ingerido por los niños al consumir agua de red sería de casi 50 µg/día. Los investigadores estimaron así que la ingesta de plomo de los niños platenses a través de alimentos y agua sería de 186,2 µg/día. Esta cifra está por encima de los valores reportados por estudios realizados en otros países para la población en general. La ingesta de los niños platenses se ubica muy lejos de los menos de 10 µg/día reportados por Corea o el Reino Unido, es el triple de lo estimado en Catalunya y sólo resulta superado por Chile, que alcanza los 206 µg/día.&lt;/p&gt;
&lt;h3 id=&#34;la-pregunta-del-millón&#34;&gt;La pregunta del millón&lt;/h3&gt;
&lt;p&gt;En el artículo se discute también cuál sería la causa de las altísimas cantidades de este metal en los alimentos consumidos en la región: “La ubicuidad del plomo en el ambiente, sobre todo, en el agua y el polvo, hace que las malas condiciones de salubridad en la producción, procesamiento, conservación y presentación para la venta de los alimentos sean potenciales explicaciones de la carga de plomo observada en los alimentos.”&lt;/p&gt;
&lt;h3 id=&#34;de-postre-limones&#34;&gt;De postre, limones&lt;/h3&gt;
&lt;p&gt;Como solución a este exceso de plomo, los investigadores sugieren, por un lado, mejorar las condiciones de salubridad de la producción y provisión de alimentos. Pero, por otro lado, si la comida es el mecanismo que nos está aportando cantidades excesivas e indeseables de plomo, también podría ser el mecanismo que nos ayude a mitigar los efectos del pesado metal. Los investigadores sugieren que la ingestión de alimentos con propiedades antioxidantes podría reducir la toxicidad del plomo. Y dentro de estos alimentos la vitamina C sería la estrella, por su capacidad antioxidante y por un posible efecto quelante: tendría un efecto protectivo frente a la toxicidad hematopoyética y aumentaría la excreción urinaria del plomo, según se ha encontrado en animales.&lt;/p&gt;
&lt;p&gt;Seguir dietas altas en hierro y calcio también favorece que la toxicidad o el tiempo de permanencia del contaminante en el cuerpo sea menor. Como líneas a seguir, los autores indican la mejora en la precisión de la determinación de ingestión de plomo en la población pediátrica, e incluso los niveles de otros posibles tóxicos. Y nos muestran cómo una ciencia vinculada con su entorno tiene mucho para aportar al bienestar de la ciudadanía.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; &lt;a href=&#34;http://www.sap.org.ar/docs/publicaciones/archivosarg/2018/v116n1a05.pdf&#34;&gt;Martins E, Malpeli A, Asens D, et al. Contribución de la dieta a la exposición al plomo de niños de 1 a 7 años en La Plata, Buenos Aires. Arch Argent Pediatr 2018;116(1):14-20.&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>¿Qué estación te va mejor? Cambios estacionales de luz y competencia de cianobacterias</title>
      <link>https://ciencianet.com.ar/post/que-estacion-te-va-mejor-cambios-estacionales-de-luz-y-competencia-de-cianobacterias/</link>
      <pubDate>Fri, 26 Jan 2018 16:04:53 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/que-estacion-te-va-mejor-cambios-estacionales-de-luz-y-competencia-de-cianobacterias/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero.&lt;/strong&gt; Investigadora de CONICET en el INIFTA (Instituto de Investigaciones Fisicoquímicas Teóricas y Aplicadas). Docente en la UNLP.&lt;/p&gt;
&lt;p&gt;En esta reseña abordamos el artículo “&lt;strong&gt;What season suits you best? Seasonal light changes and cyanobacterial competition&lt;/strong&gt;” realizado por los investigadores de CONICET Guadalupe Cascallares y Pablo M. Gleiser, del Centro Atómico Bariloche. En él estudian el papel que juega en la competencia entre diferentes cepas de cianobacterias la relación entre su ritmo circadiano y la distribución diaria de horas de luz y oscuridad. Mediante un modelo matemático sencillo, los autores del trabajo exploran el efecto de la transición entre estaciones y a su vez sugieren nuevos experimentos de laboratorio para validar el mecanismo propuesto.&lt;/p&gt;
&lt;h3 id=&#34;ritmos-y-bacterias&#34;&gt;Ritmos y bacterias&lt;/h3&gt;
&lt;p&gt;El ritmo circadiano es un ciclo interno, propio de la inmensa mayoría de los organismos vivos, que tiene una duración cercana a la del día terrestre (en general, se considera circadiano al ciclo cuya duración está comprendida entre 20 y 30 horas), dentro del cual se alternan una fase activa y una inactiva. Estos ciclos parecen conferir una ventaja adaptativa al organizar sus funciones biológicas (como alimentación, floración o reproducción) en sintonía con el ciclo natural de luz y oscuridad.&lt;/p&gt;
&lt;p&gt;Estos ritmos son diferentes para distintas especies, e incluso existe variabilidad dentro de una misma especie. Los ritmos circadianos no dependen de estímulos externos, es decir, se mantienen aún en condiciones constantes de laboratorio, pero sí pueden sincronizarse con ciclos externos de luz y oscuridad. Las cianobacterias, primitivos microorganismos unicelulares sin núcleo pero con capacidad de realizar fotosíntesis, son los organismos más simples que muestran este comportamiento rítmico. Las cinanobacterias presentan muchas propiedades interesantes. Se las considera responsables de generar el oxígeno en la atmósfera inicial de nuestro planeta.&lt;/p&gt;
&lt;p&gt;Se encuentran ampliamente distribuidas tanto en medios terrestres como acuáticos y son importantes fijadoras de nitrógeno atmosférico, es decir, transforman el nitrógeno de la atmósfera en compuestos que incorporan a la biósfera, volviéndose aprovechables para otros seres vivos, como las plantas. Desde la década del 80 se sabe que estas bacterias tienen un ritmo interno de fijación de nitrógeno y la cianobacteria Synechococcus se convirtió por su simpleza en uno de los modelos experimentales más usados para estudiar los ciclos circadianos.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2018/01/Synechococcus_PCC_7002_BF-300x259.jpg&#34; alt=&#34;Figura 1. Imagen por microscopía de la cinanobacteria Sinechococcus. By Masur - Own work, Public Domain.:left&#34;&gt;´&lt;/p&gt;
&lt;h3 id=&#34;experimentos&#34;&gt;Experimentos&lt;/h3&gt;
&lt;p&gt;Para los experimentos que estudian el ritmo circadiano en Synechococcus se usan diferentes cepas: la silvestre, con un ciclo interno de 25h, y otras cepas mutantes con ciclos alterados (más cortos o largos). En estos estudios se ha reportado que existen ventajas adaptativas en las cepas con ritmo circadiano próximo al ritmo externo, y que estas ventajas desaparecen cuando no existe ninguna variación en las condiciones ambientales. Para ello, se estudió el crecimiento competitivo de colonias de diferentes cepas de cianobacterias en laboratorios exponiéndolas a intervalos iguales de luz y oscuridad.&lt;/p&gt;
&lt;p&gt;Estos experimentos muestran que cuando el período externo día/noche se acerca al ritmo propio de una cepa de cianobacteria, esta crece más que las cepas que se encuentran “desfasadas”. Es decir, si el ciclo externo es de 24 horas, entonces la cepa natural, con un ritmo circadiano de 25h, gana la competencia frente a las cepas mutantes de ritmos circadianos de 22h o de 30 hs. La interpretación de estos resultados sugiere que el crecimiento de las bacterias ocurre durante el período de luz externa mientras que la producción de inhibidor de crecimiento de las otras cepas ocurre durante la fase activa intrínseca.&lt;/p&gt;
&lt;h3 id=&#34;modelado-matemático-y-propuesta&#34;&gt;Modelado matemático y propuesta&lt;/h3&gt;
&lt;p&gt;El modelado matemático es una herramienta habitual para estudiar sistemas biológicos. Consiste, dicho de modo muy general, en la expresión matemática del problema. En este tipo de situaciones el modelado consiste en definir las variables de interés, las interacciones sobre el sistema, los parámetros y las relaciones funcionales, determinando así un conjunto de ecuaciones diferenciales que, al ser resueltas, dan cuenta de la dinámica de las variables y permiten conocer el estado del sistema en función del tiempo.&lt;/p&gt;
&lt;p&gt;En el caso de las cianobacterias, las variables de interés serán las poblaciones relativas de las distintas cepas bajo distintas condiciones de ciclo externo de luz y oscuridad. Existen diversos modelos matemáticos propuestos para el crecimiento de las cianobacterias y el efecto del ciclo externo. Pero todas las simulaciones computacionales y todos los experimentos reportados hasta entonces se habían realizado con ciclos de igual exposición luz/oscuridad.&lt;/p&gt;
&lt;p&gt;Y aquí encontramos la innovación que presenta el trabajo de Cascallares y Gleiser: incorporar a la versión matematizada del problema el hecho de que las horas de luz respecto de las de oscuridad no son en la Naturaleza iguales sino que dependen de la estación y de la latitud. Esta modificación incorpora así las estaciones, cuyo efecto anual es fuertemente regulatorio en gran parte de los seres vivos al disparar respuestas como por ejemplo las hibernaciones, las migraciones y la senescencia.&lt;/p&gt;
&lt;p&gt;Para esto, los autores del trabajo construyeron un modelo matemático fenomenológico basado en uno ya existente (modelo de Gonze) de crecimiento bacteriano con producción de inhibidor de crecimiento selectivo, y generaron el ritmo circadiano de la bacteria con un oscilador sensible a la luz (versión modificada del modelo de Van der Pol). El ritmo externo de luz/oscuridad y el ciclo interno de cada cepa de cianobacteria quedan entonces acoplados, dependiendo de la intensidad de la luz.&lt;/p&gt;
&lt;p&gt;En el modelo, la bacteria crece siempre cuando su ciclo interno activo coincide con el ciclo externo de luz, mientras que si su ciclo intrínseco está inactivo sólo podrá crecer si además de luz externa su inhibidor selectivo (producido por las otras cepas) está por debajo de un valor umbral fijo. Lo que hicieron los investigadores fue modular el ritmo externo como si estuviera ocurriendo una transición entre estaciones en la superficie terrestre, en el cual el día y la noche no tienen una duración igual, es decir, manteniendo fija la duración del “día” pero cambiando la relación de horas de luz y de oscuridad.&lt;/p&gt;
&lt;p&gt;Por ejemplo, agregando 12 minutos de luz por ciclo a un estado inicial de 24 horas con 12 horas de luz y 12 de oscuridad, luego de 10 días la diferencia será de 120 minutos: el ciclo simulado seguirá teniendo 24 horas de las cuales 14 serán de luz y 10 de oscuridad. Con estas herramientas, pudieron avanzar respecto de los experimentos existentes que estudian el efecto de la duración del ciclo externo día/noche (con distribución equitativa de luz y oscuridad) y estudiar cómo afecta a la competencia entre cepas la variación de horas de luz dentro de un ciclo de duración fija.&lt;/p&gt;
&lt;p&gt;En una primera simulación el ciclo externo fue fijado en 28 horas, para explorar con un “día” que tuviera una duración intermedia entre los ritmos circadianos de las cepas usadas: la silvestre, de 25 horas, y la mutante, de 28. Inicialmente, en la simulación se las expuso a 14 horas de luz y otras tantas de oscuridad, agregando 3 minutos de luz extra cada ciclo, durante un “mes” de 30 ciclos. Lo que se observó en la simulación es que la coexistencia de ambas cepas termina aproximadamente a los 8 días, cuando la mutante gana a la silvestre (Figura 2).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2018/01/fig-02.png&#34; alt=&#34;Figura 2: A. Competición entre las cepas silvestre (línea negra continua) y mutante (línea roja a trazos) para un ciclo externo de 28 horas. B Si se agregan 3 minutos de luz por día la cepa mutante se impone a partir del día 8.&#34;&gt;&lt;/p&gt;
&lt;p&gt;Pero el resultado que sorprende es que al aumentar teóricamente las horas de luz de a 12 minutos por día dentro de un ciclo de 24 horas, las cepas mutantes de ciclo largo son las que predominan sobre la silvestre (cuyo ciclo es 25 h). En estas condiciones, inicialmente predomina la cepa silvestre, en acuerdo con los experimentos de laboratorio, pero con el transcurso de los días ocurre una “inversión” en la curva de crecimiento de las cepas. La cepa mutante logra imponerse y predominar por encima de la silvestre a medida que el tiempo activo de su ciclo interno, más largo, se aproxima al período de luz del ciclo externo (Figura 3).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2018/01/fig-03.png&#34; alt=&#34;Figura 3: Competición entre las cepas silvestre (línea continua roja) y mutante (línea azul a trazos) para un ciclo externo de 24 horas. Si se agregan 12 minutos de luz por día inicialmente predomina la cepa silvestre pero la cepa mutante se impone a partir del día 8.&#34;&gt;&lt;/p&gt;
&lt;p&gt;En este trabajo se plantea un ida y vuelta entre experimentos y simulaciones. El modelo propuesto reproduce los resultados de los experimentos de laboratorio, y a su vez va más allá explorando situaciones para los cuales no existen aún respuestas empíricas. Los resultados predichos por el modelo para las situaciones planteadas por los investigadores pueden ser contrastados con experimentos factibles de ser realizados, y de este modo, validar o no el mecanismo propuesto.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; Guadalupe Cascallares y Pablo M. Gleiser, &lt;a href=&#34;https://doi.org/10.4279/pip.070005&#34;&gt;&amp;quot;&lt;em&gt;What season suits you best? Seasonal light changes and cyanobacterial competition&lt;/em&gt;”&lt;/a&gt;,  Papers in Physics, vol. 7, art. 070005 (2015).&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Matemáticas para estudiar el corazón: Pérdida de correlaciones de largo alcance en el ritmo cardíaco de enfermos de Chagas</title>
      <link>https://ciencianet.com.ar/post/matematicas-para-estudiar-el-corazon-perdida-de-correlaciones-de-largo-alcance-en-el-ritmo-cardiaco-de-enfermos-de-chagas/</link>
      <pubDate>Sun, 17 Jul 2016 22:02:42 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/matematicas-para-estudiar-el-corazon-perdida-de-correlaciones-de-largo-alcance-en-el-ritmo-cardiaco-de-enfermos-de-chagas/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero.&lt;/strong&gt; Investigadora de CONICET en el INIFTA (Instituto de Investigaciones Fisicoquímicas Teóricas y Aplicadas). Docente en la UNLP.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;En este artículo reseñamos el trabajo “Losses of Long-Range Correlation in the Heart Rate Variability of Patients With Chagas’ Disease”, publicado en la edición de Junio de 2016 de la revista Global Heart. Los investigadores Isabel Irurzun, Magdalena Defeo, Regina de Battista y Eduardo Mola presentaron recientemente los resultados de un desarrollo proveniente de técnicas de análisis de series temporales a registros electrocardiográficos de personas con la enfermedad de Chagas.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2017/12/Fig1-768x230.gif&#34; alt=&#34;Figura 1&#34;&gt;&lt;/p&gt;
&lt;p&gt;En trabajos previos, los investigadores del INIFTA (Instituto de Investigaciones Fisicoquímicas Teóricas y Aplicadas, UNLP-CONICET) desarrollaron un índice denominado FNNF10 basándose en las características no lineales de la variabilidad del ritmo cardíaco humano. El nuevo índice, cuya sigla significa &lt;em&gt;Fracción residual de falsos vecinos a dimensión 10&lt;/em&gt;, fue testeado en aproximadamente 500 personas con diferentes condiciones cardíacas, observándose su sensibilidad para detectar alteraciones en el balance de la acción de los sistemas nerviosos simpático y parasimpático sobre el corazón. Esta capacidad señalaba al índice FNNF10 como una potencial herramienta de diagnóstico temprano de enfermedades cardíacas.&lt;/p&gt;
&lt;p&gt;El principal objetivo del trabajo interdisciplinar que aquí se reseña es evaluar la utilidad del FNNF10 para la detección temprana de casos de disautonomía parasimpática chagásica. Las afecciones cardíacas son la principal causa de muerte en enfermos de Chagas. La disautonomía parasimpática es uno de los mecanismos posibles de daño, en este caso por la pérdida de conexiones entre los nervios y el músculo cardíaco, que puede estar presente antes de que exista una disfunción detectable. El índice propuesto por los investigadores se basa en el método de falsos vecinos.&lt;/p&gt;
&lt;p&gt;Según este método, dos puntos vecinos en el espacio de fase de un sistema determinístico (de dimensión de &lt;em&gt;embedding&lt;/em&gt; &lt;em&gt;d&lt;/em&gt;) se mantendrán como vecinos en dimensiones superiores. Si los puntos en cambio se separan, se los identifica como falsos vecinos. Se supone que para dimensiones mayores a &lt;em&gt;d&lt;/em&gt; la fracción de falsos vecinos será muy pequeña. El FNNF10 es en este caso la fracción de falsos vecinos cuando la serie temporal de intervalos entre latidos se analizan a dimensión 10, que es la máxima dimensión de embedding obtenida en individuos sanos.&lt;/p&gt;
&lt;p&gt;El índice fue calculado sobre las series temporales de intervalos entre latidos de 130 individuos caracterizados como sanos, 128 sujetos con Chagas pero asintomáticos (con electrocardiograma y holter de 24 horas normales) y 27 pacientes con Chagas con registros de actividad cardíaca eléctrica anormales. Los resultados obtenidos reflejan, por un lado, que el índice depende de la edad en individuos sanos: el valor de FNNF10 decrece con la edad siguiendo una ley de potencia.&lt;/p&gt;
&lt;p&gt;Por otro lado, los pacientes chagásicos presentan valores de FNNF10 mayores que los individuos sanos, aún en los casos en que no presentan alteraciones cardíacas detectables por los métodos usuales de diagnóstico. Esta elevación de la fracción residual de falsos vecinos a dimensión 10 es interpretada como una pérdida en las correlaciones de largo alcance, es decir, como si la “memoria” del corazón del ritmo cardíaco que llevaba un tiempo atrás fuera menor para los pacientes chagásicos que para las personas sanas.&lt;/p&gt;
&lt;p&gt;Esta diferencia significativa hallada en el comportamiento de los intervalos entre latidos sugiere una potencial utilidad del índice FNNF10 para el diagnóstico temprano de la disautonomía chagásica en su estadio asintomático. Respecto de las perspectivas futuras en esta temática, Isabel Irurzun nos cuenta que el grupo que dirige está avanzando en dos líneas de trabajo: establecer correlaciones de FNNF10 con el nivel de anticuerpos antimuscarínicos (que son indicadores bioquímicos de disautonomía) y determinar si la la disautonomía es anterior al daño miocárdico producido por fibrosis. El artículo original, titulado “Losses of Long-Range Correlation in the Heart Rate Variability of Patients With Chagas’ Disease” está disponible &lt;a href=&#34;https://doi.org/10.1016/j.gheart.2016.03.166&#34;&gt;aquí&lt;/a&gt;.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Planificación y gestión de la comunicación desde el lugar del científico – El caso CONICET (2007-2015)</title>
      <link>https://ciencianet.com.ar/post/planificacion-y-gestion-de-la-comunicacion-desde-el-lugar-del-cientifico-el-caso-conicet-2007-2015/</link>
      <pubDate>Mon, 11 Jul 2016 22:05:30 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/planificacion-y-gestion-de-la-comunicacion-desde-el-lugar-del-cientifico-el-caso-conicet-2007-2015/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero.&lt;/strong&gt; Investigadora de CONICET en el INIFTA (Instituto de Investigaciones Fisicoquímicas Teóricas y Aplicadas). Docente en la UNLP.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Reseña de la Tesis &amp;quot;Planificación y gestión de la comunicación desde el lugar del científico&amp;quot; de Silvia Montes de Oca.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;El día miércoles 8 de junio de 2016, en la facultad de Periodismo y Comunicación Social de la UNLP se realizó la defensa de la tesis de maestría de la Comunicadora Silvia Montes de Oca. La tesis, de título “Planificación y gestión de la comunicación desde el lugar del científico – El caso CONICET (2007-2015)”, fue dirigida por Paula Porta, y el tribunal evaluador estuvo constituido por Nancy Díaz Larrañaga, Carlos Ciappina y Roberto Salvarezza, quien fuera presidente del CONICET durante el período 2012-2015.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2016/07/IMG_2798.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;La tesis presenta una reseña del CONICET, la mayor institución de Ciencia y Tecnología de Argentina y definido por la autora como “portador del modelo social de investigación científica”. Luego analiza el proceso recorrido por el Estado nacional desde la creación del Ministerio de Ciencia y Tecnología en 2007 en búsqueda de una mayor llegada de la Ciencia y Tecnología argentina a la sociedad, acorde a su valoración altamente positiva de la CyT en el desarrollo integral y sostenible de un país.&lt;/p&gt;
&lt;p&gt;Montes de Oca presenta las distintas experiencias realizadas en el plano de comunicación pública de la ciencia. Entre ellas se encuentran Tecnópolis, Tec-TV, Canal Encuentro, el portal Educ.ar, así como la exploración de distintas herramientas y formatos, como el stand up científico, las redes sociales y las tecnologías digitales. La participación intensa y variada del CONICET en este proceso en que la ciencia fue una política de estado es analizada, profusamente documentada y valorada en su significado en términos de los cambios de modelos comunicacionales adoptados para incluir a la audiencia.&lt;/p&gt;
&lt;p&gt;En este último sentido, dice Montes de Oca, se dio voz y visibilidad a los propios científicos para establecer un diálogo con la sociedad, el cual es considerado como un agente reconstructor de la ciencia en el período analizado. Según palabras de la autora “Formar parte del CONICET es estar legitimado frente a la sociedad como prestigioso portavoz del modelo social de investigación científica, y por ende, de sus discursos, en forma mediada o directa. La experiencia de los últimos años demuestra que cuando la intervención del científico es presencial y su relato adaptado para la comprensión del lego, captura la atención de las audiencias con independencia del tema que aborde”.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2016/07/IMG_2800-768x663.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;El cambio de contexto político que tuvo lugar a fines de 2015, y con él la distinta concepción del rol de la CyT imperante en el Gobierno actual, dan al trabajo la potencia de servir de herramienta para el análisis de la continuidad de las políticas públicas. Según fue apreciado por el propio Salvarezza durante la defensa de la tesis, el estudio realizado por Silvia Montes de Oca constituye un importante documento para el análisis y la puesta en valor de la gestión realizada. La tesis refleja el enorme esfuerzo de CONICET en reconstruir la comunicación pública de la ciencia y nos deja a los científicos en una posición privilegiada para sostener el papel que pretendemos para la CyT en Argentina. Queda en los miembros del sistema científico comprender que este privilegio nos es otorgado junto a la responsabilidad de asumir el compromiso de formarnos y capacitarnos, no ya -como se suele decir- “bajando los contenidos al llano” sino elevándonos nosotros mismos para estar a la altura de la audiencia.&lt;/p&gt;
&lt;p&gt;Silvia Montes de Oca es autora de un libro sobre Contravientos, el antiboom de la la divulgación científica, donde relata el recorrido de esta actividad en Argentina. El libro puede ser descargado libremente desde la dirección &lt;a href=&#34;http://www.contravientos.com.ar&#34;&gt;http://www.contravientos.com.ar&lt;/a&gt; gracias a la Fundación Instituto Leloir. Su tesis de maestría está también disponible en el Repositorio de la UNLP, SEDICI para ser &lt;a href=&#34;http://hdl.handle.net/10915/53714&#34;&gt;descargada&lt;/a&gt; de modo libre y gratuito. *Planificación y Gestión de la Comunicación.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Epidemia de dengue 2015-2016: Evitemos transmitir la enfermedad al  mosquito</title>
      <link>https://ciencianet.com.ar/post/epidemia-de-dengue-2015-2016-evitemos-transmitir-la-enfermedad-al-mosquito/</link>
      <pubDate>Thu, 28 Jan 2016 22:12:59 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/epidemia-de-dengue-2015-2016-evitemos-transmitir-la-enfermedad-al-mosquito/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Hernán G Solari&lt;/strong&gt;. Miembro de la Red de Modelización de Enfermedades Infecciosas - Investigador Principal del CONICET Profesor Asociado, FCEN-UBA.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Hacía tiempo que no veía a Juan. Cuando me llamó y me propuso venir a casa a tomar unos mates, no lo dudé y convenimos un día. Llegado el día, Juan no se sentía con todas las pilas, pero la idea de verme después de tanto tiempo lo entusiasmaba. Lo recibí al atardecer en el estar que tengo algo arreglado con una plantitas, unos potus que cuido con esmero. Charlamos de viejos y nuevos tiempos, de la epidemia de dengue y de tantas cosas. Quince días después enfermé de dengue. Cuando yo enfermé la Municipalidad fumigó el barrio, pero de nada sirvió, siguieron mi mujer y mis hijos. Ninguno fue al hospital, para qué, si apenas estaban mal, ¿para hacer la cola de dos horas en la guardia? Mi experiencia.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;El criterio aceptado, histórico, dice que cuando el número de casos de una enfermedad supera significativamente el número de casos usuales se está frente a una epidemia. Esta es la situación hoy día en varias provincias argentinas, sobre todo en el Noreste del país, situación que naturalmente nos alarma.&lt;/p&gt;
&lt;p&gt;Este criterio hoy en día se ha modificado por los aportes realizados desde el punto de vista de la evolución (dinámica) de los procesos epidémicos. Desde el desarrollo de la epidemiología matemática se tiene claro que el índice a considerar como indicador de una situación epidémica es el número reproductivo básico, familiarmente conocido como R0. Esté número es (entre otras cosas) la cantidad de casos secundarios producidos por una persona contagiosa. Claramente, si R0 es mayor que 1 estamos frente a un cuadro epidémico, pero siendo menor que 1 la circulación del virus puede sostenerse en el mediano plazo solamente por el flujo de casos importados, R0 nos dice como se va multiplicando el problema.&lt;/p&gt;
&lt;p&gt;Recordemos que el virus del dengue se reproduce a gran escala en el ser humano y a muy pequeña escala en el mosquito. su ciclo se puede describir como: producción en el ser humano posiblemente acompañada de transporte, entrega al mosquito para su distribución a nuevas unidades reproductoras. El mosquito hace el “delivery” casa por casa, pero el producto lo produce y transporta largas distancias el ser humano. Las estimaciones más confiables indican que el mosquito solo excepcionalmente se aleja más de 50 metros del peri-domicilio (alrededores de la casa) durante un ciclo reproductivo (hablamos aquí de unos pocos días, para fijar ideas cuatro o cinco días). Es esta distancia la que determina la zona que se fumiga al verificarse un caso de dengue. Sin embargo, los seres humanos se movilizan mucho más y contribuyen con esto significativamente a la epidemia y a la supervivencia del virus que sin esta contribución estaría condenado a extinguirse.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2016/01/Aedes_aegypti-300x288.jpg&#34; alt=&#34;Mosquito Aedes aegypti, fotografía tomada del sitio del Grupo de Estudio de Mosquitos (GEM), FCEN- UBA.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Es bueno saber que se llama caso importado a aquel caso de dengue en el que la persona ha viajado distancias considerables comparadas con el vuelo del mosquito y ha visitado zonas donde circula el virus, siendo entonces probable (y solo probable, no cierto) que haya adquirido el virus en la zona visitada. Por el contrario, el caso autóctono corresponde a cuando, por no haber salido de la región, estamos obligados a reconocer que el contagio se produjo en la misma, localmente. Los casos importados en Quilmes no vienen necesariamente del extranjero, pueden venir del noreste por ejemplo. Pero el ser humano no solo hace el transporte de larga distancia, también lo lleva de su casa al trabajo y del trabajo a la casa, o a la escuela, el hospital y cuando visita a sus amigos y familiares, etc. etc. La atención puesta sobre el mosquito suele hacer olvidar estas contribuciones. Solo recientemente (últimos diez años como mucho) se ha empezado a considerar esta contribución en estudios científicos del dengue.&lt;/p&gt;
&lt;p&gt;Debemos procurar que nuestros mosquitos estén sanos, no solo hay que concebir al mosquito como el que inocula el virus al ser humano, sino también como el que adquiere el virus del ser humano. En el año 2009 circuló el virus del dengue en Capital Federal y el conurbano bonaerense. Poco después de un periodo de sequía comenzó la circulación del virus. Pero a esa altura del año, las poblaciones de mosquitos se encontraban en declinación (ya era otoño), la probabilidad de un caso importado de producir más de uno autóctono era por tanto baja y el dengue circuló solo gracias a un flujo de casos importados que pensamos como proveniente del desarrollo de la epidemia en otras regiones del país.&lt;/p&gt;
&lt;p&gt;En un trabajo que realizamos antes de la epidemia de 2009 analizábamos el posible desarrollo de epidemias de dengue en el área metropolitana, una conclusión clave: el momento en que comienza la circulación del virus es un elemento determinante. Las epidemias de dengue en la región metropolitana no pueden comenzar en abril. Considerando los niveles actuales de poblaciones de &lt;em&gt;Aedes aegypti&lt;/em&gt; en esta región, si la circulación del virus comienza tardíamente como en 2009 solo habrá una circulación forzada del virus (R0 menor que 1): Por el contrario, pueden producirse brotes grandes (unos pocos miles de casos considerando los clínicos y subclínicos) si la epidemia comienza en enero. Esto sin contar con los brotes secundarios producidos por la movilidad de las personas.&lt;/p&gt;
&lt;p&gt;La fiebre amarilla (FA), que es transmitida por el mismo mosquito que el dengue, chikungunya, zika y 100 otros virus, el mosquito &lt;em&gt;Aedes (Stegomyia) aegypti&lt;/em&gt;, nos proporciona un ejemplo. El virus de la fiebre amarilla es el prototipo de la familia flaviviridae a la que pertence el dengue. Ambas enfermedades son en extremo similares excepto por la tasa de mortalidad y el cuadro agudo de la fiebre amarilla. En febrero de 1870 se dio un caso importado de FA en el hotel Roma de Buenos Aires. Para fines de marzo se daban las primeras muertes en la zonas aledañas al hotel. Su número es incierto pero treparon a algo entre 100 y 200 casos. En 1871 se dio la epidemia histórica de Buenos Aires con más de 13.000 muertos. Estimamos que la circulación del virus comenzó por casos importados arribados a principios de enero de 1871 y sabemos con certeza que las primeras muertes ocurrieron a fines de enero. La variable que explica tamaña diferencia es la fecha de inicio de la epidemia.&lt;/p&gt;
&lt;p&gt;La epidemia de dengue actual en la región metropolitana podría ser unas 50 o 100 veces más severa que la circulación del virus en 2009. Focos de hasta 2000 casos son probables si no median medidas de control. Me refiero solo a la situación en el área metropolitana porque es la región de la que poseemos datos como para realizar los estudios. Similares estudios se pueden hacer para todas las ciudades del país, el dengue es fundamentalmente un problema urbano. ¿Podemos/debemos hacer algo más de lo que usualmente se hace frente a una epidemia de dengue? La respuesta al problema del dengue es siempre la misma que se da en aquellos lugares donde es endémico, campañas de concientización para que la población colabore eliminando potenciales criaderos del mosquito y vistosas fumigaciones.&lt;/p&gt;
&lt;p&gt;Sobre la importancia de las primeras no caben dudas, sobre la efectividad de las segundas podríamos escribir una larga hoja de dudas. Bien efectuadas, las fumigaciones pueden ser de gran ayuda, pero esa condición de efectuarse bien es en extremo difícil de lograr. Hay que fumigar donde el mosquito puede estar y no donde casi seguro no está. Se suele ver fumigaciones en plazas (donde rara vez se los encuentra) y con vehículos que se desplazan por las calles cuando muchos mosquitos de esta especie pasan casi toda la vida dentro de las casas y prefieren/necesitan los lugares húmedos y con sombra que en nuestras urbanizaciones asociamos con el pulmón de manzana. Los medios suelen mostrarnos casi siempre la fumigación en el lugar equivocado, ¡pero visible! un acto de propaganda fundamentalmente. Los equipos de fumigación deben estar cuidadosamente calibrados y mantenidos, el tamaño de la gota en la niebla importa. El tiempo entre que se detecta el estado febril de un enfermo de dengue (aproximadamente al segundo día de viremia) y la fumigación también es un factor determinante, las demoras tienen costos en eficiencia.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2016/01/larva_aedes-300x166.jpg&#34; alt=&#34;Larva de Aedes aegypti. Muy movediza e inquieta, suele cumplir su ciclo en floreros y bebederos. Fotografía tomada de https://dengueinfoar.wordpress.com.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;¿Podemos hacer más? Si, podemos anticipar donde es probable que se produzcan brotes considerando que no solo el mosquito transporta el virus sino que el principal transporte son los seres humanos. Podemos bajar la contribución de los casos sub-clínicos a la propagación de la epidemia y seguramente más cosas. Dejo de lado las propuestas que probablemente escucharemos para convertirnos en conejillos de india liberando en el ambiente organismos de bio-ingeniería (mosquitos genéticamente modificados, mosquitos alterados con endo-bacterias y la propagandizada vacuna quimera contra el dengue consistente en un virus de fiebre amarilla modificado), estas propuestas comerciales no han sido sometidas al escrutinio de científicos independientes de las empresas productoras, no pueden por el momento considerarse científicas, pero las epidemias también pueden concebirse como oportunidades de negocios.&lt;/p&gt;
&lt;p&gt;Fundamentalmente, podemos preguntarle a los que saben y no tienen conflictos de intereses. Es necesario formar un comité científico amplio de asesoramiento para este problema que es complejo y no se lo puede considerar como contenido dentro de una disciplina científica. La visión del médico requiere el complemento de la del biólogo, el sociólogo, el funcionario de salud pública y la epidemiología matemática. Es necesario adelantarse a los hechos y no correr detrás de ellos. El estado nacional cuenta con recursos humanos en su sistema de ciencia y técnica. Hacer buen uso de ellos representará un cambio en la política sanitaria.&lt;/p&gt;
&lt;p&gt;La historia del comienzo podría ocurrir. Según un estudio en Tailandia, tres de cada cuatro casos de dengue son sub-clínicos y los enfermos continúan con sus tareas habituales, la proporción depende -claro está- de la cultura, pero que son más los casos sub-clínicos que los clínicos es seguro. Juan era un enfermo contagioso (para los mosquitos) de dengue, sea que fuere asintomático o estuviera en su primer día de viremia. A &lt;em&gt;Aedes aegypti&lt;/em&gt; es común encontrarlo como larva en el agua de la plantitas de potus, también en la bandeja del secaplatos y otros lugares dentro de la casa, los mosquitos de casa son mis-mosquitos. Como adulto puede seguir viviendo en ella y posiblemente volverá para su ingesta de sangre. Después, esperar a que se reproduzca el virus en él, mientras su cría crece en el agua del potus. Más tarde contagiar al resto de la casa directa o inderactamente. ¿Quién trajo el virus a la casa? No fueron los mosquitos, fue Juan. ¿Pudo hacer algo Juan? Claro que si, además del antipático no venir, debió haber usado repelente para no transmitir el virus a los mosquitos. ¿Pude hacer algo yo? Si, claro, eliminar esas larvas de la planta de potus. ¿Sirvió de algo la fumigación? De poco, los mosquitos del cuento estaban dentro de la casa cuando ocurrió.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Supercomputadoras para el diseño de fármacos</title>
      <link>https://ciencianet.com.ar/post/supercomputadoras-para-el-diseno-de-farmacos/</link>
      <pubDate>Sat, 14 Feb 2015 22:22:53 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/supercomputadoras-para-el-diseno-de-farmacos/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Diego Masone&lt;/strong&gt;. CONICET - Facultad de Ciencias Exactas y Naturales, Universidad Nacional de Cuyo.&lt;/p&gt;
&lt;p&gt;La mayor parte de los procesos biológicos que tienen lugar en los seres vivos involucran interacciones entre proteínas, estas agrupaciones organizadas de aminoácidos que cumplen funciones vitales de la célula. Es así que la correcta identificación y caracterización de las interacciones entre proteínas y el conocimiento de la estructura y las propiedades de los complejos que forman, resulta crucial a la hora de comprender cómo funcionan las células. Este tipo de problema es abordado desde hace décadas utilizando modelos computacionales para el desarrollo de métodos de diseño de nuevos fármacos. Aunque no podía anticiparse en 1938, el problema se parece al descripto por &lt;a href=&#34;http://es.wikipedia.org/wiki/Albert_Einstein&#34;&gt;Albert Einstein&lt;/a&gt; y Leopold Infeld en el inolvidable libro “La evolución de la física”. Dice así:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“...los conceptos físicos son creaciones libres del espíritu humano y no están, por más que lo parezca, determinados unívocamente por el mundo exterior. En nuestro empeño de concebir la realidad, nos parecemos a alguien que tratara de descubrir el mecanismo invisible de un reloj, del cual ve el movimiento de las agujas, oye el tic-tac, pero no le es posible abrir la caja que lo contiene. Si se trata de una persona ingeniosa e inteligente, podrá imaginar un mecanismo que sea capaz de producir todos los efectos observados; pero nunca estará segura de si su imagen es la única que los pueda explicar. Jamás podrá compararla con el mecanismo real, y no puede concebir, siquiera, el significado de una tal comparación. Como él, el hombre de ciencia creerá ciertamente que, al aumentar su conocimiento, su imagen de la realidad se hará más simple y explicará mayor número de impresiones sensoriales. Puede creer en la existencia de un límite ideal del saber, al que tiende el entendimiento humano, y llamar a este límite la verdad objetiva.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;En la segunda mitad del siglo XIX el límite de resolución de un microscopio óptico había sido definido matemáticamente por &lt;a href=&#34;http://es.wikipedia.org/wiki/Ernst_Abbe&#34;&gt;Ernst Karl Abbe&lt;/a&gt; y se había alcanzado el punto en que ya no era posible ir más allá de las observaciones de medio micrómetro (0,0000005 m). Sin embargo el desarrollo del tubo de rayos catódicos impulsó el gran salto en microscopía: usando electrones en lugar de luz visible &lt;a href=&#34;http://es.wikipedia.org/wiki/Gustav_Hertz&#34;&gt;Gustav Ludwig Hertz&lt;/a&gt; demostró teóricamente cómo un pequeño solenoide podía hacer converger haces de electrones de forma análoga a como una lente convergía rayos de luz visible.&lt;/p&gt;
&lt;p&gt;Aunque originalmente concebido por &lt;a href=&#34;http://es.wikipedia.org/wiki/Le%C3%B3_Szil%C3%A1rd&#34;&gt;Leó Szilárd&lt;/a&gt; en 1933, los ingenieros alemanes &lt;a href=&#34;http://es.wikipedia.org/wiki/Ernst_Ruska&#34;&gt;Ernst Ruska&lt;/a&gt; y &lt;a href=&#34;http://es.wikipedia.org/wiki/Max_Knoll&#34;&gt;Maximillion Knoll&lt;/a&gt; construyeron el primer microscopio electrónico que tenía una resolución de 50 nanómetros (0,000000050 m). Lamentablemente el estallido de la Segunda Guerra Mundial retrasó los avances y sólo después de 20 años de terminada la guerra los microscopios electrónicos fueron capaces de alcanzar 1 nanómetro de resolución (0,000000001 m). Estos fueron los primeros pasos hacia las técnicas de cristalografía por rayos X con las que hoy se estudian las proteínas en gran detalle.&lt;/p&gt;
&lt;p&gt;En un esfuerzo por almacenar, organizar y compartir información cristalográfica la base de datos de proteínas de libre acceso en internet (véase &lt;a href=&#34;www.pdb.org&#34;&gt;www.pdb.org&lt;/a&gt;) fue creada en los Laboratorios Nacionales de Brookhaven (BLN) en 1971. En sus comienzos contenía sólo 13 estructuras, hoy cuenta con más de 100.000. Esto significa que la humanidad dispone de la información estructural de esta cantidad de proteínas a sólo un click de distancia, muchas de ellas responsables directa o indirectamente de las más horribles patologías y enfermedades. Es sin duda una situación sin precedentes para la investigación biomédica.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2015/02/fig1-290x300.jpg&#34; alt=&#34;Figura 1: Estructura tridimensional de la proteína de la albúmina humana.:left&#34;&gt; Es entonces muy fácil para nosotros descargar de internet una proteína de nuestro agrado y “ver” cómo luce su disposición tridimensional en una computadora de escritorio, como muestra la figura 1. Pero podemos hacer mucho más, y de eso se encarga la biomedicina computacional. Podemos estudiar cómo interacciona nuestra proteína con otras proteínas o cómo lo hace con pequeñas moléculas. Podemos diseñar nuestras propias moléculas en la computadora y simular su interacción con nuestra proteína para por ejemplo, inhibir cierta función relacionada con alguna enfermedad. Es decir, podemos diseñar fármacos virtuales que luego sean probados experimentalmente hasta llegar a convertirse en fármacos reales.&lt;/p&gt;
&lt;p&gt;Desde luego, detrás de este proceso computacional están las leyes físicas y químicas que gobiernan los átomos que componen los aminoácidos y que su vez forman las proteínas. La dinámica de moléculas es en términos computacionales un experimento virtual donde hemos incluido todo lo que sabemos sobre fuerzas interatómicas. Es el descomunal poder de cálculo de una máquina quien simula el paso del tiempo para que observemos la evolución de nuestro sistema virtual y podamos dilucidar sus mecanismos de funcionamiento, detectar problemas y proponer modelos. Con este fin se han construido centenares de supercomputadoras de uso académico en diversas universidades del mundo (véase &lt;a href=&#34;www.top500.org&#34;&gt;www.top500.org&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Siguiendo esta línea de investigación hemos desarrollado junto a nuestros colaboradores del Centro Nacional de Supercomputación en Barcelona (véase &lt;a href=&#34;www.bsc.es&#34;&gt;www.bsc.es&lt;/a&gt;), un método de análisis de complejos entre proteínas. Dado un par de proteínas que sabemos se asocian para formar una nueva estructura y de la cual poseemos información cristalográfica detallada, generamos de forma virtual centenares de posibles complejos según un criterio geométrico. Luego, para decidir cuales de ellos son factibles de existir en la naturaleza, utilizamos un tipo muy especial de interacción intermolecular: los enlaces de hidrógeno. Este tipo de interacción resulta sumamente importante durante el reconocimiento molecular como se ha demostrado experimentalmente mediante cálculos de estructura electrónica.&lt;/p&gt;
&lt;p&gt;De esta forma extendiendo el proceso a una buena cantidad de proteínas distintas, verificamos que los cálculos de energías de interacción que incluían una completa optimización de los enlaces de hidrógenos de las proteínas en su totalidad, introducían una significativa mejora en el discernimiento entre los complejos generados geométricamente. Esto significa que luego de generar en una computadora centenares de complejos entre proteínas de una manera más bien caprichosa, somos capaces de distinguir cuales tienen una buena posibilidad de existir realmente y con un poco de suerte, tendremos un número reducido de candidatos.&lt;/p&gt;
&lt;p&gt;Este último conjunto de estructuras es hasta donde la biología computacional puede llegar (por ahora) y que a continuación sigue la demoledora etapa experimental. Pero no es difícil imaginar un futuro con diseño de fármacos “a la carta” específicos para cada necesidad. Como “Los cazadores de microbios” de Paul de Kruif, queda en manos de los científicos la búsqueda de la verdad objetiva que nos permita avanzar en el entendimiento de las interacciones entre proteínas.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; &lt;a href=&#34;mailto:dmasone@fing.uncu.edu.ar&#34;&gt;dmasone@fing.uncu.edu.ar&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Artículo original&lt;/strong&gt;: H-bond network optimization in protein–protein complexes: Are all-atom force field scores enough? Diego Masone, Israel Cabeza de Vaca, Carles Pons, Juan Fernandez Recio and Victor Guallar. &lt;em&gt;Proteins: Structure, Function, and Bioinformatics&lt;/em&gt; Volume &lt;strong&gt;80&lt;/strong&gt;, Issue 3, pages 818–824, March 2012. &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1002/prot.23239/abstract&#34;&gt;http://onlinelibrary.wiley.com/doi/10.1002/prot.23239/abstract&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Flujo intermitente de sistemas particulados y transición de atasco</title>
      <link>https://ciencianet.com.ar/post/flujo-intermitente-de-sistemas-particulados-y-transicion-de-atasco/</link>
      <pubDate>Thu, 15 Jan 2015 23:01:34 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/flujo-intermitente-de-sistemas-particulados-y-transicion-de-atasco/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Gago.&lt;/strong&gt; Facultad Regional La Plata. Universidad Tecnológica Nacional y CONICET.&lt;/p&gt;
&lt;p&gt;¿Existen similitudes en el comportamiento de la sal en un salero, ovejas saliendo del establo a pastar y un grupo de personas abandonando un edificio? Intentaré convencerlos de que sí, contando algunas de las similitudes que se han encontrado recientemente en un trabajo conjunto que realizamos con colegas de la Universidad Tecnológica Nacional, el Instituto Tecnológico Buenos Aires, la Universidad de Navarra, la Universidad de Edimburgo, la Universidad de Zaragoza, la Escuela Superior de Física y Química Industrial de París y la Universidad de Barcelona.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2015/01/ilustracion-300x190.jpg&#34; alt=&#34;Imagen tomada de http://despicableme.wikia.com/wiki/Minions?file=Minions3.png. CC-BY-SA.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;La primera similitud que podemos sugerir es que todos estos sistemas (junto con una gran variedad de otros) pertenecen a la categoría de sistemas particulados; esto es, son &lt;em&gt;un conjunto numeroso de elementos de tamaño macroscópico (que se aprecian a simple vista) que interactúan entre ellos&lt;/em&gt;. Pensemos por ejemplo en los diferentes recorridos que atraviesa una persona caminando por una calle vacía o por una calle repleta de otras personas. En el primer caso el recorrido típico será en línea recta, del lugar donde se encuentra al lugar a donde desea dirigirse, en el segundo caso será necesario realizar un camino zigzagueante que permita evitar el contacto con los demás peatones.&lt;/p&gt;
&lt;p&gt;En la definición dada de sistema particulado no nos interesará entonces el comportamiento individual de cada elemento, sino el comportamiento del conjunto, causado por las interacciones presentes. Si aceptamos esta primera similitud podemos avanzar a la que nos convoca, el flujo de estos sistemas particulados a través de orificios o aberturas angostas.&lt;/p&gt;
&lt;h3 id=&#34;un-salero-que-se-tapa&#34;&gt;Un salero que se tapa&lt;/h3&gt;
&lt;p&gt;Podemos decir que los sistemas particulados poseen la capacidad de &amp;quot;fluir&amp;quot; a través de un orificio o abertura, tal como lo hace la arena del reloj de arena, o la gente a la salida del cine. Para esto necesita algún tipo de fuerza impulsora (en el caso de la arena la gravedad, en el caso de las personas alguna motivación). En estos casos el sistema se comporta como una especie de fluido que puede adaptarse a la forma de la abertura y a la situación (es decir, intentar moverse más rápido o más lento dependiendo de la fuerza o motivación).&lt;/p&gt;
&lt;p&gt;Sin embargo, un salero que se tapa es un fenómeno de todos los días y esta es otra propiedad que tienen en común los sistemas particulados. Si prestamos atención veremos que el flujo de personas saliendo del cine no es continuo ni homogéneo. Presenta interrupciones de diferente duración sólo que aquí, a diferencia de lo que ocurre con la sal, las mismas &amp;quot;partículas/personas&amp;quot; son las que se encargan, mediante tirones y empujones, de destrabar el embotellamiento y seguir saliendo. En el caso del salero, como bien sabemos, resulta necesario sacudirlo o golpearlo un poco para obtener el mismo efecto de reanudación del flujo.&lt;/p&gt;
&lt;p&gt;Al aplicar algún mecanismo para reactivar el flujo de estos sistemas se obtiene lo que se llama un &lt;em&gt;flujo intermitente de sistemas particulados&lt;/em&gt;. En estos casos el tiempo que le tomará al sistema reanudar el flujo una vez producido el atasco dependerá de muchos factores: la humedad de la sal, el tamaño del orificio del salero, la fuerza que impulsa al sistema a fluir, etc; así como del mecanismo de desbloqueo aplicado.&lt;/p&gt;
&lt;p&gt;Para muchos fines prácticos, conocer el tiempo que lleva restablecer estos flujos tiene gran importancia. Para esto, lo que haríamos generalmente sería repetir varias veces el &amp;quot;experimento&amp;quot; midiendo el tiempo de desatasco en cada caso y después promediando estos tiempos obtendríamos una estimación del tiempo característico de atascamiento. Sin embargo, este método de promediado sólo es válido para algunos fenómenos, en el problema que nos convoca no lo es.&lt;/p&gt;
&lt;p&gt;Este problema de flujo intermitente en sistemas particulados requiere de un análisis diferente. Tratemos de entender porqué.&lt;/p&gt;
&lt;h3 id=&#34;las-leyes-de-probabilidad&#34;&gt;Las leyes de probabilidad&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2015/01/gauss_3-300x120.png&#34; alt=&#34;Ejemplo de distribución normal. Vemos que este tipo de distribución es simétrica y podemos notar además que de las 1000 repeticiones del experimento, aproximadamente 700, es decir la gran mayoría, duraron entre 55 y 65 minutos, es decir que esta es una distribución localizada.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Cuando queremos dar información sobre un hecho repetitivo (el tiempo que tarda el tren en ir de A a B por ejemplo) hablamos de forma intuitiva del promedio de estos valores sobre el número total de medidas realizadas (el número de veces que viajamos). No nos importa cuanto tardó ayer ni cuanto tardó hoy; no queremos el detalle del tiempo de cada viaje, queremos información general, una cantidad que nos sirva para predecir de un modo aceptable la duración de los viajes siguientes. En la figura de la izquierda podemos ver un ejemplo de lo que se llama una &lt;em&gt;&lt;a href=&#34;http://es.wikipedia.org/wiki/Distribuci%C3%B3n_normal&#34;&gt;ley de probabilidad &amp;quot;normal&amp;quot;&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Supongamos que viajamos 1000 veces de A a B. Cada linea en la figura representa la cantidad de veces que el tren tardó 40, 45, 50, 55, etc minutos en el trayecto. Claramente, la mayoría de las veces, aproximadamente 160, el tren tarda una hora, aunque puede tardar unos minutos más o unos minutos menos (seguramente dependiendo del tránsito). Según este experimento, con gran probabilidad llegaremos a destino a tiempo si tomamos el tren una hora antes. Este tipo de distribución también se distingue por ser simétrica, es decir que podemos con igual probabilidad llegar 5 minutos antes o cinco minutos después y el promedio o media coincide con el tiempo que se repitió la mayor cantidad de veces, llamado &lt;em&gt;moda&lt;/em&gt;. Las distribuciones normales ocupan un lugar importante en el estudio probabilístico de algunos fenómenos de la naturaleza, es por esto que sus propiedades (es decir, las propiedades de las cantidades que poseen este tipo de distribución) han sido intensamente estudiadas.&lt;/p&gt;
&lt;h3 id=&#34;flujo-intermitente-de-sistemas-granulados&#34;&gt;Flujo intermitente de sistemas granulados&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2015/01/long_tail-300x165.png&#34; alt=&#34;Ejemplo del tipo de distribución de probabilidades obtenida para los tiempos de atasco en el flujo intermitente de sistemas particulados. La línea de trazo roja indica el valor promedio de esta distribución.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Si miramos ahora la distribución que resulta de medir los tiempos de duración de los atascos en el flujo intermitente de los sistemas particulados veremos que esta distribución no resulta del tipo normal sino que tiene la forma mostrada en la figura de la derecha. En esta puede apreciarse que existe una alta probabilidad asociada a tiempos cortos pero que también existe probabilidad, aunque cada vez menor, de tener atascos de muy larga duración.&lt;/p&gt;
&lt;p&gt;En esta figura la línea de trazo en rojo marca el valor correspondiente al promedio de los tiempos obtenidos. Podemos ver que en este caso esta cantidad no nos provee mayor información de lo que pasa en el sistema. No nos permite predecir, ya que la mayoría de las veces los atascos se romperán rápidamente, pero existe una probabilidad de que duren mucho tiempo más de lo que indica el promedio.&lt;/p&gt;
&lt;p&gt;En el caso de estar interesados, por ejemplo, en predecir de forma aceptable el tiempo que lleva evacuar determinado edificio será necesario entonces intentar un nuevo enfoque. Construyamos entonces las llamadas &lt;em&gt;&lt;a href=&#34;http://es.wikipedia.org/wiki/Funci%C3%B3n_de_distribuci%C3%B3n&#34;&gt;distribuciones acumuladas&lt;/a&gt;&lt;/em&gt;, que en este caso significa simplemente &amp;quot;la probabilidad de que la duración de un atasco sea mayor que&amp;quot;. Es decir, no el número de veces que un atasco duró 3, 5 o 7 minutos, sino el número de atascos que duraron más de 3, más de 5 o más de 7 minutos. En la figura de abajo podemos ver el resultado de tomar la distribución acumulada para los tiempos de atascos correspondiente a la descarga de un silo con granos que posee una abertura pequeña y se vibra constantemente para romper los atascos que se forman durante el flujo.&lt;/p&gt;
&lt;p&gt;En esta figura las dos curvas representadas corresponden a dos fuerzas de gravedad efectiva distintas con las que las partículas son empujadas a moverse a través del orificio de salida del silo. Esto se consigue cambiando la inclinación del silo con respecto a la vertical. Podemos ver que para la curva roja, la probabilidad de tener atascos largos es mayor que para la curva azul, correspondiente a una aceleración mayor. Este es un fenómeno interesante, que sin embargo no voy a poder contar en esta nota.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2015/01/distribucion_plano.png&#34; alt=&#34;(Der.) Esquema del dispositivo empleado para los experimentos de atascos de granos. Consiste en un silo bidimensional sobre un plano inclinado que se vibra usando un parlante. El silo cuanta con una abertura pequeña que se atasca en forma permanente si no se usan vibraciones. (Izq.) Distribución de probabilidades de los tiempos de atascos correspondientes a la descarga del silo. Las distintas curvas corresponden a distintas inclinaciones del silo. Pueden verse los valores de alpha obtenidos en cada caso.&#34;&gt;&lt;/p&gt;
&lt;p&gt;Los datos de este gráfico se encuentran representados en una escala llamada escala logarítmica y así representados puede verse que en la región de tiempos largos estas gráficas se asemejan a una linea recta. Este tipo de comportamiento puede describirse matemáticamente mediante lo que se conoce como una ley de potencias: $$P(T &amp;gt; τ) = τ^{(-α)}$$
Sin entrar en detalles, lo que quiero es remarcar la importancia del parámetro α en esta ecuación. El valor de α será diferente para las distintas curvas en el gráfico y nos indica que tan rápido o lento decrece la probabilidad de tener tiempos de atascos largos. Por ejemplo, para la evacuación de un edificio, querríamos conseguir un exponente α lo mayor posible, lo que aseguraría muy baja probabilidad de tiempos de atasco, y por consiguiente tiempos de evacuación, largos.&lt;/p&gt;
&lt;p&gt;Sin embargo, ¿cuál sería el valor mínimo aceptable o el valor óptimo en cada caso? Esto es algo que no puede definirse de forma simple y necesita de un estudio más profundo a partir de las situaciones específicas. Lo que si podemos decir es que estas distribuciones presentan características completamente diferentes para valores de α por encima y por debajo de 2. En la mayoría de los casos queremos asegurar un valor α &amp;gt; 2.&lt;/p&gt;
&lt;h3 id=&#34;transición-de-atasco&#34;&gt;Transición de &amp;quot;atasco&amp;quot;&lt;/h3&gt;
&lt;p&gt;El tipo de función matemática propuesta para los tiempos de atasco tiene la particularidad de que para un valor de α mayor que 2, luego de realizar un número suficientemente grande de repeticiones (suficientemente grande dependiendo del caso puede se 100, 1000, 10000 o más), el valor promedio de la distribución no variará significativamente con las repeticiones subsiguientes. Sin embargo, valores α menores que 2 implican que el valor promedio que obtendremos si realizamos repetidamente este experimento (por ejemplo la descarga de un silo o salero) no será el mismo si repetimos el experimento 100, 1000 o 10000 de veces, dependerá de las repeticiones realizadas y su valor puede cambiar de forma radical entre la medida número 10000 y la 10001!&lt;/p&gt;
&lt;p&gt;En la figura vemos que para los tiempos de atasco de un silo bidimensional es posible que el valor del parámetro α tome valores por debajo y por encima del valor crítico 2, tan solo cambiando la fuerza que impulsa a las partículas hacia la salida. Este fenómeno, que ha sido observado también en el flujo intermitente de otros sistemas partículados, como un rebaño de ovejas o en la evacuación de personas (este último estudiado mediante modelos de simulación por computadora) ha sido llamado transición de atasco (&lt;em&gt;clogging transition&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Las transiciones son saltos bruscos de las propiedades de los sistemas al variar levemente algún parámetro de control. En el caso del agua hirviendo, por ejemplo, la densidad cambia de forma brusca de líquido a vapor, mientras que la temperatura del sistema sólo necesita cambiar de digamos 99C a 101C. En el caso que nos ocupa es posible, variando características propias del sistema estudiado (ancho del orificio de descarga o puerta, aceleración del sistema, urgencia por salir, etc), que el exponente α tome valores por encima y por debajo del 2. Por lo tanto, los tiempos medios de evacuación pasarían de estar bien definidos luego de un número suficientemente grande de medidas, a seguir teniendo la capacidad de variar siempre que agreguemos más medidas al experimento.&lt;/p&gt;
&lt;h3 id=&#34;entonces&#34;&gt;Entonces...&lt;/h3&gt;
&lt;p&gt;Existen similitudes entre la descarga de un silo o el comportamiento de la gente a la salida de un estadio. Entre estas similitudes pudimos ver que el flujo intermitente presenta, para los tiempos de atasco, una transición de fase conocida como transición de atasco. Este tipo de resultado tiene uno de sus mayores puntos de interés en la nueva perspectiva que ofrece al estudio de las normas de seguridad para evacuación peatonal, donde resulta fundamental tener la mayor precisión posible en los tiempos que el proceso lleva.&lt;/p&gt;
&lt;p&gt;También resulta útil a la hora de analizar procesos productivos, donde los materiales particulados son ampliamente manipulados y suelen encontrarse con la necesidad de lidiar con este tipo de atascos. Pero además de sus aplicaciones técnicas, desde un punto de vista científico, la existencia de una transición de fase en este tipo de sistemas ofrece un amplio e interesante campo de investigación. Las transiciones de fase y los saltos bruscos y discontinuidades que implican, siempre han resultado fascinantes para quienes estudian la naturaleza y quieren intentar comprender el cómo y del porqué de estas discontinuidades.&lt;/p&gt;
&lt;p&gt;La existencia de esta transición en flujos intermitentes de sistemas particulados con características tan variadas hacen que su estudio y generalización resulten atrayentes para intentar dilucidar los fenómenos que subyacen a la naturaleza de los mismos.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; I. Zuriguel, D. R. Parisi, R. Cruz Hidalgo, C. Lozano, A. Janda, P. A. Gago, J. P. Peralta, L. M. Ferrer, L. A. Pugnaloni, E. Clement, D. Maza, I. Pagonabarraga, A. Garcimartin, &lt;a href=&#34;http://dx.doi.org/10.1038/srep07324&#34;&gt;Clogging transition of many-particle systems flowing through bottlenecks&lt;/a&gt;, Scientific Reports 4, 7324 (2014)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; Paula Gago (&lt;a href=&#34;mailto:paulaalejandrayo@gmail.com&#34;&gt;paulaalejandrayo@gmail.com&lt;/a&gt;)&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Un centinela en aguas patagónicas</title>
      <link>https://ciencianet.com.ar/post/un-centinela-en-aguas-patagonicas/</link>
      <pubDate>Thu, 08 Jan 2015 23:05:38 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/un-centinela-en-aguas-patagonicas/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Anabella Fassiano&lt;/strong&gt;. Laboratorio de Enzimología, Estrés y Metabolismo, Facultad de Ciencias Exactas y Naturales, Universidad de Buenos Aires.&lt;/p&gt;
&lt;p&gt;El incremento poblacional en la Patagonia provocó un aumento en la contaminación de ríos y lagos por efluentes cloacales. Investigadores de la Facultad de Ciencias Exactas y Naturales evalúan el uso de una almeja nativa como señal de alarma frente a la contaminación. Entre montañas y estepas, cuando paseamos por la tierra del fin del mundo no podemos evitar admirar la belleza de sus paisajes. Una de las imágenes más reconocidas de la Patagonia son sus hermosos lagos. Aunque no todas las aguas son tan transparentes como lo muestran las postales. El crecimiento demográfico que se ha dado en la Patagonia en los últimos años fue muy importante. Los asentamientos urbanos, por lo general, se sitúan cerca de algún curso de agua o una laguna que actúa como receptor de los desechos de las ciudades. Estos residuos impactan y modifican en la ecología del lago. ¿Cómo conocer de forma temprana si esas aguas están contaminadas y poder actuar antes de que sea demasiado tarde? La doctora Iara Rocchetta y la licenciada María Soledad Yusseppone, dos investigadoras del CONICET y de la Facultad de Ciencias Exactas y Naturales (FCEN-UBA), estudian una almeja de agua dulce autóctona que podría ayudar a dar la señal de alerta de contaminación en el agua.&lt;/p&gt;
&lt;h3 id=&#34;centinelas-biológicos&#34;&gt;Centinelas biológicos&lt;/h3&gt;
&lt;p&gt;Una &lt;strong&gt;especie centinela&lt;/strong&gt; es una especie que vive en un ecosistema y a través de la interacción que puede tener con lo que lo rodea (se alimenta, respira, etc.), puede dar información sobre el estado de su hábitat. Esta información viene codificada como alteraciones en su fisiología o morfología. La almeja autóctona &lt;em&gt;Diplodon chilensis&lt;/em&gt; actuaría como &lt;strong&gt;centinela&lt;/strong&gt;, pues los cambios que se produzcan en su forma o en su fisiología darán indicios de la contaminación.&lt;/p&gt;
&lt;p&gt;Estos organismos tienen una aplicación muy clara: detectar cambios en un sistema. Estas variaciones pueden ser, por ejemplo, la aparición de contaminantes en un lago. &lt;em&gt;“Es una manera fácil y rápida de detectar de forma temprana cambios en el ecosistema que puedan alterar su equilibrio, evitando llegar a condiciones extremas”&lt;/em&gt;, explica Rocchetta y agrega: &lt;em&gt;“De esta manera se puede remediar fácilmente si hubo algún daño en el ambiente, ya que, seguramente, no ha sido irreversible”&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Las ventajas de los &lt;strong&gt;centinelas&lt;/strong&gt; frente a otros métodos de sondeo de contaminación es que los límites de detección son menores, es decir, permiten “ver” la contaminación aún cuando es muy baja y no se puede medir por métodos analíticos. Son como un termómetro que hacen posible conocer el estado de salud del ecosistema y ayudan a dar una advertencia temprano de alarma frente a una situación de contaminación.&lt;/p&gt;
&lt;h3 id=&#34;el-elegido&#34;&gt;El elegido&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2015/01/Fig1-300x225.jpg&#34; alt=&#34;Almejas de agua dulce, Diplodon chilensis, aclimatándose en los laboratorios de la FCEN-UBA. Foto: Marisol Yusseppone.:left&#34;&gt;“Diplodon &lt;em&gt;es un organismo abundante en la zona, presente en zonas de aguas muy limpias y en zonas contaminadas&lt;/em&gt;”, según explica Rocchetta. Esta almeja de agua dulce es una especie nativa y muy abundante en todos los ríos y lagos de Patagonia Norte. Es un bivalvo que habita sobre el lecho de ríos y lagos. &lt;em&gt;D. chilensis&lt;/em&gt; funciona como un colador: toma el agua del río o lago donde habita y la filtra con sus branquias, unas estructuras que le sirven para respirar y también para retener partículas que se encuentran en el agua. Luego mueve estas partículas filtradas hasta su boca y las ingiere. Esas partículas pueden ser microorganismos (su alimento natural), polvo, materia orgánica o fragmentos de suelo o rocas.&lt;/p&gt;
&lt;p&gt;En otras palabras, esta almeja, para alimentarse, filtra el agua del lago o río donde habita, acumulando y digiriendo los microoganismos o partículas que están en ella. &lt;em&gt;“Estos organismos tienen una alta tasa de filtración y, en general, la capacidad de acumular diferentes compuestos en sus tejidos, presentando niveles detectables por encima de los que se encuentra en el agua”&lt;/em&gt;, destaca Rocchetta. Cuando los contaminantes ingresan al sistema de la almeja pueden generar daño y hasta inducir una respuesta de defensa por parte del organismo. &lt;em&gt;“Dado que es un organismo filtrador capaz de acumular diversos contaminantes, es un excelente candidato para el control de efectos adversos por contaminación”&lt;/em&gt;, manifiesta Yusseppone.&lt;/p&gt;
&lt;h3 id=&#34;un-guardián-patagónico&#34;&gt;Un guardián patagónico&lt;/h3&gt;
&lt;p&gt;Para poder interpretar cuándo un &lt;strong&gt;organismo centinela&lt;/strong&gt; nos está dando una señal de contaminación se requiere de expertos que lo conozcan en profundidad y que cuenten con información sobre las características de los organismos de la misma especie que habiten en lugares no contaminados. &lt;em&gt;“Antes de pensar a&lt;/em&gt; Diplodon &lt;em&gt;como&lt;/em&gt; &lt;strong&gt;organismo centinela&lt;/strong&gt;, &lt;em&gt;tuvimos que hacer estudios preliminares”&lt;/em&gt;, afirma Yusseppone y añade: “&lt;em&gt;Hubo que explorar la zona para ver si encontrábamos al organismo en los lugares de interés y tuvimos que investigar niveles basales de las variables que queríamos medir”&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Necesitaron conocer cómo era la anatomía, el crecimiento y el metabolismo normal de las poblaciones para tener una referencia con la cual comparar los ambientes impactados y reconocer las señales de alarma. Los investigadores tuvieron que elegir que parámetros iban a medir y en que órganos. Otro factor que puede influir en los datos obtenidos es la temperatura, por lo tanto, las investigadoras también tuvieron que estudiar las poblaciones en distintas estaciones del año para asegurarse de nos malinterpretar un cambio relacionado con la variación estacional como una señal de contaminación.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;“Para evaluar el estado de diferentes cuerpos de agua de la Patagonia Norte, mediante el uso de&lt;/em&gt; Diplodon chilensis, &lt;em&gt;se debe conocer no sólo el estado general de esta especie sino también su respuesta ante diferentes tipos de contaminación y las posibles estrategias que este bivalvo desarrolla para soportar condiciones desfavorables”&lt;/em&gt;, enfatiza Yusseppone.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2015/01/Fig2-292x300.jpg&#34; alt=&#34;Investigadores y buzos asistentes en el lago Lacar (Neuquén) durante una campaña de recolección de Diplodon chilensis. Foto: Marisol Yusseppone.:left&#34;&gt; Uno de los sitios de interés para este estudio fue el lago Lacar, donde se realizan las descargas cloacales urbanas de la ciudad de San Martín de los Andes. &lt;em&gt;“Se puede encontrar a&lt;/em&gt; Diplodon &lt;em&gt;en ambientes con altas concentraciones de bacterias que provienen, justamente, de desechos cloacales, soportando la contaminación orgánica”&lt;/em&gt;, asevera Yusseppone.&lt;/p&gt;
&lt;p&gt;Los estudios sobre la población de almejas expuesta a los desechos cloacales de la ciudad de San Martín de los Andes han demostrado variaciones en la fisiología de la almeja, así como alteraciones en el crecimiento. “&lt;em&gt;Se han podido realizar todas las determinaciones bioquímicas de nuestro interés de manera fácil y utilizando diferentes tejidos y se pudieron detectar cambios en estos parámetros como respuesta a cambios ambientales producidos en la zona&lt;/em&gt;”, señala Rocchetta, quien ha estado trabajado con esta especie desde 2007. Actualmente las investigadoras continúan los estudios de caracterización sobre la especie, y evalúan la capacidad de &lt;em&gt;D. chilensis&lt;/em&gt; para tolerar la falta de oxígeno que se da en ambientes contaminados y las estrategias de supervivencia que adopta la almeja para sobrellevar la adversidad del ambiente. Los datos recabados en el transcurso de toda esta investigación permitirán el uso de &lt;em&gt;D. chilensis&lt;/em&gt; como un vigía de las aguas patagónicas, dando aviso de eventos de contaminación.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; &lt;a href=&#34;mailto:afassiano@qb.fcen.uba.ar&#34;&gt;afassiano@qb.fcen.uba.ar&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; &lt;a href=&#34;https://doi.org/10.1016/j.limno.2013.06.004&#34;&gt;&amp;quot;Growth, abundance, morphometric and metabolic parameters of three populations of Diplodon chilensis subject to different levels of natural and anthropogenic organic matter input in a glaciar lake of North Patagonia&amp;quot;&lt;/a&gt;. Iara Rocchetta, Betina J. Lomovasky, Maria S. Yusseppone, Sebastián E. Sabatini, Flavia Bieczynski, María C. Ríos de Molina, Carlos M. Luquet. Limnologica 44 (2014) 72–80.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>¿Quién se beneficia cuando usamos una computadora?</title>
      <link>https://ciencianet.com.ar/post/quien-se-beneficia-cuando-usamos-una-computadora/</link>
      <pubDate>Wed, 31 Dec 2014 23:07:34 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/quien-se-beneficia-cuando-usamos-una-computadora/</guid>
      <description>
        
          &lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Rodrigo J. Gonçalves.&lt;/strong&gt; Estación de Fotobiología Playa Unión.&lt;img src=&#34;https://ciencianet.com.ar/images/2017/12/conectarIgualdad-768x499.jpg&#34; alt=&#34;Imagen: http://www.argentina.ar&#34;&gt;&lt;/p&gt;
&lt;p&gt;A una computadora (y esto incluye PCs de escritorio, portátiles, smartphones, tablets, smartTVs y otros miles de dispositivos que nos rodean hoy en día) en realidad la usamos siempre a través de un programa (software). Pero tecnicismos aparte, ¿cómo aprendemos a usar un programa? Para determinadas actividades específicas (estadística, programación, base de datos, etc.) los programas se enseñan en la escuela y/o universidad como parte de una materia.&lt;/p&gt;
&lt;p&gt;Del universo de programas disponibles para realizar una determinada tarea, ¿qué programas se enseñan y por qué? ¿Quién lo decide y sobre la base de qué criterio? En la escuela secundaria los alumnos aprenden a escribir documentos de texto con el Word; algunos cálculos, planillas y gráficos usando Excel; introducción a bases de datos con Access (todos de Microsoft). Más adelante en el ambiente terciario y universitario estos programas se siguen usando y en carreras técnicas se enseña mucho MatLab (programa muy completo y poderoso para ingeniería, cálculo, análisis entre otras muchas aplicaciones). En muchas carreras de diseño se usa mucho el Photoshop, Illustrator (de Adobe), etc. En diseño y animación 3D se usa mucho 3DMax. En estadística todavía se enseña en muchos casos con programas como SPSS, y Statistica.&lt;/p&gt;
&lt;p&gt;En fin, distintos programas para distintas tareas, actividades o áreas del conocimiento. Adelanto mi opinión: esto es un &amp;quot;problema invisible&amp;quot;, un sinsentido en el que sin darnos cuenta hemos caído como sociedad en general y el sistema educativo en particular. Para el Estado, esto es una enorme contradicción y pasa en casi todo el mundo, no solo Argentina. ¿Por qué es un sinsentido? Porque todos los programas mencionados son productos privativos: legalmente solo se pueden instalar en una cantidad limitada de computadoras y por una cantidad limitada de tiempo, pertenecen a empresas privadas que los cobran miles y miles de dólares (y probablemente lo valen porque son productos excelentes). Estos programas pueden ser modificados y/o discontinuados (por la empresa que los producen) en cualquier momento y de la manera que la empresa dueña lo decida, con lo cual el usuario queda sujeto a los vaivenes “del mercado” (en el mejor de los casos) y atrapado con un programa que es el dominante en cierta área del conocimiento y que con el tiempo define los estándares (por ejemplo hoy en día todos sabemos lo que es un “documento Word”, se volvió una palabra natural del lenguaje cuando en realidad es un producto comercial, como cuando uno dice “una Gilette” para referirse a una hojita de afeitar , etc.).&lt;/p&gt;
&lt;p&gt;Por suerte, a diferencia de 20 años atrás, hoy en día existe para cada uno de los mencionados programas una alternativa libre, abierta y gratuita. Vamos a usar en esta nota la denominación “software libre” (SL). ¿Por qué, habiendo alternativas libres y abiertas, el sistema educativo todavía nos enseña, nos entrena y nos especializa usando programas privados?. Tomemos el caso de MatLab. MatLab es un programa excelente, casi el estándar de facto en muchas ramas de ingeniería. Básicamente por eso ha sido tradicionalmente usado para enseñar ingeniería, en casi todo el mundo. El resultado neto es que el Estado está, hace décadas, formando generaciones y generaciones de usuarios de MatLab. La mejor manera de apoyar un producto es usándolo y difundiéndolo. Dicho de otra forma: el Estado le provee, gratis, usuarios a MathWorks (empresa dueña de MatLab) que van a sostener el poderío y dominancia de su producto privado (MatLab) durante décadas.&lt;/p&gt;
&lt;p&gt;Tomemos el caso de una universidad europea: allá se paga la licencia del MatLab cada año, que cuesta literalmente miles y miles de dólares. Es decir que no solo la universidad está formando usuarios de MatLab (que cuando terminen la carrera van a ser profesionales expertos en un programa que no pueden pagar) sino que además le paga a MathWorks miles de dólares por las licencias para instalar y actualizar el MatLab. Un negocio redondo para MathWorks, un sinsentido para el Estado. Y ojo que no hablo de corrupción ni nada &amp;quot;raro&amp;quot;, es simplemente (nuestra) desidia y falta de reacción. Incluso en el caso de universidades más modestas que no pueden pagar la licencia, se enseñan programas privados, lo cual es aún más contradictorio: te entrenan para usar algo que 1) no podés pagar, 2) la universidad no puede pagar, 3) beneficia gratis (y en forma sostenida en el tiempo) a una empresa privada (normalmente extranjera) lo cual no redunda en beneficio para la comunidad.&lt;/p&gt;
&lt;p&gt;MatLab es solo un ejemplo de lo que ocurre con miles de programas que se enseñan rutinariamente en el sistema educativo de Argentina y el mundo. Pero lo más preocupante es que esto no solo ocurre con casos puntuales de software especializado sino que empieza bien temprano en nuestra formación como usuarios de informática, cuando nuestros chicos aprenden en la escuela a escribir documentos de texto con Word, planillas con Excel, base de datos en Access, etc. El resultado es el mismo: la escuela secundaria está creando, con dinero estatal (gratis para las empresas), generaciones de miles y miles de usuarios de sus programas privativos; estos serán por supuesto más fáciles de aprender (y por lo tanto preferidos) cuando uno va a la universidad o terciaria. Todo esto se traduce en un flujo continuo de usuarios para empresas como Microsoft, asegurándoles que sigan dominando el mercado por mucho tiempo.&lt;/p&gt;
&lt;p&gt;Este flujo está financiado e incentivado por el sistema educativo que aparentemente no presta atención a este problema. ¿Cuál es la solución para salir de este sinsentido? Por supuesto la respuesta es: ¡software libre! El Estado tiene que fomentar el uso del software libre, abierto y gratuito. Enseñemos &lt;a href=&#34;https://www.gnu.org/software/octave/&#34;&gt;Octave&lt;/a&gt;/&lt;a href=&#34;http://www.scilab.org/&#34;&gt;Scilab&lt;/a&gt; en vez de MatLab, usemos &lt;a href=&#34;http://es.wikipedia.org/wiki/Distribuci%C3%B3n_Linux&#34;&gt;Linux&lt;/a&gt; en vez de Windows, &lt;a href=&#34;https://es.libreoffice.org/&#34;&gt;LibreOffice&lt;/a&gt; en vez de Microsoft Office, enseñemos &lt;a href=&#34;http://www.r-project.org/&#34;&gt;R&lt;/a&gt; en vez de SPSS, &lt;a href=&#34;http://www.blender.org/&#34;&gt;Blender&lt;/a&gt; en vez de 3D Max, &lt;a href=&#34;https://inkscape.org/es/&#34;&gt;InkScape&lt;/a&gt; en vez de Illustrator, &lt;a href=&#34;http://gimp.es/&#34;&gt;GIMP&lt;/a&gt; en vez de Photoshop y así podría seguir.&lt;/p&gt;
&lt;p&gt;En muy, pero muy pocos casos se justifica usar un software privativo en vez de uno libre. Si la universidad puede pagar una licencia de miles de dólares, ¿por qué no donar esos miles de dólares para apoyar un programa libre y excelente como por ejemplo LibreOffice? Cada uno de esos pesos/dólares donados se va a convertir en una mejora para ese software y quedará disponible para la comunidad y generaciones venideras. Si el Estado y sistema educativo no hacen el cambio, es más difícil que los usuarios lo hagan por si mismos porque el que ya sabe usar un programa siempre tiene por supuesto reticencia a aprender desde cero otro programa.&lt;/p&gt;
&lt;p&gt;Por lo tanto lo que hay que hacer es empezar desde temprano en la educación (escuela primaria) y mantenerlo hasta el nivel universitario. La adopción de software libre ya es un hecho en varios ambientes, desde gubernamentales hasta científicos. Mucha gente se da cuenta que, dado que hoy en día un sistema de programas manejan absolutamente todo (desde trenes de alta velocidad hasta nuestro celular), no es buena idea que una institución entera (¡o conjunto de instituciones!) quede cautiva de un programa que no controlamos.&lt;/p&gt;
&lt;p&gt;El ámbito educativo sin embargo pareciera estar relegado en este sentido. En los últimos años se han llevado a cabo algunas iniciativas: en Argentina vale la pena mencionar que por medio del programa Conectar Igualdad se han distribuido, a alumnos de escuelas secundarias, aproximadamente 4 millones de notebooks [&lt;a href=&#34;#9&#34;&gt;9&lt;/a&gt;] que incluyen software libre además de algún software privativo. También hay casos en el sistema universitario que merecen destacarse por ejemplo la UTN Regional Buenos Aires ha declarado institucionalmente el interés estratégico del software libre.&lt;/p&gt;
&lt;p&gt;Estas iniciativas son muy bienvenidas y creo que van en la dirección correcta, sin embargo en mi opinión se necesita una decisión estratégica por parte del Estado y sistema educativo para que estos cambios positivos puedan amplificarse. Para ello se necesita que las acciones institucionales como las mencionadas permanezcan en el tiempo (por ejemplo que no queden asociadas al logro de un gobierno en particular sino que se transformen en política de estado). Ojalá los alumnos que hoy usan los programas libres gracias al Conectar Igualdad (por ejemplo) puedan mañana amplificar esto y la tendencia al software libre sea algo sustentable en el tiempo.&lt;/p&gt;
&lt;p&gt;Por último, quiero expresar que si bien esta nota menciona principalmente motivos económicos, esto es un tema que tiene numerosas aristas y hay más (y mejores) razones para usar software libre en educación (por ejemplo la justicia). Mi planteo está muy lejos de ser original, de hecho ya se han escrito artículos mucho más completos y contundentes y por divulgadores con más autoridad [&lt;a href=&#34;#1&#34;&gt;1&lt;/a&gt;] - [&lt;a href=&#34;#8&#34;&gt;8&lt;/a&gt;]. Invito al lector a consultar la bibliografía citada para explorar otras razones incluso más importantes que la económica [&lt;a href=&#34;#1&#34;&gt;1&lt;/a&gt;] - [&lt;a href=&#34;#8&#34;&gt;8&lt;/a&gt;]. Pero para terminar la nota basta pensar que si desde temprano aprendemos a manejarnos con programas de código abierto, libres y gratuitos, los beneficios quedan para nosotros y la gran comunidad de voluntarios que hacen del software libre una expresión de cultura e inteligencia colectiva.&lt;/p&gt;
&lt;h3 id=&#34;fuentes&#34;&gt;Fuentes:&lt;/h3&gt;
&lt;p&gt;&lt;a id=&#34;1&#34;&gt; 1. &lt;/a&gt; Why Educational Institutions Should Use and Teach Free Software. &lt;a href=&#34;https://www.gnu.org/education/edu-why.html&#34;&gt;https://www.gnu.org/education/edu-why.html&lt;/a&gt; (visitado 04 diciembre 2014).&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;2&#34;&gt; 2. &lt;/a&gt; Why Schools Should Exclusively Use Free Software. &lt;a href=&#34;https://www.gnu.org/education/edu-schools.html&#34;&gt;https://www.gnu.org/education/edu-schools.html&lt;/a&gt; (visitado 04 diciembre 2014).&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;3&#34;&gt; 3. &lt;/a&gt; Free Software Is Even More Important Now. &lt;a href=&#34;https://www.gnu.org/philosophy/free-software-even-more-important.html&#34;&gt;https://www.gnu.org/philosophy/free-software-even-more-important.html&lt;/a&gt; (visitado 04 diciembre 2014).&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;4&#34;&gt; 4. &lt;/a&gt; How Does Free Software Relate to Education?. &lt;a href=&#34;https://www.gnu.org/education/education.html&#34;&gt;https://www.gnu.org/education/education.html&lt;/a&gt; (visitado 04 diciembre 2014).&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;5&#34;&gt; 5. &lt;/a&gt; Video en castellano: ¿Cuál es la relación entre el software libre y la educación? &lt;a href=&#34;https://www.gnu.org/education/education.es.html&#34;&gt;https://www.gnu.org/education/education.es.html&lt;/a&gt; (visitado 04 diciembre 2014).&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;6&#34;&gt; 6. &lt;/a&gt; &lt;a href=&#34;https://schoolforge.net/&#34;&gt;https://schoolforge.net/&lt;/a&gt; (visitado 04 diciembre 2014)&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;7&#34;&gt; 7. &lt;/a&gt; Free and Open Source Software for Education. &lt;a href=&#34;http://fossee.in/&#34;&gt;http://fossee.in/&lt;/a&gt; (visitado 04 diciembre 2014).&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;8&#34;&gt; 8. &lt;/a&gt; Terbuc, M., &amp;quot;Use of Free/Open Source Software in e-education,&amp;quot; Power Electronics and Motion Control Conference, 2006. EPE-PEMC 2006. 12th International , vol., no., pp.1737,1742, Aug. 30 2006-Sept. 1 2006. doi: 10.1109/EPEPEMC.2006.4778656 &lt;a href=&#34;http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=4778656&amp;amp;isnumber=4778360&#34;&gt;http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=4778656&amp;amp;isnumber=4778360&lt;/a&gt; (visitado 04 diciembre 2014).&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;9&#34;&gt; 9. &lt;/a&gt; Conectar Igualdad. &lt;a href=&#34;http://www.conectarigualdad.gob.ar/&#34;&gt;http://www.conectarigualdad.gob.ar/&lt;/a&gt; (visitado 04 diciembre 2014).&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>La comunicación por SMS: un aliado en la enseñanza de la lengua</title>
      <link>https://ciencianet.com.ar/post/la-comunicacion-por-sms-un-aliado-en-la-ensenanza-de-la-lengua/</link>
      <pubDate>Thu, 23 Oct 2014 23:13:12 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/la-comunicacion-por-sms-un-aliado-en-la-ensenanza-de-la-lengua/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Lucía Cantamutto&lt;/strong&gt;. Universidad Nacional del Sur - CONICET - AAHD.&lt;/p&gt;
&lt;p&gt;Una rápida mirada a la ambigüedad de la situación nos coloca de frente al problema: el móvil se ha tornado en un elemento indispensable dentro de los efectos personales (tanto que existe un síndrome asociado a la ansiedad que genera su olvido, nombrada nomofobia) pero, dentro del contexto escolar, se convierte en una alteridad al que enfrentar o silenciar porque irrumpe e interrumpe. Cada escuela, o cada docente, toma decisiones protésicas que no se enraízan en el motivo inicial. Posicionarnos en los extremos niega la complejidad del asunto y es por ello que presentamos un recorrido sobre la potencialidad del teléfono celular en el marco de la enseñanza de la lengua. En particular, focalizamos en dos aspectos: i) en las posibilidades que ofrece el teléfono como herramienta para el aula y ii) en el universo discursivo del estilo electrónico desde donde pueden recuperarse múltiples aspectos de la estructura de lengua.&lt;/p&gt;
&lt;h3 id=&#34;explotar-el-aula-en-la-tecnología-un-cambio-en-el-modelo&#34;&gt;Explotar el aula en la tecnología: un cambio en el modelo&lt;/h3&gt;
&lt;p&gt;Los informes a gran escala en torno al beneficio obtenido tras la incorporación de la tecnología en al aula no arrojan resultados promisorios. Las políticas educativas, bien intencionadas, no siempre pueden aplicarse en cada contexto escolar específico y los efectos pueden ser tragicómicos: escuelas rurales con computadoras sin conectividad, escuelas técnicas con enchufe insuficientes para cargar las netbooks, disposición y forma de los bancos no adecuada para el trabajo con pantallas. Este último aspecto resulta, al menos, paradójico. Los pupitres han ido adecuándose a las necesidades tecnológicas y didácticas del alumno: desde el hueco para el tintero, el plano inclinado para el dibujo técnico, el banco individual a las mesas de dos, cuatro y seis estudiantes, el tamaño adecuado a un cuaderno pequeño o a una carpeta; sin embargo, los dispositivos electrónicos no encontraron aún su lugar en la cartografía áulica.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2014/10/celularaula.jpg&#34; alt=&#34;Fuente: http://portal.educ.ar/:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Situados en una ecología del aprendizaje con anacronismos y oportunos cambios, la cotidianeidad de las aulas presenta situaciones de aprendizaje que se conjugan con el entramado de instancias donde los sujetos aprenden: es el cambio de paradigma hacia las situaciones o entornos personales de aprendizaje y a la concepción del aprendizaje como una experiencia a lo largo de toda la vida. Nos alejamos, entonces, de la escolarización universal para centrarnos en los trayectos personales.&lt;/p&gt;
&lt;p&gt;Incorporar, al contexto del aula, propuestas que sinteticen las situaciones de aprendizaje fuera de la escuela con los contenidos de dentro, permitirá que se potencie el aprendizaje. La pregunta ya no es si debe o no incluirse el teléfono en el contexto del aula, sino cómo incorporarlo en la dinámica escolar (Botha et al., 2012:2).&lt;/p&gt;
&lt;p&gt;Todos los elementos que llevan los estudiantes a la escuela tienen un uso prescriptivo y un uso lúdico, el temor a este proceso no debe detener a ningún docente. El teléfono móvil, del mismo modo, requiere de diferenciar su potencia como herramienta dentro del aula y como elemento portador de prácticas sociales y comunicativas de los chicos.&lt;/p&gt;
&lt;p&gt;Una de las ventajas principales del teléfono móvil es su bajo costo en relación con otros dispositivos portátiles y en función de las diferentes funciones que ofrece al usuario. En otras palabras, “dejaron hace tiempo de ser meros mediadores comunicativos para convertirse en centros de información, comunicación, registro y edición de audio y video, depósito de recursos y contenidos, etc” (Cantillo Valero et al., 2012: 9).&lt;/p&gt;
&lt;h3 id=&#34;estilo-electrónico-convergencia-de-competencias&#34;&gt;Estilo electrónico: convergencia de competencias&lt;/h3&gt;
&lt;p&gt;Admitir la escritura en medios electrónicos como una práctica de sujetos/usuarios supone la interacción entre dispositivos particulares (con sus constricciones propias) y sujetos con una trayectoria formativa particular. De tal manera, la escritura electrónica se da en una trama de relaciones entre niveles de alfabetización y de literacidad digital (Cassany, 2012), de cada hablante, y las características de la(s) plataforma(s) digitales que median la comunicación.&lt;/p&gt;
&lt;p&gt;En este complejo entramado, las producciones realizadas revelan el grado de conocimiento de competencias comunicativas y competencias tecnológicas (Palazzo, 2008). En este sentido, los fenómenos lingüísticos de mayor interés observados (Cantamutto, 2009-2014) son aquellos que apelan funcionalmente a la rapidez, economía lingüística y brevedad; a la expresividad y a la claridad como tendencia predominante en la construcción y dinámica discursiva de los SMS; características propias dentro del llamado estilo electrónico caracterizado “como un estilo económico, en el que el objetivo fundamental de su organización retórica se centra en un principio de economía” y que “en sus manifestaciones más interactivas, se halla, no obstante marcado por la acumulación de recursos expresivos” (Vela Delfa, 2005:670).&lt;/p&gt;
&lt;p&gt;En este sentido, el teléfono móvil en el aula se convierte en una manera de recuperar y hacer explícitas las estrategias comunicativas (y los conocimientos lingüísticos) que los estudiantes utilizan a diario en sus intercambios. La hipótesis que subyace a este recorrido es que, en la economía propia de los SMS, los elementos lingüísticos presentes condensan en sí más funciones (pragmáticas, especialmente) de las que en otros contextos tendrían. Desarticular las estrategias y recursos de la comunicación por SMS permite, entonces, reconocer cuáles son los elementos preferentes de los hablantes para alcanzar sus metas comunicativas. De esta manera, algunas de las actividades que se pueden realizar en el aula son: i) reglas del uso del predictivo, ii) desambiguación de enunciados, iii) estrategias comunicativas, iv) puntuación, v) reglas intuitivas de acortamiento de palabras, vi) análisis de anáforas, correferencias y elipsis, vii) uso de signos diacríticos y entonación.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; Este estudio forma parte de la investigación doctoral en curso financiada por una beca del Consejo Nacional de Investigaciones Científicas y Tecnológicas, para optar por el Doctorado en Letras en la Universidad Nacional del Sur. Pertenece a la Asociación Argentina de Humanidades Digitales. Artículos relacionados publicados por la autora se pueden encontrar en &lt;a href=&#34;https://www.researchgate.net/profile/Lucia_Cantamutto&#34;&gt;este enlace&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; &lt;a href=&#34;mailto:luciacantamutto@gmail.com&#34;&gt;luciacantamutto@gmail.com&lt;/a&gt; &lt;strong&gt;Referencias:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cassany, D. (2012) &lt;em&gt;En línea. Leer y escribir en la red&lt;/em&gt;, Madrid: Anagrama.&lt;/li&gt;
&lt;li&gt;Palazzo, G. (2008),&amp;quot;Consideraciones sobre el uso y las representaciones sociales del discurso juvenil en el ciberespacio”, &lt;em&gt;Actas del 10º Congreso REDCOM, Conectados, Hipersegmentados y Desinformados en la Era de la Globalización&lt;/em&gt;, Salta: Universidad Católica de Salta.&lt;/li&gt;
&lt;li&gt;Botha, A., J. Batchelor, J. Traxler, I. De Waard y M. Herselman (2012), “Towards a Mobile Learning Curriculum Framework”, IST-Africa Conference Proceedings, (pp. 1-9).&lt;/li&gt;
&lt;li&gt;Cantillo Valero, C., M. Roura Redondo y A. Sánchez Palacín (2012), “Tendencias actuales en el uso de dispositivos móviles en educación”, La Educ@ación, 147.&lt;/li&gt;
&lt;li&gt;Vela Delfa, C. (2005), El correo electrónico: un nuevo género en nacimiento, Madrid: Universidad Complutense de Madrid en &lt;a href=&#34;http://www.galanet.be/publication/%20fichiers/tesis_cristina_vela_delfa.pdf&#34;&gt;http://www.galanet.be/publication/ fichiers/tesis_cristina_vela_delfa.pdf&lt;/a&gt; (consulta: marzo 2012).&lt;/li&gt;
&lt;/ul&gt;

        
      </description>
    </item>
    
    <item>
      <title>Develando los misterios de la germinación - Alemania y la ciudad de Santa Fe comparten investigaciones en el IAL</title>
      <link>https://ciencianet.com.ar/post/develando-los-misterios-de-la-germinacion-alemania-y-la-ciudad-de-santa-fe-comparten-investigaciones-en-el-ial/</link>
      <pubDate>Wed, 01 Oct 2014 23:30:49 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/develando-los-misterios-de-la-germinacion-alemania-y-la-ciudad-de-santa-fe-comparten-investigaciones-en-el-ial/</guid>
      <description>
        
          &lt;p&gt;Entrevista del Lic. &lt;strong&gt;Enrique A. Rabe&lt;/strong&gt; (ÁCS/CONICET Santa Fe).&lt;/p&gt;
&lt;p&gt;La Dra. Jessica Schmitz, de Düsseldorf, especialista en bioquímica vegetal, realiza una pasantía en el Laboratorio de Biología Molecular que dirige el Dr. Daniel González -investigador del CONICET en el Instituto de Agrobiotecnología del Litoral (UNL/CONICET) y docente-investigador universitario-.&lt;/p&gt;
&lt;p&gt;“La pasantía se relaciona con un tópico específico: el Dr. González y la Dra. Verónica Maurino obtuvieron fondos del Servicio Alemán de Intercambio Académico (Daad) y del Conicet para integrar investigación y conocimientos entre Santa Fe y Düsseldorf. Esta es mi primera experiencia fuera de Alemania. Además, participaré en el XV Congreso Latinoamericano de Fisiología Vegetal (FV) y en la XXX Reunión Argentina de FV que se realizarán en Mar del Plata dentro de pocos días”, narra la pasante.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;¿En qué tema investiga?&lt;/strong&gt; En mi país, trabajo en el grupo de la Dra. Maurino -rosarina, en Alemania desde 1998-, abocado a la fisiología y biotecnología molecular vegetal. Investigamos en la caracterización de la participación de 2-hidroxiácidos en el metabolismo de las plantas, en particular en maíz y semilla de ricino, a fin de comprender su presencia en la producción de biomasa vegetal y en el ciclo vital de una planta. Estudio las secuencias metabólicas y las enzimas que se necesitan en los primeros días de la vida de una planta, por ejemplo, cuando las semillas comienzan a germinar y están dependiendo de las reservas de energía de la semilla. En el laboratorio tenemos algunos estudiantes y graduados que trabajan en diferentes temas relacionados con la bioquímica de las plantas, actividad que financian la DFG (alemana) y la Comisión Europea.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2014/10/Jessica-300x297.jpg&#34; alt=&#34;Dra. Jessica Schmitz, pasante del Laboratorio de Biología Molecular, Instituto de Agrobiotecnología del Litoral (UNL/Conicet).:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;¿Por qué es importante estudiar este tema?&lt;/strong&gt; El proceso de germinación y la movilización de las reservas de energía en los tejidos heterotróficos aún no se comprende del todo. La recolección de información sobre dichas reservas (y la síntesis) que alimentan el embrión de la planta puede ayudar a mejorar la germinación de la planta y la producción de biomasa.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;¿Por qué lo eligió?&lt;/strong&gt; Para mí, las plantas son los organismos más fascinantes de la Tierra. Al estar fijas a un sustrato necesitan hacer frente a una gran cantidad de factores tales como calor, frío, sequía y elevadas intensidades lumínicas. En la fase reproductiva, la semilla experimenta una etapa de vida latente en la cual puede permanecer durante algunas semanas, o aun cientos de años, antes de que el agua conduzca de nuevo a la germinación. En este proceso, al embrión se lo provee de todo lo que necesita - en forma independiente- y que está incorporado en el proceso de maduración. Hasta ahora, caracterizamos las funciones de algunas proteínas en las semillas, algo que antes no estaba descripto en la semilla en germinación. Creemos que esas funciones son importantes para la provisión de aminoácidos.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;¿Qué opinión le merece el IAL?&lt;/strong&gt; El edificio es nuevo y puede, sin dudas, competir con los laboratorios alemanes. El equipamiento es bueno pero un poco desactualizado. El grupo del Dr. González colabora mucho conmigo y me incorporaron como miembro desde el primer día, brindándome su apoyo y trabajando como equipo en un proyecto. En mi país, cuando un estudiante practica, la mayoría de las veces debe hacerlo solo.&lt;/p&gt;
&lt;p&gt;Heinrich-Heine Universität y Cluster of Excellence in Plant Sciences. Entrevista en inglés y traducción: Lic. Enrique A. Rabe (ÁCS/CONICET Santa Fe).&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Investigar y divulgar, para aprender ciencia</title>
      <link>https://ciencianet.com.ar/post/investigar-y-divulgar-para-aprender-ciencia/</link>
      <pubDate>Mon, 24 Mar 2014 23:32:17 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/investigar-y-divulgar-para-aprender-ciencia/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Daniela Lorena Lamas&lt;/strong&gt;. Departamento de Química e Ingeniería Química. Planta Piloto de Ingeniería Química- CONICET – Universidad Nacional del Sur.&lt;/p&gt;
&lt;p&gt;Una conducta del ser humano que se ha visto afectada por la tecnología y cultura que él mismo ha creado, es el aprendizaje formal. Algunas décadas atrás, estudiar y aprender, si bien significaba un reto de magnitud, no competía con otras opciones mucho más atractivas a la hora de dejar que nuestro cerebro emocional decidiera si acomodaría su cuerpo en una silla para comenzar con la tarea o con la lectura.&lt;/p&gt;
&lt;p&gt;La cultura, por su parte, acompañaba el esfuerzo cognitivo de aprender otorgando significado y sentido simbólico a los contenidos curriculares y también en gran medida a las instituciones educativas y sus docentes y directivos. Los avances de las tecnologías de la información y comunicación generan impactos significativos en los procesos educativos, permitiendo el acceso a nuevos modelos de enseñanza.&lt;/p&gt;
&lt;p&gt;Marcial Perez, neuropsicoeducador y director de la Empresa Neuraltis, indica que “la creatividad no se produce dentro de la cabeza de las personas, sino en la interacción entre las personas y un contexto sociocultural”. Este contexto es el que ha promovido el desarrollo de un plan de enseñanza a través de actividades de investigación y extensión.&lt;/p&gt;
&lt;p&gt;El proyecto se inició con actividades de divulgación de la Química de los Alimentos en la cátedra Tecnología de Alimentos III 2010 de las ciudades de Tres Arroyos y Coronel Suárez, correspondientes a la carrera Técnico Universitario en Emprendimientos Agroalimentarios que la Universidad Provincial del Sudoeste (UPSO), comparte mediante el sistema PEUZO con la Universidad Nacional del Sur (UNS) de Bahía Blanca, Argentina.&lt;/p&gt;
&lt;p&gt;Paralelamente se desarrollaron talleres de motivación al aprendizaje destinados a distintas carreras de la Universidad Nacional del Sur. Consecuentemente, se realizaron planes de enseñanza en cursos de temáticas afines en Jornada Universitarias. En todos los casos, la actividad principal se centró en un análisis de requerimientos y necesidades del grupo de trabajo, que determinó las actividades a desarrollar. En segundo término se delinearon varias actividades, propuestas en conjunto por el alumnado y el cuerpo docente. En la etapa siguiente se formuló una exposición y evaluación oral sobre lo aprendido. Por último se realizó una evaluación integral. Se definió un campo de acción, se adjudicaron grados de libertad, y se planteó un tiempo estipulado de resultados obtenidos.&lt;/p&gt;
&lt;p&gt;La actividades comprendieron la investigación de un proceso químico de interés actual a través de búsqueda bibliográfica tradicional, seguido de la elaboración de un informe y una exposición oral. Por otro lado, se realizó el seguimiento del deterioro de un alimento elegido al azar por cada alumno, se estipuló un tiempo de observación, y se discutió junto a todo el grupo las posibles causas y consecuencias de los cambios ocurridos. Paralelamente otro grupo de alumnos desarrolló un alimento innovador, y explicó sus usos, ventajas y alternativas de aplicación. El conocimiento a campo se realizó a través de visitas a plantas industriales de la zona.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2014/03/divulg-01-768x391.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;las-destrezas-logradas&#34;&gt;Las destrezas logradas&lt;/h3&gt;
&lt;p&gt;La forma de presentación del material permitió que los estudiantes desarrollen la capacidad de síntesis, de organización, y planificación, el razonamiento crítico, la creatividad, la toma de decisiones y el aprendizaje autónomo. A su vez, el trabajo permitió a los alumnos ampliar conocimientos aportados como una base o posibilidad para ser originales en el desarrollo y/o aplicación de ideas, a menudo en un contexto de investigación, con la idea de que desplieguen sus capacidades para elaborar trabajos de investigación, desarrollo e innovación como competencias específicas dentro de la disciplina de la química, y la tecnología de los alimentos. En las exposiciones orales, los alumnos descubrieron la imagen que proyectan al comunicarse, y perdieron el temor a hablar en público, luego de varios ensayos previos.&lt;/p&gt;
&lt;h3 id=&#34;reflexión&#34;&gt;Reflexión&lt;/h3&gt;
&lt;p&gt;El aprendizaje se concibe como uno de los fenómenos más complejos de la existencia del ser humano, ya que se trata de un proceso de cambios. La enseñanza tradicional utiliza los métodos didácticos comúnmente conocidos: el analítico, sintético, inductivo y deductivo que son los métodos generales lógicos, es decir un método rígido, no dinámico, y poco propicio para la innovación y el aprendizaje. En una clase tradicional, una persona habla mientras que las demás escuchan. Lo importante es la transmisión de conocimientos, unidireccional.&lt;/p&gt;
&lt;p&gt;En la concepción moderna, los momentos del &amp;quot;pensar&amp;quot; es &amp;quot;pensar para actuar&amp;quot;, pensar con fin. Los métodos didácticos deberán asentarse sobre una nueva lógica, la que explica la estructura del método científico: delimitar o definir el problema, buscar los datos necesarios, formular hipótesis o alternativas de solución, búsqueda de nuevos datos para cada una de las hipótesis, previsión de las consecuencias en caso de elegir una determinada alternativa, prueba o comprobación de las alternativas.&lt;/p&gt;
&lt;p&gt;En la concepción moderna el eje de la actividad se traslada constantemente, y así, los educandos conocen y comprenden el fin y sentido del aprendizaje, y asumen responsabilidades para la obtención de los mismos. Se manifiesta de este modo un aprendizaje por descubrimiento propio, que estimula la creatividad, la innovación y motiva a seguir conociendo. Este método basal es el innato, el sistema que el ser humano admite desde su nacimiento. El valor agregado del docente, se destaca en la labor indispensable para conseguir la actitud favorable hacia el aprendizaje, despertar y afianzar motivaciones duraderas y una acentuada acción de revalorización humana.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajos originales:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&amp;quot;La enseñanza de la química de los alimentos a través de actividades de investigación y extensión&amp;quot;. Daniela Lorena Lamas.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;quot;Taller de motivación al aprendizaje para alumnos de ingeniería química&amp;quot;. Daniela Lamas y Marcial Pérez. Disponibles en &amp;quot;Enseñanza y Divulgación de la Química y la Física&amp;quot;. Ibergarceta, Madrid (2012), ISBN 978-84-1545-224-9, accesible en este &lt;a href=&#34;http://quim.iqi.etsii.upm.es/vidacotidiana/EnsenanzayDivulgacion%282012%29.pdf&#34;&gt;enlace&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; &lt;a href=&#34;mailto:dlamas@plapiqui.edu.ar&#34;&gt;dlamas@plapiqui.edu.ar&lt;/a&gt; &lt;a href=&#34;mailto:Contacto@neuraltis.com&#34;&gt;Contacto@neuraltis.com&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Regeneración de tejidos. Entrevista a Osvaldo Chara</title>
      <link>https://ciencianet.com.ar/post/regeneracion-de-tejidos-entrevista-a-osvaldo-chara/</link>
      <pubDate>Fri, 07 Mar 2014 23:55:58 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/regeneracion-de-tejidos-entrevista-a-osvaldo-chara/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Luis Pugnaloni.&lt;/strong&gt; Dpto. Ing. Mecánica, UTN-Fac. Reg. La Plata.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2014/03/chara1-225x300.jpg&#34; alt=&#34;Osvaldo Chara junto al cluster de cálculo del IFLYSIB.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Osvaldo Chara trabaja en el Instituto de Física de Líquidos y Sistemas Biológicos (CONICET-UNLP) en La Plata. Desde hace unos años coopera con colegas de Alemania, Suiza, EEUU y España en un proyecto que tiene como objetivo comprender el proceso de regeneración de tejidos en especies con capacidad de crecer un miembro amputado. Además del interés académico de una capacidad tan peculiar de algunos seres (como las salamandras), resulta esencial comprender estos mecanismos para nuevos tratamientos médicos que se basan en la regeneración de tejidos para tratar diversas enfermedades o sus secuelas. Recientemente Osvaldo publicó un artículo en la revista Science con un valioso avance en la comprensión del proceso de regeneración (ver &lt;a href=&#34;http://dx.doi.org/10.1126/science.1241796&#34;&gt;aquí&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;- La ciencia ficción siempre inspira. Sé que te apasionan los comics ¿estudiás este problema de regeneración motivado por historias fantásticas de seres extraterrestres que pueden recuperar partes de su cuerpo luego de una amputación?&lt;/strong&gt; - Es cierto que me gustan los comics, o historietas como decimos en Argentina. Un buen ejemplo en esa dirección es Spiderman. Uno de los enemigos del hombre araña es, precisamente, &lt;em&gt;the lizard&lt;/em&gt; (conocido en estas latitudes como el lagarto). Originalmente, este villano era un biólogo que precisamente investigaba la habilidad que ciertos reptiles tienen para regenerar sus patas, motivado por la ausencia de su brazo derecho. Él va a inyectarse un suero extraído de un lagarto para poder regenerar su brazo. Infortunadamente eso lo transforma en un monstruo.&lt;/p&gt;
&lt;p&gt;De todas formas, la motivación por la cual comencé a interesarme en estos temas fue más bien por azar. En la época en que estaba haciendo mi tesis doctoral, en la UBA, me contactó un médico cirujano (el Dr. Daniel Wainstein del Hospital Tornu) que estaba lidiando con un problema relacionado con el daño de tejidos. Él estaba trabajando en un método que implicaba aplicación de vacío, lo cual implicaba algún conocimiento de física, así que lo llamo por teléfono al Prof. Mario Parisi, que es un referente en la biofísica argentina. Yo estaba, de casualidad, en la misma oficina de Mario, discutiendo con él sobre la física de un problema de transporte de agua. Cuando Mario entendió que se precisaba alguien que supiera de “presiones”, me mira a mí y le dice al interlocutor que tiene que hablar conmigo.&lt;/p&gt;
&lt;p&gt;Así comencé a interiorizarme en este tipo de trabajos. Por lo demás, me dedico a hacer modelos matemáticos de sistemas biológicos complejos. La regeneración de tejidos es un excelente ejemplo de estos últimos.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;- ¿Qué seres de nuestro mundo real pueden regenerar partes de su cuerpo?&lt;/strong&gt; - Si bien no todas las especies pueden regenerar, hay algunas que no están necesariamente cerca filogenéticamente que si pueden hacerlo. Entre ellas podes encontrar Hydra, Axolotl, Planaria. A su vez hay animales que, en ciertos periodos embrionarios pueden regenerar una parte de su cuerpo. Un ejemplo de estos es la mosca de la fruta (Drosophila melanogaster).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2014/03/chara2.jpg&#34; alt=&#34;Axolotl. Gentileza del Center for Regenerative Therapies Dresden (CRTD). Esta fotografía sólo puede ser reproducida con autorización del tenedor del Copyright Center for Regenerative Therapies Dresden&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;- En tu artículo se dice algo sobre que las células de las yemas de los dedos son diferentes a las del resto del brazo. ¿Entendí bien? ¿Eso pasa en los humanos o en todos los seres con dedos? ¿Hay otras partes del cuerpo con células así?&lt;/strong&gt; - En las células de las patas de los animales se manifiesta una propiedad que podríamos llamar “distalización”. Si la célula se encuentra más lejos del hombro (más distal), esta propiedad se intensifica. Esencialmente, esta propiedad se ve reflejada en la expresión de una serie de genes (o sea por la generación o no de ciertas proteínas codificadas en esos genes). Esto sucede en los miembros de diversos animales, incluido el hombre.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;- En tu artículo aparece mucho la palabra &amp;quot;blastema&amp;quot; ¿Qué es eso?&lt;/strong&gt; - Primero valdría la pena entender que las células pueden ser más o menos diferenciadas. Cuando una célula es poco diferenciada, puede reproducirse mucho y no se especializa en una función determinada. Un ejemplo de esto son las células madre (o &lt;em&gt;stem cells&lt;/em&gt;, en la literatura angloparlante). Las células más diferenciadas pierden un poco la capacidad de reproducirse y se especializan en determinadas funciones. Ejemplos de estas son las neuronas, los glóbulos rojos, etc.&lt;/p&gt;
&lt;p&gt;Cuando se amputa la pata de un Axolotl, las células de los tejidos que formaban pate de la pata a la altura de la amputación (musculo, piel, etc.) están bastante diferenciadas. Al amputar, estas células se de-diferencian y forman células poco diferenciadas como las células madre y se cubren por otras que forman una capa de células epiteliales. Todo esto (las células de-diferenciadas y la capa que las rodea) es lo que se conoce como blastema.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;- Si me lastimo superficialmente generalmente se me forma una cicatriz, ¿es diferente este proceso de la regeneración de una cola completa de una salamandra? ¿Por qué?&lt;/strong&gt; - Bueno, la respuesta corta es que no lo sabemos. Es cierto que cuando un humano se lastima en la piel, por ejemplo, en la herida se desencadena el proceso de cicatrización. Una hipótesis que concebimos es que el proceso de regeneración de alguna manera apareció tempranamente en la evolución y que luego desapareció. Lo cierto es que los procesos regenerativos son muy costosos, así que, tal vez, la cicatrización es una suerte de solución de compromiso evolutivo que especies como la nuestra tienen.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;- Entiendo que tu trabajo demuestra finalmente que el mecanismo de regeneración de una pata para una salamandra mexicana (el axolotl) es uno de los dos que se han propuesto en forma teórica (especificación progresiva versus intercalación de segmentos). ¿Podrías explicar estos dos mecanismos teóricos?&lt;/strong&gt; - En el desarrollo normal de un brazo nuestro, o en el caso de un Axolotl, de una pata, las células van adquiriendo identidades desde el tronco hacia afuera (del hombro a los dedos). Se van haciendo progresivamente más “distales”, como te decía antes. En los procesos regenerativos de la pata de un Axolotl, esto es, luego de amputarlo, se creía que el mecanismo que operaba era de intercalación. De acuerdo a este mecanismo, las células cercanas a la zona de amputación, adquirían una característica extremadamente distal. En otras palabras, se transformaban en células típicas de los dedos.&lt;/p&gt;
&lt;p&gt;Entonces, la vecindad entre las células inmediatamente cercanas a estas, y estas células típicas de los dedos recién formadas inducia que las primeras se reproduzcan rápidamente y vayan adquiriendo características intermedias entre ambos tipos de células, es decir “intercalen”. El mecanismo alternativo, que encontramos nosotros, es que las células cercanas a la zona de amputación van, gradualmente, adquiriendo características más distales.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;- ¿Cuál fue tu trabajo dentro del equipo de investigación? ¿Qué formación tuviste para poder hacer este tipo de cosas?&lt;/strong&gt; - Nosotros participamos en distintas áreas relacionadas con lo que hoy se llama biología de sistemas. Esencialmente hicimos análisis de datos experimentales, que incluye análisis estadístico así como análisis de imágenes, y modelado matemático. Mi formación es la que me brindo la universidad pública argentina, tanto la Universidad de Buenos Aires como la Universidad Nacional de la Plata. Estudié farmacia, bioquímica y física cuando era joven y luego hice un doctorado en biofísica seguido de un postdoctorado en física.&lt;/p&gt;
&lt;p&gt;Si bien comencé el doctorado trabajando en biofísica y fisiología experimental, rápidamente me di cuenta que lo que realmente me encanta es hacer modelado matemático. Pero mi formación experimental previa me es fundamental para trabajar en combinación con grupos experimentales. Por eso, el trabajo de modelado en biología de sistemas lo pensamos de forma de involucrarnos fuertemente con la problemática biológica que estamos estudiando, tanto desde el punto de vista de la pregunta y el marco epistemológico de esta como desde el punto de vista de las técnicas experimentales empleadas.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;- ¿Qué te gustaría comprender sobre la regeneración de tejidos que imaginás que puede tomar 10 o 20 años en llegar a dilucidarse?&lt;/strong&gt; - Las preguntas que nos gustaría responder en ese plazo están relacionadas con la posición evolutiva de los procesos regenerativos así como el vínculo que estos procesos tengan con otros procesos de la fisiología de estos animales. Está claro que no todos los animales pueden regenerar, ¿por qué hay animales que pueden regenerar y otros, como los humanos, que no pueden hacerlo? ¿Será que el proceso de regeneración apareció más temprano en la evolución luego se perdió, como te mencionaba antes? ¿La regeneración se produce mediante un mecanismo único compartido por las especies que te mencionaba antes?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;- ¿Cuáles serán tus siguientes pasos en esta línea de investigación?&lt;/strong&gt; - Nuestra propuesta es modelar los procesos regenerativos que tienen lugar en los distintos animales capaces de hacerlo. Para ello tenemos que desarrollar modelos que sean capaces de reproducir datos experimentales obtenidos hasta el presente y hacer predicciones que sean contrastadas por nuestros colaboradores experimentales. Todo esto implica un trabajo importante, imagínate que el descubrimiento oficial de que un animal, la Hydra puede regenerar se lo debemos a Abraham Trembley en 1744. Luego, trataremos de ver si estos modelos pueden clasificarse de forma de identificar características que apunten a un mecanismo común o no y al mismo tiempo, trataremos de identificar si hay una línea temporal evolutiva subyacente.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;- ¿Cuáles serán tus siguientes pasos en cuanto a tu carrera profesional?&lt;/strong&gt; - Me apasiona la ciencia y la docencia en la frontera entre la biología y la física. Planeo seguir encaminando mis pasos a seguir trabajando en problemas localizados en esta frontera y dar clases en alguna universidad para entusiasmar más gente en el camino. Como hace poco que volví al país, está claro que seguiré haciendo este tipo de trabajo científico, porque el CONICET siempre me abrió sus puertas. Sera cuestión de encontrar una Universidad a la que también le interese este tipo de investigación y docencia.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Las enzimas, una alternativa bio al refinado de aceites</title>
      <link>https://ciencianet.com.ar/post/las-enzimas-una-alternativa-bio-al-refinado-de-aceites/</link>
      <pubDate>Tue, 25 Feb 2014 00:08:00 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/las-enzimas-una-alternativa-bio-al-refinado-de-aceites/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Daniela Lorena Lamas&lt;/strong&gt;. Departamento de Química e Ingeniería Química. Planta Piloto de Ingeniería Química- CONICET – Universidad Nacional del Sur.&lt;/p&gt;
&lt;p&gt;Desde sus inicios, la biotecnología ha revolucionado diferentes procesos productivos y ha hecho realidad muchas de las expectativas creadas en su entorno para la disminución del impacto ambiental. Entre los sectores donde ha tenido un particular desarrollo se destaca el sector alimentación, con una importante aplicación de la tecnología enzimática a la industria aceitera.&lt;/p&gt;
&lt;p&gt;Los mercados internacionales de aceite comestible se caracterizan por la alta competitividad y las crecientes exigencias de calidad, lo que conlleva a la mejora de métodos para la caracterización y control de calidad de las materias primas, productos y subproductos, y a la implementación y desarrollo de tecnologías o métodos alternativos que beneficien las condiciones de procesamiento y almacenamiento del producto final.&lt;/p&gt;
&lt;p&gt;La calidad y estabilidad del aceite de girasol resultan fundamentales para su aceptación y comercialización, éstas propiedades se ven influenciadas por la presencia de algunos componentes minoritarios como tocoferoles, ácidos grasos libres, fosfolípidos, ceras y metales que tienen propiedades antioxidantes y pro-oxidantes. Históricamente, estos componentes se eliminaban mediante procesos físicos o químicos, y en las últimas décadas se han investigado y desarrollado metodologías alternativas que además de ser eficientes y seguras no perjudican el medio ambiente.&lt;/p&gt;
&lt;p&gt;En este sentido, una de las estrategias más competentes es la utilización de enzimas que son catalizadores biológicos altamente específicos, biodegradables y fácilmente removibles. En el año 1992, las empresas Lurgi y Röhm patentaron un proceso para eliminar los fosfolípidos presentes en el aceite, utilizando enzimas. Con el avance de la biotecnología, este proceso denominado desgomado enzimático protagonizó, en la última década, investigaciones mundiales. El objetivo principal se enfocó en lograr alta eficiencia en el proceso a través de la reducción de fosfolípidos, y un aumento en el rendimiento de aceite.&lt;/p&gt;
&lt;h3 id=&#34;aplicación-enzimática&#34;&gt;Aplicación enzimática&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2014/02/enzimas-01.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Durante las IX Jornadas Regionales de Estudiantes de Química e Ingeniería Química &lt;a href=&#34;#1&#34; title=&#34;1&#34;&gt;[1]&lt;/a&gt; desarrolladas en la Universidad Nacional del Sur de la ciudad de Bahía Blanca estudiamos, junto a un grupo de alumnos, el cambio producido por las enzimas sobre las características organolépticas del aceite crudo de girasol procesado.&lt;/p&gt;
&lt;p&gt;Los ensayos de desgomado enzimático se realizaron en reactores batch en paralelo, utilizando dos enzimas comerciales diferentes, fosfolipasas y aciltransferasas, y una mezcla de las mismas. El agregado de enzimas se realizó cuando la temperatura del baño de agua conectado al sistema, marcó 50 °C. El análisis realizado reveló que las enzimas modificaban drásticamente el color, la apariencia y consistencia del aceite crudo, pero no afectaba el olor característico.&lt;/p&gt;
&lt;p&gt;Paralelamente, en un trabajo reciente &lt;a href=&#34;#2&#34; title=&#34;2&#34;&gt;[2]&lt;/a&gt;, fue resumido el efecto del desgomado enzimático sobre los metales y el color del aceite crudo, desarrollando un nuevo enfoque que considera la posibilidad de reducir las tierras de blanqueo necesarias para el proceso de decoloración. En dicho estudio se ha logrado comprobar que la intensa reducción de metales era consecuencia del arrastre de los mismos por parte de los fosfolípidos, componentes “blanco” de las enzimas utilizadas. La eliminación de ambos compuestos parecería ser responsable del cambio en la apariencia del aceite.&lt;/p&gt;
&lt;p&gt;Por otro lado, el conocimiento de los componentes específicos que se eliminan durante este proceso permitió estudiar la influencia del mismo sobre calidad y estabilidad oxidativa del producto tratado. Asimismo la diversidad de enzimas comerciales, disponibles en el mercado gracias a la ingeniería genética, permite encontrar diferencias en el producto procesado. Esto constituye un beneficio si se conoce la materia prima de partida, ya que permite seleccionar con que enzima trabajar dependiendo de las condiciones iniciales y las características deseadas del producto final.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2014/02/enzimas-02.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Las enzimas fosfolipasas han demostrado superioridad en la eliminación de trazas de metales con respecto de las aciltransferasas. Las enzimas aciltransferasas logran un mayor rendimiento de proceso cuando el aceite a tratar es rico en esteroles. Ambas enzimas manifestaron ser altamente efectivas al momento de actuar sobre los fosfolípidos.&lt;/p&gt;
&lt;p&gt;La mezcla de enzimas se manifiesta como un verdadero punto medio entre las diferentes propiedades destacadas de cada una, mostrando una gran capacidad de eliminación de metales, y buen rendimiento del aceite rico en esteroles. Este proceso muestra varias ventajas con respecto de los procesos tradicionales, ya que no se requieren altas temperaturas, en algunos casos las enzimas se pueden reciclar y reutilizar, y el agua de desecho se presenta en un volumen ínfimo, lo que constituye una tecnología amigable con el medio ambiente.&lt;/p&gt;
&lt;h3 id=&#34;conclusiones&#34;&gt;Conclusiones&lt;/h3&gt;
&lt;p&gt;Los últimos avances en este tipo de procesos demuestran que la combinación de enzimas es la mejor alternativa para el rendimiento de aceite y la eliminación de sustancias no deseables. Hoy, la ingeniería genética está en pleno desarrollo, y el diseño de biocatalizadores para ser aplicados a nivel industrial es una de las temáticas claves para la mejora de procesos a gran escala. Esta técnica moderna permite “diseñar enzimas” siendo capaz de aislar los genes de interés de una enzima determinada, combinarlos con la de otra enzima de utilidad; e insertarlos en un microorganismo recombinante con capacidad de reproducción inmediata.&lt;/p&gt;
&lt;p&gt;La tendencia futura utilizando esta herramienta podría intentar lograr el efecto de la combinación de dos o más enzimas en una sola cepa. Con esta mirada, la biotecnología sería capaz de estudiar los catalizadores biológicos idóneos para remover todos los componentes indeseables, y unificarlos en una sola célula enzimática, a fin de refinar aceites comestibles en pasos mínimos con tecnologías limpias. De este modo, es de esperar que en años venideros el uso de biocatalizadores sea aún más importante de lo que lo es hoy.&lt;/p&gt;
&lt;h3 id=&#34;referencias&#34;&gt;Referencias:&lt;/h3&gt;
&lt;p&gt;&lt;a id=&#34;1&#34;&gt; 1. &lt;/a&gt; IX Jornadas Regionales de Estudiantes de Química e Ingeniería Química de la Universidad Nacional del Sur, “Alternativas Bio al Procesamiento de Alimentos: Análisis y Conclusiones”. Agosto de 2013.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;2&#34;&gt; 2. &lt;/a&gt; DL LAMAS, GH CRAPISTE, DT CONSTENLA “Physicochemical characterization of crude and enzymatic degumming sunflower oil.” 15th Latin American Congress and Exhibition on Fats and Oils - /AOCS/ Santiago de Chile, Chile, 20 a 23 de Agosto de 2013. &lt;strong&gt;Contacto:&lt;/strong&gt; &lt;a href=&#34;mailto:dlamas@plapiqui.edu.ar&#34;&gt;dlamas@plapiqui.edu.ar&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Una reconstrucción matemáticamente asistida del foco inicial de la epidemia de fiebre amarilla en Buenos Aires (1871)</title>
      <link>https://ciencianet.com.ar/post/una-reconstruccion-matematicamente-asistida-del-foco-inicial-de-la-epidemia-de-fiebre-amarilla-en-buenos-aires-1871/</link>
      <pubDate>Wed, 08 Jan 2014 00:12:33 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/una-reconstruccion-matematicamente-asistida-del-foco-inicial-de-la-epidemia-de-fiebre-amarilla-en-buenos-aires-1871/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Hernán Solari.&lt;/strong&gt; Departamento de Física, Facultad de Ciencias Exactas y Naturales, UBA.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2014/01/solari-foto1.jpg&#34; alt=&#34;Calle de Buenos Aires en 1871.&#34;&gt;&lt;/p&gt;
&lt;p&gt;La epidemia de fiebre amarilla que asoló la Ciudad de Buenos Aires en 1871 dejó una huella imborrable en la memoria colectiva. Con sus aproximadamente 177.787 habitantes, la ciudad sufrió alrededor de 13.025 muertes por causa de la fiebre, es decir, más de 7 muertos cada 100 habitantes; una tasa de mortalidad altísima aún para una enfermedad con alta tasa de mortalidad como la Fiebre Amarilla.&lt;/p&gt;
&lt;p&gt;Consecuentemente, dejó una serie de historias, controversias y leyendas urbanas; pero también dejó un registro de la mortalidad diaria, discriminado por nacionalidad, edad, sexo y distrito-policial que es único en el mundo considerando que epidemias semejantes como la que devastó a Nueva Orleans en 1853 con 7.849 fallecimientos dejó registros detallados de solo unos 400 de ellos. Este tesoro estadístico producido por la Policía Federal permaneció oculto a la historia por más de 100 años.&lt;/p&gt;
&lt;p&gt;En una apropiada lectura de estos datos reside la respuesta a las controversias históricas y la posibilidad de echar luz sobre las leyendas urbanas. La recuperación de esta información, no citada en las reseñas históricas, no es resultado de la casualidad. La fiebre amarilla es producida por un arbovirus (virus transmitidos por artópodos) del género Flavivirus y familia flaviviridae. A esta misma familia pertenece el virus del dengue, hoy día una de las enfermedades emergentes más preocupantes a nivel mundial. De hecho, dengue y fiebre amarilla tienen mucho en común: son transmitidas por el mismo mosquito (Aedes (Stegomyia) aegypti) dentro del cual el virus se multiplica, suelen producir dos periodos febriles separados por aproximadamente un día de remisión de los síntomas, y ambos producen hemorragias en los pacientes aunque la severidad de las mismas es dramáticamente mayor en Fiebre amarilla que en dengue.&lt;/p&gt;
&lt;p&gt;Estas similaridades llevaron al Grupo de estudio de Dinámica en Sistemas Complejos, del Departamento de Física de la UBA, a intentar reproducir la epidemia de Fiebre amarilla de 1871 utilizando versiones adaptadas de los modelos desarrollados para estudiar epidemias de dengue. Es en este contexto que un miembro del grupo, María Laura Fernández, en su intensa búsqueda de información da con los olvidados libros de la Policía Federal con la estadística de la mortalidad.&lt;/p&gt;
&lt;p&gt;El desarrollo de una epidemia está tanto determinado por factores causales como aleatorios, o mejor dicho, por una aleatoriedad no neutra. Por lo tanto, la única pregunta que tiene sentido hacerse para un hecho histórico es: ¿se encuentra este hecho histórico único dentro del espectro de epidemias posibles en las circunstancias dadas? En otras palabras ¿la epidemia de 1871, tiene características tales que hubiera sido probable que sucediera y se desarrollara tal como lo hizo? Una respuesta por la negativa indicaría que nuestro conocimiento actual de la evolución de estas epidemias deja aún mucho que desear.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2014/01/solari-foto2.jpg&#34; alt=&#34;:left&#34;&gt; La epidemia de 1871 se desarrolla en dos etapas claramente definidas. En la primera la mortalidad se encuentra geográficamente confinada al barrio de San Telmo, expandiendose solo a los distritos policiales adyacentes. En un segundo periodo, de comienzo súbito, la epidemia abarca la totalidad de los distritos policiales de la ciudad. Este desarrollo de la epidemia sugiere la existencia de factores causales adicionales a los mosquitos que no han sido estudiados, por lo que el Grupo de estudio de Dinámica en Sistemas Complejos centró su atención en el foco inicial de San Telmo y su expansión.&lt;/p&gt;
&lt;p&gt;El estudio matemático de la evolución de la epidemia requiere de información más o menos precisa. Esta información abarca la formas y tiempos característicos del desarrollo de la enfermedad y las tasas de mortalidad. Los tiempos de desarrollo del virus en el mosquito, la descripción del ciclo de vida completo del mosquito y las disponibilidades ambientales para el desarrollo de los mismos, el clima imperante en la región durante la epidemia y finalmente, la fecha en que se produce el ingreso del virus a la ciudad, el llamado caso índice.&lt;/p&gt;
&lt;p&gt;Esta información no está exenta de controversias, por ejemplo, los tiempos de desarrollo de la enfermedad reconocidos por la Organización Mundial de la Salud no son compatibles con los observados en las últimas epidemias en África. Otros son casi imposibles de conseguir, como por ejemplo los que corresponden a las oportunidades para el desarrollo de larvas de mosquitos en aquellos días o la llegada del caso índice, el cual no fue detectado y sobre el que existe una controversia.&lt;/p&gt;
&lt;p&gt;En efecto, si bien todas las fuentes coinciden en que el virus arribó desde Brasil, la forma en que lo hizo ofrece dos versiones. En la primera el virus habría arribado directamente por el puerto de Buenos Aires en pasajeros o tripulantes procedentes de Río de Janeiro. En la segunda, el virus procedería por la vía de las epidemias que se desarrollaban en Asunción (1870) y Corrientes (1870-1871) asociadas al final de la llamada Guerra de la Triple Alianza. El resultado de esta controversia es que toda interpretación puramente histórica estará teñida de intereses políticos.&lt;/p&gt;
&lt;p&gt;En un trabajo publicado recientemente, junto a M. L. Fernández, M. Otero y N. Schweigmann hemos mostrado que solo es posible reconstruir la evolución de la mortalidad en el foco inicial de la epidemia en forma precisa (confidencia del 90%) si se hubieran dado las siguientes circunstancias:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Que existíeran condiciones ambientales para criar simultáneamente unas 25.000-30.000 larvas del mosquito por manzana en el momento del año de mayor producción, las cuales resultaban en “tan solo” un promedio de 5 picaduras por hembras de mosquitos adultos Aedes aegypti por día y por persona. La falta de aguas corrientes, el costo del agua de río distribuida por los tradicionales aguateros, y una basta población en condiciones sociales marginales y en rápido crecimiento apoyan estas estimaciones.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Que el desarrollo clínico de la fiebre amarilla fuese similar a lo observado en las epidemias en África a mediados del siglo XX (Etiopía y Nigeria).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Que el caso índice hubiese arribado en los primeros días de enero de 1871.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;La reconstrucción matemática de la epidemia muestra que de aceptarse la tesis histórica que sitúa al caso índice, como proveniente directamente de Río de Janeiro, a no más tardar a mediados de Diciembre de 1870, es decir, antes de comenzar la mortalidad en Corrientes (16 de diciembre de 1870) el desarrollo de la epidemia no podría reconstruirse, pero de adoptarse la tesis sobre las causas de la epidemia como secuela de las epidemias en Asunción y Corrientes, el acuerdo entre simulaciones y datos es excelente.&lt;/p&gt;
&lt;p&gt;Algo semejante ocurre con el relato histórico sobre un despoblamiento de Buenos Aires ocurrido durante la epidemia. Tal despoblamiento hubiera tenido una influencia visible en la evolución de la epidemia y consecuentemente, hubiera surgido un apartamiento claro de la epidemia real de las epidemias simuladas (las epidemias simuladas no tienen en cuenta estos movimientos migratorios). Nada de esto ocurre. Las fuentes históricas de la versión del despoblamiento de Buenos Aires remiten a un artículo de Diego de la Fuente (organizador del censo nacional de 1869) aparecido en La República el 15 de marzo 1871, en plena epidemia. El autor sostenía en el mismo diario, el mismo día, que el origen de la epidemia era racial/cultural, focalizado en la nacionalidad italiana.&lt;/p&gt;
&lt;p&gt;Por ese entonces se llegó a conocerse localmente a la fiebre amarilla como “la enfermedad de los italianos”. La excelente estadística de mortalidad de la Policía Federal combinada con la reconstrucción matemática de la epidemia representa una oportunidad sin igual para estudiar un hecho histórico con métodos interdisciplinarios que rara vez, si alguna, han sido utilizados en la reconstrucción histórica. Hoy podemos anticipar que la asociación de la fiebre amarilla con los italianos no es más que el resultado de un mal análisis estadístico de la época. A diferencia de la causalidad inmediata de las reconstrucciones por métodos históricos, la causalidad en matemática está mediada por largos desarrollos y computaciones.&lt;/p&gt;
&lt;p&gt;Esta rigidez lógica obliga a los investigadores a hacerse cargo de las consecuencias imprevisibles de las hipótesis. Así, no es posible pensar que se sesgó un modelo para dar un resultado políticamente conveniente, mucho menos un modelo preestablecido como en este caso. Mientras que las observaciones de las personas están teñidas de intereses e intencionalidades, y muy particularmente de la posición social desde la que se realizan las mismas. La reconstrucción matemática del hecho histórico (en las raras veces en que es posible) nos acerca a la verdad del mismo.&lt;/p&gt;
&lt;p&gt;Podemos decir que el modelo sobrevivió a la prueba y no surgen de este trabajo elementos que permitan objetarlo. La rigidez lógica de modelo y la precisión de la información que requiere obligan a pensar la reconstrucción histórica en nuevos términos. Así, en lugar de focalizar en el momento en que se produce la expansión de la epidemia a toda la ciudad dejando atrás su desarrollo en un foco grande pero aislado (mediados de abril de 1871), podemos inferir la fecha en que ocurrió la dispersión del virus. Los cálculos señalan a los primeros días de febrero. La historia indica que el inicio de la mortalidad en San Telmo fue el 27 de enero y el comienzo de la política de desalojo de los lugares donde ocurrían casos entre el 5 y 8 de febrero, fundamentada en la idea de que la enfermedad era producida por miasmas. El mosquito aparece así ya no más como el único responsable por la transmisón de la enfermedad.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Bajas temperaturas en La Plata</title>
      <link>https://ciencianet.com.ar/post/bajas-temperaturas-en-la-plata/</link>
      <pubDate>Tue, 31 Dec 2013 00:19:40 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/bajas-temperaturas-en-la-plata/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro&lt;/strong&gt;. Instituto de Física de Líquidos y Sistemas Biológicos (CONICET).&lt;/p&gt;
&lt;p&gt;En la oficina vecina a la que ocupo en el &lt;a href=&#34;http://www.iflysib.unlp.edu.ar/&#34;&gt;IFLYSIB&lt;/a&gt; trabaja Santiago Grigera, responsable de la flamante instalación y puesta en marcha del Laboratorio de Bajas Temperaturas (BT La Plata). Aprovechamos una pausa durante una tarde de junio para charlar sobre esta singularidad térmica en el Universo.
&lt;img src=&#34;https://ciencianet.com.ar/images/2013/12/BTLP-01-300x200.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CN&lt;/strong&gt;: -¿Qué actividad se realiza en el BT La Plata?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SG&lt;/strong&gt;: -La idea básica es que en el Laboratorio se pueden alcanzar temperaturas muy bajas y campos magnéticos muy altos. Las temperaturas que alcanzamos son realmente bajas, la menor de 260 mK (-273 °C), que es más baja que la del &lt;a href=&#34;http://es.wikipedia.org/wiki/Radiaci%C3%B3n_de_fondo_de_microondas&#34;&gt;espacio exterior&lt;/a&gt; (aproximadamente 2.7 K). Estas temperaturas muy bajas se alcanzan en un volumen relativamente pequeño, de forma cilíndrica de 30 mm de diámetro por 100 mm de alto. Los campos que podemos alcanzar son de hasta 9 T (esto es, unas 200 mil veces el campo magnético de la Tierra en su superficie).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CN&lt;/strong&gt;: -¿Cuál es el propósito de medir algo a tan baja temperatura?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SG&lt;/strong&gt;: -La temperatura de un sistema se puede pensar en términos de la agitación térmica de los átomos y moléculas que componen un material. Esta agitación destruye el orden que existe en estos sistemas, así que al enfriarlo se pueden observar las propiedades del mismo con la menor perturbación posible, recuperando el orden que no se puede observar a mayores temperaturas. En especial, ganan relevancia a bajas temperaturas los efectos cuánticos.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2013/12/BTLP-04-200x300.jpg&#34; alt=&#34;:right&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CN&lt;/strong&gt;: -¿Qué tipos de muestras analizan?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SG&lt;/strong&gt;: -Se puede analizar lo que uno quiera, siempre y cuando haya alguna propiedad de interés que se pueda manifestar a baja temperatura. Por ejemplo, se pueden analizar gases, que se licúan a esas temperaturas tan frías, y es entonces cuando aparecen propiedades cuánticas que no se observan a temperaturas más altas. Una de esas propiedades conocidas es la superfluidez del helio. Nosotros apuntamos a mirar sistemas electrónicos y magnéticos. Nos gustaría entender cómo funciona un sistema de muchas partículas (aproximadamente 1024, esto es, un uno seguido de veinticuatro ceros) con interacciones fuertes entre sí a un nivel cuántico, que están muy correlacionadas. Decimos que están muy correlacionadas en el sentido de que una perturbación de una partícula, afecta no solo a ella y sus vecinas inmediatas, sino también al resto del sistema.&lt;/p&gt;
&lt;p&gt;Uno de los casos que uno puede pensar como sistema con comportamiento cuántico colectivo es el de los electrones dentro de un metal. Lo mismo se puede pensar con otra de las propiedades fundamentales de interés, en este caso magnética y de origen relativista, como es el espín. Dentro de los sistemas magnéticos, un caso de interés para nosotros son los sistemas frustrados. Estos sistemas tienen, además de una interacción fuerte, una o varias interacciones adicionales no compatibles con las primeras. Cada &amp;quot;individuo&amp;quot; del sistema está sujeto a muchas condiciones y no puede satisfacer todas al mismo tiempo. Se pueden analizar y comparar las propiedades clásicas y cuánticas de estos sistemas, y ver cómo se resuelven estas frustraciones y qué tipo de fenómeno colectivo emerge en cada caso. A bajas temperaturas, la frustración evita la aparición de los estados simples ya conocidos, y quedan en evidencia comportamientos nuevos y complejos. La descripción de estos sistemas se complementa con modelos teóricos cuyos resultados y predicciones se ponen a prueba con las mediciones que realizamos.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2013/12/BTLP-03-300x200.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CN&lt;/strong&gt;: -Además de realizar investigación básica, ¿es posible que el Laboratorio tenga alguna vinculación con el desarrollo tecnológico?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SG&lt;/strong&gt;: - Nosotros en este momento estamos trabajando en problemas de  ciencia básica. A veces mirando este tipo de problemas nos encontramos con materiales que tienen propiedades de interés tecnológico (como ha sido el caso histórico de los semiconductores). Por otra parte, en principio, el Laboratorio también es una herramienta, que puede ser utilizada para estudiar otros tipos de problemas con interés más aplicado. Nuestro objetivo es justamente ampliar gradualmente nuestro ámbito de investigación para incluir problemas aplicados y desarrollo tecnológico en criogenia (la producción de bajas temperaturas).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CN&lt;/strong&gt;: -¿Quiénes trabajan en el Laboratorio?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SG&lt;/strong&gt;: -Actualmente somos tres investigadores, tres becarios doctorales y un estudiante de grado que está haciendo su tesis de licenciatura.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CN&lt;/strong&gt;: -El Laboratorio es nuevo, recién poniéndose en marcha. ¿Cómo fue posible su construcción?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SG&lt;/strong&gt;: -Como es usual, lo primero es conseguir el financiamiento. Más del 90% del mismo provino de la Agencia Nacional de Promoción Científica y Tecnológica, con un subsidio para grandes equipamientos. La Agencia misma puso dinero también para acondicionar el lugar. Algunas donaciones de equipamiento más pequeño llegaron desde la Universidad de Saint Andrews de Escocia.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2013/12/BTLP-02-300x200.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CN&lt;/strong&gt;: -Una vez obtenido el financiamiento, ¿qué dificultades hubo para llevar a cabo la construcción?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SG&lt;/strong&gt;: -Parece que obtener el dinero es la parte difícil, pero en realidad es más complicado llevar el proyecto a la realización. Una de las dificultades es que los mecanismos para ello, pese a las buenas intenciones, no están pensados para proyectos de gran alcance o larga duración. También hay algunas incoherencias como que podés recibir dinero para comprar el equipamiento pero no para su funcionamiento. Otra dificultad es la forma que tienen las unidades administrativas (en este caso la Universidad Nacional de La Plata) para contratar los servicios de construcción o instalación eléctrica. Son mecanismos complejos y tortuosos, y uno siente que pierde tiempo y dinero.&lt;/p&gt;
&lt;p&gt;Hay otra cuestión que afecta seriamente, y que surge del hecho que Argentina es un país que no está desarrollado industrialmente. Comprar insumos sencillos es muy difícil, por ejemplo conectores o tornillos de precisión. A veces se consigue algo en Buenos Aires, o hay que comprar cosas usadas. No hay demanda para ingeniería de precisión, por lo que no hay muchos comercios que vendan esos insumos. Determinados materiales tampoco se consiguen, así como servicios. Hay poco conocimiento en el país de determinadas implementaciones técnicas. Nosotros compramos muchas veces las cosas básicas, y al resto lo hacemos porque no lo conseguimos en el mercado, y este aprendizaje puede ser útil para problemas de bajas temperaturas en la industria, como en el desarrollo electrónico y en ámbitos más lejanos a los que nos movemos. De algún modo, esto también es un producto de nuestro Laboratorio, porque para armarlo es necesario desarrollar cierto conocimiento técnico que también es exportable. Nosotros vemos con buenos ojos esta iniciativa de CONICET de acercar los laboratorios a la industria, y que exista un ida y vuelta de demanda y oferta de soluciones.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CN&lt;/strong&gt;: -¿Cuál es la perspectiva para el Laboratorio en el largo plazo?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SG&lt;/strong&gt;: -El BT La Plata recién empieza. Cuando uno recibe algo tiene que justificar lo recibido, lo que significa que lo que se invirtió en nosotros tenemos que justificarlo con resultados. No queremos ampliarnos hasta no demostrar que estamos construyendo sobre una báse sólida, por lo que no vamos a incorporar más equipamiento hasta no tener suficientes resultados obtenidos con lo que ya tenemos funcionando. Esperamos poder hacer contribuciones interesantes a la ciencia básica, pero que además, que el BT La Plata funcione como punto de consulta para temas que estén relacionados con lo que hacemos.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nota del Editor&lt;/strong&gt;: La ironía de publicar una nota sobre temperaturas extremadamente bajas en La Plata justo cuando una ola de calor récord golpea la ciudad es producto del deseo del autor que tal inclemencia climática cese de inmediato.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>El Agronegocio de la soja en la Argentina: problemas socioeconómicos, medioambientales y alternativas</title>
      <link>https://ciencianet.com.ar/post/el-agronegocio-de-la-soja-en-la-argentina-problemas-socioeconomicos-medioambientales-y-alternativas/</link>
      <pubDate>Sat, 28 Dec 2013 00:24:31 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/el-agronegocio-de-la-soja-en-la-argentina-problemas-socioeconomicos-medioambientales-y-alternativas/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Javier Carlos Quagliano Amado.&lt;/strong&gt; Instituto de Investigaciones Científicas y Técnicas para la Defensa (CITEDEF), Ministerio de Defensa.&lt;/p&gt;
&lt;p&gt;Desde que a medidos de la década del noventa se autorizó el uso de plantas transgénicas, el modelo de agricultura extensiva basada en la biotecnología de organismos genéticamente modificados (OGM) y elevado uso de agroquímicos se aplicó en nuestro país en forma casi excluyente. Así como en otros países de América del Sur, por ejemplo Bolivia, Brasil, Paraguay y Uruguay, este modelo, exitoso desde el punto de vista comercial, se implementa sin una evaluación rigurosa ni información adecuada sobre el impactos en la salud humana y el medio ambiente.&lt;/p&gt;
&lt;p&gt;Argentina es el tercer mayor productor de soja del mundo y es responsable de un tercio de las ventas de soja en todo el mundo. Estos datos nos hablan de la magnitud del beneficio económico para el país, pero también de las consecuencias negativas que acompañan este beneficio (también llamadas “externalidades negativas”, en el lenguaje de negocios).
`
&lt;img src=&#34;https://ciencianet.com.ar/images/2013/12/373_3_1-300x199.jpg&#34; alt=&#34;Fuente: http://www.ecoportal.net:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;El crecimiento de la soja transgénica ha originado dependencia del paquete tecnológico provisto por empresas multinacionales como Monsanto y Novartis. Éstas no sólo proveen la semilla, sino también el paquete tecnológico y los herbicidas. El uso exclusivo de la tecnología transgénica ha causado que biotipos de cultivos tiendan a desaparecer, debilitando el suelo, y por lo tanto menos productivo para la agricultura futura. En tal sentido, &lt;a href=&#34;http://www.ipni.net/publication/ia-lahp.nsf/0/AE9C457B83C1813685257A2F004F6397/$FILE/IAH-2012-02.pdf#page=7&#34;&gt;Gustavo Cruzate y Roberto Casas&lt;/a&gt; del Instituto de Suelos del INTA determinaron que para la campaña 2010/11 se extrajeron 3.93 millones de toneladas de nitrógeno, fósforo, potasio, sodio y calcio, de los cuales sólo se repusieron 1.26 millones de toneladas. Esto representa un 34.6 por ciento de reposición, es decir, un déficit de casi el 65 por ciento. Al mismo tiempo, el éxito de este modelo permitió el desarrollo paralelo industrias conexas, como la industria de maquinaria agrícola en las últimas dos décadas. Esta industria metalmecánica es una de las más competitivas del mundo.&lt;/p&gt;
&lt;p&gt;Por otro lado, el costo de la maquinaria hace que solamente sea posible amortizarlo a través de asociación en agrupaciones como son los “pooles” de siembra. Como la mayor parte de la producción se realiza en campos arrendados (alrededor del 60%) no se crea un compromiso acerca la conservación del suelo, sino más bien con la maximización de la renta. Esto contribuye a que se refuerce la adopción del sistema de monocultivo.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2013/12/tala_soja_monocultivo-300x180.jpg&#34; alt=&#34;Fuente: http://www.contrainfo.com/1411/el-monocultivo-mental-argentino/:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Algunos datos adicionales nos hablan de la magnitud de las externalidades negativas que mencionamos arriba para este modelo. Respecto de la desertificación, la región de Santiago del Estero tiene un promedio de 0,81% de bosque nativo deforestado cada año, comparado con un valor promedio de 0,23% a nivel mundial. Si vamos al uso masivo de agroquímicos que trae aparejado, la Organización de las Naciones Unidas (ONU) considera que la tasa de intoxicaciones en los países de América Latina podría ser unas 13 veces mayor que en los países industrializados. En nuestro país específicamente se registran en forma continua muertes por intoxicación aguda y crónica, mayormente en las provincias de Córdoba y Santa Fe. Esto ha dado lugar a movimientos como las Madres de Ituzaingó y redes de profesionales médicos y científicos urgente y justificadamente preocupados por estos hechos.&lt;/p&gt;
&lt;p&gt;Recientemente, Amigos de la Tierra Internacional exhortó a los gobiernos del mundo a que limiten el uso del pesticida glifosato, después de que resultados de análisis de laboratorio publicados demostraron que se hallaron restos del pesticida en personas de 18 países europeos. Es la primera vez que se realiza un seguimiento en toda Europa de la presencia del pesticida en humanos. Los participantes del estudio, que proporcionaron muestras en forma voluntaria, vivían en ciudades y ninguno de ellos había manipulado ni utilizado productos con glifosato antes de los análisis. Lamentablemente el llamado de atención surge desde el Primer Mundo, y en tanto y cuanto este es afectado como consecuencia de los productos agroindustriales exportados al mundo desarrollado.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.reduas.fcm.unc.edu.ar/wp-content/uploads/downloads/2012/04/Situaci%C3%B3n-pueblos-fumigados-ARG.pdf&#34;&gt;Avila Vázquez Medardo&lt;/a&gt;, de la Red Universitaria de Ambiente y Salud–Médicos de Pueblos Fumigados concluye que en la Argentina impera una agricultura de monocultivos, una agricultura química, derivada de un modelo agroindustrial que utiliza un paquete tecnológico que incluye siembra directa, semillas transgénicas y aplicación a discreción de agrotóxicos. Un informe reciente, oficial y reconocido por ser realizado por investigadores del Consejo Nacional de Investigaciones Científicas y Técnicas (CONICET) del Ministerio de Ciencia y Tecnología de la Nación concluyó que aunque existen estudios para evaluar los impactos del glifosato en las especies no blanco, la mayoría de ellos no considera importantes aspectos ecológicos. Por lo tanto, concluyeron que en la Argentina no existen suficientes datos sobre los efectos del glifosato en la salud humana.&lt;/p&gt;
&lt;p&gt;Quizás estos datos soslayan, en la frialdad de lo escrito en el papel, la realidad de las intoxicaciones y fallecimientos de argentinos que lamentablemente siguen produciéndose. Las consecuencias del referido modelo de agronegocios seguirán agudizándose en nuestra opinión, dado que el Estado argentino ha apoyado este modelo desde los comienzos de su implementación. En el último Plan Estratégico agroindustrial presentado en el predio de Tecnópolis en Septiembre de 2011, se espera incrementar la producción de granos en un 60% hasta 160 millones de toneladas en 2020. Se preveé aumentar la superficie plantada de soja de 18,8 a 20 millones entre 2010 y 2020, así como la de trigo, girasol y maíz. Al mismo tiempo, y en una eventual contradicción, el gobierno argentino no incentiva actualmente la producción de cultivos alternativos como el maíz, trigo o girasol a través de beneficios impositivos.&lt;/p&gt;
&lt;h3 id=&#34;posibles-alternativas&#34;&gt;Posibles alternativas&lt;/h3&gt;
&lt;p&gt;A pesar de todos estos problemas, no puede soslayarse que el agronegocio de la soja tiene un gran peso en la economía nacional. Según un trabajo presentado por el Programa de Agronegocios y Alimentos de la Universidad de Buenos Aires, presentado en un reciente congreso internacional de agronegocios en Chicago, el complejo de la soja contribuye con el 22% del PBI nacional. El trabajo recalca el nivel de desarrollo tecnológico y preeminencia de la Argentina en el mercado internacional, aunque menciona la necesidad que la producción sea en el futuro más amigable con el medio ambiente. Sin embargo, este tipo de estudios de cadena agroindustrial tiende a soslayar los evidentes efectos negativos del modelo sobre la salud y el medio ambiente.&lt;/p&gt;
&lt;p&gt;Es innegable que el modelo fomenta industrias conexas como fábricas de tractores, cosechadoras, sembradoras y fumigadoras, de fertilizantes, semilleras, plantas de biodiesel o bioalcohol, que crean puestos de trabajo. Sin embargo, expertos provenientes del ámbito académico, como el Profesor Otto Solbrig, investigador argentino radicado hace décadas en los Estados Unidos y trabajando en estos temas en la Universidad de Harvard, alertan sobre la primarización de la economía de las Pampas y la soja-dependencia (diario Clarín, 25 de diciembre de 2010). Se plantea entonces el claro dilema entre un desarrollo acelerado de la actividad agroindustrial que privilegie el desarrollo a corto o mediano plazo frente al paradigma deseable de un modelo más sustentable a largo plazo.&lt;/p&gt;
&lt;p&gt;Si se piensa en actuar en forma inmediata, la aplicación del principio precautorio (prevista en el artículo 4 de la ley nacional 25.675 del medio ambiente) puede aplicarse cuando se pone en peligro el medio ambiente y la salud humana, especialmente cuando la frontera agrícola se acerca a las zonas urbanas. Algunos jueces han logrado aplicarla en ciertos casos, donde el sentido común daba clara evidencia del daño. Obviamente, esto no es practicable en gran escala. Desde el punto de vista tecnológico, medidas simples como elegir los métodos de aplicación de pesticidas ayudarían a evitar la dispersión de los agrotóxicos. Por ejemplo, puede evitarse la aplicación por aspersión aérea cuando la temperatura es alta y humedad relativa baja, limitándose a realizarla de este modo en horas tempranas del día, de baja temperatura, alta humedad y ausencia de vientos fuertes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2013/12/374_2_2-213x300.jpg&#34; alt=&#34;Fuente: http://www.ecoportal.net:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Existe la tecnología para aplicar agroquímicos optimizando la dosis y régimen de aplicación (tamaño de gota de aspersión, aplicaciones repetidas en dosis más diluídas). Dado que el glifosato no tiene una vida media excesivamente larga en los suelos (del orden de 2 a 6 meses dependiendo de las condiciones medioambientales) es posible practicar medidas de veda y así evitar la exposición a largo plazo. La implementación de un sistema de monitoreo ambiental y toxicológico oficial es imperiosa para proveer de información válida en el ámbito legal.&lt;/p&gt;
&lt;p&gt;La implementación de Buenas Prácticas Agrícolas (BPA) por parte de los productores y “pooles” de siembra como parte integral del modelo contribuiría a revertir la situación actual de inacción en el área medioambiental. Ampliando el marco donde pueda aplicarse un “punto de apalancamiento positivo”, Henk Hobbelink, de la red GRAIN (&lt;a href=&#34;http://www.grain.org/es/&#34;&gt;http://www.grain.org/es/&lt;/a&gt;), señala que simplemente aumentando el tiempo de los contratos a 3 o 5 años se puede lograr la rotación de los cultivos necesaria para evitar que los suelos pierdan su calidad y que se limite el monocultivo. Esto requiere, para este reconocido activista, que los gobiernos estabilicen sus políticas y establezcan impuestos claros, de modo que las partes involucradas puedan tomar decisiones en un marco de previsibilidad.&lt;/p&gt;
&lt;p&gt;Ninguna acción individual podrá por sí sola ser efectiva sino en el marco de un plan conjunto, coordinado y consensuado, donde el Estado tenga el rol de impulsor, coordinador y ejecutor. En la situación económica actual del país, solamente se puede seguir con el modelo, dado que el sistema agroindustrial es fundamental para balancear las cuentas nacionales. Quizás el futuro permita la implementación de medidas para lograr un modelo para el cultivo de la soja más acorde con los criterios de sustentabilidad. Hoy por hoy, siguen siendo válidas las palabras del premio Nobel de la Paz Adolfo Pérez Esquivel, entrevistado por el periodista Darío Aranda:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“los gobiernos hasta ahora han privilegiado los intereses económicos por sobre la vida de las personas”.&lt;/p&gt;
&lt;/blockquote&gt;

        
      </description>
    </item>
    
    <item>
      <title>El microbioma humano</title>
      <link>https://ciencianet.com.ar/post/el-microbioma-humano/</link>
      <pubDate>Fri, 06 Dec 2013 00:26:05 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/el-microbioma-humano/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Emiliano Salvucci&lt;/strong&gt;. Instituto de Ciencia y Tecnologia de Alimentos - CONICET – Universidad Nacional de Córdoba.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2013/12/Salvucci.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;La medicina de hoy se enfrenta a una serie de epidemias aparecidas en la era postindustrial, entre ellas las alergias, obesidad, enfermedades autoinmunes y enfermedades inflamatorias. Las defensas del cuerpo se han vuelto peligrosamente hipersensibles a elementos como el polvo, las mascotas y la comida. La clave de este aumento está en la obsesión por la higiene. En el mundo actual parece existir una guerra contra los microorganismos. Pero es este ataque constante a los microorganismos lo que está detrás de la emergencia de una constelación de enfermedades.&lt;/p&gt;
&lt;p&gt;En la década del 90 a partir de las investigaciones de la Dra. Erika von Mutius y el Dr. Stracham se conoció la hipótesis de la higiene que explicaba por qué los niños de familias numerosas, que tienen mayor contacto con la naturaleza, que tienen en general mayor exposición al ambiente, así como aquellos cuyas mamás estuvieron en contacto con animales, o que viven en zonas rurales, son menos propensos a desarrollar asma y alergias. Esto se debe al incremento en el número y la función de ciertas células del sistema inmune llamadas células T reguladoras, que bajan la producción de un tipo específico de citoquinas, moléculas mensajeras o señales del sistema inmunológico, lo que evita que el organismo reaccione exageradamente ante la presencia de moléculas comunes del medio ambiente.&lt;/p&gt;
&lt;p&gt;Se trata de una forma de inmunoterapia natural, formando el desarrollo inmunitario para lo cual es necesario y fundamental la interacción con microorganismos. En años recientes los científicos han comenzado a comprender la verdadera dimensión evolutiva, estructural y funcional de los microorganismos asociados a nuestro organismo. El organismo humano es, entonces, una red compleja que presenta diez células microbianas para cada célula humana. A este conjunto de microorganismos (bacterias, virus y eucariotas) que comparten el espacio del cuerpo se lo denomina microbioma.&lt;/p&gt;
&lt;p&gt;Particularmente, el microbioma intestinal constituye una comunidad taxonómicamente compleja y ecológicamente dinámica e influye en la maduración y la regulación, estimulación y supresión, del sistema inmune. El microbioma comienza a formarse inmediatamente después del nacimiento y hasta los 3 años está evolucionando hasta adquirir la conformación característica de la de un adulto. Durante este tiempo se desarrolla por completo el sistema inmune.&lt;/p&gt;
&lt;p&gt;El microbioma humano ha definido en conjunto no solo al sistema inmunológico con el cual convive, sino que también forma parte integral de procesos fundamentales como la producción de vitaminas, la digestión, la homeostasis energética, la integridad de la barrera intestinal y la angiogénesis en el cuerpo humano. El microbioma provee la capacidad para degradar polisacáridos vegetales que habitualmente consumimos, ricos en carbohidratos conteniendo xilanos, pectinas y arabinosa ya que realiza el metabolismo de sacarosa, glucosa, galactosa, fructosa y manosa. La fermentación de las fibras y los glicanos requiere la cooperación y asociación de diversos microorganismos.&lt;/p&gt;
&lt;p&gt;El microbioma metaboliza el butirato a butiril-CoA un ácido graso de cadena corta que es la principal fuente de energía de los colonocitos, cuyo desarrollo establece una barrera intestinal saludable. Participa, además, en la síntesis esencial de aminoácidos y vitaminas. Además regula funciones relacionadas al sistema nervioso y endocrino.&lt;/p&gt;
&lt;p&gt;En un trabajo reciente, he resumido estos aspectos del microbioma humano y desarrollando un nuevo enfoque que considera que todo organismo, incluido el ser humano, no puede concebirse como aislado, sino coevolucionando, coexistiendo, con un conjunto de organismos y microoganismos que lo definieron evolutivamente por medio de procesos integrativos. Los humanos no evolucionaron como una sola especie (¿compitiendo contra quién?, ¿seleccionado por quién? ¿entre quiénes?), sino asociados con un complejo microbioma en una suerte de “superorganismo”.&lt;/p&gt;
&lt;p&gt;Esta idea ha sido planteada por otros autores como Savinov (lo llama autocenosis) o Zilber-Rosenberg (holobionte) aunque siempre se considera a este nuevo superorganismo como un resultado de la selección natural. En cambio, en este trabajo considero que es el resultado de procesos integrativos que se observan a diferentes niveles en la naturaleza, donde cada nuevo nivel contiene propiedades únicas e indisociables de esa entidad, superior a la suma de las partes que lo componen. Es el carácter integrativo de estos sistemas, que generan propiedades emergentes y estables lo que ha dado origen también a organismos como el ser humano, construido por integración con su microbioma.&lt;/p&gt;
&lt;p&gt;Hoy en día, muchas enfermedades son consideradas nuevas epidemias. La incidencia de un grupo de enfermedades ha aumentado desde la era industrial. Estas se relacionan con un sistema inmune hiperreactivo y este desequilibrio está relacionado con la separación de nuestra contraparte simbionte a lo largo de los últimos años. Entre ellas, se encuentran la obesidad, el síndrome metabólico, las enfermedades inflamatorias intestinales y la diabetes.&lt;/p&gt;
&lt;p&gt;El equilibrio entre los simbiontes que construyen al superorganismo u holobionte se ha ido perdiendo con la medicina moderna, las nuevas tecnologías y los cambios en el estilo de vida. A ello debemos sumar los notables cambios en las características y composición de los alimentos y los hábitos alimentarios. Y, de acuerdo a la hipótesis de la higiene, el conjunto de factores como el poco contacto con microorganismos ambientales que incide en el riesgo a desarrollar estas enfermedades.&lt;/p&gt;
&lt;p&gt;Restaurar el microbioma perdido es una tarea imposible, pero la comprensión del organismo humano con estas perspectivas holísticas, permiten alentar la búsqueda de soluciones más adecuadas conociendo mejor la etiología de las mismas y el trasfondo evolutivo que las genera. En ese sentido, por ejemplo, existen terapias alternativas como el tratamiento con helmintos para restablecer el equilibrio inmunológico. Comprendiendo la complejidad del superorganismo humano y entendiendo la decisiva influencia del medio ambiente en los fenómenos de integración que se observan en la naturaleza nos aproximamos a una mejor comprensión evolutiva de nuestra especie.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajos originales:&lt;/strong&gt; E. Salvucci, &lt;a href=&#34;http://www.revista.unal.edu.co/index.php/actabiol/article/view/28331&#34;&gt;El agotamiento del bioma y sus consecuencias&lt;/a&gt;, Acta biol. Colomb. 18, 31 (2013). E. Salvucci, &lt;a href=&#34;http://dx.doi.org/10.3389%2Ffcimb.2012.00054&#34;&gt;Selfishness, warfare and economics; or integration, cooperation and biology&lt;/a&gt;, Front. Cell. Inf. Microbiol. 2, 54 (2012).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; Emiliano Salvucci (&lt;a href=&#34;mailto:esalvucci@agro.unc.edu.ar&#34;&gt;esalvucci@agro.unc.edu.ar&lt;/a&gt;)&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Variables financieras y personas de a pie</title>
      <link>https://ciencianet.com.ar/post/variables-financieras-y-personas-de-a-pie/</link>
      <pubDate>Fri, 29 Nov 2013 00:29:32 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/variables-financieras-y-personas-de-a-pie/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Daniel Parisi.&lt;/strong&gt; Instituto Tecnológico Buenos Aires y CONICET.&lt;/p&gt;
&lt;p&gt;[&lt;img src=&#34;https://ciencianet.com.ar/images/2013/11/parisi.jpg&#34; alt=&#34;Bolsa de Nueva York&#34;&gt;&lt;/p&gt;
&lt;p&gt;La fotografía muestra el piso de la Bolsa de Comercio de Nueva York. La dinámica de las personas caminando por ese espacio físico tal vez esté más relacionada de lo que parece con la dinámica de los precios de las acciones que estos mismos agentes generan. En un estudio reciente mostramos que un sistema de peatones simulados exhibe características muy similares a las que se observan en las series de precios de activos financieros o divisas.&lt;/p&gt;
&lt;p&gt;Existen distintos modelos físico-computacionales que permiten simular y predecir el movimiento de peatones. Uno de estos es el conocido &amp;quot;Modelo de Fuerza Social&amp;quot;. Con este modelo se puede simular, por ejemplo, dos habitaciones conectadas por una puerta y dos grupos de personas que quieren atravesarla en sentidos opuestos. A este sistema se le llama flujo bi-direccional (o contrapuesto) a través de un cuello de botella (la puerta). Se ha observado ya (con este modelo y en la vida real) que el flujo en la puerta presenta oscilaciones: por momentos dominan las personas que cruzan en una dirección y por momentos las que cruzan en sentido opuesto. Estas variaciones (fluctuaciones) han sido comparadas cualitativamente con las subas y bajas de los precios de las acciones de empresas.&lt;/p&gt;
&lt;p&gt;Sin embargo, que dos cosas se parezcan a simple vista no significa que sean equivalentes desde un punto de vista más riguroso. Inspirados por esta semejanza, implementamos una simulación computacional que consideró dos grupos de peatones: un grupo siempre quiere atravesar la puerta de izquierda a derecha y el otro en el sentido opuesto. Se midió entonces el número de personas por m&lt;sup&gt;2&lt;/sup&gt; (la densidad) en las cercanías de la puerta. Se encontró una relación muy estrecha entre las variaciones de densidad y las variaciones de precio de las acciones en la bolsa. Las variaciones o &amp;quot;retornos&amp;quot; de una variable financiera son el porcentaje que subió o bajó una acción, por ejemplo. Dada una serie temporal de precios, o de densidades de personas cerca de la puerta con flujo contrapuesto, es fácil calcular la serie de los retornos haciendo la diferencia entre un valor y el anterior en el tiempo.&lt;/p&gt;
&lt;p&gt;Es conocido en el mundo de las finanzas que los retornos tienen ciertas características estadísticas que lo diferencian de cualquier otro sistema. Hay aproximadamente 10 de estas características y se las conoce como &amp;quot;hechos estilizados&amp;quot; (&lt;a href=&#34;http://en.wikipedia.org/wiki/Statistical_finance&#34;&gt;&lt;em&gt;stylized facts&lt;/em&gt;&lt;/a&gt;) de los sistemas financieros. Es muy difícil, imposible diría, encontrar un sistema que no sea financiero y que exhiba todos estos hechos estilizados. Es más, los modelos que se construyen específicamente para intentar simular mercados financieros solo reproducen algunos pocos de estos hechos estilizados.&lt;/p&gt;
&lt;p&gt;Lo sorprendente del sistema peatonal simulado es que, sin ser un modelo que intenta simular series financieras, logra reproducir por lo menos 8 de los principales hechos estilizados propios de las finanzas. Veamos un ejemplo de hecho estilizado financiero que se cumple débilmente en las simulaciones de los peatones en flujo contrapuesto arriba descripta. La volatilidad, es una medida del tamaño de la variación sin importar si la acción subió o bajó. Una forma de medir la volatilidad es tomando el valor absoluto del retorno. Es un echo estilizado que los sistemas financieros muestran una alta autocorrelación de la volatilidad a la vez que los retornos de las acciones no están autocorrelacionados.&lt;/p&gt;
&lt;p&gt;Esto significa que no hay relación entre la variación de precios de hoy con la de mañana (retorno no correlacionado) pero grandes fluctuaciones son generalmente seguidas por grandes fluctuaciones y pequeñas por pequeñas (volatilidad autocorrelacionada). Obviamente, no se sabe el signo de las variaciones (si subirán o bajarán). Dicho de otra manera, sabiendo la variación de hoy sabré que la variación de mañana será de tamaño similar, pero no sabré si será negativa o positiva (si supiera sería fácil ganar dinero operando en la bolsa,... y no lo es).&lt;/p&gt;
&lt;p&gt;El modelo descripto arriba de peatones, que siempre quieren ir en la misma dirección, sin tomar ninguna decisión es suficiente para reproducir casi todos los hechos estilizados. Sin embargo, no permite lo análogo a una crisis financiera, en la cual todos los agentes buscan masivamente deshacerse de las acciones (una corrida). Para poder lograr este comportamiento en el sistema peatonal, hemos permitido que los agentes puedan decidir si seguir con su destino original o si cambiar de sentido en su recorrido al atravesar la puerta. La decisión se toma al acercarse a la puerta y ver hacia donde se dirigen los vecinos. Esto permite que cada grupo tenga un número variable de personas pudiéndose dar el caso extremo que todos los agentes quieran ir hacia el mismo lado, lo que equivaldría a una crisis financiera.&lt;/p&gt;
&lt;p&gt;Con esta modificación, no sólo se obtienen &amp;quot;crisis peatonales&amp;quot; sino que además se observa una autocorrelación de la volatilidad tan grande como la de los sistemas financieros, mientras que la serie de los retornos de la densidad no tiene ninguna autocorrelación. Es decir, que este importante hecho estilizado requiere que los precios se vayan del equilibrio y que haya corridas, grandes o pequeñas, para que exista. Sólo para esto es relevante que los peatones puedan tomar decisiones al mirar que hacen otros agentes cercanos. Todos los demás hechos estilizados que caracterizan a los sistemas financieros pueden ser explicados simplemente por la competencia entre dos grupos de personas con intereses o expectativas contrapuestas, los que creen que una acción bajará y los que creen que subirá.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; D. R. Parisi, D. Sornette y D. Helbing. &amp;quot;&lt;a href=&#34;http://link.aps.org/doi/10.1103/PhysRevE.87.012804&#34;&gt;Financial price dynamics and pedestrian counterflows: A comparison of statistical stylized facts&lt;/a&gt;&amp;quot;. Phys. Rev. E 87, 012804 (2013).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; Daniel Parisi (&lt;a href=&#34;mailto:dparisi@itba.edu.ar&#34;&gt;dparisi@itba.edu.ar&lt;/a&gt;)&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Propagación de Algarrobo blanco mediante la técnica de miniestacas</title>
      <link>https://ciencianet.com.ar/post/propagacion-de-algarrobo-blanco-mediante-la-tecnica-de-miniestacas/</link>
      <pubDate>Fri, 04 Oct 2013 00:32:57 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/propagacion-de-algarrobo-blanco-mediante-la-tecnica-de-miniestacas/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Abelardo Veggeti.&lt;/strong&gt; (Facultad de Ciencias Agrarias, Universidad Nacional del Litoral).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2013/10/algarrobo1.jpg&#34; alt=&#34;Clones de Algarrobo blanco en cámara de enraizamiento.&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Prosopis alba&lt;/em&gt; - o Algarrobo blanco- es una especie leguminosa de gran importancia en la composición arbórea de zonas áridas y semiáridas. Posee un gran potencial para la producción de material forestal, siendo tradicional e intensivamente empleado en el uso de madera para muebles, construcción y carpintería, producción de leña y carbón, alimentación animal y también humana, fijadora de nitrógeno y muy utilizada en sistemas silvopastoriles.&lt;/p&gt;
&lt;p&gt;Actualmente este recurso proviene principalmente de plantaciones naturales, a través de la explotación intensiva de los montes nativos. Por estos motivos hay un gran interés en obtener tecnologías de domesticación, mejoramiento, propagación y conservación de esta especie. Debido a su difícil enraizamiento &lt;em&gt;Prosopis alba&lt;/em&gt; es propagada a través de semillas. Se han realizado numerosos trabajos en el país y también en el exterior con el objetivo de lograr una técnica de clonación viable para la comercialización de plantines de clones elites de &lt;em&gt;Algarrobo blanco&lt;/em&gt;, la mejor de las técnicas probadas hasta la fecha era el injerto, trabajos realizados por Ewens y Felker, y Felker y Guevara (2003), sin embargo aún no ha podido ser llevada a cabo comercialmente.&lt;/p&gt;
&lt;p&gt;Todavía no hay una técnica de propagación vegetativa monoclonal que sea adecuada para la producción de grandes cantidades de clones de &lt;em&gt;Prosopis alba&lt;/em&gt; que serian necesarios para las plantaciones a escala comercial. La propagación clonal posibilita acelerar el crecimiento y desarrollo de especies forestales, lo que es de suma importancia en plantíos comerciales, ya que disminuye el turno de corte (tala rasa) de la forestación comercial, así como multiplica genotipos seleccionados de características silvícola de interés económico para el mercado regional.&lt;/p&gt;
&lt;p&gt;La utilización de la propagación de especies forestales a través de la técnica de miniestacas, debido al rejuvenecimiento regenerado por el manejo de las plantas madres y técnicas aplicadas al enraizamiento, posibilita un aumento considerable de las tasas de crecimiento y de enraizamiento lo que es de suma importancia para especies de bajas tasas de enraizamiento, en un corto plazo de tiempo, generando un aumento de la productividad por unidad de área, una menor variación estacional y con costos más económicos que los plantines generados por otros medios.&lt;/p&gt;
&lt;h3 id=&#34;técnica-de-miniestacas&#34;&gt;Técnica de miniestacas&lt;/h3&gt;
&lt;p&gt;Desde la introducción de la clonación forestal por macropropagación monoclonal a través de la técnica de rejuvenecimiento en minijardín clonal y generación de plantines a través de la técnica de miniestacas, la silvicultura clonal viene experimentando grandes y exitosos avances, responsables del establecimiento de montes homogéneos de elevada productividad. Estas técnicas actualmente son las más adoptada por las principales empresas forestales de Brasil para la producción comercial de clones de &lt;em&gt;Eucalyptus&lt;/em&gt;. La técnica de miniestacas consiste en el uso de propágulos (brotes) rejuvenecidos de plantas madres (donantes) que están bajo manejo y condiciones específicas para generar un mayor grado de rejuvenecimiento y menor grado de lignificación de los explantes.&lt;/p&gt;
&lt;p&gt;La utilización de esta técnica ha posibilitado el suficiente rejuvenecimiento de los materiales como para eliminar o reducir la utilización de reguladores de crecimiento entre otras ventajas como las citadas anteriormente. Uno de los problemas que tiene la propagación vegetativa de las especies forestales se relaciona con problemas en el desarrollo de raíces adventicias que son las que posibilitan el logro de la clonación.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2013/10/algarrobo2-225x300.jpg&#34; alt=&#34;Plantines clonales de Algarrobo blanco en fase de enraizamiento.:right&#34;&gt;&lt;/p&gt;
&lt;p&gt;En esta dirección, investigadores de la Facultad de Ciencias Agrarias pertenecientes al Instituto de Agrobiotecnología del Litoral (CONICET-UNL) vienen trabajando desde hace 3 años en el uso de la técnica de miniestacas para la propagación vegetativa de esta especie. La investigación es parte del plan de tesis doctoral de la Ing. Agr., Mr. Sc., Jonicélia Cristina Araujo Vieira de Souza, Becaria de Postgrado Tipo II (CONICET) bajo la Dirección del Dr. Abelardo C. Vegetti (IAL-FCA) y la Co-Dirección del Ing. Agr. Luis Mroginski (IBONE-FCA). El trabajo está subsidiado por la Universidad Nacional del Litoral (CAI+D y CAI+D Orientado). El proyecto tiene por objetivos&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Estudiar la capacidad de enraizamiento de miniestacas de esta especie nativa en relación con la influencia de diferentes propágulos y la aplicación de diferentes concentraciones de reguladores de crecimiento.&lt;/li&gt;
&lt;li&gt;Analizar la relación entre las características histológicas de las miniestacas con la capacidad de enraizamiento.&lt;/li&gt;
&lt;li&gt;Interrelacionar y utilizar la información resultante en la propuesta de protocolos optimizados de propagación comercial de la especie.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;resultados&#34;&gt;Resultados&lt;/h3&gt;
&lt;p&gt;En Esperanza (FCA-IAL) se logró entre 98% a 100% de miniestacas enraizadas para 55 genotipos seleccionados de características silvícolas de interés económico, evaluados, mediante la técnica de miniestacas. El tratamiento fisiológico de rejuvenecimiento, sumado al de la etiolación, ambos conferidos por la aplicación de la técnica de miniestacas, promovieron el alto porcentaje de enraizamiento logrado para &lt;em&gt;Prosopis alba&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2013/10/algarrobo3-225x300.jpg&#34; alt=&#34;Cuantificación de sistema radical de plantines clonales de Algarrobo blanco en fase de enraizamiento.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Como otro resultado a destacar, hallaron que que el grado de juvenilidad y el grado de lignificación en los explantes utilizados influenciaron en el enraizamiento. Se encontraron variaciones entre clones de &lt;em&gt;P. alba&lt;/em&gt;; es  por ello que se sugiere que la técnica debería ser ajustada para cada clon para lograr un índice satisfactorio en el enraizamiento. Estos resultados exitosos serán puestos a prueba para la producción comercial de plantines por la Escuela de Educación Agropecuaria Nº 13 - Jardinería de Resistencia (Chaco) dirigida por el Ing. José Ruchesi en el marco del Convenio de Cooperación científica y tecnológica entre el CONICET y el Ministerio de Educación, Cultura, Ciencia y Tecnología del Chaco.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Nuevo libro de Ana Guglielmucci: La consagración de la memoria</title>
      <link>https://ciencianet.com.ar/post/nuevo-libro-de-ana-guglielmucci-la-consagracion-de-la-memoria/</link>
      <pubDate>Thu, 03 Oct 2013 00:43:18 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/nuevo-libro-de-ana-guglielmucci-la-consagracion-de-la-memoria/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Ana Guglielmucci&lt;/strong&gt; (Dra. en Antropología e Investigadora asistente de Conicet) reseña su nuevo libro, editado en 2013 por Antropofagia: &lt;strong&gt;La consagración de la memoria - Una etnografía acerca de la institucionalización del recuerdo sobre los crímenes del terrorismo de Estado en la Argentina&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2013/10/memoria.jpg&#34; alt=&#34;ISBN: 978-987-1238-99-6:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;En Argentina, qué recordar y qué olvidar respecto a la llamada violencia política de los setenta se ha constituido en un tema de interés y de debate entre militantes de derechos humanos, familiares de detenidos-desaparecidos, sobrevivientes, políticos, periodistas y miembros de las Fuerzas Armadas y de Seguridad, además de otros actores. Asimismo, la interpretación y adjudicación de responsabilidades jurídicas sobre eventos pasados, su documentación y transmisión a las nuevas generaciones ha sido un tema central de la “agenda pública” de los gobiernos constitucionales instaurados con posterioridad a ese momento histórico.&lt;/p&gt;
&lt;p&gt;Desde mediados de la década del noventa, la categoría memoria comenzó a instalarse como una consigna del movimiento de derechos humanos, sumándose a la tradicional demanda de Verdad y Justicia. Paralelamente, se multiplicaron los estudios sobre la construcción social de recuerdos sobre el pasado reciente. Y, a través del activismo político-militante y el trabajo profesional de diversos actores, la categoría memoria sobre el terrorismo de Estado fue incorporada en proyectos, leyes y programas gubernamentales, ceremonias oficiales y obras materiales destinados a preservarla y promoverla públicamente.&lt;/p&gt;
&lt;p&gt;En este contexto, el libro de Ana Guglielmucci, a partir de una perspectiva etnográfica, nos permite encontrar lineamientos para analizar la consagración de la memoria como objeto de políticas públicas en Argentina y, más específicamente, en la Ciudad de Buenos Aires. A lo largo del recorrido analítico de la autora, el lector puede comprender los procesos sociales y culturales que han alentado a construir, mantener y difundir recuerdos comunes sobre eventos pasados que se pretenden como aún presentes.&lt;/p&gt;
&lt;p&gt;Hablar de memoria en Argentina, a diferencia de otros países, conlleva una marca indeleble ligada a las prácticas terroristas ejecutadas por el Estado, como la instauración de centros clandestinos de detención y la desaparición forzada de personas. El análisis antropológico del proceso de diseño e implementación de una serie de políticas públicas para evocar estos eventos pasados permite revisar los sentidos hegemónicos locales materializados en obras públicas como monumentos y sitios de memoria.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>La complejidad del desarrollo del mosquito Aedes aegypti</title>
      <link>https://ciencianet.com.ar/post/la-complejidad-del-desarrollo-del-mosquito-aedes-aegypti/</link>
      <pubDate>Sun, 08 Sep 2013 00:45:02 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/la-complejidad-del-desarrollo-del-mosquito-aedes-aegypti/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero.&lt;/strong&gt; Facultad de Ciencias Exactas (UNLP).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Modeling the complex hatching and development of Aedes aegypti in temperate climates&lt;/strong&gt; es una publicación aparecida en la revista &lt;em&gt;Ecological Modelling&lt;/em&gt; en 2013, producto de un proyecto interdisciplinar, donde conviven teoría y experimentación. El trabajo fue llevado adelante por Victoria Romeo Aznar, Marcelo Otero, y Hernán G. Solari, del Departamento de Física, junto a María Sol De Majo y Sylvia Fischer, del Departamento de Ecología, Genética y Evolución, ambas instituciones de la Facultad de Ciencias Exactas y Naturales de la Universidad de Buenos Aires.&lt;/p&gt;
&lt;p&gt;Los autores presentan y discuten aquí un modelo estocástico y compartimental para el Aedes aegypti (mosquito doméstico, responsable de la transmisión de Dengue y Fiebre amarilla) con la idea de primeramente elucidar y luego predecir los fenómenos biológicos relacionados con la eclosión de los huevos y el desarrollo del mosquito (ver el esquema del modelo en la Figura). El trabajo está enmarcado en contexto de la eco-epidemiología matemática, es decir, el desarrollo de métodos matemáticos -tanto de modelado como de inferencia estadística- que permitan el estudio de procesos epidémicos.&lt;/p&gt;
&lt;p&gt;Este tipo de tratamientos incluyen en general múltiples aspectos: la biología de la enfermedad en el huésped y vector, la dispersión de vectores, las variables climáticas que pudieran estar influyendo en los procesos, cuestiones geográficas locales, movilidad de personas, entre otros.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2013/09/paper_solari1-616x472.jpg&#34; alt=&#34;Los compartimentos representan las respectivas poblaciones. Las transiciones se representan con flechas.&#34;&gt;&lt;/p&gt;
&lt;p&gt;En los últimos años los autores han desarrollado un modelo llamado aedesBA, previo al presentado en trabajo que reseñamos aquí, que describe el comportamiento de poblaciones de Aedes aegypti en la ciudad de Buenos Aires. Según describen, se trata de un &lt;em&gt;“developmental model”&lt;/em&gt;, es decir, un modelo que ha sido diseñado con el objeto de ser usado en el proceso de construcción de conocimiento teórico. El aedesBA fue confrontado con datos reales de postura de huevos en el barrio de Mataderos en un año con clima regular, generando un perfil de actividad compatible con las observaciones.&lt;/p&gt;
&lt;p&gt;Posteriormente fue usado para analizar la posibilidad de un brote de dengue en Buenos Aires, prediciendo una circulación no epidémica del virus que –a pesar de las expectativas en sentido contrario de las autoridades del sistema sanitario- fue efectivamente reportado en 2009, un año de notable sequía en la región. Sin embargo, el aedesBA no tenía en cuenta los registros de lluvias, correspondiendo la predicción correspondía a un año de clima normal y no de sequía, por lo cual podía pensarse que el éxito predictivo del modelo se debía a motivos erróneos.&lt;/p&gt;
&lt;p&gt;Tampoco consideraba la alimentación del mosquito adulto como un factor limitante. En este sentido, el nuevo modelo se adentra en la complejidad del problema: incorpora el efecto de las lluvias en el mecanismo regulatorio de la población de mosquitos. Así, considera dos factores climáticos que pueden influir en ambientes de clima templado: las lluvias y las temperaturas diarias. La lluvia tiene el efecto disparador de la eclosión de los huevos y por lo tanto modula la densidad de larvas, aunque también se considera en el modelo una tasa de eclosión independiente de las precipitaciones, como ocurría en el aedesBA. Por otra parte, la mortalidad y el desarrollo de las larvas en su estadio posterior (pupa), dependen aquí de la disponibilidad de nutrientes en el ambiente, que es modelado mediante una población de bacterias que crece regulada por la temperatura.&lt;/p&gt;
&lt;p&gt;Los parámetros fisiológicos reportados en la literatura, como la cantidad de huevos por puesta, la duración de la vida del mosquito, o el rango de vuelo, suelen presentar una gran variación, propia de la variabilidad intrínseca de la especie (diferencias entre poblaciones locales o cepas) pero también de las técnicas de experimentación/medición. Los autores consideraron entonces diferentes conjuntos de parámetros, encontrando como resultado que las predicciones obtenidas por el modelo presentan una gran sensibilidad a las características locales de la población y del ambiente (como la disponibilidad de alimento), y que deben ser tenidas en cuenta a la hora de evaluar posibles métodos de control del mosquito, como por ejemplo la liberación de machos estériles.&lt;/p&gt;
&lt;p&gt;Con respecto a las lluvias, el modelo indica que las poblaciones se recuperan rápidamente luego de una sequía, por lo cual las predicciones del aedesBa y del nuevo modelo, que incorpora registros de lluvias, están en acuerdo. Entre las conclusiones del trabajo, y más allá de los buenos resultados del modelo modificado, los autores plantean la necesidad de obtener más y mejor información biológica. Y en esta búsqueda de conectar el mundo matemático con el empírico, proponen en su trabajo nuevas preguntas biológicas cuyas respuestas guiarán el futuro progreso.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt;  &lt;a href=&#34;https://doi.org/10.1016/j.ecolmodel.2012.12.004&#34;&gt;Ecological Modelling 253 (2013) 44– 55&lt;/a&gt;.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>CienciaNet gana proyecto de CONICET y expande su política editorial</title>
      <link>https://ciencianet.com.ar/post/ciencianet-gana-proyecto-de-conicet-y-expande-su-politica-editorial/</link>
      <pubDate>Mon, 26 Aug 2013 00:49:10 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/ciencianet-gana-proyecto-de-conicet-y-expande-su-politica-editorial/</guid>
      <description>
        
          &lt;p&gt;El Consejo Nacional de Investigaciones Científicas y Técnicas de Argentina (CONICET) ha calificado a &lt;strong&gt;CienciaNet&lt;/strong&gt; con el primer puesto en el orden de mérito en su Convocatoria 2012 para Proyectos de Divulgación. La financiación otorgada por CONICET permitirá sostener en el tiempo este portal y enriquecer su contenido.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CienciaNet&lt;/strong&gt; invita a investigadores del sistema científico nacional y a periodistas científicos a enviar notas cortas de divulgación, para ser consideradas para su publicación en nuestro portal.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nueva política editorial&lt;/strong&gt; Anteriormente, &lt;strong&gt;CienciaNet&lt;/strong&gt; aceptaba notas que describieran un artículo científico recientemente publicado o una línea de investigación de un grupo de trabajo con la condición de que estuviera escrito por un autor que no hubiere formado parte del trabajo descripto. Esta política ha sido revisada y actualmente &lt;strong&gt;CienciaNet&lt;/strong&gt; acepta también artículos de divulgación sobre trabajos científicos publicados en revistas con referato, escritos por los propios autores del trabajo de investigación. &lt;strong&gt;CienciaNet&lt;/strong&gt; se reserva el derecho de editar los textos en caso de que fuera necesario ajustarlos a los objetivos y formato del portal, aunque la versión final contará siempre con la aprobación del autor.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; &lt;a href=&#34;mailto:manuel@iflysib.unlp.edu.ar&#34;&gt;Manuel Carlevaro&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Proyecto Choiols: la astronomía como vehículo para la emancipación</title>
      <link>https://ciencianet.com.ar/post/proyecto-choiols-la-astronomia-como-vehiculo-para-la-emancipacion/</link>
      <pubDate>Mon, 24 Jun 2013 00:53:07 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/proyecto-choiols-la-astronomia-como-vehiculo-para-la-emancipacion/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Patricia Knopoff.&lt;/strong&gt; Facultad de Ingeniería, Universidad Nacional de La Plata.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2013/06/choiols1-768x456.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;¿Por qué y para qué didáctica de la astronomía? Hubo tiempos en que la Verdad y la Mentira sobre el cielo estaban en manos de una institución que ostentaba el poder. La Tierra era el Centro del Universo, morada del Rey de la Creación, y estaba quieta. La causa del movimiento de los astros era el Primer Movil aristotélico, que Santo Tomás identificó con Dios. La materia celeste era perfecta, obviamente diferente de la sublunar, corrompible como nosotros mismos. Discutir estas Verdades era blasfemar. Y un día, esas Verdades fueron cuestionadas. Tal vez la Tierra sí se movía. Tal vez el Cielo era de la misma materia que la Tierra. Tal vez Júpiter era también centro de movimientos.&lt;/p&gt;
&lt;p&gt;Con un telescopio y paciencia, gente en el margen o directamente fuera del Poder planteó modelos, discutió. Y ese acto fue una puerta que invitó a abrir otras. Ocurrió una Revolución, y no sólo en el sentido Kuhniano: el Clero y la Nobleza fueron reemplazados por la Burguesía al tiempo que Dios era reemplazado por “El Hombre”. Hoy una nueva Institución se arroga la detentación de la Verdad. Esa institución se hace llamar “Ciencia”, pero lo que de Ciencia llega a las escuelas y los medios de comunicación es tan dogmático como lo de aquella otra, la de la Institución derrocada. ¿Qué diferencia hay entre un niño de hoy que repite que la Tierra gira alrededor del Sol y de su propio eje, y el monje medieval que repetía que la Tierra es el centro del Universo y Dios el Primer Móvil? Ambos repiten sin comprender, so pena de ser tratados de ignorantes (o peor) si disienten.&lt;/p&gt;
&lt;p&gt;Prueba de ello es la enorme cantidad de gente que “descubre” que “en realidad” la Tierra es plana, o está quieta, y patenta su “descubrimiento”, y trajina universidades y medios en una obsesión inclaudicable. ¿Están locos? Un poco, puede ser. Pero la forma en que canalizan su locura algo dice de su entorno. Que niños y adultos tienen representaciones salvajemente diferentes de la representación “oficial” está reconocido ampliamente por didactas y pedagogos. Pero la respuesta es con frecuencia el diseño de modos más eficientes para implantar en cada mente el dogma “correcto”.&lt;/p&gt;
&lt;p&gt;Creemos que la Didáctica no debe ser un vehículo para hacer aceptar con mayor eficacia una producción de conocimiento ajena. No importa lo razonables o útiles que sean los modelos, serán dogmas si se imponen desde afuera, desde la autoridad. La astronomía es particularmente potente como vehículo de emancipación. Los fenómenos astronómicos son tan conspicuos que no hay quien no repare en ellos, de modo que es fácil plantear cuestiones que resulten en genuinos problemas para cualquiera: ¿Es casualidad que la época más fría sea también la época en que los días son más cortos? ¿Cómo es que hay muchas estrellas que parecen moverse en masa y unas pocas que parecen tener movimientos propios? ¿Por qué me sigue la Luna cuando me muevo? ¿Qué pasa con el Sol cuando es de noche? ¿Por qué no es siempre luna llena?&lt;/p&gt;
&lt;p&gt;Pero una vez instaladas las preguntas, si queremos formar ciudadanos críticos con criterio propio sería un error imponer las respuestas dogmáticas. Una búsqueda de modelos diversos que den cuenta de estos fenómenos es una manera de entrar en los modos de hacer de la Ciencia (la Ciencia real, la que hacen los Científicos, la que es una obra cultural, humana, provisional y falible) y de comprender que no hay Verdades sino representaciones útiles. Si una persona se adueña de esta pieza epistemológica fundamental sabrá que el Sur está para donde él decida colocarlo, será una persona más libre y más sabia.&lt;/p&gt;
&lt;h3 id=&#34;el-proyecto&#34;&gt;El Proyecto&lt;/h3&gt;
&lt;p&gt;El Proyecto Choiols nace en el año 2010 con la intención de sistematizar acciones en favor de una Didáctica de la Astronomía especialmente orientada hacia los docentes que curricularmente deben abordar los contenidos conceptuales básicos que hacen a la comprensión de los fenómenos astronómicos en los que se encuentra involucrada la Tierra y de sus consecuencias sobre nuestro planeta y nuestra vida cotidiana. Choiols se constituye como un grupo independiente y está integrado por profesores y licenciados en física, profesores de matemática, historia y estudiantes de Astronomía, en su mayoría de la Universidad Nacional de La Plata.&lt;/p&gt;
&lt;p&gt;Choiols es entonces una propuesta para aprender Astronomía a ras del suelo: propone investigar cómo se mueve la Tierra, y comprender las implicancias de esos movimientos: las estaciones del año y los ciclos del día y la noche. Y para ello, se propone el uso de dispositivos muy sencillos que se observan en el suelo: &lt;strong&gt;una tríada didáctica constituida por un Globo Terráqueo Paralelo, un Gnomon y una Esfera Lisa&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;fundamentación-de-la-propuesta&#34;&gt;Fundamentación de la propuesta&lt;/h3&gt;
&lt;p&gt;Existen estudios que revelan una inadecuada conceptualización, por parte de los docentes de Escuela Primaria, de los movimientos relativos del Sistema Tierra-Sol y sus consecuencias. Así, estos docentes replican tales conceptualizaciones inadecuadas en los estudiantes, debido a la fuerza de la palabra -“sabia” para el alumno-, cual es el discurso de sus profesores (Camino, 1995).&lt;/p&gt;
&lt;p&gt;La propuesta de Choiols es abandonar la clase magistral y los aprendizajes memorísticos. Se pretende fomentar el aprendizaje de los fenómenos astronómicos que acontecen en la Tierra por observación directa de los mismos. Para ello se trabaja con dispositivos que informan directamente y en tiempo real sobre estos fenómenos.&lt;/p&gt;
&lt;h3 id=&#34;la-tríada-didáctica&#34;&gt;La tríada didáctica&lt;/h3&gt;
&lt;p&gt;Choiols propone el uso de tres dispositivos que aumentan su capacidad explicativa y su potencialidad didáctica cuando se utilizan de modo conjunto. Se complementan y completan, formando una verdadera tríada de trabajo observacional.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;¿Por qué un Globo Terráqueo Paralelo?&lt;/strong&gt; El Proyecto propone que los globos terráqueos salgan de su estado de abandono en las bibliotecas y vuelvan a ocupar su lugar como dispositivos didácticos. Para ello, suscribimos a la Liberación del Globo (&lt;a href=&#34;http://www.globolocal.net/esp/index_esp.html&#34;&gt;www.globolocal.net&lt;/a&gt;) mediante la cual se libera de su soporte tradicional al Globo Terráqueo y se lo coloca en posición homotética con el planeta. Un globo terráqueo tradicional pretende representar la inclinación del eje de rotación respecto de la Eclíptica, encontrándose en una posición que no representa a nadie en particular y no informa de nada, específicamente. Y no existe ningún lugar en particular desde donde se pueda observar al planeta en esa posición.&lt;/p&gt;
&lt;p&gt;Un Globo Terráqueo en posición Paralela representa al planeta homotéticamente para cada posición topocéntrica. Y en esta posición (en la que el eje del globo es paralelo al eje del planeta y el punto más alto del globo es el la posición del observador en la Tierra, por ejemplo La Plata) adquiere una enorme potencialidad explicativa, informando en tiempo real respecto del estado de iluminación en cualquier lugar del Planeta.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;¿Por qué un Gnomon?&lt;/strong&gt; Porque un dispositivo tan simple como una vara recta en posición vertical nos informa del movimiento diurno del Sol en el cielo, observando la trayectoria de los rayos de luz que pasan por su extremo y las posiciones que ocupan las sombras que proyecta en el suelo durante el devenir del movimiento del Sol durante el día.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;¿Por qué una Esfera Lisa?&lt;/strong&gt; Porque no podemos observar al planeta “desde fuera”. Pero podemos determinar a partir de la dinámica del estado de iluminación de la Esfera Lisa el correspondiente estado de iluminación del Planeta. Porque ambos se encuentran vinculados por una homotecia y son iluminados en tiempo real, simultáneamente, por la misma fuente de luz –el Sol-. La esfera lisa es un dispositivo ideado por el Dr. Néstor Camino.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2013/06/choiols2-300x300.jpg&#34; alt=&#34;:right&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;el-nombre-choiols&#34;&gt;El nombre Choiols&lt;/h3&gt;
&lt;p&gt;En varias culturas de América del Sur se registran leyendas que asocian a la Constelación de la Cruz del Sur con el Ñandú. Choiols es el nombre Aonikenk de esa constelación y la palabra significa “la huella de ñandú en el cielo”. Choiols es la huella en el cielo de lo que debería estar en el suelo, una pisada de ñandú. El Proyecto invita a conocer más sobre el Cielo, pero mirando al suelo. Choiols es, para quienes lo integran, &lt;strong&gt;&lt;em&gt;la huella del Cielo en el Suelo&lt;/em&gt;&lt;/strong&gt;. Es por eso que propone hacer Astronomía a ras del suelo. El Proyecto Choiols tiene su base en la ciudad de La Plata y realiza regularmente variadas acciones educativas, como jornadas de observación en espacios públicos, actividades en escuelas y talleres para docentes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; &lt;a href=&#34;mailto:choiols@yahoo.com.ar&#34;&gt;choiols@yahoo.com.ar&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Más información:&lt;/strong&gt; &lt;a href=&#34;http://choiols.unlp.edu.ar/&#34;&gt;http://choiols.unlp.edu.ar/&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Física y biología para develar misterios del control motor durante el canto de las aves</title>
      <link>https://ciencianet.com.ar/post/fisica-y-biologia-para-develar-misterios-del-control-motor-durante-el-canto-de-las-aves/</link>
      <pubDate>Thu, 25 Apr 2013 01:04:14 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/fisica-y-biologia-para-develar-misterios-del-control-motor-durante-el-canto-de-las-aves/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Ana Amador.&lt;/strong&gt; Departamento de Física, Facultad de Ciencias Exactas y Naturales, Universidad de Buenos Aires. CONICET.&lt;/p&gt;
&lt;p&gt;El modo en que las aves aprenden a cantar es uno de los pocos ejemplos que existen en el reino animal de aprendizaje vocal, siendo un proceso similar al que seguimos los humanos para aprender a hablar. Además del aprendizaje, la ejecución de un canto es en sí misma una tarea compleja ya que requiere una fina coordinación del sistema respiratorio y el sistema muscular que controla la fuente sonora (similar a las cuerdas vocales de humanos). Debido a esto, es de gran interés el estudio de las instrucciones neuronales utilizadas durante la ejecución de un comportamiento complejo como es el canto.&lt;/p&gt;
&lt;p&gt;Un trabajo de colaboración entre la Universidad de Buenos Aires (Yonatan Sanz Perl y Gabriel B. Mindlin) y la Universidad de Chicago (Ana Amador y Daniel Margoliash) permitió mostrar, midiendo cómo el sistema neuronal procesa la información recibida, que un ave puede reconocer un canto sintético (simulado por un modelo físico-matemático) como su propio canto.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2013/04/amador-300x211.jpeg&#34; alt=&#34;Diamante mandarín.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;La reducción del número de variables necesarias propuesta desde la física permitió generar una nueva hipótesis sobre el código neuronal utilizado para el procesamiento y generación del canto. En el Laboratorio de Sistemas Dinámicos de la Facultad de Ciencias Exactas y Naturales (UBA), Gabriel Mindlin y colaboradores generaron un modelo físico para la producción de canto de aves cuyo resultado es un canto sintético que puede compararse con el canto real del ave.&lt;/p&gt;
&lt;p&gt;Este modelo logró una reducción muy importante del número de variables incluidas en el problema: ajustando sólo dos parámetros fue posible reproducir el canto del diamante mandarín (Taeniopygia Guttata). Si bien los cantos sintéticos y reales eran muy similares, la pregunta del millón era si estos cantos sintéticos sonaban reales para el ave. Para evaluarlo, en este trabajo nos propusimos medir las respuestas neuronales.&lt;/p&gt;
&lt;p&gt;En el cerebro de las aves canoras, existen núcleos neuronales que responden específicamente al escuchar el canto propio, y no responden al escuchar otros estímulos como ruido o tonos, pero más importante, tampoco responden a cantos de otros individuos de la misma especie (conespecíficos) o al canto del propio ave si es modificado intencionalmente. Es por esto que estas neuronas se conocen como selectivas al canto propio.&lt;/p&gt;
&lt;p&gt;Para poner a prueba al modelo sintético de canto, se presentó el canto propio del ave y su versión sintética, y se midió la respuesta neuronal del núcleo HVC (un núcleo con neuronas selectivas al canto propio), obteniendo el mismo patrón de respuesta para ambos cantos. Se concluyó entonces que los cantos sintéticos, generados con este modelo matemático que requiere de pocas variables, es suficiente para captar las características relevantes del canto del ave.&lt;/p&gt;
&lt;p&gt;Este modelo físico así validado permitió identificar dos parámetros (“gestos motores”) que se usaron para estudiar el código neuronal utilizado por el ave para el procesamiento y generación del canto. Los resultados sugieren que la información auditiva del canto que llega a áreas de la corteza cerebral está codificada según coordenadas asociadas a los movimientos musculares necesarios para producir el canto.&lt;/p&gt;
&lt;p&gt;Además, debido a que la actividad neuronal está sincronizada con el canto, las neuronas no podrían estar actuando de manera promotora ya que desde la activación neuronal en la corteza hasta la generación del canto se estima que existe un retraso considerable debido a la cantidad de sinapsis (conexiones entre neuronas) intervinientes. De esta manera se propone una codificación neuronal novedosa donde los gestos motores representados en la corteza del cerebro son utilizados para realizar predicciones del comportamiento necesario para generar el canto. Esto lleva a un marco de referencia conceptual novedoso para estudiar el código neuronal.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Articulo original:&lt;/strong&gt; “&lt;a href=&#34;http://www.nature.com/nature/journal/v495/n7439/full/nature11967.html&#34;&gt;Elemental gesture dynamics are encoded by song premotor cortical neuons&lt;/a&gt;”, Nature 495, 59 (2013).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Comentario de Todd Troyer:&lt;/strong&gt; &lt;a href=&#34;http://www.nature.com/nature/journal/v495/n7439/full/nature11957.html&#34;&gt;Neuroscience: The units of a song&lt;/a&gt;, Nature 495, 56 (2013).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Otros enlaces relacionados:&lt;/strong&gt;  &lt;a href=&#34;http://www.scholarpedia.org/article/Models_of_birdsong_%28physics%29&#34;&gt;Models of birdsong&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Nanobiotecnología y nanomedicina</title>
      <link>https://ciencianet.com.ar/post/nanobiotecnologia-y-nanomedicina/</link>
      <pubDate>Thu, 11 Apr 2013 01:09:24 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/nanobiotecnologia-y-nanomedicina/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero.&lt;/strong&gt; Investigadora de CONICET en el INIFTA (Instituto de Investigaciones Fisicoquímicas Teóricas y Aplicadas). Docente en la UNLP.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reseña del artículo Nanotecnología, hacia un nuevo portal científico-tecnológico de&lt;/strong&gt; Fiona M. Britto y Guillermo R. Castro (&lt;em&gt;Laboratorio de Nanobiomateriales, CINDEFI (CONICET-UNLP, CCT La Plata), Dto. de Química, Facultad de Cs. Exactas, UNLP) publicado en la revista Química Viva de Diciembre de 2012, número 3 año 11&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2013/04/quimica_viva.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;En este artículo de revisión los autores plantean ofrecen un panorama general de las aplicaciones más prometedoras de la Nanomedicina –disciplina de reciente creación, a partir de los avances de la Nanobiotecnología-, como ingeniería de tejidos, liberación controlada de fármacos, y detección precoz de enfermedades. También comentan y discuten algunas de las tendencias y desafíos actuales del área.&lt;/p&gt;
&lt;p&gt;El artículo comienza describiendo el proceso que se denomina Nanotecnología, que esencialmente consiste en la producción de objetos tecnológicos debido a la manipulación de la materia a escala atómica y que ha generado un cambio de paradigma que involucra tanto la concepción y diseño de los objetos materiales como su producción.&lt;/p&gt;
&lt;p&gt;Existe una definición establecida por la &lt;em&gt;National Science Foundation&lt;/em&gt; según la cual un objeto es “nano” si alguna de sus tres dimensiones físicas está en una escala comprendida entre 1 y 100 nanómetros (un nanómetro es igual a 10&lt;sup&gt;-9&lt;/sup&gt; metros). Así, la Nanotecnología comprende el estudio, diseño, síntesis, manipulación y aplicación de materiales funcionales, dispositivos y sistemas a través del control de la materia a escala nanométrica, y también el uso de las nuevas propiedades en esa escala.&lt;/p&gt;
&lt;p&gt;La convergencia de la Nanotecnología con la Biología surge de pensar a las moléculas que participan en los procesos biomoleculares como lo que los autores denominan “complejas nano-bio-máquinas”. Las proteínas, ácidos nucleicos, lípidos y polisacáridos, entre otros componentes de la materia viva, tienen sus dimensiones dentro de la escala nanométrica, sugiriendo que su comportamiento está regido por las mismas reglas que se comportan los nanomateriales.&lt;/p&gt;
&lt;p&gt;Así surgen dos amplios campos para la investigación y desarrollo: la Nanobiotecnologia, y su aplicación al cuidado de la salud, la Nanomedicina. La Nanomedicina ofrece muchísimas aplicaciones potenciales y otras que encuentran ya en el mercado. Algunas de las más prometedoras son la liberación controlada y dirigida de fármacos, la ingeniería de tejidos, el diagnóstico por imágenes y la detección temprana de patologías, de las cuales el artículo ofrece detallada descripción y que reseñaremos brevemente aquí.&lt;/p&gt;
&lt;h3 id=&#34;drug-delivery&#34;&gt;Drug delivery&lt;/h3&gt;
&lt;p&gt;Es una metodología de administración de medicamentos que logra la reducción de los posibles efectos secundarios tóxicos y aumenta la eficiencia de las terapias, ya que los nanosistemas administran los fármacos de una manera programada, durante períodos prolongados, permitiendo niveles constantes de la droga en el organismo, e incluso con la habilidad de dirigir la droga hacia el sitio específico de acción.&lt;/p&gt;
&lt;p&gt;De esta forma, las dosis necesarias se reducen y los efectos nocivos también. Los nanocomponentes actúan entonces como portadores de la droga, permitiendo controlar: cuánta se libera, en qué órganos, en qué tipo de célula, en qué tiempo y por cuánto tiempo reside el fármaco en el organismo. De especial interés resultan los nano biopolímeros, pues al tratarse de sustancias naturales tienen buena compatibilidad y degradabilidad en el organismo, ocasionando pocas reacciones adversas. Como además responden a condiciones ambientales, como el pH, pueden ser “programados”.&lt;/p&gt;
&lt;h3 id=&#34;ingeniería-de-tejidos&#34;&gt;Ingeniería de tejidos&lt;/h3&gt;
&lt;p&gt;Conocida también como medicina regenerativa, consiste en el desarrollo de sustitutos biológicos, con la finalidad de resolver problemas clínicos y quirúrgicos asociados a la pérdida de tejido o al fallo funcional de órganos. Los materiales y terapias tradicionales que se usan en este campo presentan una serie de problemas de biocompatibilidad, degradabilidad, y como el rechazo por inflamación e infección, que serían resueltos por el desarrollo de nanosistemas híbridos biocompatibles, con propiedades mecánicas, eléctricas y de superficie que se acerquen a las de los tejidos reales.&lt;/p&gt;
&lt;p&gt;Actualmente el área de mayor desarrollo es la regeneración del tejido óseo, también está bajo el foco de la Nanoingeniería de tejidos la regeneración de vasos sanguíneos y neuronas, y los implantes dentales. Finalmente, el avance de esta rama de la Nanociencia dependerá del entendimiento que se logre de de las interacciones entre los nanomateriales y las células, a nivel molecular.&lt;/p&gt;
&lt;h3 id=&#34;detección-y-diagnóstico&#34;&gt;Detección y diagnóstico&lt;/h3&gt;
&lt;p&gt;El objetivo de este campo es el desarrollo de sistemas de análisis y de imagen utilizando nanobiomateriales, que superen la velocidad, sensibilidad y selectividad de los métodos existentes para el diagnóstico temprano de enfermedades. La clave para este desafío es potenciar las propiedades ópticas y eléctricas especiales de los nanomateriales con la capacidad de reconocimiento específico del material biológico. Los nanomateriales actúan aquí como biosensores, compuestos de un receptor biológico (por ejemplo, enzimas, ADN, anticuerpos) y dispositivo que envíe una señal medible, que –sin necesidad de otras sustancias que actúen de contraste- trabajan in vitro sobre pequeñas muestras de tejido.&lt;/p&gt;
&lt;p&gt;Por otra parte, para los estudios que sí requieren el empleo de sustancias de contraste, la utilización de nanopartículas metálicas, semiconductoras o magnéticas como agentes de contraste provee mejoras significativas en la precisión de imágenes obtenidas &lt;em&gt;in vivo&lt;/em&gt;. La aplicación más conocida son los “puntos cuánticos”: nanopartículas semiconductoras, muy útiles como marcadores biológicos gracias a su fluorescencia. Como esta propiedad es dependiente del tamaño de las partículas, pueden obtenerse puntos cuánticos que emiten de modo intenso y bien definido en una amplia gama de colores, mejorando la calidad de la imagen en resonancias magnéticas, tomografías computadas, imágenes de fluorescencia, etc.&lt;/p&gt;
&lt;p&gt;Sin embargo, su uso no está muy extendido pues estas sustancias resultan actualmente costosas, y tampoco se conoce completamente su efecto en el organismo.&lt;/p&gt;
&lt;h3 id=&#34;nanotoxicología&#34;&gt;Nanotoxicología&lt;/h3&gt;
&lt;p&gt;Como puede apreciarse a lo largo del trabajo de Britto y Castro, son numerosas las aplicaciones de interés, con vistas a un uso extendido. Sin embargo, aún no se conocen completamente los alcances y consecuencias del uso de los nuevos materiales y tecnologías, de modo que los estudios toxicológicos de los mismos presentan una gran importancia a nivel mundial. Así, la Nanotoxicología tiene como objetivo el desarrollo de protocolos para la fabricación, uso, reciclaje, y descontaminación de nano-objetos, teniendo en cuenta que muchas de las propiedades de los nanomateriales dependen del tamaño de su particulado. Por ejemplo, las propiedades ópticas del oro -en particular, su color- dependen vistosamente del tamaño de las nanopartículas.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2013/04/gold_nanoparticles.jpg&#34; alt=&#34;Cambios en las propiedades ópticas del oro en función del tamaño de las nanopartículas. Créditos de la imagen: http://www.malvern.com/LabEng/industry/nanotechnology/gold_silver_nanoparticles.htm&#34;&gt;&lt;/p&gt;
&lt;p&gt;En cuanto al impacto de los nanoproductos sobre el medio ambiente, aún resta resolver cuestiones como la evaluación del ciclo de vida, la cuantificación de su emisión, los efectos sobre la cadena alimentaria, entre otros.&lt;/p&gt;
&lt;h3 id=&#34;a-modo-de-conclusión&#34;&gt;A modo de conclusión&lt;/h3&gt;
&lt;p&gt;El trabajo plantea interesantes observaciones respecto de las nuevas disciplinas. Además de la necesidad de seguir investigando en ellas, los autores reafirman la necesidad del trabajo conjunto entre áreas académicas, gobiernos y empresas que promuevan la inversión en investigación y desarrollo. También resaltan su naturaleza interdisciplinar y su consecuente requerimiento de profesionales especialmente formados en el área.&lt;/p&gt;
&lt;p&gt;Con los potenciales de las nanociencias y nanotecnologías, en combinación con los factores antes mencionados, y bajo una correcta implementación, los autores auguran un salto cuali y cuantitativo en la solución de los problemas de los países en desarrollo, y más aún, en el futuro de la humanidad.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Artículo original disponible en &lt;a href=&#34;http://www.quimicaviva.qb.fcen.uba.ar/&#34;&gt;http://www.quimicaviva.qb.fcen.uba.ar/&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>De la difusión de partículas a la difusión de la información: visualizando datos geoestadísticos mediante cartogramas</title>
      <link>https://ciencianet.com.ar/post/de-la-difusion-de-particulas-a-la-difusion-de-la-informacion-visualizando-datos-geoestadisticos-mediante-cartogramas/</link>
      <pubDate>Sat, 09 Mar 2013 01:11:21 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/de-la-difusion-de-particulas-a-la-difusion-de-la-informacion-visualizando-datos-geoestadisticos-mediante-cartogramas/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Karina Mazzitello&lt;/strong&gt; (Universidad de Mar del Plata) y &lt;strong&gt;Julián Candia&lt;/strong&gt; (Universidad Nacional de La Plata).&lt;/p&gt;
&lt;p&gt;La forma tradicional que se utiliza para visualizar resultados de censos y datos estadísticos en general es a través de diagramas de barras, de tortas y de cajas. En un diagrama de tortas a cada dato de una variable estadística dada le corresponde una porción de tamaño proporcional a su valor. Si el número de valores a representar es muy grande el diagrama de tortas resulta ilegible, debido a la gran cantidad de porciones requeridas en que se debería dividir la torta.&lt;/p&gt;
&lt;p&gt;Por este motivo vamos a encontrar muchas veces en la literatura extensas tablas muy detalladas con datos estadísticos tediosas de leer, sin ser volcadas a ningún tipo de diagrama para no perder información. Si en lugar de tablas o los diagramas tradicionales, se utiliza un mapa que represente, por ejemplo, la incidencia de una epidemia dada con código de colores, inevitablemente se verá alta incidencia en ciudades y baja incidencia en zonas rurales debido a sus diferencias en la cantidad de habitantes.&lt;/p&gt;
&lt;p&gt;Parecería a primera vista, que las zonas rurales fueran mas seguras, pero no necesariamente es así. Una alternativa es graficar la incidencia per cápita, que resuelve el problema a costa de descartar toda la información acerca de dónde ocurre la mayoría de los casos. Afortunadamente se puede combinar la densidad de población con datos geográficos para crear lo que podríamos llamar “cartogramas de difusión”, una forma mucho más eficiente y rápida para visualizar datos estadísticos de diferentes regiones, desde censos a mapas electorales.&lt;/p&gt;
&lt;p&gt;A modo de ilustración de ésta técnica se ha aplicado recientemente el método de los cartogramas a la problemática de la criminalidad en los estados de Brasil (ver el &lt;a href=&#34;http://link.springer.com/article/10.1007%2Fs13538-012-0091-0%5D(http://link.springer.com/article/10.1007%2Fs13538-012-0091-0)&#34;&gt;artículo original&lt;/a&gt;. Veremos con este ejemplo la utilidad de los mismos y luego explicaremos cómo se construyen. Comenzaremos mostrando un mapa original de Brasil y luego lo compararemos con un cartograma de dicho país, conteniendo ambas gráficas información similar.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2013/03/fig1a-300x257.jpg&#34; alt=&#34;Fig. 1(a): Un mapa de Brasil con los estados coloreados en escala de grises representando la tasa de homicidios cada cien mil habitantes. Los estados mas oscuros tienen mayor tasa de homicidios que los claros.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;En la figura 1(a) se muestra un mapa original de Brasil que ha sido coloreado según las tasas de homicidio cada 100.000 habitantes, correpondientes al año 2008. Las regiones oscuras son más peligrosas que las claras. En la figura 1(b) se grafica un cartograma de Brasil con los estados deformados según la tasa de homicidios cada 100.000 habitantes (año 2008). Los estados más oscuros son los más poblados y los más claros los menos poblados. Las áreas deformadas del cartograma son proporcionales a las tasas de homicidio y revelan una alta tasa de crímenes en la región noreste. Se puede apreciar que no necesariamente los estados más poblados tienen una alta tasa de homicidios como por ejemplo San Pablo y Minas Gerais, cuyas regiones no son tan grandes y sin embargo están muy poblados.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2013/03/fig1b-300x300.jpg&#34; alt=&#34;Fig. 1(b): Un cartograma de Brasil con los estados deformados para reflejar el número de homicidios cada cien mil habitantes. Revela una taza elevada en la región noreste. La escala de grises está asociada a la densidad de población (más oscuro implica mayor población).:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Resulta mas fácil visualizar los datos en el mapa deformado de la figura 1(b) que en el mapa convencional de la figura 1(a). La utilidad de los cartogramas queda en evidencia: rápidamente se puede atribuir el tamaño de la región de un mapa a una variable elegida, que en nuestro ejemplo es la tasa de homicidios. Además, los mismos pueden colorearse siguiendo un código con otra variable, permitiendo así dos capas de visualización de datos. En la figura 1(b) se colorearon los estados en escala de grises de acuerdo a sus densidades de población y como se mencionó anteriormente la tasa de homicidios no está relacionada a la cantidad de habitantes. Claramente, la construcción de mapas deformados ayuda a develar correlaciones.&lt;/p&gt;
&lt;p&gt;Las mediciones estadísticas realizadas sobre una población están frecuentemente correlacionadas entre sí, pero estas correlaciones permanecen ocultas si se utilizan directamente tablas o las herramientas tradicionales de visualización. Por ejemplo, en la figura 2 se muestra el mismo cartograma que en la figura 1(b) pero incluyendo un índice socioeconómico para cada estado. Las regiones rojas tienen menor nivel socioeconómico y tienden a una mayor tasa de homicidios. Es decir, hay una clara relación entre tasa de homicidios y el nivel socioeconómico dado por el acceso a empleo e ingresos, educación y salud. Verdaderamente esta correlación es fácil de visualizar en el cartograma de la figura 2.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2013/03/fig2-300x300.jpg&#34; alt=&#34;Fig. 2: El mismo cartograma que en la fig. 1(b) incluyendo datos socio-económicos. Las regiones rojas tienen menor nivel socio-económico que las azules y tienden a razones más altas de homicidios. Las correlaciones son mas difíciles de detectar en otros tipos de gráficas tales como los scatter plots.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Los cartogramas son gráficos elaborados a partir de una idea simple de la física asociada a los procesos difusivos de partículas. Supongamos que en una caja dividida por la mitad tenemos un gas que ocupa ambos lados de la caja a tempertura constante. Supongamos además, que en uno de los lados hay mayor densidad que en el otro. Si quitamos la pared que divide la caja habrá un flujo neto de partículas desde la región de mayor densidad a la región de menor densidad. El proceso difusivo alcanza un estado de equilibrio cuando la redistribución de partículas se homogeiniza. ¿Cómo se puede utilizar este fenómeno en la elaboración de cartogramas?&lt;/p&gt;
&lt;p&gt;Si representamos una variable estadística (por ejemplo, las tasas de homicidios por provincia) como diferentes densidades de partículas podemos estudiar su difusión hasta alcanzar el estado homogéneo. Con su desplazamiento, estas partículas arrastran los contornos de cada región del cartograma. El resultado es un mapa deformado cuya distorsión refleja la información estadística que se proporcionó como punto de partida. Retornando a los homicidios se pueden construir cartogramas de barrios en lugar de estados y hacer el análisis más focalizado.&lt;/p&gt;
&lt;p&gt;Este artículo pretende ilustrar la potencialidad y riqueza del método en una temática concreta para llamar la atención sobre su empleo como algo novedoso para la visualización de datos geográficos multidimensionales. También se puede aplicar esta técnica a diversas problemáticas locales o regionales, como por ejemplo la incidencia y distribución de determinadas enfermedades, epidemias, resultados de elecciones, censos y otros datos estadísticos con información geográfica. Recientemente el CONICET ha aprobado un proyecto para lanzarlo como herramienta de divulgación de datos geo-estadísticos a escala nacional y de alcance latinoamericano a través de una página web que será construida en el presente año.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>La conjetura de Maldacena</title>
      <link>https://ciencianet.com.ar/post/la-conjetura-de-maldacena/</link>
      <pubDate>Wed, 06 Feb 2013 01:15:02 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/la-conjetura-de-maldacena/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Gastón Giribet.&lt;/strong&gt; Universidad de Buenos Aires y CONICET.&lt;/p&gt;
&lt;p&gt;Gastón Giribet comparte con &lt;strong&gt;CienciaNet&lt;/strong&gt; su columna para el diario La Prensa, publicada en la edición en papel del lunes 7/1/13, donde ofrece una descripción de la idea detrás de la llamada &#39;conjetura de Maldacena&#39;. Para leer la nota del mismo diario, donde Juan Maldacena comenta su trabajo, cliquear &lt;a href=&#34;http://www.laprensa.com.ar/401132-Una-explicacion-que-puede-unir-dos-grandes-teorias.note.aspx&#34;&gt;aquí&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Algunas conferencias del propio Maldacena sobre su valiosa contribución, que cumplió 10 años en 2007, pueden encontrarse aquí: &lt;a href=&#34;http://video.ias.edu/search/node/maldacena&#34;&gt;http://video.ias.edu/search/node/maldacena&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2013/02/ten_years_maldacena-300x225.gif&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;La conjetura de Maldacena, también conocida con el nombre técnico de “correspondencia AdS/CFT”, es sin lugar a dudas uno de los resultados más importantes de la física teórica de las últimas décadas. Esto se debe en particular a que esta conjetura ofrece una novedosa y promisoria forma de abordar problemas cuya resolución eludió a los físicos teóricos durante muchísimas décadas. Entre las preguntas que la conjetura de Maldacena permite responder se encuentra la pregunta acerca de por qué las partículas fundamentales tales como los quarks tienden a permanecer unidos cuando no tienen demasiada energía. Esta pregunta es crucial para entender la constitución de la materia.&lt;/p&gt;
&lt;p&gt;Otra pregunta que gracias a este resultado podemos abordar es la de cuál es la explicación para las exóticas propiedades térmicas de los astros conocidos como agujeros negros. Así, tal como estos dos ejemplos muestran, el rango de aplicación del resultado de Maldacena va desde el infinitamente pequeño mundo subatómico hasta las escalas astrofísicas y cosmológicas.&lt;/p&gt;
&lt;p&gt;El logro de Maldacena ha sido, precisamente, mostrar que ciertas teorías físicas que rigen la física microscópica de las partículas fundamentales son totalmente equivalentes a ciertas teorías físicas con las que describimos fenómenos macroscópicos tales como la fuerza de gravedad entre los astros. Más precisamente, la conjetura de Maldacena establece la equivalencia entre dos teorías físicas que, si bien eran previamente conocidas, hasta fines de 1997 nadie había notado que estaban intrínsecamente relacionadas.&lt;/p&gt;
&lt;p&gt;Una de esas dos teorías es la denominada “teoría de cuerdas en el espacio-tiempo curvo”, que puede considerarse como una generalización de la teoría de la Relatividad General que Einstein propuso para describir el campo gravitatorio. La otra teoría es una teoría cuántica de campos, similar a la que los físicos emplean para estudiar las interacciones nucleares entre las partículas subatómicas. Según Maldacena, estas dos teorías son, en realidad, dos formas de describir las mismas ecuaciones o, como se suele decir, dos caras de la misma moneda.&lt;/p&gt;
&lt;p&gt;Para acercar al lector un poco más al valor que la conjetura de Maldacena tiene, permítaseme recurrir a la siguiente metáfora: Imaginémonos recorriendo los pasillos de una biblioteca inmensa; podemos pensar en la Biblioteca de Babel que Jorge Luis Borges ideó, una biblioteca cuyos anaqueles albergan todos los libros que podrían haber sido escritos. Imaginemos también que cada uno de esos libros corresponde a una teoría física. Algunos de esos libros -algunas de esas teorías- tendrán sentido y otros no; algunos de esos libros contendrán las fórmulas para entender ciertos fenómenos de la naturaleza mientras que otros se ocuparán de otros rincones de la ciencia. Debemos imaginar a Maldacena como un brillante bibliotecario con quien nos ha sido dado toparnos en nuestro recorrido.&lt;/p&gt;
&lt;p&gt;Con intuición genial, este bibliotecario toma dos libros de anaqueles distantes y nos los entrega, y al hacerlo nos afirma que esos dos libros, de apariencia tan distinta, en realidad contienen exactamente la misma historia, la misma información, aunque está esa historia escrita en dos idiomas distintos en cada uno de los libros. Nuestra sorpresa ante un hallazgo tal es infinita ya que, entre tantos libros, encontrar dos ejemplares del mismo parece &lt;em&gt;a priori&lt;/em&gt; impracticable; pero la sorpresa es mayor cuando constatamos que, además, esos dos libros no contienen una historia cualquiera sino que contienen las fórmulas que describen todas las leyes fundamentales de la física, desde las leyes que rigen lo más pequeño hasta las que rigen lo más grande, desde los constituyentes últimos de la materia hasta la geometría del mismo universo.&lt;/p&gt;
&lt;p&gt;Maldacena nos proporciona, además, un diccionario para poder traducir el idioma de uno de esos libros en el idioma del otro, lo que nos permite constatar que, en efecto, lo que nos dice es cierto y que ambos libros hablan en realidad de lo mismo, de la misma historia, de las mismas leyes. Para ir más allá con la analogía y entender en mayor profundidad la importancia del aporte de Maldacena, imaginemos que ambos libros hubieran sido víctimas de vandalismo en el pasado y que se hubieran arrancado las primeras páginas de uno de ellos y las últimas páginas del otro. Entonces, la importancia del diccionario que Maldacena nos provee sería capital ya que con él podemos nosotros reconstruir totalmente la historia; podríamos traducir lo que no sabíamos leer de uno de esos libros en términos del lenguaje del otro y descifrar, así, finalmente las leyes del universo.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>La ideología de la enseñanza de las ciencias</title>
      <link>https://ciencianet.com.ar/post/la-ideologia-de-la-ensenanza-de-las-ciencias/</link>
      <pubDate>Fri, 09 Nov 2012 01:20:01 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/la-ideologia-de-la-ensenanza-de-las-ciencias/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Ariel Dobry.&lt;/strong&gt; Instituto de Física Rosario (UNR)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;¿Qué cuestión ideológica puede encontrarse en la explicación de un problema de física por un docente? El físico y docente &lt;strong&gt;Ariel Dobry&lt;/strong&gt; comparte aquí algunas reflexiones a partir de su experiencia.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Ingresé a la docencia universitaria en el año 1983. La recuperación reciente de la democracia trajo como consecuencia el ingreso irrestricto a la universidad. La apertura de las puertas de la universidad después de años de cupos y examen de ingreso, aumentó el número de ingresantes e hizo necesaria la incorporación de nuevos docentes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2012/11/arieldobry.jpg&#34; alt=&#34;:left&#34; title=&#34;arieldobry&#34;&gt;&lt;/p&gt;
&lt;p&gt;Yo terminaba mi carrera de Licenciatura en Física y me había propuesto presentarme a cuanto concurso docente se abriera con el objetivo de conseguir varios cargos que me permitieran acumular un salario. Por otro lado la docencia me apasionaba. Esa búsqueda de argumentos sencillos para transmitir ideas complejas. Esa necesidad de profundizar los conceptos, de separar lo esencial de lo superfluo para cada etapa del conocimiento.&lt;/p&gt;
&lt;p&gt;Hoy podría identificar una parte de las motivaciones que condujeron mi trabajo de investigación posterior, como conectadas con esa búsqueda pedagógica. Pensando en cómo contar una idea, estudiando y haciendo pruebas mucho tiempo hasta encontrar palabras sencillas que me permitieran explicar una idea sin ropaje formal, buscando analogías, conectando distintas realidades, tratando de sacar a luz conexiones impensadas entre hechos aparentemente desconectados.&lt;/p&gt;
&lt;p&gt;Recuerdo especialmente un concurso de los tantos a los que me presenté en esa época. Era para enseñar física básica a estudiantes de segundo año. En el sorteo previo de temas salió un problema relacionado a la estructura electrónica del átomo de Helio. El Helio es el segundo elemento de la tabla periódica después del Hidrogeno. Tiene dos electrones orbitando alrededor del núcleo. Después de cierto análisis y consultas con otros docentes, elegí una resolución sencilla del problema, basada en argumentos de electrostática que los alumnos habían adquirido en ese mismo curso. De esa forma podía reducir el problema a uno efectivo del átomo de Hidrogeno que seguramente los alumnos conocían también.&lt;/p&gt;
&lt;p&gt;Me pareció lo más adecuado para el nivel al que iba dirigido. Por otra parte era la solución más simple que yo podía entender en el corto tiempo de preparación que tenía. Mis compañeros del concurso, o contrincantes si se quiere, prefirieron desarrollos más formales basados en la Mecánica Cuántica. De esa forma se acercaban a la explicación moderna y a los cálculos actuales de la estructura electrónica de los átomos. Al terminar el concurso el jurado alumno me hizo un comentario que en ese momento me sorprendió un poco. ‘Se notó una diferencia ideológica entre tu explicación y la de los otros’.&lt;/p&gt;
&lt;p&gt;¿Qué cuestión ideológica podía diferenciarse en la explicación de un problema de física? Probablemente me pregunté en aquel momento. Con el tiempo comprendí, o por lo menos tuve mi propia interpretación de aquel mensaje. Una explicación muy compleja, por encima del nivel de quien escucha, puede generar frustración, puede también delinear una relación de poder del docente hacia el alumno. Podría también estar anticipando actitudes sobre cómo se producirá su inserción en el sistema científico o productivo. Quizás pueda también dificultar el desarrollo de un análisis crítico reemplazándolo por una idealización del discurso del docente, por el querer ser como él.&lt;/p&gt;
&lt;p&gt;Desde aquellos años iniciáticos en la docencia para mi, muchas veces me enfrenté a dilemas parecidos al que tuve al resolver aquel problema. Entre elegir una explicación simple pero muy alejada del nivel de abstracción que se requiere para la comprensión de los conceptos en juego. Dependiendo del nivel de formación de a quién iba dirigido el mensaje he intentado respuestas diversas. A veces apelando a lo intuitivo. Muchas otras, como mis compañeros de concurso de aquel entonces, tratando de llevar a los alumnos a ideas más actuales. Fluctuando entre lo claro y lo obscuro. Entre lo fácilmente comprensible y lo que requiere de un estudio más profundo.&lt;/p&gt;
&lt;p&gt;Sin embargo desde aquella respuesta autocomplaciente sobre el rol de la ideología en el proceso de enseñanza-aprendizaje me he replanteado varias veces el problema y hoy creo que la respuesta podría necesitar de un abordaje más global. El proceso de enseñanza-aprendizaje requiere de un compromiso activo tanto de docentes como alumnos. Aunque la presentación del docente su formato, su lenguaje, su actitud juega un rol importante, lo que configura el eje de la cuestión ideología es el canal de comunicación que se establece. Si la educación se plantea en un solo sentido como que el docente debe explicar, clarificar, conducir al alumno al conocimiento, mientras este mantiene una actitud pasiva, desde el lugar del no-saber, difícilmente se generará un espacio conjunto de enseñanza-aprendizaje. El conocimiento no circulará sino que se ‘transferirá’. El marco ideológico para una relación de poder estática se estará comenzando a configurar.&lt;/p&gt;
&lt;p&gt;Si en cambio, se genera un compromiso activo tanto de docentes como de alumnos para convertir una clase de ciencias en un espacio de discusión de problemas, de planteo de dudas de ambas partes sin ocultar las dificultades del tema a tratar en pos de una claridad a veces ficticia. Si el docente asume un rol de coordinador y facilitador de este espacio de discusión y los alumnos se comprometen a asumir un rol activo sin sentirse inhibidos por su aparente ignorancia pero asumiendo ambos la responsabilidad de generar espacios reflexivos que no se agoten en la solución de un problema con una receta conocida. Quizás si estos presupuestos se dan, la clase se convertirá en un laboratorio de iniciación a la investigación científica en un sentido creativo. Se comenzaran a generar aptitudes de pensamiento más libres, se pensará al conocimiento en todos sus aspectos como en permanente evolución y siempre posible de recrear.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Diego Petrucci&lt;/strong&gt;, desde la didáctica de las ciencias, nos ofrece un comentario sobre el artículo de &lt;strong&gt;Ariel Dobry&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Es un artículo interesante. Parte de asumir que enseñar es transmitir, quizá porque ha sido su idea en aquellos años. Y su preocupación pasaba por cómo presentar el conocimiento. Más adelante aparece la idea de que enseñar es explicar. En ambos casos el alumno está ausente, o en todo caso, pasivo. Sin embargo a medida que avanza el artículo, aparece la necesidad de que el alumno sea activo, comprometido, de generar un espacio conjunto, Allí aparece el análisis que hace Ariel de la cuestión del poder en el aula. La Sociología de la Educación desde la década del 70 analiza el aula en términos del poder (Candela, 2008). También está presente en el discurso de Paulo Freire y su concepción bancaria de la educación tradicional.&lt;/p&gt;
&lt;p&gt;En los últimos años se utiliza el enfoque de Foucault por ejemplo el de Vigilar y castigar (1989). El docente es inicialmente el depositario del poder, pues es él quien posee el saber. Además es quien decide quién aprueba y quién no. A partir de este análisis Ariel fundamenta la propuesta de que el docente sea coordinador y facilitador, ya no “explicador” y los alumnos se comprometan, participen, pongan en juego sus saberes y su creatividad. Es un ideal por demás deseable.&lt;/p&gt;
&lt;p&gt;La cuestión es ¿cómo hace un docente comprometido con su curso para que los estudiantes se comprometan? ¿Cómo lograr estudiantes que pregunten, que expongan -se expongan- sus dudas, que debatan, que demanden ser enseñados? Desde la primera clase los estudiantes se dedican a analizar al docente, a clasificarlo en alguna categoría de docente: sabe, no sabe, explica bien, no se le entiende, es muy exigente, etc. Por ello son importantes las actitudes y propuestas del docente desde el primer momento. Es necesario ser auténtico y coherente, cumplir con los compromisos y que haga lo que dice que hay que hacer. Si pretende que los estudiantes pregunten, debe olvidarse de burlarse de una pregunta tonta y por el contrario, valorarla, mostrando que ella le da pie a hacer una aclaración que no duda será útil para más de uno. Si pretende fomentar el debate entre estudiantes debe evitar responder preguntas, dando tiempo a la reflexión y fomentando que un compañero conteste, valorando nuevamente las participaciones.&lt;/p&gt;
&lt;p&gt;Con el correr de las clases, si hay auténtico aprendizaje y buen clima, el compromiso de los estudiantes crecerá. Otro tema es analizar cómo la evaluación atraviesa este proceso.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ariel Dobry&lt;/strong&gt; es Doctor en Física. Es Investigador Independiente del CONICET y Profesor Adjunto en la Universidad Nacional de Rosario, en el Departamento de Física. Su área de trabajo abarca Sistemas magnéticos y electrónicos en bajas dimensiones y Teoría de campos en Materia Condensada. También es autor de artículos de divulgación y opinión y creador del blog Humanizando la Ciencia &lt;a href=&#34;http://huciencia.blogspot.com.ar/&#34;&gt;http://huciencia.blogspot.com.ar/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Diego Petrucci&lt;/strong&gt; es es Profesor de Físico-Matemática y doctor en Didáctica de las Ciencias Experimentales por la Universidad de Granada, España. Trabaja en el Espacio Pedagógico de la Facultad de Ciencias Exactas (UNLP).&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>El último elemento: El bosón de Higgs</title>
      <link>https://ciencianet.com.ar/post/el-ultimo-elemento-el-boson-de-higgs/</link>
      <pubDate>Wed, 05 Sep 2012 01:24:20 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/el-ultimo-elemento-el-boson-de-higgs/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Gaston Giribet&lt;/strong&gt;. Este artículo es una versión modificada del texto publicado en la Revista Noticias el 01/09/12. Gaston Giribet es Doctor en Física por la Universidad de Buenos Aires, es Profesor de la Facultad de Ciencias Exactas y Naturales de la Universidad de Buenos Aires e Investigador del CONICET.&lt;/p&gt;
&lt;p&gt;El pasado 4 de julio científicos de la Organización Europea para la Investigación Nuclear (CERN), en Ginebra, anunciaron el descubrimiento de una nueva especie de partícula, jamás observada hasta el momento, la cual exhibe las propiedades que, según predice la teoría, debería tener la largamente buscada “partícula de Higgs”. La partícula de Higgs, también llamada “bosón de Higgs” por pertenecer al sub-conjunto de partículas que los físicos denominan “bosones”, es la última partícula que quedaba por descubrir del edificio teórico denominado Modelo Estándar de las partículas fundamentales, y es la explicación del hecho de que la materia posea masa.&lt;/p&gt;
&lt;h3 id=&#34;el-modelo-estándar-de-las-partículas-elementales&#34;&gt;El modelo estándar de las partículas elementales&lt;/h3&gt;
&lt;p&gt;El Modelo Estándar es un ambicioso esquema teórico que propone una descripción minuciosa de cómo serían absolutamente todos los constituyentes microscópicos que componen la materia en el universo. Además de esto, el Modelo describe también las interacciones entre dichos constituyentes; es decir, lo que usualmente llamamos fuerzas. Según la teoría, toda la materia que observamos en el universo y las fuerzas actuando entre esa materia están constituidas por partículas fundamentales infinitamente pequeñas. El electrón, el fotón, los quarks, los neutrinos, son sólo algunas de esas veintitantas partículas del Modelo Estándar.&lt;/p&gt;
&lt;p&gt;De comprobarse que, en efecto, la partícula observada recientemente en el colisionador de partículas LHC del CERN se trata de la partícula de Higgs, entonces se habría hallado el último bloque de ese colosal edificio teórico y estaríamos en condiciones de afirmar que tenemos una descripción acabada de todos los fenómenos hasta hoy observados en el universo, con excepción de fenómenos que involucran a la fuerza de gravedad, para la cual la física parece reservar un capítulo a parte.&lt;/p&gt;
&lt;h3 id=&#34;el-origen-de-la-masa&#34;&gt;El origen de la masa&lt;/h3&gt;
&lt;p&gt;La importancia de la partícula de Higgs no viene dada sólo por su propiedad de ser la última partícula predicha por la teoría que quedaba por ser descubierta, sino también porque ella desempeña un papel crucial: es precisamente esta partícula la responsable de que las otras tengan masa. Por ello, descubrir el Higgs es, de suyo, encontrar la razón por la cual la materia tiene masa.&lt;/p&gt;
&lt;p&gt;En cuanto construcción teórica, el Modelo Estándar debe su éxito a un conjunto de hermosas propiedades matemáticas que uno encuentra en sus ecuaciones. Estas propiedades reciben el nombre de “simetrías de gauge” y son parte esencial de cómo hoy los físicos conciben la naturaleza. Sin ánimo de renunciar a esas propiedades, los físicos entendieron desde un comienzo que la simetría de gauge es posible sólo si la masa de las partículas no es una propiedad intrínseca de ellas sino, por el contrario, es una propiedad emergente debido a algún otro fenómeno. Fue así como en 1964 Peter Higgs junto a otros físicos (correspondería también mencionar los nombres de Brout, Englert, Hagen, Guralnik y Kibble) propusieron un mecanismo según el cual las partículas podrían adquirir masa.&lt;/p&gt;
&lt;p&gt;Esta idea fue posteriormente implementada por Salam y Weinberg en la formulación del Modelo Estándar. Según el mecanismo de Higgs, la “masa” que observamos de las partículas, lo que solemos asociar a la renuencia de éstas a ser frenadas o aceleradas, no es una propiedad intrínseca de ellas sino que es el producto de la interacción de ellas con otra partícula, la hoy llamada “bosón de Higgs”. Este bosón interacciona con otras partículas adosándoseles y de ese modo confiriéndoles la masa. Así, las partículas en realidad no tendrían masa en un sentido estricto sino que su comportamiento inercial se debería a que se propagan en el universo surcando un omnipresente campo plagado de partículas de Higgs que les entorpece su andar “como si fueran masivas”.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2012/09/higgs-300x201.jpg&#34; alt=&#34;Peter Higgs. Imagen: Universidad de Edinburgo.:left&#34; title=&#34;Higgs&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;la-manifestación-del-bosón-de-higgs&#34;&gt;La manifestación del bosón de Higgs&lt;/h3&gt;
&lt;p&gt;La historia se vuelve aún más interesante cuando uno advierte que, según la teoría, el bosón de Higgs no sólo puede manifestarse indirectamente a través de su propiedad de otorgarles la masa a otras partículas, sino que podría también hacerse presente per se. Para lograrlo, es necesario producir colisiones de partículas de muy alta energía, similares a las que se han alcanzado en el CERN.&lt;/p&gt;
&lt;p&gt;La razón por la cual los procesos de colisiones necesarios para exhortar al Higgs a salir de su cueva deben ser muy energéticos está relacionada con el hecho de que el Higgs tiene una masa muy grande. Por ejemplo, si la partícula observada recientemente en el CERN se tratara en efecto del bosón de Higgs entonces su masa se calcula entre 125 y 126 GeV (Giga-electronVoltios), lo que equivale a decir que sería 245.000 veces más pesado que un electrón.&lt;/p&gt;
&lt;p&gt;Para entender esto es conveniente recurrir a la siguiente analogía: Asumamos que el universo está lleno de ese “campo de Higgs”. Esto es, supongamos que el universo es un medio en el cual las partículas se mueven. Imaginemos a ese medio denso, como si fuera algún tipo de pegamento de esos que uno usa en la escuela. Ocurre que algunas de las partículas fundamentales tienen la cualidad de pegarse a ese medio gomoso más que otras, dependiendo esto de sus propiedades. La idea es que las partículas en realidad no tienen masa sino que tienen “diferente adhesión a ese medio gomoso en el que se propagan”. Así, las partículas que se pegan con mayor afinidad se comportan como si fueran pesadas, como si tuvieran mucha masa, con un andar lento y costoso. Por el contrario, aquéllas que se adhieren muy poco al medio gomoso son las que se mueven rápidamente, de manera ligera, como si estuvieran en el vacío; éstas son las que uno llama partículas sin masa, o con masa muy pequeña.&lt;/p&gt;
&lt;p&gt;Ahora bien, hasta ahí esta descripción sólo viene a reforzar lo que decíamos arriba, que el campo de Higgs es lo que les confiere la masa a las otras partículas. Pero uno puede ir más allá con la metáfora y pensar que, así como ese medio tiene la propiedad de pegarse a las partículas que transitan en él, tiene también la propiedad intrínseca de generar olas. Es decir, aun cuando no haya una partícula moviéndose en ese medio gomoso, el mismo puede moverse formando olas de pegamento, o grumos que se propagan en el mismo medio del que están hechos. Son precisamente esas “olitas” las que uno llama “partículas de Higgs”, y la razón por la que ha demandado tanto esfuerzo observarlas es que esas olas del medio gomoso no se generan tan fácilmente como lo hacen las olas en el agua o en un medios menos densos; por el contrario, para generarlas uno necesita “más energía”. Es necesario lograr procesos muy energéticos dentro de ese medio para que las repercusiones de los mismos devengan en una “olita” en el campo de Higgs.&lt;/p&gt;
&lt;p&gt;Lo que en el CERN parece haberse observado es precisamente esta manifestación más directa del Higgs, esas olitas. Hubo que esperar casi cinco décadas desde su predicción hasta poder alcanzar la energía suficiente para experimentar este fenómeno, y es esto lo que hace de este momento uno tan especial. De todos modos, y a pesar de estar embriagados con el entusiasmo propio de quienes probablemente estén frente a la última partícula que quedaba por descubrir, los físicos prefieren la cautela: lo que los dos experimentos (llamados ATLAS y CMS) que funcionan en el colisionador LHC han anunciado el 4 de julio es la observación de una nueva partícula, jamás observada anteriormente, y cuyas propiedades coinciden con las esperadas para el escurridizo bosón de Higgs.&lt;/p&gt;
&lt;p&gt;Investigaciones venideras nos dirán si efectivamente se trata del Higgs o si, por el contrario, se trata de algo aún más exótico. En lo personal, yo estaría feliz con lo primero.&lt;/p&gt;
&lt;h3 id=&#34;el-colisionador-de-partículas-lhc&#34;&gt;El colisionador de partículas LHC&lt;/h3&gt;
&lt;p&gt;Pero propongo que hagamos aquí un descanso. Abramos un paréntesis para saldar una deuda, la de explicar qué es precisamente el colisionador de partículas LHC que funciona en Ginebra y cómo una máquina de esas características nos permitiría aprender sobre la estructura microscópica de la materia. El colisionador LHC, acrónimo de la traducción al inglés de “gran colisionador de hadrones”, es el acelerador de partículas más grande que haya sido construido. Funciona acelerando protones e iones pesados. Los protones pertenecen al conjunto de partículas que los físicos denominan “hadrones” y es por eso que el LHC recibe ese nombre; por otro lado, los iones pesados son átomos cargados eléctricamente cuyos núcleos tienen una gran cantidad de protones y de neutrones.&lt;/p&gt;
&lt;p&gt;El LHC consiste en un túnel circular de 27 kilómetros de perímetro enterrado a cien metros de profundidad en la frontera entre Suiza y Francia. Aunque los detalles técnicos de una máquina de las características del LHC son infinitos y ciertamente exceden cualquier tipo de descripción que un profano como yo podría intentar, el principio de funcionamiento es curiosamente simple: gracias a un intenso campo magnético, los protones del LHC son acelerados a lo largo del túnel y finalmente forzados a colisionar entre sí a velocidades altísimas, generando de esa manera choques de partículas de muy alta energía.&lt;/p&gt;
&lt;p&gt;El producto de esas colisiones es colectado en complejos detectores y luego analizado por los físicos de partículas. Los expertos se dedican entonces a estudiar los detritos de esos choques sub-atómicas con la intención de entender cuáles son los constituyentes mínimos de la materia. Cuanto más energéticos son esos choques, más se adentra uno en los secretos íntimos de la materia.&lt;/p&gt;
&lt;p&gt;Para entender cómo es esto pensemos en un choque entre colectivos: si exploráramos los restos de un choque entre dos colectivos que venían a moderada velocidad, entonces sólo podríamos concluir que los colectivos están compuestos de guardabarros, parabrisas y neumáticos, ya que son ésos los vestigios que quedarían diseminadas en el pavimento luego del impacto. En cambio, si nos fuera dado estudiar los restos de una colisión entre colectivos que venían a muy alta velocidad, entonces aprenderíamos más de su composición, y al explorar sus restos comprobaríamos la existencia de tuercas, tornillos, carburadores y bujías. Esto ejemplifica lo que ocurre en los aceleradores de partículas: es necesario alcanzar velocidades muy altas si lo que se pretende es escudriñar en lo más profundo de la materia.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2012/09/cern-300x200.jpg&#34; alt=&#34;Detección de partículas en el LHC. Imagen: CERN.:left&#34; title=&#34;Cern&#34;&gt;&lt;/p&gt;
&lt;p&gt;En el caso particular del LHC las partículas allí aceleradas son protones, y al estar éstos compuestos a su vez por otras partículas más pequeñas (los quarks) el análisis de los restos de sus colisiones resulta ciertamente muy complejo. Participan en la realización y en el análisis de esos experimentos muchísimos científicos de muchísimos países, los cuales suman esfuerzos y recursos para llevar a cabo una de las empresas científicas más grandes de la historia.&lt;/p&gt;
&lt;p&gt;Entre los muchos expertos involucrados en los experimentos del LHC hay grupos de nuestro país realizando tareas de gran importancia; cabe mencionar el grupo de Ricardo Piegaia, de la Universidad de Buenos Aires, y el grupo de María Teresa Dova, de la Universidad Nacional de La Plata. De todos modos, la porción de la comunidad científica interesada en los resultados de las colisiones de protones en Ginebra excede ampliamente al grupo de físicos directamente involucrados con el experimento. Un ejemplo particular de esto es el entusiasmo, compartido ampliamente por la comunidad científica, ante anuncio del reciente descubrimiento de esa partícula “que tanto se le parece al Higgs”.&lt;/p&gt;
&lt;h3 id=&#34;predicción-teórica-y-experimento&#34;&gt;Predicción teórica y experimento&lt;/h3&gt;
&lt;p&gt;Un corolario que podemos -y acaso debemos- extraer de descubrimientos tales como el acaecido recientemente en el LHC es el que nos habla de la fecundidad de las construcciones teóricas a la hora de guiar las investigaciones en física. Es remarcable que la existencia de partículas como el Higgs u otros componentes del Modelo Estándar haya sido primero conjeturada sobre un pizarrón o sobre un pedazo de papel y sólo después -en muchos casos, décadas después- hayan los físicos asistido a sus descubrimientos.&lt;/p&gt;
&lt;p&gt;Este aspecto suele a veces quedar tristemente desplazado a un segundo plano, pero es uno de los logros más importantes de las ciencias naturales: ese poder de predicción, de anticipación de fenómenos, siendo la investigación a veces guiada sólo por intuiciones y argumentos que al comienzo pueden lucir tan frágiles y etéreos como aquella reticencia a abandonar la “simetrías de gauge” como forma de entender el mundo.&lt;/p&gt;
&lt;p&gt;Un caso paradigmático de cómo cogitaciones puramente teóricas pueden eventualmente desembocar en el descubrimiento de nuevas partículas en la naturaleza es la historia del descubrimiento de la partícula llamada Omega. Esta historia transcurre en los años 60s. En aquel momentos los físicos conocían ya un vasto catálogo de partículas y se abría con ello un gran abanico de preguntas: ¿Por qué había tantos tipos distintos de partículas sub-atómicas?, ¿había, acaso, una manera conveniente de clasificarlas? Los más osados llegaron a preguntarse si podría una tarea de clasificación tal llevarlos a predecir la existencia de nuevas partículas aún no detectadas.&lt;/p&gt;
&lt;p&gt;La respuesta por la afirmativa no tardaría en llegar. A comienzos de la década de 1960, Gell-Mann y Neemann arribaron independientemente a la conclusión de que, en efecto, existía una manera conveniente de clasificar las partículas conocidas hasta ese entonces. El método consistía en organizar el bestiario sub-atómico formando octágonos, y fue por ello que uno de sus padres bautizó al método con el nombre de “la manera óctuple”.&lt;/p&gt;
&lt;p&gt;Según la manera óctuple de organizar las partículas, éstas se ubican formando octágonos sobre un papel luego de que uno traza sobre éste ejes imaginarios etiquetando sus propiedades, de forma similar a la de esos gráficos con ejes que hacíamos en las clases de matemática en el colegio. Ante la sorpresa de aquellos dos físicos, las partículas no se ubicaban caprichosamente sobre el papel sino que lo hacían formando un patrón definido, armónico, formando octágonos simétricos. Así, aquellos pioneros concluyeron que debía haber alguna razón para tal simetría y que no podían ser esos prolijos octágonos mera casualidad.&lt;/p&gt;
&lt;p&gt;Al mismo tiempo advirtieron que otro grupo de partículas, primas cercanas a las que formaban octágonos, hacían una gracia similar pero formando triángulos en grupos y ya no solamente octágonos. Nuevamente, aparecía sobre el papel una figura simétrica que remedaba a un triángulo formado por nueve partículas. –Sería un triángulo perfecto si hubiera una décima partícula aquí en la punta– pensó uno de ellos, y así se conjeturó la existencia de Omega-. Esta partícula, cuya existencia fue predicha basándose sólo en la inquietud estética de dos genios que vieron aparecer figuras geométricas en sus cuadernos de notas, fue detectada pocos años después en el acelerador de partículas del Laboratorio Nacional de Brookhaven, en los Estados Unidos.&lt;/p&gt;
&lt;p&gt;Poco más tarde, la manera óctuple dio origen a la teoría de los quarks, que son las partículas de las que, según hoy sabemos, se componen otras partículas tales como la Omega-, los protones y los neutrones. Y la teoría de los quarks me invita a contar a otro ejemplo: recuerdo cuando, siendo yo un estudiante de segundo año de la universidad, se anunciaba oficialmente el descubrimiento del quark top. Hasta el reciente anuncio del pasado 4 de julio, el quark top detentaba el título de ser la última partícula fundamental en haber sido descubierta. Se trata del quark más pesado de los seis detectados en la naturaleza, y en la dinastía de partículas fundamentales se le reserva un lugar como miembro de una “familia” de partículas cuya existencia había sido predicha a comienzos de los años 70’s por Kobayayi y Maskawa.&lt;/p&gt;
&lt;p&gt;El quark top fue finalmente observado en los aceleradores de partículas en 1995, dos décadas después de su predicción teórica, regalándonos de esta manera otro ejemplo de cómo las construcciones teóricas no siempre se limitan a la mera descripción de los fenómenos naturales en términos matemáticos sino que, en muchas ocasiones, son estas construcciones las que se adelantan a la misma observación de dichos fenómenos. Esta digresión invita a preguntarnos cuáles de las predicciones de las más especulativas teorías físicas de la actualidad acabarán finalmente manifestándose en futuros experimentos y observaciones.&lt;/p&gt;
&lt;p&gt;Construcciones teóricas de la física-matemática tales como la teoría de cuerdas podrían, quizá, terminar prediciendo fenómenos que alguna vez sean observados. ¿Existen en el universo más dimensiones espaciales que las tres que experimentamos cotidianamente?, ¿existe para cada tipo de partícula del Modelo Estándar un alter ego como nos propone la teoría de “la supersimetría”?, ¿cuál es la razón por la cual es más abundante la materia que la anti-materia en el universo?, ¿qué tipo de “materia oscura” es la que ayuda a apelmazar a las galaxias y les impide dispersarse al rotar tan rápidamente?, ¿cuál es el origen de esa “energía oscura” que acicatea al universo a acelerar su expansión? Son éstas preguntas que aún permanecen abiertas y para las cuales los físicos teóricos han ensayado varias posibles respuestas desde hace décadas. El tiempo dirá si alguna de esas teorías resulta ser verdad. Vayamos de a poco.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>¿De dónde vienen las oscilaciones epidémicas?</title>
      <link>https://ciencianet.com.ar/post/de-donde-vienen-las-oscilaciones-epidemicas/</link>
      <pubDate>Mon, 14 May 2012 01:26:48 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/de-donde-vienen-las-oscilaciones-epidemicas/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Guillermo Abramson&lt;/strong&gt;. Grupo de Física Estadística e Interdisciplinaria del Centro Atómico Bariloche, CONICET e Instituto Balseiro.&lt;/p&gt;
&lt;p&gt;Desde épocas remotas las epidemias han causado enorme sufrimiento a la Humanidad. Cada civilización sobrellevó sus plagas de la mejor manera que pudo, y éstas fueron forjando en alguna medida el curso de la Historia. Hasta hace no muchas décadas las enfermedades infecciosas tenían una presencia en la vida cotidiana mucho mayor que hoy en día. Esto era así aún en las regiones más favorecidas del mundo. La malaria, la difteria, la sífilis, la meningitis meningocóccica, y mucha, mucha tuberculosis. Gracias a los avances en áreas que van desde los antibióticos a la plomería (y sobre todo a mucho dinero) gran parte de estas pestes ya casi no existen en las regiones más desarrolladas, así como en buena parte del mundo subdesarrollado. Cada tanto, sin embargo, la sombra de una vieja o de una nueva plaga aparece en el horizonte.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://es.wikipedia.org/wiki/Ronald_Ross&#34;&gt;Ronald Ross&lt;/a&gt; (nacido en Almora, India, en 1857) fue un médico genial, descubridor del agente causante de la malaria y de su transmisión por el mosquito, por lo cual recibió el Premio Nobel de Medicina en 1902. Era aficionado a la matemática, y publicó en 1917 un artículo &lt;a href=&#34;#1&#34; title=&#34;1&#34;&gt;[1]&lt;/a&gt; en el que manifiesta su asombro de que no existiera una teoría matemática de la propagación de epidemias, un campo en el cual se tenían, desde hacía tiempo, grandes cantidades de datos estadísticos esperando ser examinados. Dice, además, que las cuestiones fundamentales de la epidemiología, en las cuales se basan las medidas preventivas (tales como la tasa de contagio, la frecuencia de brotes o la pérdida de la inmunidad) no pueden ser resueltas por métodos que no sean analíticos. ¿A qué se debe que algunas enfermedades persistan en la población, mientras que otras aparecen y desaparecen? ¿Por qué existen las epidemias? Ross sospechaba que la respuesta a estas preguntas se esconde en principios fundamentales a los que puede accederse mediante el cálculo, del mismo modo que en la Astronomía, la Física y la Mecánica.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2012/05/sir-300x266.png&#34; alt=&#34;Esquema del modelo SIR:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;El campo interdisciplinario del estudio matemático de las epidemias se ha desarrollado considerablemente en el siglo transcurrido desde la propuesta pionera de Ross, pero queda sin embargo mucho por hacer. Los modelos tradicionales están basados en la división de la población en categorías, de acuerdo a su estado infeccioso. Poco después de Ross un par de científicos escoceses, William Kermack (químico) y Anderson McKendrick (médico) propusieron el que se ha convertido en el padre de todos los modelos epidemiológicos: el modelo &lt;strong&gt;SIR&lt;/strong&gt; &lt;a href=&#34;#2&#34; title=&#34;2&#34;&gt;[2]&lt;/a&gt;. Los individuos que componen la población pertenecen a las categorías susceptibles (&lt;strong&gt;S&lt;/strong&gt;), infectados (e infecciosos, &lt;strong&gt;I&lt;/strong&gt;) o recuperados (e inmunes, &lt;strong&gt;R&lt;/strong&gt;). Existen transiciones entre estos estados, controladas por fenómenos tanto epidemiológicos como sociales. Los susceptibles pueden contagiarse por contacto con infectados, pasando del estado &lt;strong&gt;S&lt;/strong&gt; al &lt;strong&gt;I&lt;/strong&gt;. A su vez los infectados se recuperan, pasando al estado &lt;strong&gt;R&lt;/strong&gt;. El balance entre la velocidad a la cual ocurren estos dos procesos determina la evolución de la epidemia. Usualmente se condensa esto en términos de un único parámetro, la tasa reproductiva de la infección, que expresa el número de infecciones secundarias producidas por cada caso de la enfermedad. Si este número es mayor que 1, entonces ocurre una epidemia. Si es menor que 1, el “brote” inicial decae. A medida que la infección se va propagando la cantidad de susceptibles disminuye, de manera que a la infección se le hace cada vez más difícil propagarse, eventualmente empieza a decaer, y finalmente se extingue.&lt;/p&gt;
&lt;p&gt;Es fácil ver que este modelo sencillo es una caricatura de lo que ocurre en la realidad. Por un lado, la historia natural de muchos agentes infecciosos es más complicada. Pueden existir estados latentes (infectados pero no infecciosos), una respuesta inmune compleja (inclusive dependiente de la edad del anfitrión), pérdida de la inmunidad (con regreso de los &lt;strong&gt;R&lt;/strong&gt; al estado &lt;strong&gt;I&lt;/strong&gt;), vacunación y tratamientos, una demografía con nacimientos, muertes y migraciones, y un largo etcétera. Por otro lado, existen hipótesis más sutiles en la formulación matemática de estos sistemas, cuya modificación podría producir un comportamiento dinámico distinto. En nuestro trabajo hemos estudiado estos problemas desde numerosos puntos de vista [[3](#3 &amp;quot;3·), - &lt;a href=&#34;#6&#34;&gt;6&lt;/a&gt; &amp;quot;6&amp;quot;], tratando de dilucidar el rol de los distintos mecanismos que se esconden detrás de esas hipótesis. Por ejemplo, ¿qué ocurre si la población, en lugar de estar bien mezclada, con contactos de &amp;quot;todos con todos&amp;quot;, está organizada en una red social, como ocurre en la realidad? &lt;a href=&#34;#3&#34;&gt;[3]&lt;/a&gt;. ¿Y qué pasa si, en lugar de analizar poblaciones grandes que evolucionan de manera suave, la población es pequeña y las fluctuaciones que ocurren a nivel individual son relevantes? &lt;a href=&#34;#4&#34; title=&#34;4&#34;&gt;[4]&lt;/a&gt;. O, por otro lado, ¿qué pasa si los fenómenos infecciosos no ocurren a tasas constantes como se supone habitualmente, sino que deben cumplir plazos más o menos bien definidos? En los próximos párrafos veremos con algún detalle este último caso, que es el objeto de nuestros trabajos más recientes [&lt;a href=&#34;#5&#34; title=&#34;5&#34;&gt;5,&lt;/a&gt;&lt;a href=&#34;#6&#34; title=&#34;6&#34;&gt;6&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2012/05/delay-300x254.png&#34; alt=&#34;Esquema comparativo entre recuperación a tasa constante y con tiempos característicos.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;¿Qué significa que los fenómenos infecciosos ocurren a tasas constantes? Pensemos en lo que ocurre al individuo apenas contagiado en un modelo &lt;strong&gt;SIRS&lt;/strong&gt; (es decir, con pérdida de la inmunidad). Acaba de ingresar a la categoría &lt;strong&gt;I&lt;/strong&gt;, en la que permanecerá hasta que se recupere. Una tasa constante de recuperación significa que el individuo tiene una cierta probabilidad de curarse hoy, la misma probabilidad de curarse mañana, la misma probabilidad de curarse pasado mañana... ¡y así sucesivamente hasta que se cure! Esto, claramente, está lejos de la realidad para muchas enfermedades infecciosas. Es mucho más razonable suponer que, lo que ocurrirá, es que el infectado se recuperará una vez transcurrido cierto tiempo característico de su enfermedad. A lo sumo un poquito antes, o un poquito después. Lo mismo puede decirse de la duración de la inmunidad en el estado &lt;strong&gt;R&lt;/strong&gt;. Estos mecanismos de recuperación y de pérdida de la inmunidad con tiempos característicos pueden también formularse matemáticamente. El análisis es un poco más complicado, razón por la cual no se lo usa habitualmente. En [5] hemos analizado un sistema &lt;strong&gt;SIRS&lt;/strong&gt; de este tipo, en situaciones en las que el análisis &amp;quot;clásico&amp;quot; (al estilo de Kermack y McKendrick) predice un comportamiento de tipo endémico, con una pequeña fracción de la población que permanece infectada. Encontramos que la hipótesis de tiempos característicos de recuperación y pérdida de inmunidad, en cambio, produce una dinámica oscilatoria, con rebrotes periódicos de la infección. La aparición de las oscilaciones depende de dos factores. Por un lado, el tiempo de pérdida de la inmunidad debe ser mayor que el de recuperación (cuánto mayor depende, además, de la tasa reproductiva). Parece un resultado inocente, pero puede ser muy relevante en la vida real. Imaginemos que estamos ante un agente infeccioso endémico en una población, con una cierta prevalencia. Ahora imaginemos que el tiempo de inmunidad se alarga, por ejemplo por mejoras en una vacuna. Podemos empezar a ver una oscilación, ¡inclusive con aumento de la prevalencia! Eso no significa que la vacuna no esté funcionando, claro está, ya que el valor medio de la prevalencia puede disminuir. Evidentemente es algo que hay que tener en cuenta.&lt;/p&gt;
&lt;p&gt;Por otro lado, como dijimos antes, los tiempos de infección y de inmunidad pueden no ser exactos, sino más o menos difusos alrededor de un valor medio. El modelo predice que, para que aparezcan las oscilaciones, ninguno de estos procesos puede estar muy desparramado alrededor de su valor medio. Existe un ancho crítico, por debajo del cual las recuperaciones y las pérdidas de inmunidad de los distintos individuos &amp;quot;no se mezclan&amp;quot;. Esto permite la sincronización de la infección en muchos de ellos, condición necesaria para la oscilación. En ambos casos se trata de una transición en el comportamiento dinámico de la epidemia en función de la historia natural del agente infeccioso, en lugar de depender de la tasa reproductiva que controla los modelos clásicos.&lt;/p&gt;
&lt;p&gt;Muy bien, tenemos un modelo matemático de epidemias que produce oscilaciones. ¿Existen las oscilaciones epidémicas en el mundo real? Claro que sí. El sarampión, antes de la introducción de la vacunación masiva a fines de los &#39;60, rebrotaba cada dos años en muchos países. La tos convulsa, una infección para la cual también existe vacuna pero que está reapareciendo, oscila con un período de 4 años. Se ha evidenciado un período de 11 años en los brotes de sífilis en algunas ciudades norteamericanas. Varias enfermedades respiratorias, muy ligadas a la estacionalidad anual, oscilan sin embargo con período bienal. Es el caso de la parainfluenza de tipos 1 y 3, y de la propia gripe, que parece tener picos en distintas semanas en años pares o impares en algunas ciudades. Nuestro modelo predice períodos de oscilación que son compatibles con los parámetros epidemiológicos de varias de estas enfermedades. En particular, cuando se supone adicionalmente que la tasa de contagio está sujeta a oscilaciones estacionales (como en el caso de las enfermedades tipo influenza), observamos 6 oscilaciones con dos picos anuales, pero que ocurren en momentos distintos del año, tal como se observa en situaciones reales.&lt;/p&gt;
&lt;p&gt;Podemos concluir, por un lado, que es posible formular modelos matemáticos que relajen las hipótesis fuertes que hacen que, muchas veces, los modelos clásicos de enfermedades infecciosas parezcan caricaturas de la realidad. Por otro lado, que existen fenómenos dinámicos novedosos cuando estudiamos cada una de estas suposiciones por separado. Finalmente, que estos modelos se caracterizan por un fenómeno de sincronización de la fase infecciosa, que se manifiesta en la aparición de oscilaciones de distinta complejidad, cuyas propiedades pueden estudiarse detalladamente. Existen muchos mecanismos por los cuales un sistema epidémico puede manifestar oscilaciones. Ciertamente, en la naturaleza pueden operar simultáneamente más de uno, de manera que es necesario estudiar cuidadosamente sus características e interacciones.&lt;/p&gt;
&lt;h2 id=&#34;para-saber-más&#34;&gt;Para saber más:&lt;/h2&gt;
&lt;p&gt;El artículo de Wikipedia sobre el &lt;a href=&#34;http://en.wikipedia.org/wiki/SIR_Model&#34;&gt;modelo SIR&lt;/a&gt; está muy bien (en inglés). Si quiere explorar su dinámica, hay un &lt;a href=&#34;http://jsxgraph.uni-bayreuth.de/wiki/index.php/Epidemiology:_The_SIR_model&#34;&gt;simulador on line&lt;/a&gt; en la Universidad de Bayreuth. El episodio 3 de la primera temporada de la serie televisiva NUMB3ERS trata sobre los modelos SIR. Visite la &lt;a href=&#34;http://fisica.cab.cnea.gov.ar/estadistica/abramson/&#34;&gt;página web del autor&lt;/a&gt; y la del &lt;a href=&#34;http://fisica.cab.cnea.gov.ar/estadistica/&#34;&gt;Grupo de Física Estadística e Interdisciplinaria&lt;/a&gt;. Y si le gusta la astronomía, no el blog del autor: &lt;a href=&#34;http://guillermoabramson.blogspot.com.ar/&#34;&gt;En el cielo las estrellas&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;referencias&#34;&gt;Referencias&lt;/h2&gt;
&lt;p&gt;&lt;a id=&#39;1&#39;&gt;1&lt;/a&gt;. Ronald Ross, An application of the theory of probabilities to the study of a priori Pathometry, Proc. Roy. Soc. Lond. 93A, 225–240 (1917).&lt;/p&gt;
&lt;p&gt;&lt;a id=&#39;2&#39;&gt;2&lt;/a&gt;. William Kermack and Anderson McKendrick, A contribution to the mathematical theory of epidemics, Proc. Roy. Soc. Lond. 115A, 700–721 (1927).&lt;/p&gt;
&lt;p&gt;&lt;a id=&#39;3&#39;&gt;3&lt;/a&gt;. Marcelo Kuperman and Guillermo Abramson, Small world effect in an epidemiological model, Phys. Rev. Lett. 86, 2909–2912 (2001).&lt;/p&gt;
&lt;p&gt;&lt;a id=&#39;4&#39;&gt;4&lt;/a&gt;. Sebastián Risau-Gusman and Guillermo Abramson, Bounding the quality of stochastic oscillations in population, Eur. Phys. J. B 60 , 515–520 (2007).&lt;/p&gt;
&lt;p&gt;&lt;a id=&#39;5&#39;&gt;5&lt;/a&gt;. Sebastián Gonçalves, Guillermo Abramson and Marcelo F. C. Gomes, Oscillations in SIRS model with distributed delays, Eur. Phys. J. B 81, 363–371 (2011).&lt;/p&gt;
&lt;p&gt;&lt;a id=&#39;6&#39;&gt;6&lt;/a&gt;. Sebastián Gonçalves, Guillermo Abramson and Marcelo F. C. Gomes, The interaction between seasonality and delays in epidemic models, en preparación.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Pautas éticas para el quehacer académico-científico en la Facultad de Ciencias Exactas y Naturales, UBA</title>
      <link>https://ciencianet.com.ar/post/pautas-eticas-para-el-quehacer-academico-cientifico-en-la-facultad-de-ciencias-exactas-y-naturales-uba/</link>
      <pubDate>Tue, 20 Mar 2012 01:31:49 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/pautas-eticas-para-el-quehacer-academico-cientifico-en-la-facultad-de-ciencias-exactas-y-naturales-uba/</guid>
      <description>
        
          &lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2012/03/uba.jpg&#34; alt=&#34;&#34; title=&#34;uba&#34;&gt;&lt;/p&gt;
&lt;p&gt;El Consejo Directivo de esta Facultad aprobó un documento con recomendaciones y normas que &amp;quot;deberán ser interpretadas en forma consistente con la visión de la Universidad como una comunidad donde los principios de democracia, responsabilidad social, honestidad, confianza y honradez prevalezcan en todas las ocasiones&amp;quot;. En el mismo se tratan aspectos de la investigación (autoría de los trabajos, procedimiento con especies animales y vegetales, uso de los datos, financiamiento, entre otros) y la docencia (evaluación, responsabilidades). Este documento, elaborado por el Comité de ética, fue aprobado el 12/3/12 y puede consultarse en &lt;a href=&#34;http://www.exactas.uba.ar/&#34;&gt;www.exactas.uba.ar&lt;/a&gt; &amp;gt; Institucional &amp;gt; La FCEyN &amp;gt; Comité de Ética&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>TOP TEN: La revista Physics World presenta su selección de los 10 principales avances del 2011</title>
      <link>https://ciencianet.com.ar/post/top-ten-la-revista-physics-world-presenta-su-seleccion-de-los-10-principales-avances-del-2011/</link>
      <pubDate>Wed, 22 Feb 2012 01:33:34 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/top-ten-la-revista-physics-world-presenta-su-seleccion-de-los-10-principales-avances-del-2011/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero&lt;/strong&gt;.  Facultad de Ciencias Exactas (UNLP).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2012/02/physw.jpg&#34; alt=&#34;:left&#34; title=&#34;physw&#34;&gt;La selección estuvo a cargo del equipo editorial de la revista y se realizó entre 350 artículos publicados allí. La elección estuvo guiada por algunos marcadores, como la relevancia del aporte para todos los físicos, la importancia de la investigación y la conexión entre el experimento y la teoría. El artículo que los recopila fue escrito por Hamish Johnston y puede leerse en &lt;a href=&#34;http://physicsworld.com/cws/article/news/48126&#34;&gt;http://physicsworld.com/cws/article/news/48126&lt;/a&gt;. Como toda lista, es parcial y sesgada, pero de todos modos resulta interesante. Allí vamos.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Primer puesto: Medidas cuánticas “inmorales”&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;El primer lugar fue para Aephraim Steinberg y sus colegas de la Universidad de Toronto en Canadá, por su trabajo experimental en mecánica cuántica. Usando una nueva técnica llamada &amp;quot;weak measurement&amp;quot;, el equipo fue el primero en realizar un seguimiento de las trayectorias promedio de fotones individuales que pasan a través del experimento de Young de la doble rendija. Steinberg y su equipo han sido capaces de obtener información acerca de los caminos tomados por los fotones sin destruir el patrón de interferencia. En el experimento, la doble rendija se sustituye por un divisor de haz y un par de fibras ópticas. El fotón llega al divisor de haz y se desplaza por una de las fibras. Después de emerger de la fibra óptica, pasa a través de un trozo de calcita, que lo polariza levemente de acuerdo a su momento. Finalmente, crea un patrón de interferencia en una pantalla. Los fotones son entonces seleccionados según dónde impacten en la pantalla, lo cual permite a los investigadores determinar la dirección media de viaje de los mismos. El experimento revela, por ejemplo, que un fotón detectado en el lado derecho del diagrama de difracción es más probable que haya surgido de la fibra óptica que está a la derecha. Lo cual no es poco. En palabras de Steinberg, según el paradigma cuántico, &amp;quot;asking where a photon is before it is detected is somehow immoral&amp;quot;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Segundo puesto: Mediciones de la función de onda&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;El segundo lugar está ligado al anterior. Fue asignado a Jeff Lundeen del Consejo Nacional de Investigación de Canadá, en Ottawa. Este ex colega de Steinberg también utilizó la &amp;quot;weak measurement&amp;quot; para trazar la función de onda de un conjunto de fotones idénticos, sin perder información sobre su estado. Además de mejorar la comprensión de los fundamentos de la mecánica cuántica, estas mediciones podrían ser útiles en los casos en que la tomografía no se pueda utilizar**.**&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Tercer puesto: &amp;quot;Encubrimiento&amp;quot; de eventos&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Este lugar es para un aporte teórico y un dispositivo experimental relacionados. El puesto es compartido por dos equipos, uno en la Universidad de Cornell en los EE.UU. encabezado por Alexander Gaeta, y el otro en el Imperial College de Londres, dirigido por Martin McCall. El equipo de McCall publicó a principios de 2011 un análisis teórico sobre cómo puede “encubrirse” un evento en el espacio y tiempo para volverlo indetectable (descrito en &lt;a href=&#34;http://physicsworld.com/cws/article/print/46376)&#34;&gt;http://physicsworld.com/cws/article/print/46376)&lt;/a&gt;. Unos meses después, Gaeta y sus colaboradores construyeron un dispositivo que usa dos lentes temporales para lograr ese efecto al que llaman “cloaking”.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cuarto puesto: Mediciones del universo a partir de agujeros negros&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;El cuarto lugar en la lista fue otorgado a Darach Watson y sus colegas de la Universidad de Copenhague, Dinamarca, y la Universidad de Queensland, Australia. Los investigadores han elaborado una forma de usar los agujeros negros supermasivos para hacer mediciones precisas de distancias cósmicas. Estos agujeros negros supermasivos se pueden encontrar en casi todas partes en el universo, y a diferencia de las supernovas que se utilizan actualmente, son detectables por largos períodos de tiempo.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Quinto puesto: Efecto Casimir dinámico, luz a partir del vacío&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;En la mitad de la lista se encuentra otro experimento. Christopher Wilson y sus colegas de la Universidad Tecnológica de Chalmers en Suecia, junto con físicos de Japón, Australia y los EE.UU. comparten el mérito de ser los primeros en detectar el efecto Casimir dinámico en el laboratorio. Este efecto, predicho teóricamente en 1970, consiste en la aparición de pares de fotones a partir del vacío cuántico generada por un cuerpo acelerado. Además de arrojar nueva luz sobre el efecto Casimir, el uso de un dispositivo superconductor de interferencia cuántica (SQUID) que simula un espejo oscilando al 5% de la velocidad de la luz hacen, de este un experimento extremadamente inteligente. Este efecto puede ser vinculado con la posibilidad de que las fluctuaciones de energía en el vacío hayan sido las responsables de la expansión del Universo durante los primeros instantes de su creación.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Sexto puesto: Temperatura del Universo primitivo&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;El sexto puesto es para un cálculo. Fue otorgado a un equipo de físicos de EE.UU., India y China que ha hecho el mejor cálculo de la temperatura de condensación del Universo primitivo: dos billones de grados Kelvin. El trabajo es además un aporte en la comprensión de la cromodinámica cuántica (QCD), que describe las propiedades de los neutrones, protones y otros hadrones.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Séptimo puesto: Oscilaciones de neutrinos&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;El séptimo puesto es para un trabajo experimental. Corresponde al equipo internacional de físicos que trabajan en el experimento Tokai-to- Kamioka (T2K) en Japón. Los investigadores dispararon un haz de neutrinos de muón a un detector y encontraron con que había cambiado u &amp;quot;oscilado&amp;quot; a neutrinos de electrón. Aunque estos resultados no son suficientes para pretender el descubrimiento de la oscilación neutrino muón-electrón, resulta la mejor evidencia disponible en la actualidad de que un &amp;quot;sabor&amp;quot; de neutrinos pueda oscilar en otro.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Octavo puesto: Láser viviente&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Este puesto es para un experimento en Biofísica. Malte Gather and Seok Hyun Yun, de la Harvard Medical School en EE. UU. fueron los primeros en fabricar un láser a partir de una célula viva. Iluminando con una intensa luz azul una proteína fluorescente de una célula embrionaria de riñón se logró que las moléculas emitieran una luz intensa, monocromática y direccional. Este fenómeno no destruye las células y se especula que podría ser utilizado para distinguir las células cancerosas de las sanas.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Noveno puesto: Computadora cuántica de un solo chip&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;El noveno lugar fue asignado a Matteo Mariantoni y sus colegas en la Universidad de California en Santa Bárbara por implementar una versión cuántica de la arquitectura &amp;quot;Von Neumann&amp;quot; de computadoras personales. Basado en circuitos superconductores e integrados en un solo chip, el nuevo dispositivo ya ha sido utilizado para realizar dos importantes cálculos cuánticos. Su desarrollo da un paso más hacia la creación de ordenadores cuánticos prácticos capaces de resolver problemas reales.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Décimo puesto: Reliquias del Big Bang&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Este lugar fue adjudicado a Michele Fumagalli y Xavier Prochaska de la Universidad de California, Santa Cruz junto a John O&#39;Meara de Saint Michael&#39;s College en Vermont. Ellos fueron los primeros en avistar nubes de gas generadas durante el Big Bang. A diferencia de otras nubes en el Universo (que parecen estar formadas por elementos creados en las estrellas) las nubes en cuestión solo están formadas por hidrógeno, helio y litio, los elementos más livianos que fueron creados por el Big Bang. Además de confirmar predicciones de la teoría del Big Bang, estas nubes proveen información sobre los elementos que conformaron las primeras estrellas y galaxias.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Hielo solar. Una heladera que funciona con el calor del Sol. Segunda parte</title>
      <link>https://ciencianet.com.ar/post/hielo-solar-una-heladera-que-funciona-con-el-calor-del-sol-segunda-parte/</link>
      <pubDate>Wed, 09 Nov 2011 01:42:02 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/hielo-solar-una-heladera-que-funciona-con-el-calor-del-sol-segunda-parte/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero.&lt;/strong&gt; Facultad de Ciencias Exactas (UNLP).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2011/11/8df9796-300x225.jpg&#34; alt=&#34;El equipo 2011.:left&#34; title=&#34;El equipo 2011&#34;&gt; Aunque abundan los anuncios de avances científicos con posibles aplicaciones beneficiosas para la sociedad, no es tan frecuente que su desarrollo se sostenga a lo largo de los años y finalmente redunde en innovaciones concretas. Y mucho menos frecuente es que el trabajo incluya la adaptación de las tecnologías para que puedan ser apropiadas más fácilmente por la comunidad a quien fueran destinadas.&lt;/p&gt;
&lt;p&gt;Enmarcado en el perfil de la Universidad Nacional de General Sarmiento (UNGS) -institución fuertemente conectada con las necesidades sociales- el trabajo del equipo del Dr. Rodolfo Echarri (físico, investigador del CONICET, docente) es uno de estos casos. En Diciembre de 2007 en la UNGS se hacían las primeras pruebas de este desarrollo. Bajo la mirada de alumnos y docentes, el prototipo instalado en el Campus de Los Polvorines fabricó 300 gramos de hielo (ver la primera parte de esta nota &lt;a href=&#34;https://ciencianet.com.ar/post/hielo-solar-una-heladera-que-funciona-con-el-calor-del-sol/&#34;&gt;aquí&lt;/a&gt;). Para fines de este año, casi 4 después, una versión mejorada de la heladera estará instalada en el noroeste de Córdoba, y se espera que produzca 5 kilos de hielo por día.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2011/11/Archivo7-300x225.jpg&#34; alt=&#34;Montado del prototipo. Versión 2011.:left&#34; title=&#34;Prototipo 2011&#34;&gt;&lt;/p&gt;
&lt;p&gt;El proyecto contó en sus inicios con la colaboración del INTEC-Instituto Tecnológico de Santo Domingo, República Dominicana; los primeros resultados pueden encontrarse &lt;a href=&#34;http://redalyc.uaemex.mx/pdf/870/87011539007.pdf&#34;&gt;aquí&lt;/a&gt;. Actualmente Echarri está acompañado por los investigadores Andrés Sartarelli, Sergio Vera y Ernesto Cyrulies, también de UNGS.&lt;/p&gt;
&lt;h3 id=&#34;los-destinatarios&#34;&gt;Los destinatarios&lt;/h3&gt;
&lt;p&gt;La versión 2011 de la heladera solar será instalada en Los Talas, una pequeña población del noroeste de Córdoba, cuya principal actividad es la producción caprina. En esta región, sin tendido eléctrico y con un clima riguroso, la dificultad para mantener los productos lácteos refrigerados es un factor limitante para los pequeños productores, para quienes resulta inaccesible económicamente el uso de un equipo tradicional de enfriamiento por gas envasado.&lt;/p&gt;
&lt;p&gt;En busca de una solución, el Instituto de investigación y desarrollo tecnológico para la Pequeña Agricultura Familiar (IPAF) de la Región Pampeana del Instituto Nacional de Tecnología Agropecuaria (INTA) y la Asociación de Productores del Noroeste de Córdoba (APENOC) convocaron en 2010 al equipo de Echarri.
Como consecuencia del trabajo conjunto entre los diversos actores, se prevé la instalación de dos equipos para fines de este año.&lt;/p&gt;
&lt;p&gt;Uno de los logros a destacar es que la instalación de las heladeras correrá por cuenta de los mismos productores, quienes serán asesorados por los investigadores para el armado. Un análisis detallado de la situación de los productores cordobeses y del impacto de la propuesta a nivel local pueden encontrarse &lt;a href=&#34;http://adiungs.com.ar/wp-content/uploads/2011/07/ADIUNGS-JULIO-2011-web.pdf&#34;&gt;aquí&lt;/a&gt; en palabras del investigador Sergio Vera.&lt;/p&gt;
&lt;h3 id=&#34;sin-enchufe-ni-motor&#34;&gt;Sin enchufe ni motor&lt;/h3&gt;
&lt;p&gt;La heladera funciona mediante la adsorción y desorción de metanol (alcohol metílico) por carbón activado, que es un material muy poroso y adsorbente. El prototipo básico está formado por un &lt;strong&gt;recipiente colector&lt;/strong&gt; donde está el carbón activado empapado en alcohol metílico, un &lt;strong&gt;condensador&lt;/strong&gt;, que convierte en líquido los vapores del alcohol, un &lt;strong&gt;evaporador&lt;/strong&gt; que recoge el alcohol líquido para que puedan volver a evaporarse y una &lt;strong&gt;cámara fría&lt;/strong&gt;, donde el agua se enfría convirtiéndose en hielo. En la primera parte de la nota se encuentra una descripción más detallada del mecanismo de enfriamiento.&lt;/p&gt;
&lt;h3 id=&#34;los-ingredientes-necesarios&#34;&gt;Los ingredientes necesarios&lt;/h3&gt;
&lt;p&gt;Una Universidad con la mirada puesta en las necesidades de los ciudadanos, un grupo de científicos comprometidos con la investigación y abiertos al diálogo, instituciones que releven las necesidades de la sociedad en que están inmersas y puedan hacer de vínculo y finalmente ciudadanos dispuestos a sumarse a propuestas en principio tan delirantes como conseguir frío a partir del Sol.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>La supuesta detección de velocidades superlumínicas de los neutrinos</title>
      <link>https://ciencianet.com.ar/post/la-supuesta-deteccion-de-velocidades-superluminicas-de-los-neutrinos/</link>
      <pubDate>Tue, 08 Nov 2011 01:44:34 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/la-supuesta-deteccion-de-velocidades-superluminicas-de-los-neutrinos/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Gaston E. Giribet.&lt;/strong&gt; Departamento de Física, Facultad de Ciencias Exactas y Naturales, Universidad de Buenos Aires y CONICET.&lt;/p&gt;
&lt;p&gt;El Laboratorio Nacional Gran Sasso de Italia anunció el pasado 23 de septiembre la detección de partículas que, aparentemente, tendrían una velocidad mayor a la de la luz. Se trata de las partículas llamadas neutrinos, las cuales se conocen desde hace ya varias décadas, pero de las cuales hemos aprendido mucho recién en los últimos años.&lt;/p&gt;
&lt;p&gt;La existencia de los neutrinos fue predicha en la década de 1930 por W. Pauli, y éstos fueron observados recién a mediados de la década de 1950. Hace tan solo una década que se consiguió evidencia significativa de que los neutrinos tienen masa (durante mucho tiempo se creyó que no era así) y que debido a la pequeñísima masa que tienen (a lo sumo unos pocos electronvoltios, i.e. cerca de una milmillonésima parte de la masa de un átomo de hidrógeno) se comportan de manera muy curiosa, cambiando sus propiedades con alternancia, oscilando su identidad a medida que viajan.&lt;/p&gt;
&lt;p&gt;Y como si este deambular esquizofrénico no fuera ya suficiente para merecer nuestra sorpresa, se suma hoy el notable anuncio de los científicos en el Gran Sasso. Si sus mediciones resultaran ciertas, algunos neutrinos viajarían más rápidamente que la luz, excediendo la velocidad de ésta en una parte en cienmil. La forma en la que los físicos dicen haber medido este efecto es sencillo de explicar, aunque muy difícil de implementar experimentalmente dada la gran cantidad de detalles a los que es menester atender: los físicos del laboratorio italiano se disponen a medir con gran exactitud la distancia recorrida por un rayo de neutrinos y medir también el tiempo empleado en recorrer dicha distancia. Luego, la velocidad a la que los neutrinos viajan se obtiene, como sabemos desde el preescolar, al dividir la distancia por el tiempo empleado en recorrerla.&lt;/p&gt;
&lt;p&gt;Los neutrinos son partículas que interaccionan muy débilmente con la materia, razón por la cual es imposible evitar que rayos de estas partículas escapen tras las paredes de los aceleradores de partículas como producto residual de tales experimentos. Los físicos, lejos de desaprovechar esos neutrinos escapistas, aprovechan los rayos para realizar otros experimentos y entender así sus propiedades. De hecho, fue este el truco empleado para detectarlos en los 50s. Y es esto lo que ocurre también con los neutrinos producidos en el acelerador del CERN, en la frontera franco-suiza.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2011/11/OPERAdetector-300x201.jpg&#34; alt=&#34;Detector del experimento OPERA.:left&#34; title=&#34;Detector del experimento OPERA&#34;&gt;&lt;/p&gt;
&lt;p&gt;Luego de abandonar los detectores de muones del CERN, los neutrinos producidos allí viajan cerca de 732 kilómetros hasta la cordillera de los Apeninos, donde son detectados por el experimento llamado OPERA, que está ubicado bajo 1400 metros de piedra, en el Laboratorio Nacional Gran Sasso de Italia.&lt;/p&gt;
&lt;p&gt;Debido a la mencionada propiedad de interaccionar débilmente con la materia, los neutrinos ven al macizo Gran Sasso casi transparente, y es esa la razón por la que el experimento se encuentra en las profundidades, cobijado por la piedra, donde prácticamente sólo los neutrinos pueden llegar. Cabe mencionar que no es esta la primera vez que se observa este tipo de comportamiento super-lumínico de los neutrinos. Resultados similares habían ya sido obtenidos por el experimento denominado MINOS, ubicado en Medio Oeste de los Estados Unidos. No obstante, el grado de exactitud de OPERA es mucho mayor y es esto lo que hace tan importante el anuncio.&lt;/p&gt;
&lt;p&gt;También sería necesario explicar cómo el hecho de que estos neutrinos producidos en el CERN y detectados en el Gran Sasso superen la velocidad de la luz no entra en tajante contradicción con las observaciones de neutrinos provenientes de supernovas lejanas, como es el caso de la supernova SN1987a cuya emisión de anti-neutrinos fue detectada en el momento esperado, en concordancia con los modelos de evolución estelar. Los neutrinos son emitidos pocas horas antes del estallido de luz en una supernova de esas características, y los neutrinos de SN1987a fueron observados precisamente con esa antelación, y no una mayor, a la emisión de luz asociada. Si aquellos neutrinos producidos en la explosión SN1987, acaecida a una distancia de 168.000 años luz de la tierra, hubieran tenido la velocidad que se les adjudica a los neutrinos italianos, entonces habrían aquéllos arribado a la tierra varios años antes.&lt;/p&gt;
&lt;p&gt;Ahora bien, ¿por qué sería tan importante si se descubriera que una partícula diminuta y reticente a interaccionar con la materia viaja más rápido que la luz? La respuesta es que un descubrimiento de esta índole pondría en tela de juicio uno de los pilares fundamentales de la física: la existencia de partículas que viajen más rápidamente que la luz está en tajante contradicción con la teoría de la relatividad especial de Einstein, la cual establece la velocidad de la luz como un límite infranqueable. Violar dicho límite representaría una crisis fundamental en la física teórica ya que llevaría en germen el problema de poner en riesgo la noción de causalidad.&lt;/p&gt;
&lt;p&gt;Es así como si los resultados del experimento italiano llegaran a ser confirmados, sería necesario rever de cuajo varios aspectos fundamentales de la física que conocemos. No obstante, en estos casos la actitud que prevalece en la comunidad científica es siempre la cautela, y aún más en este caso. Lo más probable es que la supuesta observación de velocidades super-lumínicas de los neutrinos en el Gran Sasso se deba, en realidad, a algún error sistemático del experimento. Es por esto necesario que otros laboratorios confirmen las mediciones de manera independiente, y que se evalúe si no se ha cometido ningún error en el experimento efectuado ni en la interpretación del mismo.&lt;/p&gt;
&lt;p&gt;No sería, pues, la primera vez que la teoría de la relatividad saliera triunfante luego de haber sido desafiada.&lt;/p&gt;
&lt;p&gt;Versión ampliada de *¿Einstein se equivocó?*, por Gaston Giribet, Revista Noticias Nº 1815.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Actualización - 19 de Marzo de 2012 -&lt;/strong&gt; Después de que OPERA reconociera la existencia de errores, el experimento ICARUS informa que los neutrinos viajan a la velocidad de la luz. Los resultados están en: &lt;a href=&#34;http://arxiv.org/pdf/1203.3433v1.pdf&#34;&gt;http://arxiv.org/pdf/1203.3433v1.pdf&lt;/a&gt; Measurement of the neutrino velocity with the ICARUS detector at the CNGS beam The CERN-SPS accelerator has been briefly operated in a new, lower intensity neutrino mode with ~10^12 p.o.t. /pulse and with a beam structure made of four LHC-like extractions, each with a narrow width of ~3 ns, separated by 524 ns. This very tightly bunched beam structure represents a substantial progress with respect to the ordinary operation of the CNGS beam, since it allows a very accurate time-of-flight measurement of neutrinos from CERN to LNGS on an event-to-event basis. The ICARUS T600 detector has collected 7 beam-associated events, consistent with the CNGS delivered neutrino flux of 2.2 10^16 p.o.t. and in agreement with the well known characteristics of neutrino events in the LAr-TPC. The time of flight difference between the speed of light and the arriving neutrino LAr-TPC events has been analysed. The result is compatible with the simultaneous arrival of all events with equal speed, the one of light. This is in a striking difference with the reported result of OPERA that claimed that high energy neutrinos from CERN should arrive at LNGS about 60 ns earlier than expected from luminal speed.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>La expansión acelerada del Universo y el Premio Nobel de Física de 2011</title>
      <link>https://ciencianet.com.ar/post/la-expansion-acelerada-del-universo-y-el-premio-nobel-de-fisica-de-2011/</link>
      <pubDate>Fri, 04 Nov 2011 01:47:01 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/la-expansion-acelerada-del-universo-y-el-premio-nobel-de-fisica-de-2011/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Gastón E. Giribet&lt;/strong&gt;. Departamento de Física, Facultad de Ciencias Exactas y Naturales, Universidad de Buenos Aires y CONICET.&lt;/p&gt;
&lt;h3 id=&#34;la-expansión-del-universo-y-la-relatividad&#34;&gt;La expansión del Universo y la Relatividad&lt;/h3&gt;
&lt;p&gt;Una de las predicciones más sorprendentes de la Teoría de la Relatividad, y que al comienzo fue recibida con sorpresa por el mismo Einstein, es que si uno asume que el Universo es homogéneo (que en todas partes es similar) e isótropo (que en todas direcciones es similar) entonces indefectiblemente éste puede expandirse o contraerse con el tiempo, pero nunca permanecer estático.&lt;/p&gt;
&lt;p&gt;Este aspecto dinámico del Universo es inexorable, una conclusión ineluctable de las ecuaciones de Einstein. Muchos físicos advirtieron tempranamente sobre esta implicancia de la Teoría de la Relatividad General; el mismo Albert Einstein recibió el resultado con reluctancia al comienzo y muchos otros como Erwin Schrödinger intentaron conciliar la idea de un universo estático con la teoría. Incluso, Einstein creyó por un momento haber logrado detener el Universo en sus cuadernos de notas. Para lograrlo había el creador de la Relatividad deformado su propia teoría introduciendo un nuevo término en sus célebres ecuaciones, el cual parecía hacer el trabajo.&lt;/p&gt;
&lt;p&gt;No obstante, rápidamente se advirtió que ese nuevo y extraño término, llamado &amp;quot;el término cosmológico&amp;quot;, no era de suficiente provecho y que, aunque lograba detener el Universo ejerciendo una presión negativa para lograrlo, éste terminaba desestabilizándose ante el menor aleteo de un mosquito y continuaba así su expansión (o su contracción.) ¿Debían entonces Einstein y sus contemporáneos aceptar la idea de un universo en movimiento?, ¿o acaso el Universo se hallaba entonces en su inestable equilibrio gracias al término cosmológico que venía a corregir sus ecuaciones?&lt;/p&gt;
&lt;p&gt;Otra posibilidad, a la que no muchos parecían subscribir, era abandonar la Relatividad General como el marco teórico para describir el Universo a gran escala. Afortunadamente no fue este último el camino que tomaron los físicos sino uno que, como sabemos hoy, resultaría más provechoso. Uno se ve tentado entonces a concluir que la creencia en la Relatividad prevaleció sobre la concepción cosmogónica estática, pero esta visión de los sucesos sería incompleta si no reconociéramos los méritos que las observaciones astronómicas tuvieron a la hora de confirmar el movimiento del cosmos.&lt;/p&gt;
&lt;h3 id=&#34;la-observación-de-la-expansión-cósmica&#34;&gt;La observación de la expansión cósmica.&lt;/h3&gt;
&lt;p&gt;Muchas preguntas permanecían abiertas en esos tiempos tempranos de la cosmología. Aunque la idea de nuestro Universo como un escenario dinámico se encontraba en consonancia con la revisión de la naturaleza que la revolución darwiniana ya había iniciado como línea de pensamiento -aunque en un contexto muy distinto-, seguía siendo una concepción difícil de digerir. Y aún aceptando el movimiento del cosmos quedaban preguntas básicas por responder tales como si el Universo se expandía o se contraía, o si en caso de que ocurriere lo primero se expandiría por siempre o sólo por un tiempo.&lt;/p&gt;
&lt;p&gt;Hoy, a casi cien años de la formulación de la Relatividad General, nos es dado saber la respuesta a muchas de estas preguntas. Pero, antes de adentrarnos en lo que hoy sabemos del cosmos, volvamos a los años 10s y 20s del pasado: Einstein insistió durante un tiempo considerable en su idea de conciliar la Relatividad con una imagen estática del cosmos, y para eso había agregado aquel nuevo &amp;quot;término cosmológico&amp;quot; en sus ecuaciones deformando ligeramente la versión original de su propia teoría –en parte esto se debió a que el término cosmológico satisfacía otras propiedades matemáticas que él buscaba en su teoría-.&lt;/p&gt;
&lt;p&gt;No obstante, la historia cambiaría radicalmente poco tiempo después. Tan sólo trece años después de la formulación de la Relatividad General, el astrónomo Edwin Hubble obtendría en 1929 la primera evidencia observacional de que, en efecto, el universo se movía y que éste lo hacía expandiéndose. Vale aclarar aquí que la Relatividad General predecía el movimiento del Universo, pero no decidía si ese movimiento era en expansión o en contracción ya que ello dependía, además, de conocer de la cantidad de materia total que hay en el cosmos.&lt;/p&gt;
&lt;p&gt;Hubble observó que las estrellas distantes presentaban un tono más rojizo que el que se esperaba de ellas, y que ese tinte inesperado de la luz que nos enviaban podía entenderse como signo de que dichas estrellas estaban alejándose de la Tierra, alejándose más rápidamente aquellas estrellas que más lejos se encontraban de nosotros. Era ésa evidencia substancial de la expansión del Universo.&lt;/p&gt;
&lt;p&gt;Además, las observaciones efectuadas por Hubble indicaban que la expansión cósmica se daba en todas direcciones de la misma manera y a la misma velocidad, una observación que inmediatamente invita a aclarar que esa isotropía no se debe a que seamos nosotros el centro del universo, como algún rezagado partícipe de la visión eclesiástica podría aventurar, sino a que el Universo parece satisfacer aquellas hipótesis que Einstein había sugerido desde el comienzo: El universo es en todas partes y en todas direcciones similar, homogéneo e isótropo. Así, todo punto del cosmos es su centro y todo punto se separa de los otros de igual manera.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2011/11/TypeIaSupernova.jpg&#34; alt=&#34;Supernova tipo Ia 1994D (el punto brillante abajo a la izquierda) en la región externa de la galaxia NGC4526. Crédito: High-Z Supernova Search Team, HST, NASA and V. Rubin/CIW.&#34; title=&#34;TypeIaSupernova&#34;&gt;&lt;/p&gt;
&lt;p&gt;El mismo Einstein tuvo acceso a las observaciones de Hubble y encontró la evidencia contundente. Las estrellas se alejan de nosotros y lo hacen con más velocidad conforme más lejos de nosotros se hallan. Fue entonces cuando advirtió que su teoría de la Relatividad General iba más lejos que sus propias convicciones y preconceptos: La Relatividad había predicho la mismísima expansión del Universo antes de que ésta hubiera sido observada en el telescopio.&lt;/p&gt;
&lt;p&gt;Einstein se sintió compelido a reconocer que la inclusión de su famoso &amp;quot;término cosmológico&amp;quot; en sus ecuaciones, el que entonces parecía ser ya innecesario, había sido &amp;quot;el desatino más grande de su vida&amp;quot;. Sin ése la teoría explicaba la expansión cósmica, una predicción que habría merecido otro premio Nobel.&lt;/p&gt;
&lt;h3 id=&#34;la-aceleración-y-el-premio-nobel&#34;&gt;La aceleración y el premio Nobel&lt;/h3&gt;
&lt;p&gt;Ahora bien, aún asumiendo que el Universo se expandía, y aún teniendo evidencia observacional de ello, quedaba la pregunta abierta de si sería esa expansión eterna, o si, por el contrario, luego de una larga excursión cósmica, todas las estrellas revertirían sus carreras y recolapsarían.&lt;/p&gt;
&lt;p&gt;La respuesta a esta pregunta tardó en llegar ya que, como mencionamos antes, saber el destino del Universo depende de conocer la cantidad de materia y energía que haya en él, un dato del que sólo en las dos últimas décadas hemos aprendido lo suficiente: Desde comienzos de la década de los 90s contamos con evidencia de que no parece haber suficiente materia en el Universo como para que éste recolapse. Mucho más sorprendente fue el descubrimiento, a fines de esa misma década, de que el Universo no sólo se expande sino que, además, lo hace cada vez más rápido, acelerándose.&lt;/p&gt;
&lt;p&gt;Fue la observación de esto último lo que le valió a Saul Perlmutter, Brian Schmidt y Adam Riess el premio Nobel de física de este año. En 1997 y 1998 ellos, junto a otros coautores, publicaron las observaciones de supernovas tipo Ia lejanas (explosiones de estrellas muy distantes debidas típicamente a enanas blancas que ganan mucha masa y superan el llamado límite de Chandrasekar) que presentaban un alto vicio hacia el color rojo cuando se las observaba. Esto, combinado con su propiedad de emitir luz con una intensidad que permite ser calculada con precisión, permitió a Perlmutter et al. emplear ese tipo de supernovas como &amp;quot;candelas patrón&amp;quot; para establecer a qué velocidad se expande el Universo en un dado momento de la historia del mismo.&lt;/p&gt;
&lt;p&gt;Fue así como se logró determinar que el Universo, hoy, se expande aceleradamente. Esta expansión acelerada del Universo es desconcertante en varios aspectos teóricos, y hubo quien señaló que entender dicha aceleración es &amp;quot;el problema más desconcertante de la física teórica actual&amp;quot;. En particular, es difícil entender de una manera satisfactoria cómo es que el valor del término cosmológico es tan pequeño.&lt;/p&gt;
&lt;p&gt;Los modelos más naturales desde el punto de vista estético-matemático, si es que tal criterio existe en verdad, sugieren que el término cosmológico debería no estar presente o, si lo estudviera, su valor debería ser muchísimos órdenes de magnitud superior al observado. No obstante, sin ánimo de entrar en los detalles técnicos del caso, alcanza con decir que la mejor descripción teórica con la que hoy contamos para explicar la expansión acelerada del Universo es, paradójicamente, aquella que Einstein había ensayado en 1917; es decir, deformar la Teoría de la Relatividad General como él mismo lo había sugerido, introduciendo su &amp;quot;término cosmológico&amp;quot; -aunque en una medida mucho menor a la originalmente propuesta para detener el cosmos-.&lt;/p&gt;
&lt;p&gt;Es así como hoy asistimos al momento en el que la cosmología le da la derecha al genio alemán mostrando que incluso &amp;quot;el más grande desatino de su vida&amp;quot; fue en realidad un gracioso acierto. El término cosmológico de Einstein parece estar allí, cumpliendo el papel de acelerar la expansión.&lt;/p&gt;
&lt;h3 id=&#34;el-futuro-del-universo&#34;&gt;El futuro del Universo&lt;/h3&gt;
&lt;p&gt;Surge entonces la pregunta sobre el futuro del universo: Si no recolapsará el cosmos sobre sí mismo, si éste seguirá expandiéndose por siempre, enfriándose, ¿qué será del futuro de éste?, ¿qué ocurrirá cuando las estrellas agoten el combustible nuclear que las hace brillar?, ¿qué ocurrirá cuando la coalescencia gravitatoria de las diferentes galaxias lleve a la materia estelar a apelmazarse?&lt;/p&gt;
&lt;p&gt;Estas preguntas sobre el futuro del Universo ocupan también la mente de los cosmólogos, quienes ensayan respuestas que, aunque diversas, siempre contrastan con el pasado tumultuoso del Universo temprano y caliente. Algunas especulaciones sugieren que, en el futuro remoto, grandes agujeros negros serán creados tras la coalescencia de astros y galaxias, y que esos gigantes, oscuros y fríos terminarán radiando su masa al ritmo lento que Stephen Hawking predijo para tal fenómeno en la década de los 70s.&lt;/p&gt;
&lt;p&gt;Otras especulaciones sobre el futuro del universo tienen en cuenta la expansión acelerada del mismo, y predicen que esta expansión terminaría por alejar unas regiones del universo de otras hasta que ya no sea posible enviar luz de un lado al otro, y que el universo iría así desconectándose causalmente, desapareciendo tras el escurridizo concepto de “horizonte cosmológico”.&lt;/p&gt;
&lt;p&gt;Sea cual fuere el destino de nuestro Universo en el futuro remoto, hay algo en lo que los cosmólogos parecen estar de acuerdo: Muy probablemente será una muerte fría y solitaria.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>GRB110328A no sería un GRB - Curiosa emisión de rayos gama proveniente de la Constelación del Dragón</title>
      <link>https://ciencianet.com.ar/post/grb110328a-no-seria-un-grb-curiosa-emision-de-rayos-gama-proveniente-de-la-constelacion-del-dragon/</link>
      <pubDate>Sat, 14 May 2011 01:49:28 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/grb110328a-no-seria-un-grb-curiosa-emision-de-rayos-gama-proveniente-de-la-constelacion-del-dragon/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Gastón Giribet.&lt;/strong&gt; Departamento de Física, Instituto de Física Buenos Aires, UBA &amp;amp; CONICET.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;En este artículo el autor comenta y discute una reciente observación, relevante en el campo de la astrofísica, sobre una curiosa emisión de rayos gama proveniente de la Constelación del Dragón. Esta emisión se debería a la destrucción parcial de una estrella debido a la cercanía de la misma a un agujero negro supermasivo. Además de la información, interesante por sí misma, nos ofrece un ejemplo actual de la construcción de conocimiento en ciencia.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Hace pocas semanas, el 28 de marzo de 2011, el satélite Swift de NASA observó en la Constelación del Dragón un evento de emisión en rayos gama muy curioso y cuya interpretación ulterior es muy interesante. El evento en rayos gama, bautizado como Swift J164449.3+573451, fue originalmente interpretado como un &amp;quot;gamma ray burst&amp;quot; (GRB) &lt;a href=&#34;#1&#34;&gt;[1]&lt;/a&gt;, a lo que se debe su nombre alternativo GRB110328A.&lt;/p&gt;
&lt;p&gt;No obstante, rápidamente se advirtió que el evento no presentaba las características usualmente observadas para un GRB, por lo que la interpretación usual en términos del colapso de una estrella como mecanismo generador de tal emisión gama no parecía funcionar. En particular, se observó que la fuente de rayos gama, fuera cual fuere su naturaleza, siguió emitiendo esporádicamente destellos los días siguientes a la observación del fenómeno inicial.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2011/05/GRB110328A-1-300x300.jpg&#34; alt=&#34;Imagen del evento GRB110328A obtenida por el satélite Swift de la NASA.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Entonces, inmediatamente surge la pregunta acerca de qué tipo de fenómeno astrofísico daría lugar a un evento tal. Una respuesta a esta pregunta llegó rápidamente, y parece haber consenso al respecto: La inusual radiación gama no se debería a un GRB, sino a la &amp;quot;disrupción&amp;quot; de una estrella debida a la cercanía de la misma a un agujero negro supermasivo.&lt;/p&gt;
&lt;p&gt;La masa de este agujero negro se calcula entre uno y diez millones de masas solares. Este tipo de agujeros negros existe en los centros de las galaxias y, aunque algunas veces están cubiertos por materia circundante congregada en torno a ellos, otras veces (como en este caso, o como probablemente ocurre en el caso de Sagitario A*, la fuente asociada al agujero negro que estaría en el centro de nuestra Via Láctea).&lt;/p&gt;
&lt;p&gt;Estos gigantescos monstruos están desnudos, invisibles, expectantes, acechando a alguna estrella que pase cerca de ellos para, como en el caso del evento GRB110328A del que hablamos aquí, eventualmente destruirla con su fuerza gravitatoria (más precisamente, por el efecto centrífugo que genera la misma dinámica) y comer parte de ella. Esto parece ser lo que ocurrió en la galaxia lejana de la que GRB110328A nos llega.&lt;/p&gt;
&lt;p&gt;La razón por la que llamo la atención sobre este hallazgo es que se trata de un evento curioso, difícilmente observado, y que permite tener más información para estudiar la física de los agujeros negros supermasivos, objetos sobre los que aún hay varias preguntas abiertas.&lt;/p&gt;
&lt;p&gt;Para aquellos que quisieren saber más detalles acerca del evento GRB110328A, pueden recurrir a dos artículos muy interesantes que aparecieron en estas semanas, en los cuales se discute el evento y se adhiere a la interpretación del mismo como una &amp;quot;&lt;em&gt;tidal force disruption&lt;/em&gt;&amp;quot;; a saber: &lt;a href=&#34;https://arxiv.org/abs/1104.32577&#34;&gt;Bloom et al.&lt;/a&gt; y &lt;a href=&#34;https://arxiv.org/abs/1104.2528&#34;&gt;Barres et al.&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referencias.&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;J.S. Bloom, at al., &amp;quot;A relativistic jetted outburst from a massive black hole fed by a tidally disrupted star&amp;quot;, arXiv:1104.3257.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;U. Barres de Almeida and A. De Angelis, &amp;quot;Enhanced emission from GRB 110328A could be evidence for tidal disruption of a star&amp;quot;, arXiv:1104.2528.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a id=&#34;1&#34;&gt;1.&lt;/a&gt; Los “gamma ray burst” o GRB son emisiones puntuales de radiación electromagnética en el rango de los rayos gama, que han sido detectados –aunque poco frecuentemente- en explosiones asociadas a supernovas y colapsos estelares.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Dulces melodías, ¿con cuántas de azúcar?</title>
      <link>https://ciencianet.com.ar/post/dulces-melodias-con-cuantas-de-azucar/</link>
      <pubDate>Sat, 09 Apr 2011 01:55:22 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/dulces-melodias-con-cuantas-de-azucar/</guid>
      <description>
        
          &lt;p&gt;Por &lt;strong&gt;Damián H. Zanette&lt;/strong&gt;, Centro Atómico Bariloche&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2011/04/dulces-melodias-300x221.png&#34; alt=&#34;:left&#34; title=&#34;Dulces Melodias&#34;&gt;&lt;/p&gt;
&lt;p&gt;Los lenguajes humanos son muy avaros cuando se trata de brindarnos vocablos específicos que describan las sensaciones que nos produce la música. Es así que, desde siempre, los músicos y su público han recurrido a palabras prestadas de otros ámbitos para definir las sutiles variaciones en velocidad, volumen, y timbre que determinan el carácter de una composición. Cuando Beethoven titulaba Scherzo (broma o juego, en italiano) al tercer movimiento de una sinfonía, o Mozart indicaba dolce en las arias más líricas de sus protagonistas operísticas, apelaban a un acuerdo tácito que delinea de modo muy preciso cómo deben ejecutarse sus obras. Las palabras que hacen referencia al sabor (dolce, salao) están entre las preferidas para “traducir” nuestras percepciones musicales.&lt;/p&gt;
&lt;p&gt;En el marco de las neurociencias se ha propuesto que las asociaciones intermodales como ésta, que vincula procesos cognitivos del ámbito musical con imágenes del mundo sensorial, podrían tener una base más profunda que la de ser una simple metáfora, y estar siempre presentes en las funciones mentales normales. En el caso extremo de los individuos sinestéticos, la estimulación de un canal sensorial -por ejemplo, la audición de un sonido- es capaz de generar respuestas en otro -la visualización de un color. En un trabajo reciente, Bruno Mesz, Marcos Trevisan y Mariano Sigman, del Laboratorio de Acústica y Percepción Sonora (UNQ) y de los Laboratorios de Sistemas Dinámicos y de Neurociencia Integrativa (FCEN-UBA), han explorado empíricamente el vínculo cognitivo entre la percepción musical y las imágenes asociadas al sentido del gusto.&lt;/p&gt;
&lt;p&gt;Del experimento participaron nueve músicos, a cada uno de los cuales se solicitó que ejecutara 24 improvisaciones. Cada una, de no más de un minuto de duración, debía realizarse sobre la base de una palabra “consigna”, entre las que se encontraban dulce, salado, ácido, y amargo. Los experimentadores cuantificaron la articulación, volumen, altura y grado de disonancia de las improvisaciones, y correlacionaron estas medidas con la consigna. Las improvisaciones “dulces” se caracterizaron por ser de bajo volumen, con notas ligadas, de larga duración, y con poca disonancia; las “saladas” tuvieron notas cortas y bien articuladas; las “ácidas” resultaron ser muy disonantes, con notas agudas y largas; las “amargas”, con notas graves y ligadas.&lt;/p&gt;
&lt;p&gt;En un segundo experimento, se solicitó a 57 sujetos que, luego de escuchar cada una de las improvisaciones del experimento anterior, le asignaran una de las palabras dulce, salado, ácido, o amargo. El nivel de coincidencia entre la palabra asignada y la consigna de la improvisación alcanzó, en promedio, casi el 70 %. En el caso de las improvisaciones “amargas”, superó el 80 %. Estos experimentos contribuyen a dilucidar el entramado de percepciones sensoriales, estados emocionales, dimensiones afectivas y culturales, evocaciones y asociaciones semánticas que subyacen a los intrincados procesos cognitivos del cerebro humano.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; B. Mesz, M. A. Trevisan, M. Sigman, &lt;a href=&#34;http://www.perceptionweb.com/abstract.cgi?id=p6801&#34;&gt;The taste of music&lt;/a&gt;, Perception 40, 209 (2011).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Institutciones:&lt;/strong&gt; Laboratorio de Acústica y Percepción Sonora (Universidad Nacional de Quilmes), Laboratorio de Sistemas Dinámicos y Laboratorio de Neurociencia Integrativa (Depto. de Física, FCEN, Universidad de Buenos Aires).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; Mariano Sigman (Email: &lt;a href=&#34;mailto:sigman@df.uba.ar&#34;&gt;sigman@df.uba.ar&lt;/a&gt;)&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Cambios y permanencias en la Ciencia Argentina</title>
      <link>https://ciencianet.com.ar/post/cambios-y-permanencias-en-la-ciencia-argentina/</link>
      <pubDate>Fri, 08 Apr 2011 01:58:48 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/cambios-y-permanencias-en-la-ciencia-argentina/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Ariel Dobry&lt;/strong&gt;, Instituto de Física Rosario (UNR).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2011/04/mifoto-149x150.jpeg&#34; alt=&#34;Ariel Dobry:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Creo que uno de los efectos más positivos de las políticas implementadas en los últimos años en el país, fue el cambio de la subjetividad colectiva que consideraba imposible el cuestionamiento a los poderes instituidos.&lt;/p&gt;
&lt;p&gt;Me refiero a que desde la dictadura militar, pasando por el alfonsinismo y terminando en la década neoliberal menemista se nos había grabado a sangre y fuego que, aunque moralmente repugnante, era necesario aceptar las reglas impuestas por los poderes fácticos que actuaban tanto dentro como fuera del país.&lt;/p&gt;
&lt;p&gt;Quienes osábamos cuestionar estas reglas éramos tildados poco menos que de dinosaurios que nos habíamos quedado en el túnel del tiempo de ideologías ya perimidas. Así, no había sido posible avanzar en el juicio a los militares responsables del genocidio porque con sus levantamientos habían impuesto las leyes de impunidad.&lt;/p&gt;
&lt;p&gt;Los dictados de los organismos internacionales y de sus economistas autóctonos eran algo así como palabra santa aunque implicaban sumir en la pobreza a gran parte de nuestro pueblo. Lo mismo ocurría con el cuestionamiento a las privatizaciones de las empresas del estado y a la de los recursos naturales.&lt;/p&gt;
&lt;p&gt;Aunque el kirchnerismo no avanzó con la misma profundidad desarrollando acciones concretas en todas las cuestiones antes citadas, si permitió romper con el círculo de las verdades incuestionables aunque profundamente injustas. Hoy todo está en discusión, y aunque resistida violentamente por los sectores que ostentan los poderes económicos, es posible, al menos dar pelea, en todas las áreas donde se considere que un sector se adueña injustamente del patrimonio global o donde las desigualdades son evidentes e intolerables.&lt;/p&gt;
&lt;p&gt;La respuesta ‘simplemente no se puede´ no es considerada políticamente correcta ni aceptable por sectores cada vez más importantes de la población. Existe sin embargo un área en que este gobierno y el anterior han desarrollado políticas activas que no han aún logrado un efecto visible en cuanto al cambio de la subjetividad colectiva al respecto. Me refiero al del sector científico técnico.&lt;/p&gt;
&lt;p&gt;Durante el gobierno de Cristina Kirchner se creó el Ministerio de Ciencia, Tecnología e Innovación Productiva, cambiando así cualitativamente la importancia relativa que se le daba a este sector previamente. Se revirtió la pendiente negativa de (des)inversión en el área. Se comenzó a dar vuelta la pirámide etaria que hacía que en los organismos de ciencia la mayoría de los integrantes pertenezcan a las categorías más altas. Esto se logró aumentando considerablemente el número en becas de investigación y de ingresos a la carrera de investigador de CONICET. Se amplió substancialmente la cantidad de investigadores que tienen actualmente acceso a subsidios, esto permitió renovar o comprar equipamiento científico nuevo y mejorar las condiciones del trabajo científico. Se invirtió en bibliografía haciendo que mediante la Biblioteca Electrónica de Ciencia y Tecnología muchos investigadores tengan acceso a las revistas internacionales más importantes de su especialidad. Se comenzó también a revertir el déficit edilicio que hace que los investigadores trabajen en condiciones de hacinamiento.&lt;/p&gt;
&lt;p&gt;En otro orden, se ha comenzado a equilibrar la inversión que realiza el país en las distintas áreas del conocimiento. En particular, las áreas de Ciencias Humanas y Sociales, que históricamente eran relegadas respecto a las ciencias exactas, naturales y las biológicas, han empezado a revertir esta tendencia. Por ejemplo en 2010 el número de becarios del CONICET en ciencias humanas y sociales es aproximadamente el mismo que en ciencias biológicas y de la salud, mientras en el año 2000 la relación era de 2 a 1 a favor de estas últimas.&lt;/p&gt;
&lt;p&gt;Aunque estas políticas son aún incipientes y actúan en el contexto de un proceso histórico caracterizado por el desprecio y el desfinanciamiento del sector, implican una reversión en la tendencia y de mantenerse, ubican al área científica en un sector de potencial relevancia para colaborar en la formulación de políticas que cambien el estado de dependencia económico-cultural al que se sometió al país durante décadas, con la consecuente regresiva distribución de la riqueza que esto generó.&lt;/p&gt;
&lt;p&gt;Sin embargo creo, que la posibilidad concreta de que esto ocurra no está sólo conectado con la inversión de recursos económicos, ni aún más con el direccionamiento de los mismos hacia las áreas en que el país considere prioritarias, sino con el cambio en la subjetividad de la sociedad y aún mismo de los propios actores del sector en cuanto a que significa hacer ciencia en la Argentina.&lt;/p&gt;
&lt;p&gt;Nuestro país tiene ahora una cantidad importante de científicos trabajando en diferentes áreas del conocimiento. Sin embargo, la investigación científica no es identificada por el conjunto social como una actividad relevante. No se conocen sus instituciones, sus orientaciones, sus resultados ni sus potencialidades. Esto es en parte debido a que las continuas crisis y rupturas que sufrió el país debilitaron la formación de escuelas de pensamiento argentino en diversas áreas del conocimiento. Es también debido al relativo aislamiento en que debió desarrollarse la investigación para subsistir.&lt;/p&gt;
&lt;p&gt;Por otro lado la investigación científica en el país no constituye un cuerpo estructurado sino que es una animal de infinitas cabezas con casi tantas líneas de investigación como investigadores. La ciencia argentina debería dejar de pensarse asimismo como furgón de cola de algunas líneas de moda en la ciencia internacional.&lt;/p&gt;
&lt;p&gt;Sin caer en un aislamiento ilógico a esta altura del siglo XXI, debería reforzar sus potencialidades y crear vasos comunicantes entre investigadores de diferentes espacios. El cambio de la subjetividad de los científicos y de la sociedad hacia la ciencia es lo que está en juego. La respuesta del “no se puede” debería resquebrajarse en la ciencia como lo ha sido en el resto de las cuestiones que mencioné anteriormente.&lt;/p&gt;
&lt;p&gt;Creo que el tema merece un debate profundo entre los investigadores y entre estos y el resto de la sociedad. Aprendimos dolorosamente que la teoría del derrame económico no funciona. Me parece que tampoco funcionará en la ciencia. Quiero decir la inversión de recursos económicos, esperando que se produzca un cambio cualitativo espontáneo en cuanto a la calidad de la ciencia que se genera y en cuanto a su conexión con el medio, es una idea ingenua. Hay de hecho sectores, desde el ministerio hasta grupos de investigadores auto convocados, que están intentando dar respuestas a la inserción de la ciencia en la sociedad, con visiones a veces contrapuestas.&lt;/p&gt;
&lt;p&gt;Si no se amplía el marco de discusión incluyendo cada vez a más actores, las soluciones serán parciales y no permitirán desarrollar todas las potencialidades que el sistema científico puede ofrecer a la sociedad.&lt;/p&gt;
&lt;p&gt;Información de contacto: &lt;a href=&#34;mailto:dobry@ifir-conicet.gov.ar&#34;&gt;dobry@ifir-conicet.gov.ar&lt;/a&gt; &lt;a href=&#34;http://huciencia.blogspot.com/&#34;&gt;http://huciencia.blogspot.com/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Publicado en la revista Micropolíticas Enero/Febrero 2011.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>El valor de la educación universitaria pública y gratuita</title>
      <link>https://ciencianet.com.ar/post/el-valor-de-la-educacion-universitaria-publica-y-gratuita/</link>
      <pubDate>Tue, 15 Feb 2011 02:01:36 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/el-valor-de-la-educacion-universitaria-publica-y-gratuita/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Gabriel M. Bilmes&lt;/strong&gt;. Comisión de Investigaciones Científicas de la Provincia de Buenos Aires y Universidad Nacional de La Plata, Miembro del Espacio Varsavky.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2011/02/bilmes-foto.jpg&#34; alt=&#34;Gabriel Bilmes:left&#34; title=&#34;Gabriel Bilmes&#34;&gt;&lt;/p&gt;
&lt;p&gt;Si alguna duda cabía, un nuevo motivo confirma lo acertado de haber defendido la educación universitaria pública y gratuita en la Argentina, contra los intentos de privatización y arancelamiento que ensombrecieron la vida universitaria en nuestro país en distintos periodos y especialmente durante la década del 90. En un reciente artículo de la prestigiosa revista científica de circulación internacional Nature (&lt;a href=&#34;http://www.nature.com/news/2010/101222/full/4681003a.html&#34;&gt;diciembre del 2010, volumen 468, página 1003&lt;/a&gt;), Gregory Petsko, profesor de bioquímica de la Universidad de Brandeis, Walthman, EEUU, alertaba sobre un fenómeno que recorre los centros de educación superior del mundo: el corte de presupuestos y la desaparición de programas y carreras humanísticas y artísticas, perseguidas por el espectro del mercado.&lt;/p&gt;
&lt;p&gt;El fenómeno se está haciendo sentir cada vez con mayor fuerza en los EEUU, Canadá, Australia, Nueva Zelanda, Inglaterra, Francia y otros países de la Comunidad Europea, que están reduciendo drásticamente sus sistemas educativos universitarios. En Inglaterra, por ejemplo, ya se prevé que en los próximos cuatro años habrá una reducción del 40 % del presupuesto para la educación pública superior. Las razones de tales decisiones son sencillas: en esos países las carreras humanísticas y artísticas son económicamente deficitarias, no son rentables y no son las que masivamente eligen “los consumidores”, es decir, los alumnos.&lt;/p&gt;
&lt;p&gt;Sobre la base de la idea de que las fuerzas del mercado deben controlar la dirección de la educación para que ésta impacte en la economía, la formación superior se ha transformado en un negocio. Por tanto, todo lo que no contribuya a este negocio es reducido o eliminado. Este enfoque se ha reforzado por el hecho de que la administración de una gran cantidad de instituciones universitarias está en manos de burócratas reclutados del mundo de las finanzas y los negocios.&lt;/p&gt;
&lt;p&gt;Petsko, que proviene de las ciencias duras, reclama la defensa y reivindica la necesidad de las carreras humanísticas y artísticas. No solo porque forman parte del conocimiento universal e imprescindible para una persona culta, en el sentido más democrático de esta palabra (alguien que tiene conocimientos que le permiten tomar mejores decisiones para su vida y la de la comunidad), sino también por su rol crucial en la formación de un individuo. Poniendo como ejemplo su propia experiencia, Petsko reconoce que aprendió a pensar críticamente, a analizar en profundidad y a escribir con claridad gracias a los cursos humanísticos, y no a los científicos, que tomó en su Universidad. Más aún, las carreras humanísticas y artísticas ampliaron los horizontes de su formación, enriquecieron sus contactos y lo ayudaron a comunicarse mejor.&lt;/p&gt;
&lt;p&gt;Todas éstas, aptitudes imprescindibles para su trabajo como científico en el campo de la bioquímica. Imaginemos, entonces, por un momento qué hubiera pasado si las presiones políticas de los que todavía hoy aspiran en nuestro país a una educación universitaria no gratuita hubieran triunfado. Así como desaparecieron ramales enteros de ferrocarriles, hubieran desaparecido carreras enteras. Aún más, a diferencia de lo que ocurre en los países arriba mencionados, aquí ni siquiera hubieran sobrevivido las ciencias duras que, como lo demuestra la estructura de las universidades privadas argentinas, en ellas no tienen lugar porque no son rentables, requieren mucha inversión y tienen comparativamente a otras carreras, pocos alumnos.&lt;/p&gt;
&lt;p&gt;Por otro lado, en países donde la educación universitaria es privada o está arancelada, sobre todo de América Latina, mucha gente, instituciones y organizaciones políticas y sociales, expresan su admiración y reconocimiento a la posibilidad de acceder gratuitamente a los estudios superiores con la calidad y el nivel que las universidades argentinas tienen. Esta situación que nuestro país comparte con muy pocos otros en el mundo, no deja de ser sin duda un sano motivo de orgullo para nosotros y, por qué no, un modelo a imitar para otras sociedades.&lt;/p&gt;
&lt;p&gt;Para quienes pensamos que la educación es un derecho y no un negocio. Que el acceso a la Universidad debe ser gratuito, porque, además de garantizar este derecho, un país con equidad social, mayor distribución del ingreso y mayor crecimiento para todos requiere cada vez más profesionales comprometidos que puedan llevar adelante estas políticas, el artículo de la revista Nature es un alerta más de lo que puede ocurrir si los sectores de poder reaccionarios y conservadores, lograran torcer el rumbo que la Argentina ha emprendido en los últimos años. Algo, como reclama Petsko, para pensar y no quedarse callado.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Sobre la pista del grafeno en Argentina: efectos electromecánicos en la nanoescala</title>
      <link>https://ciencianet.com.ar/post/sobre-la-pista-del-grafeno-en-argentina-efectos-electromecanicos-en-la-nanoescala/</link>
      <pubDate>Tue, 14 Dec 2010 02:05:01 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/sobre-la-pista-del-grafeno-en-argentina-efectos-electromecanicos-en-la-nanoescala/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Héctor Riojas Roldán&lt;/strong&gt;. Universidad Nacional de La Plata.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Este artículo ha sido financiado por el proyecto INVOFI de la Asociación Física Argentina.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Este año han sido galardonados con el premio Nobel de Física 2010 los rusos Andre Geim (51) y Konstantin Novoselov (36) por sus estudios sobre el grafeno que posibilitaría nuevos avances en la física cuántica como así también numerosas aplicaciones tecnológicas. En este artículo reseñamos algunos trabajos realizados por investigadores argentinos.&lt;/p&gt;
&lt;h3 id=&#34;grafeno-en-argentina&#34;&gt;Grafeno en Argentina&lt;/h3&gt;
&lt;p&gt;En nuestro país el investigador Luis Foa Torres (32) Dr. en Física, investigador adjunto del Instituto de Física Enrique Gaviola (CONICET) y profesor adjunto de la FaMAF, en la Universidad Nacional de Córdoba, nos dice “Nuestro trabajo se centra principalmente en el estudio de las propiedades eléctricas de materiales nanoestructurados, notablemente materiales basados en carbono como los nanotubos de carbono y el grafeno. Mis primeras experiencias con estos materiales se remontan al 2005 cuando trabajaba para la Comisión de Energía Atómica de Grenoble (Francia), una línea que continué luego en Dresden (Alemania) y posteriormente aquí en Córdoba, donde desde el 2009 trabajo junto a un pequeño equipo de investigadores”.&lt;/p&gt;
&lt;h3 id=&#34;pero-primero-qué-es-el-grafeno&#34;&gt;Pero primero, ¿qué es el grafeno?&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/12/Fig1.jpg&#34; alt=&#34;Figura 1:left&#34; title=&#34;Figura 1&#34;&gt; El grafeno es una estructura de átomos de carbono formando una red bidimensional de tan sólo un átomo de espesor. La red cristalina que los contiene le da la forma de un panal de abejas. Cuando esta plancha de átomos tiene una de sus dimensiones en el orden de las decenas de nanometros se dice que es una “cinta” (&lt;em&gt;ribbon&lt;/em&gt;) de grafeno y se suele usar las siglas en inglés GNRs por &lt;em&gt;graphene nanoribbons&lt;/em&gt;. Dependiendo de la forma de los bordes de la cinta, las mismas se denominan tipo zig-zag o tipo sillón (&lt;em&gt;armchair&lt;/em&gt;), como se puede ver en la figura 1.&lt;/p&gt;
&lt;h3 id=&#34;veamos-algunas-de-sus-características&#34;&gt;Veamos algunas de sus características&lt;/h3&gt;
&lt;p&gt;Tienen alta conductividad térmica y eléctrica debido a que los electrones tienen una alta movilidad y baja dispersión. Combina alta elasticidad y ligereza, lo que lo convierte en unos de los materiales más resistentes ya que su dureza es extrema (varias veces más fuerte que el acero). Puede reaccionar con otros elementos y compuestos químicos y además es transparente. Doblando estas láminas se tienen los nanotubos (buckytubos o &lt;em&gt;buckytubes&lt;/em&gt;) y nanoesferas (buckyesferas o &lt;em&gt;buckyballs&lt;/em&gt; ) como se ilustra en la figura 2, aunque como nos aclara nuestro investigador “los nanotubos se obtuvieron mucho antes que el grafeno. Sólo recientemente lograron &#39;cortar&#39; tubos para fabricar cintas de grafeno, en un proceso similar a la apertura de un cierre, logrando así producir una estructura a partir de la otra.[&lt;a href=&#34;#ref1&#34;&gt;1&lt;/a&gt;]&lt;/p&gt;
&lt;h3 id=&#34;posibilidades-tecnólogicas-y-aplicaciones&#34;&gt;Posibilidades tecnólogicas y aplicaciones&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/12/Fig21.jpg&#34; alt=&#34;Figura 2.:left&#34; title=&#34;Figura 2&#34;&gt; En teoría el grafeno es más estable a nanoescalas que el silicio y dado que que la tendencia en la eléctrónica es cada vez hacer cosas más pequeñas, todo indica que es el mejor candidato a revolucionar el campo tecnológico electrónico. Pantallas táctiles flexibles, sensores de gases y otras sustancias y nueva generación de baterías ultracompactas. La dificultad fundamental para algunas aplicaciones es que el grafeno no tiene una brecha en la banda de energías permitidas (&lt;em&gt;energy band-gap&lt;/em&gt;). Solamente cuando el sistema se confina en una de las direcciones, formando cintas de grafeno se encuentra que algunas de ellas exhiben una brecha energética que depende de la topología. Aun así, el desafío es crear una brecha energética mediante un estímulo externo. Esto permitiría controlar las propiedades eléctricas y generar aplicaciones útiles.&lt;/p&gt;
&lt;h3 id=&#34;aportes-teóricos-nacionales&#34;&gt;Aportes teóricos nacionales&lt;/h3&gt;
&lt;p&gt;En una publicación científica reciente el Dr Luis Foa Torres [&lt;a href=&#34;#ref2&#34;&gt;2&lt;/a&gt;] , conjuntamente con otros científicos, estudiaron las propiedades de transporte de cargas cuando las láminas de grafeno se someten a esfuerzos mecánicos como se indica en la figura 3 (en este caso la fuerza es uniaxial). Se encuentra que la conductancia eléctrica tiene altas influencias debido a la simetría de los bordes de las láminas de grafeno. Se comparan los bordes brazos de silla y zigzag.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/12/Fig3-112x300.jpeg&#34; alt=&#34;Figura 3.:left&#34;&gt;] Mientras que los bordes zigzag resultan altamente resistentes a los esfuerzos de corte por fuerzas mecánicas, en la configuración tipo brazo de silla se induce una brecha energética generando así una transición metal-semiconductor. Estos novedosos resultados ubican a las tipo brazos de silla en una opción mucho mejor para aplicaciones electromécanicas. En palabras del autor “En algunas condiciones, nuestras simulaciones computacionales predicen que la tensión puede hacer que el sistema, originalmente conductor, se vuelva un semiconductor o viceversa. Esto podría ser de gran utilidad para la generación de nuevos dispositivos nanoelectromecánicos”.&lt;/p&gt;
&lt;p&gt;El Dr Luis Foa Torres dice además “su obtención en el laboratorio no requiere de elementos costosos ni exóticos: el grafeno puede aislarse a partir del grafito, el material de la mina de lápiz, el mismo que todos usamos en la escuela. Reexaminando un material conocido, con tenacidad y voluntad para ir más allá del camino marcado, Geim y Novoselov lograron un descubrimiento revolucionario que podría marcar el inicio de una nueva era.” También podemos encontrar en la reunión de la AFA de este año en Malargüe, Mendoza, dos resúmenes de sus últimos trabajos en progreso bajo los títulos “Bombeando electrones en materiales basados en carbono: la influencia de los defectos” (P584) y “Nanodispositivos en el límite cuántico: transporte alterno en un mundo de carbono” (P210).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referencias.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;ref1&#34;&gt; 1. &lt;/a&gt; Ver &lt;a href=&#34;https://doi.org/10.1038/nature07872&#34;&gt;Nature Vol 458, 16 April 2009.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;ref2&#34;&gt; 2. &lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1103/PhysRevB.81.193404&#34;&gt;Physical Review B 81, 193404 (2010)&lt;/a&gt;.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Avances en la lucha contra la placa dental</title>
      <link>https://ciencianet.com.ar/post/avances-en-la-lucha-contra-la-placa-dental/</link>
      <pubDate>Mon, 06 Dec 2010 20:28:15 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/avances-en-la-lucha-contra-la-placa-dental/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro&lt;/strong&gt;. Instituto de Física de Líquidos y Sistemas Biológicos.&lt;/p&gt;
&lt;p&gt;Los profesores Bauke Dijkstra y Lubbert Dijkhuizen de la Universidad de Groningen (Holanda) analizaron la enzima glucansucarasa obtenida de la bacteria &lt;em&gt;Lactobacillus reuteri&lt;/em&gt;, que se encuentra en la boca y en el tracto digestivo. Las bacterias usan la enzima glucansucarasa para convertir el azúcar de los alimentos en largas cadenas de azúcar &amp;quot;pegajosa&amp;quot;. Las bacterias usan este &amp;quot;pegamento&amp;quot; para adherirse al &lt;a href=&#34;http://es.wikipedia.org/wiki/Esmalte_dental&#34;&gt;esmalte dental&lt;/a&gt;. La principal causa del decaimiento dental, la bacteria &lt;em&gt;Streptococcus mutans&lt;/em&gt;, también utiliza esta enzima. Una vez adheridas al esmalte dental, estas bacterias fermentan &lt;a href=&#34;http://es.wikipedia.org/wiki/Az%C3%BAcar&#34;&gt;azúcares&lt;/a&gt; liberando ácidos que disuelven el calcio de los &lt;a href=&#34;http://es.wikipedia.org/wiki/Diente&#34;&gt;dientes&lt;/a&gt;. Así es como se desarrolla la &lt;a href=&#34;http://es.wikipedia.org/wiki/Caries&#34;&gt;caries&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/12/sonrisas-300x300.jpg&#34; alt=&#34;Fuente: Wikimedia:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Por medio de métodos cristalográficos, los investigadores fueron capaces de elucidar la estructura tridimensional (3D) de la enzima, siendo los primeros en lograr cristalizar glucansucarasa. La estructura cristalina reveló un mecanismo único de plegamiento: los distintos dominios de la enzima no están formados por una única cadena lineal de aminoácidos, sino por dos cadenas que se unen vía una estructura en forma de U. Este es el primer reporte en la literatura de un mecanismo de plegamiento de estas características. La determinación de la estructura 3D permitió a los investigadores obtener evidencias del mecanismo funcional de la enzima. La glucansucarasa divide la &lt;a href=&#34;http://es.wikipedia.org/wiki/Sacarosa&#34;&gt;sacarosa&lt;/a&gt; en &lt;a href=&#34;http://es.wikipedia.org/wiki/Fructosa&#34;&gt;fructosa&lt;/a&gt; y &lt;a href=&#34;http://es.wikipedia.org/wiki/Glucosa&#34;&gt;glucosa&lt;/a&gt;, y luego adhiere la molécula de glucosa en la cadena azucarada que de este modo va creciendo.&lt;/p&gt;
&lt;p&gt;Hasta ahora, la comunidad científica asumía que ambos procesos eran realizados por diferentes partes de la enzima. Sin embargo, el modelo creado por los investigadores de Groningen reveló que ambas actividades ocurren en el mismo sitio activo de la enzima. Dijkhuizen confía en que inhibidores específicos de la glucansucarasa puedan ayudar a prevenir que las bacterias se adhieran al esmalte dental, y los resultados obtenidos son cruciales para desarrollar estos inhibidores. No obstante, tal desarollo no ha sido exitoso, ya que los inhibidores de glucansucarasa también inhiben la enzima amilasa presente en la saliva, que es necesaria para degradar el almidón de los alimentos.&lt;/p&gt;
&lt;p&gt;El presente estudio ayuda a comprender por qué sucede esto, ya que es muy probable que las proteínas glucansucarasa hayan evolucionado a partir de la enzima amilasa. &amp;quot;Sabemos que las dos enzimas son similares&amp;quot;, dice Dijkhuizen, &amp;quot;ya que la estructura cristalográfica muestra que los sitios activos son virtualmente idénticos. Los futuros inhibidores necesitan ser dirigidos a blancos muy específicos debido a que ambas enzimas están muy cercanas evolutivamente&amp;quot;.&lt;/p&gt;
&lt;p&gt;Dijkhuizen menciona que en un futuro, los inhibidores de glucansucarasa pueden agregarse a la pasta dental y enjuages bucales. &amp;quot;Pueden ser incluso agregados a golosinas&amp;quot;, sugiere. &amp;quot;Un inhibidor puede prevenir que el azúcar liberada en la boca pueda causar daño&amp;quot;. Sin embargo, Dijkhuizen no espera que los cepillos dentales sean abandonados: &amp;quot;será siempre necesario que limpies tus dientes&amp;quot;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fuente:&lt;/strong&gt; &lt;a href=&#34;http://www.rug.nl/corporate/nieuws/archief/archief2010/persberichten/190_10&#34;&gt;Nota de prensa&lt;/a&gt; de la Universidad de Groningen.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; Remarkable fold of a 117 kDa glucansucrase fragment: Insights into evolution and product specificity of GH70 enzymes. Autores: Andreja Vujicić-Žagar, Tjaard Pijning, Slavko Kralj, Cesar A. López, Wieger Eeuwema, Lubbert Dijkhuizen y Bauke W. Dijkstra. PNAS, 30 de noviembre de 2010. El artículo se encuentra disponible en: &lt;a href=&#34;http://www.pnas.org/content/early/2010/11/24/1007531107&#34;&gt;www.pnas.org/cgi/doi/10.1073/pnas.1007531107&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Informe de la UBA sobre el INDEC</title>
      <link>https://ciencianet.com.ar/post/informe-de-la-uba-sobre-el-indec/</link>
      <pubDate>Sat, 04 Dec 2010 20:29:22 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/informe-de-la-uba-sobre-el-indec/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero&lt;/strong&gt;. Facultad de Ciencias Exactas (UNLP).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/12/indec-dyn.jpg&#34; alt=&#34;Manifestación en el INDEC - Fuente: DyN.:left&#34; title=&#34;indec-dyn&#34;&gt;&lt;/p&gt;
&lt;p&gt;El viernes 26 de noviembre de 2010 la Universidad dio a conocer el &amp;quot;INFORME TECNICO DE LA UNIVERSIDAD DE BUENOS AIRES (UBA) CON RELACION A LA SITUACION DEL INDEC&amp;quot; que se puede consultar en: &lt;a href=&#34;http://www.uba.ar/download/informe.pdf&#34;&gt;http://www.uba.ar/download/informe.pdf&lt;/a&gt; El informe es elocuente en sus conclusiones. Transcribimos algunos párrafos: &amp;quot;Finalmente, resulta relevante remarcar que la UBA ha estado muy lejos de avalar o convalidar la actual situación del INDEC, tanto en lo que hace a la confiabilidad de los datos y estimaciones que produce, como en cuanto a las cuestiones vinculadas a la estructura institucional y la administración del personal que allí trabaja. Asimismo, la UBA reafirma la necesidad de definir una propuesta de reestructuración del Instituto, a fin de que a futuro quede a salvo de cualquier sospecha de posible manipulación política, principal resguardo para que el INDEC pueda cumplir adecuadamente su misión de proveer estadísticas confiables para el uso de toda la sociedad.&amp;quot;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/12/noriega-indec.post_.jpeg&#34; alt=&#34;El libro &#39;El Indec&#39;, de Gustavo Noriega.:left&#34; title=&#34;noriega-indec.post&#34;&gt;&lt;/p&gt;
&lt;p&gt;&amp;quot;El examen realizado en este documento fundamenta la menguada confianza pública en dicho indicador. En este sentido, no es posible considerar al IPC elaborado por el INDEC como una medida fiable del nivel agregado de los precios del consumo, ni como un indicador adecuado para emplear como deflactor a efectos de estimar variables como el tipo real de cambio o el salario real o los niveles de pobreza. Los argumentos presentados por el INDEC, en tanto,no levantan las dudas sobre la calidad del índice, ni modifican la percepción acerca de la existencia de problemas en la estimación de la serie del IPC.&amp;quot; &amp;quot;En efecto, el descrédito sobre las estadísticas se ha dado paralelamente con un deterioro en el funcionamiento del Instituto y con la pérdida de valiosos recursos humanos debido a alejamientos y desplazamientos de carácter involuntario.&amp;quot;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>¿Democratizar la Ciencia? Diálogo, reflexividad y apertura</title>
      <link>https://ciencianet.com.ar/post/democratizar-la-ciencia-dialogo-reflexividad-y-apertura/</link>
      <pubDate>Mon, 29 Nov 2010 20:30:15 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/democratizar-la-ciencia-dialogo-reflexividad-y-apertura/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero&lt;/strong&gt;. Facultad de Ciencias Exactas (UNLP).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/11/asambleistas_creditoNA_0.post_.jpg&#34; alt=&#34;Asambleístas de Gualeguaychú - Crédito de la foto: NA.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;En agosto de 2010, la prensa argentina y uruguaya daba cuenta de noticias relacionadas con la el conflicto de Botnia. La Asamblea de Gualeguaychú había elevado a la Cancillería los nombres de dos científicos, que contaban con su aval para integrar el comité binacional de monitoreo de la cuenca del río Uruguay y de las instalaciones fabriles de ambas márgenes. Sin embargo, días después el gobierno nombró otros dos especialistas, desatando una polémica. Los asambleístas esperaban que al menos uno de los elegidos fuera propuesto por ellos, y que fuera además especialista en temas de celulosa. Criticaban además que el contrato de trabajo con los científicos de la Universidad de Buenos Aires que lograron la mayor parte de las pruebas de contaminación aérea de Botnia había cesado en mayo y no había sido renovado.&lt;/p&gt;
&lt;p&gt;Este sería un ejemplo de los procesos de inclusión de los ciudadanos en la toma de decisiones sobre asuntos científicos, que se conocen como ‘democratización de la ciencia’. Esta creciente participación en la política de ciencia y tecnología promovida por académicos, grupos de ciudadanos (como movimientos sociales) y algunos políticos se basa en una imagen de ciencia que la considera parcial, falible y en gran medida contextual.&lt;/p&gt;
&lt;p&gt;Consecuentemente, como sus aplicaciones pueden fallar o tener efectos indeseados o inesperados, se ha argumentado que para un mejor manejo de los riesgos, los ciudadanos deben ser incluidos en la toma decisiones técnicas para encontrar soluciones más eficientes y democráticas. Sin embargo, la democratización de la ciencia en sí misma es un proceso heterogéneo en el que están implicados distintos actores, motivaciones y valores y que se presta a múltiples miradas. En esta disputa entre grupos civiles e instituciones, lo que está en juego es la misma definición de ‘democratización’ y de quiénes deben ser los líderes de los nuevos procesos.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/11/tapa15_democratizacion_CN.post_.jpg&#34; alt=&#34;Revista CTS.:right&#34;&gt;&lt;/p&gt;
&lt;p&gt;El artículo de Delgado avanza sobre esta complejidad de factores y de análisis posibles. Para ello, comienza por definir los elementos considerados clave en los procesos de democratización, que esbozamos aquí:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Apertura: inclusión de una pluralidad de puntos de vista.&lt;/li&gt;
&lt;li&gt;Reflexividad: concepto central de la teoría social crítica. Se puede interpretar como conciencia crítica, conciencia de la situación/contexto o conciencia de las ideas propias y de las implicaciones prácticas de esas ideas.&lt;/li&gt;
&lt;li&gt;Diálogo: el diálogo directo aparece como la forma ideal de la relación entre expertos, ciudadanos y políticos.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Analiza luego tres enfoques teóricos. &lt;strong&gt;El enfoque optimista:&lt;/strong&gt; Ulrich Beck y Anthony Giddens En este enfoque los procesos de democratización se originan por un aumento en la conciencia de la crisis ambiental y tecnológica, representan un “progreso” de la ciencia. El dominio de la producción de conocimiento (la toma de decisiones sobre asuntos técnicos) está aún reservada a los expertos. &lt;strong&gt;El enfoque pragmático:&lt;/strong&gt; Helga Nowotny, Peter Scott y Michael Gibbons Este enfoque plantea que la democratización consiste en que la actividad científica sea guiada por las necesidades sociales. Así, las soluciones técnicas deberían ser generadas desde los contextos político-sociales particulares y, al mismo tiempo, deben estar orientadas a contextos de aplicabilidad concretos (“&lt;em&gt;goal-driven research&lt;/em&gt;”). En este enfoque se enfatiza el carácter potencialmente productivo de las relaciones ciencia-sociedad. &lt;strong&gt;El enfoque crítico y reflexivo:&lt;/strong&gt; Brian Wynne, Sheila Jasanoff y Alan Irwin El argumento básico en este enfoque es que el nuevo giro participativo en las políticas de ciencia y tecnología reproduce viejos patrones de relaciones de poder-saber.&lt;/p&gt;
&lt;p&gt;Las propuestas y ejercicios deliberativos organizados por las autoridades tienen una apariencia de ser inclusivos cuando, en realidad, no solucionan sino que exacerban el papel privilegiado de la ciencia que mantienen a los ciudadanos excluidos. Según este modelo, los ejercicios institucionales aparentemente democratizadores buscan recuperar la confianza de los ciudadanos en la ciencia y la tecnología, y establecer nuevas formas de legitimidad, pero sin cuestionar en profundidad las estructuras de poder que dan forma al conocimiento científico. Estos tres modelos comparten una misma idea base: diálogos plurales y abiertos entre ciencia y sociedad resultarán en procesos políticos más reflexivos. Sin embargo, según el enfoque, las prácticas deliberativas concretas serán interpretadas de formas muy diferentes.&lt;/p&gt;
&lt;p&gt;La autora describe y analiza en su artículo el “&lt;em&gt;GM Nation Debate&lt;/em&gt;”, un debate nacional sobre biotecnología organizado en 2003 por el gobierno del Reino Unido para evaluar la seguridad y el impacto de los organismos genéticamente modificados. A modo de conclusión la autora destaca que cada punto de vista lleva a una opción metodológica que se relaciona con una serie de paradojas y dilemas. Se pone de manifiesto que, en la práctica, ideales como la apertura, el diálogo, la pluralidad o la transparencia no proporcionan “prescripciones definitivas” sobre cómo conducir acciones participativas, sino que pueden funcionar como principios regulativos que orientan las prácticas, sin estar rígidamente fijados de antemano. En este sentido, se puede cuestionar hasta qué punto la reflexividad debe ser impuesta como un criterio fundamental para evaluar la calidad de los procesos de democratización de la ciencia (como se ha hecho comúnmente desde los estudios sociales de la ciencia y la tecnología) o si es necesario crear nuevos criterios de calidad más inclusivos, que estén sujetos a negociación, siendo el producto del diálogo y de la experiencia de las prácticas deliberativas concretas.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Artículo original:&lt;/strong&gt; &lt;em&gt;¿Democratizar la Ciencia? Diálogo, reflexividad y apertura&lt;/em&gt; CTS Revista Iberoamericana de Ciencia Tecnología y Sociedad VOLUMEN 5 número 15 - Septiembre de 2010 - &lt;a href=&#34;https://www.revistacts.net&#34;&gt;www.revistacts.net&lt;/a&gt; Dra. Ana Delgado, Centre for the Study of the Sciences and the Humanities de la Universidad de Bergen, Noruega.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Correo electrónico:&lt;/strong&gt; &lt;a href=&#34;mailto:Ana.Delgado@svt.uib.no&#34;&gt;Ana.Delgado@svt.uib.no&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Supernova SN 1979C: ¿El agujero negro más joven?</title>
      <link>https://ciencianet.com.ar/post/supernova-sn-1979c-el-agujero-negro-mas-joven/</link>
      <pubDate>Tue, 23 Nov 2010 20:32:59 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/supernova-sn-1979c-el-agujero-negro-mas-joven/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Cecilia Garraffo&lt;/strong&gt;, Harvard-Smithsonian Center for Astrophysics y &lt;strong&gt;Gastón Giribet&lt;/strong&gt;, Universidad de Buenos Aires - Conicet.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/11/lanzamiento_chandra_0.jpg&#34; alt=&#34;Lanzamiento del Chandra. El Observatorio Espacial fue lanzado en Agosto de 1999 por el transbordador Columbia. Crédito: NASA y el Centro de Ciencias del Chandra.:left&#34; title=&#34;lanzamiento_chandra_0&#34;&gt; Según entendemos hoy la astrofísica de los objetos compactos, los agujeros negros [&lt;a href=&#34;#1&#34;&gt;1&lt;/a&gt;] astrofísicos (aquéllos cuyas masas superan a la masa de nuestro sol en unos pocos factores) tienen su origen en algunas de las llamadas supernovas. Las supernovas son las explosiones de algunas estrellas al final de sus vidas. En muchos casos, estas explosiones dejan como detrito una estrella que aún tiene suficiente masa como para colapsar debido a la fuerza de su propia gravedad y convertirse finalmente en un agujero negro.&lt;/p&gt;
&lt;p&gt;El recientemente anunciado descubrimiento de lo que, según se cree, sería el agujero negro más jóven detectado en nuestro universo cercano se relaciona con este tipo de “nacimiento”. El observatorio de rayos X Chandra de la Nasa descubrió recientemente evidencia de que un agujero negro en la galaxia M100, ubicada a 50 millones de años luz de nuestro sistema solar, habría nacido de la explosión de supernova SN 1979C, observada hace 31 años. Si este descubrimiento se confirmara, estaríamos frente al agujero negro negro más jóven que hayamos observado en nuestro universo cercano.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/11/chandra_01.jpg&#34; alt=&#34;Imagen de SN 1979C: Imagen compuesta que muestra una supernova en la Galaxia M100. Crédito: X-ray: NASA/CXC/SAO/D.Patnaude et al, Optical: ESO/VLT, Infrared: NASA/JPL/Caltech.:left&#34; title=&#34;chandra_0&#34;&gt; Este es un hallazgo de gran importancia ya que nos permitiría observar un agujero negro y el comportamiento de la materia en sus inmediaciones desde sus primeros años de vida. Por otro lado, ésta puede considerarse la primera observación directa de un agujero negro en el centro de una supernova, por lo que representa una pieza fundamental para comprobar nuestras teorías acerca de la formación de objetos compactos en el cosmos.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Artículo original:&lt;/strong&gt; &lt;a href=&#34;http://www.cfa.harvard.edu/news/2010/pr201023.html&#34;&gt;http://www.cfa.harvard.edu/news/2010/pr201023.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referencia:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;1&#34;&gt; 1. &lt;/a&gt; ¿Qué es un agujero negro? Alrededor de 1930 se probó matemáticamente que si la masa de una estrella supera dos veces y media la de nuestro Sol, la presión del gas que la forma no puede equilibrar el efecto de la gravedad. Comienza entonces un proceso de contracción que la convierte en un objeto super denso, con un campo gravitatorio muy intenso. Ninguna radiación o partícula puede escapar del campo de una estrella colapsada, pues necesitaría superar la velocidad de la luz en el vacío para hacerlo. Como la luz que incide sobre estas estrellas colapsadas no es reflejada se les dio el nombre de &amp;quot;agujeros negros&amp;quot;.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Estabilidad como un mecanismo de selección natural en redes interactuantes</title>
      <link>https://ciencianet.com.ar/post/estabilidad-como-un-mecanismo-de-seleccion-natural-en-redes-interactuantes/</link>
      <pubDate>Tue, 26 Oct 2010 20:34:18 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/estabilidad-como-un-mecanismo-de-seleccion-natural-en-redes-interactuantes/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Pablo Gleiser&lt;/strong&gt;. Centro Atómico Bariloche.&lt;/p&gt;
&lt;p&gt;Una gran variedad de sistemas en la naturaleza pueden describirse como redes, donde las unidades del sistema están representadas por nodos, y la interacción entre estas unidades está representada por conexiones entre los nodos. Así, podemos pensar al cerebro como una red de neuronas, a una organización social como una red de personas, o a Internet como una red de computadoras.&lt;/p&gt;
&lt;p&gt;Muchas de estas redes poseen propiedades específicas que no se observan en sistemas aleatorios donde los nodos son conectados al azar. Estas redes se denominan redes complejas. Sorprendentemente, se ha observado que muchas propiedades cuantitativas de las redes complejas son compartidas por sistemas muy diversos, incluyendo sistemas sociales, biológicos y tecnológicos. Esto sugiere que existen mecanismos muy básicos que gobiernan la formación y evolución de estas redes.&lt;/p&gt;
&lt;p&gt;Descubrir cuáles son estos mecanismos genéricos es un desafío para la ciencia actual. Existen diferentes formas en las que la física intenta modelar estos sistemas para comprender su funcionamiento. Una forma consiste en tener en cuenta la mayor cantidad de elementos presentes en el sistema. Esto lleva a la formulación de modelos con un gran número de parámetros que pueden ser ajustados mediante datos experimentales, y dan una descripción realista de un sistema específico.&lt;/p&gt;
&lt;p&gt;En el otro extremo, un enfoque posible es el de considerar un modelo abstracto con un número mínimo de parámetros. Con este enfoque es posible obtener resultados muy generales, que permiten describir cuáles son los ingredientes fundamentales necesarios para que se pueda observar un fenómeno dado. Éste es precisamente el enfoque que Juan Perotti, Orlando Billoni, Francisco Tamarit y Sergio Cannas, de la Facultad de Matemática, Astronomía y Física de la Universidad Nacional de Córdoba, han seguido en el artículo titulado &amp;quot;Estabilidad como un mecanismo de selección natural en redes interactuantes&amp;quot; que ha sido publicado recientemente en la revista &amp;quot;Papers in Physics&amp;quot;.&lt;/p&gt;
&lt;p&gt;En este trabajo ellos proponen un mecanismo para la formación de redes complejas basado en un algoritmo con dos ingredientes fundamentales que se encuentran acoplados entre sí: crecimiento y estabilidad de la red. En el modelo, los nodos de la red tienen estados dinámicos que cambian con el tiempo. A través de las interacciones de la red los nodos llegan a un estado estable. Esto es, un estado en el que una pequeña perturbación no altera el estado del sistema. Una alteración posible es la incorporación de un nuevo nodo a la red, que trae como consecuencia la presencia de nuevas conexiones que modifican el estado de los otros nodos en la red y pueden desestabilizar al sistema.&lt;/p&gt;
&lt;p&gt;Por ejemplo, en un sistema biológico podría pensarse en un ecosistema donde una nueva especie se hace presente. El efecto de esta nueva especie podría ser sólo el incremento del número de especies, o ésta podría alterar al ecosistema llevando a la extinción de muchas especies. Estos ingredientes son tenidos en cuenta en el modelo, por lo que la incorporación de un nuevo nodo sólo se acepta si se dan las condiciones matemáticas para que el sistema permanezca estable.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/10/estabilidad-red.post_.jpg&#34; alt=&#34;:left&#34; title=&#34;estabilidad-red.post&#34;&gt;&lt;/p&gt;
&lt;p&gt;Los estudios realizados sobre el modelo muestran que las reglas que tienen en cuenta el crecimiento y la estabilidad de la red establecen una limitación sobre las posibles estructuras de red que se pueden obtener. Los autores analizaron las redes obtenidas con el modelo y observaron características presentes en redes reales, tales como redes biológicas de células. Por ejemplo, las redes obtenidas con el modelo presentan un grán número de nodos con pocas conexiones y unos pocos nodos con un gran número de conexiones. Los nodos con pocas conexiones forman pequeños módulos, donde los nodos están muy conectados entre sí, y cumplen un rol funcional de estabilizar la red. Por otro lado los nodos con un gran número de conexiones establecen un nexo entre los módulos, lo que permite reducir la distancia entre nodos en diferentes módulos.&lt;/p&gt;
&lt;p&gt;La generalidad del modelo también permite hacer un análisis de los resultados obtenidos en el contexto de redes ecológicas. Los autores señalan que la incorporación de ingredientes específicos tales como el envejecimiento de los nodos y una capacidad limitada de establecer conexiones con otro nodos permitirá extender el alcance del modelo a nuevos sistemas tales como las redes tróficas.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo Original:&lt;/strong&gt; Juan I. Perotti, Orlando V. Billoni, Francisco A. Tamarit, Sergio A. Cannas, &lt;a href=&#34;https://doi.org/10.4279/pip.020005&#34;&gt;Stability as a natural selection mechanism on interacting networks&lt;/a&gt;. Papers in Physics 2, 020005 (2010).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; S. A. Cannas (Email: &lt;a href=&#34;mailto:cannas@famaf.unc.edu.ar&#34;&gt;cannas@famaf.unc.edu.ar&lt;/a&gt; )&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Modelo matemático para la propagación de la tos convulsa en Argentina</title>
      <link>https://ciencianet.com.ar/post/modelo-matematico-para-la-propagacion-de-la-tos-convulsa-en-argentina/</link>
      <pubDate>Mon, 25 Oct 2010 20:37:52 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/modelo-matematico-para-la-propagacion-de-la-tos-convulsa-en-argentina/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero&lt;/strong&gt;. Facultad de Ciencias Exactas (UNLP).&lt;/p&gt;
&lt;p&gt;La tos convulsa, también llamada coqueluche o pertussis, es una enfermedad infecciosa respiratoria inmunoprevenible causada por la bacteria Bordetella pertussis. Se caracteriza por episodios muy intensos de tos que pueden prolongarse por varias semanas, y en el caso de niños pequeños puede causar complicaciones y hasta la muerte. En los adultos los síntomas suelen ser más leves. La enfermedad se transmite de persona a persona a través de gotitas de aerosol dispersadas por la tos de un infectado. La tasa de comunicabilidad es alta: pueden contagiarse entre el 80 y 90 % de las personas susceptibles en contacto con un individuo infectado.&lt;/p&gt;
&lt;p&gt;Antes de la existencia de vacunas específicas, era la principal causa de muerte en niños. El desarrollo y la inclusión de vacunas en los calendarios nacionales de vacunación redujeron de manera significativa la morbi mortalidad de la enferemdad. En nuestro país el esquema consiste en tres dosis que se administran a los 2, 4 y 6 años de vida, más dos refuerzos que se dan a los 18 meses y al ingreso escolar. Desde el año 2009 se incluye un refuerzo más a los 11 años de edad.&lt;/p&gt;
&lt;p&gt;Pese a años de utilización de vacunas, la enfermedad no se ha podido erradicar. Más aún, en las últimas dos décadas se ha reportado en varios países un notable aumento en el número de enfermos registrados por año. Mundialmente se estima que hay por año 40 millones de casos, de los cuales 300.000 son fatales, con un 70% de incidencia en menores de 5 años, un 90% en menores de 1 año de los cuales 75% son menores de 6 meses. El 50% de los afectados necesita internación. En nuestro país el número de casos reportados, según la Organización Mundial de la Salud, pasó de 267 en el año 2002 a 1742 en el 2009. Además del aumento de casos se ha registrado un cambio en la proporción de casos según la edad de los pacientes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/10/Graph1-s.jpg&#34; alt=&#34;:left&#34; title=&#34;Graph1-s&#34;&gt;&lt;/p&gt;
&lt;p&gt;Para explicar el resurgimiento de la tos convulsa se han propuesto varias hipótesis, entre las cuales se cuentan: coberturas de vacunación por debajo de las óptimas; baja efectividad de la vacuna (se estima alrededor del 70% de efectividad luego de la 3ra dosis); corta duración de la inmunidad conferida por vacunación; presencia de nuevas variantes de B. pertussis contra las que las vacunas en uso podrían no ser tan eficaces y cuadros clínicos atípicos sin diagnóstico o con diagnóstico tardío. La importancia de cada uno de estos factores no es igual en todos los países por lo que no se ha podido arribar a una conclusión que permita sentar las bases para mejorar estrategias de control contra la enfermedad.&lt;/p&gt;
&lt;p&gt;En este marco, los modelos matemáticos predictivos surgen como una herramienta importante que puede contribuir a la toma de decisiones en el área de la salud pública. Si bien se han desarrollado modelos para esta patología en otros países, los mismos no contemplan nuestras particularidades epidemiológicas locales. En esta dirección se encamina el trabajo encabezado por el físico Gabriel Fabricius –especializado en mecánica estadística y física computacional- y la bioquímica Daniela Hozbor, quien dirige un grupo de investigación que aborda distintos aspectos de la epidemiología de la enfermedad y del desarrollo de vacunas. También han participado de este trabajo el bioquímico Aníbal Lodeiro, el físico Augusto Melgarejo y el matemático Alberto Maltz.&lt;/p&gt;
&lt;h3 id=&#34;el-modelo&#34;&gt;El modelo&lt;/h3&gt;
&lt;p&gt;El modelo matemático diseñado por el grupo plantea que la población está dividida en tres grupos de individuos: los susceptibles de adquirir la infección, los que están infectados y los que poseen algún grado de inmunidad. Este último grupo se subdivide, a su vez, según sea el grado de inmunidad de los individuos: adquirido naturalmente (por haber estado infectados) o adquirido artificialmente (por haber recibido distintas dosis de vacuna). Como la tos convulsa reviste distinta gravedad según la edad, y como el contagio es mayor en ciertas etapas de la vida -como ocurre en el período escolar- cada uno de los grupos de individuos es a su vez dividido en clases etarias.&lt;/p&gt;
&lt;p&gt;Por otra parte, tener los resultados discriminados por edades permite saber por ejemplo qué fracción de los infectados corresponden a las clases etarias más bajas (menores de un año), los cuales son los que pueden perecer a causa de la enfermedad. La transferencia de individuos entre los distintos grupos poblacionales es descripta por un conjunto de ecuaciones diferenciales no lineales acopladas que se resuelven numéricamente.&lt;/p&gt;
&lt;p&gt;Al modelo se incorporan los datos reales: a partir de información epidemiológica, poblacional y clínica deben establecerse algunos parámetros como las tasas de contagio entre individuos susceptibles e infectados de distintas clases etarias, el tiempo que dura la infección, la velocidad a la que se pierde la inmunidad, las tasas de nacimiento y muerte.&lt;/p&gt;
&lt;h3 id=&#34;algunos-resultados&#34;&gt;Algunos resultados&lt;/h3&gt;
&lt;p&gt;El modelo descrito permite estudiar el efecto de un retraso en las dosis de vacunación que es simulado alterando las coberturas de las distintas dosis vacunales y observando los nuevos valores obtenidos para la incidencia de la pertussis en los distintos grupos etarios. El objetivo de este trabajo fue evaluar si es posible establecer alguna conexión entre el aumento registrado en casos de pertussis a partir de 2002 en Argentina con la ausencia/retraso de algunas dosis de vacuna registradas en la población. Los resultados indican que un retraso en la primera dosis produce un aumento significativo en la incidencia global de la enfermedad y en particular para los niños de hasta 1 año de edad. Los retrasos en las siguientes dosis también producen efectos significativos, pero siempre menores que el producido por un retraso en la primera dosis.&lt;/p&gt;
&lt;p&gt;Estos resultados enfatizan la importancia de aplicar a tiempo la primera dosis vacunal y muestran que este tipo de estudios pueden resultar muy útiles a la hora de evaluar estrategias y definir políticas de prevención y control de pertussis en nuestro país.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original.&lt;/strong&gt; &lt;em&gt;Fabricius, G., Lodeiro, A., Melgarejo,A., Graieb, A. y Hozbor,D.A. Deterministic epidemiology model to study pertussis in Argentina. 27th Annual Meeting of the European Society for Paediatric Infectious Disease, Brussels, Belgium, June 9-13, 2009.&lt;/em&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>¿La dimensión, importa?</title>
      <link>https://ciencianet.com.ar/post/la-dimension-importa/</link>
      <pubDate>Thu, 21 Oct 2010 20:36:18 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/la-dimension-importa/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Luis A. Pugnaloni.&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET La Plata, UNLP).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/10/dimensionalidad-768x328.jpg&#34; alt=&#34;Foto grupal de los participantes al evento.&#34; title=&#34;dimensionalidad.preview&#34;&gt;&lt;/p&gt;
&lt;p&gt;Hace poco participé de un taller (&lt;em&gt;workshop&lt;/em&gt;) sobre coloides y granulares. El evento se llamó &amp;quot;&lt;a href=&#34;http://www.mpipks-dresden.mpg.de/~pardim10/&#34;&gt;Particulate matter: Does dimensionality matter?&lt;/a&gt;&amp;quot;. Cuya significación es: Es importante la dimensión en sistemas formados por partículas? Aquí &amp;quot;partículas&amp;quot; se refiere a objetos visibles, al menos en un microscopio convencional.&lt;/p&gt;
&lt;p&gt;Participaron colegas de Europa (la mayoría), América (casi todos del norte) y Asia (muy pocos). Durante el evento se propuso la pregunta, usada como título del encuentro, multiples veces. Diferentes especialistas ofecían diferentes respuestas. La semana de charlas y exposiciones terminó y no se llegó a conclusión alguna: unos insistían en que la dimensión no es importante y otros en que si.&lt;/p&gt;
&lt;p&gt;En esta nota intento explicar las razones de lo inútil que es responder preguntas como estas desde un aspecto técnico. Usaré este evento científico como &amp;quot;mal&amp;quot; ejemplo. Pero al final comentaremos la verdadera función de preguntas como estas en contextos como el taller mencionado.&lt;/p&gt;
&lt;p&gt;Una pregunta del estilo: ¿La dimensión, importa? es tan vaga que cada interlocutor puede interpretar de forma diferente la consigna. Quiero destacar que esta pregunta usa dos palabras cuyo significado es extremadamente ambiguo.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dimensión.&lt;/strong&gt; Esta palabra parece ser algo muy bien definido. Sin embargo en esta conferencia había concepciones diferentes. Lamentablemente, se discutía sin darse cuenta de que en ocasiones se referían a cosas distintas. Para algunos (en especial para los experimentalistas) la dimensión de un sistema se puede controlar confinando las partículas en espacios planos o lineales. Por ejemplo, unas cuantas bolitas entre dos placas de vidrio bien cercanas constituiría un sistema en dos dimensiones (2D).&lt;/p&gt;
&lt;p&gt;Así, es posible cambiar lentamente a un sistema tridimansional (3D) alejando poco a poco las placas de vidrio una de otra. Para otros (los más teóricos) la dimensión podía cambiar de uno a infinito pero sólo de uno en uno. Claramente unos piensan en &amp;quot;dimensión&amp;quot; como sinónimo de &amp;quot;confinamiento&amp;quot;. Esto es, ¿en cuántas direcciones dejo que las partículas comunes y tridimensionales de nuestro mundo se muevan?. Otros pensaban en partículas que eran de otro mundo, bidimensionales, que ni podían imaginar la tercera dimensión, o n-dimensionales incluso.&lt;/p&gt;
&lt;p&gt;Quiero aclarar que es posible describir un mundo estrictamente 2D. Pero en ese caso, las fuerzas de interacción entre objetos en ese mundo deben ser diferentes al mundo 3D. La fuerza electrostática de Coulomb, por ejemplo, en un mundo estrictamente unidimensional (1D) no es inversamente proporcional al cuadrado de la distancia entre las cargas eléctricas sino independiente de la distancia. Mucho más complejo sería conocer las complicadas fuerzas de fricción entre objetos macroscópicos en un mundo extrictamente 1D.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Importa&lt;/strong&gt; Lo que a mi me importa quizá no te importa. Preguntar si algo importa en una conferencia como esta implica que queremos saber la opinión subjetiva de los participantes, no se puede esperar una respuesta factual. La importancia de las cosas es de caracter relativo y subjetivo. Así, escuché científicos (Frederic Lechenault) decir que sus experimentos eran más simples en 2D (confinados a un plano, para hablar con propiedad) y por eso es importante la dimensión; porque complica o simplifica los experimentos. Lo mismo puede decir un teórico porque sus cálculos son más simples en unas dimensiones que en otras.&lt;/p&gt;
&lt;p&gt;Otros (Corey O&#39;Hern), pensando de un modo más general, decían que sus resultados no cambiaban en forma &amp;quot;importante&amp;quot; con la dimensión y por lo tanto ésta no importaba. Este último argumento es más fuerte pero merece ser atacado. ¿Qué significa que un resultado cambia en forma &amp;quot;importante&amp;quot;? La respuesta puede ser variada.&lt;/p&gt;
&lt;p&gt;Doy como ejemplo tres visiones estereotipadas posibles según un ingeniero, un físico y un matematico (y no intento contar un chiste). Ingeniero: Si la propiedad A como función del parámetro B cambia en un factor 2 al pasar de 2D a 3D entonces la dimensión importa. El valor cuantitativo de una propiedad es importante. Físico: Si la propiedad A como función del parámetro B cambia al menos su forma funcional al pasar de 2D a 3D entonces la dimensión importa. La forma funcional es valorada por encima de los detalles cuantitativos. Matemático: Si la propiedad A como función del parámetro B cambia al menos su topología al pasar de 2D a 3D entonces la dimensión importa. No deslumbra un cambio de forma funcional, hace falta la aparición de discontinuidades para que resulte importante.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lo que importa a los que importan&lt;/strong&gt; Como es de esperar, no se puede responder en forma definitiva la pregunta propuesta en la conferencia de ejemplo. Pero si se puede aprovechar para escuchar opiniones y aprender qué le importa a los importantes. Estos pueden decidir donde poner dinero y donde publicar resultados. Varios científicos influyentes estaban presentes. No pude intuir que alguno de estos se dieran cuenta de lo inapropiada de la pregunta dado su afán por responderla según su leal entender.&lt;/p&gt;
&lt;p&gt;Queda claro que sus respuestas guían a los menos influyentes sobre que tipo de investigaciones debieran seguir en el futuro si desean recibir la aprobación de aquellos otros. Quizá ese era el objetivo de los organizadores.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Siempre se aprenden cosas interesantes&lt;/strong&gt;. Más allá de esto, siempre se aprenden cosas en las reuniones científicas. Cuento aquí un par de cosas que escuche (no por vez primera), que considero suficientemente generales para que le interese a cualquiera.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Teorías locas:&lt;/strong&gt; Un matemático (Henry Cohn) dictó una conferencia sobre cuál es la máxima densidad con que se pueden acomodar esferas en cualquier dimensión (especialmente de 4D a infinitoD). Comentó sobre las peculiaridades de la 8D que la hacen especialmente interesante (para él). Cómo Henry parece estar acostumbrado a que le digan que eso de n-dimensiones es una locura matemática sin utilidad, empezó por explicar que un famoso matemático en la década de 1940 explicó que las señales que intentaban mandar en una línea de transmisión de información (radio, teléfono, etc) estaban limitadas por el número de &amp;quot;esferas de ruido&amp;quot; en el espacio multidimensional de los parámetros que controlan las propiedades de la señal que podían acomodarse. Esto es una aplicación muy concreta del problema de acomodar esferas en n-dimensiones para la industria de las telecomunicaciones. Aún así, me pregunto que tan impostante es que sean hiperesferas (nombre para referirse a esferas matemáticas en dimensiones mayores a 3) en lugar de hipercubos (cubos matemáticos en n-dimensiones) cuyo ordenamiento intuyo es tan fácil como el de cubos convencionales (3D), cuadrados (2D) y segmentos (1D).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;¿Para qué investigar en 1D y en infinitoD?&lt;/strong&gt; Un prestigioso científico de la audiencia (Bob Behringer), preguntó qué sentido tenían estudios teóricos sobre 1D o infinitoD que nunca se podrán llevar a la realidad. Atinadamente uno de los organizadores (Patrick Charbonneau) le aclaró que esas teorías admiten soluciones matemáticas exactas y que pueden servir como modelos de prueba para simulaciones numéricas. Dado un algoritmo numérico para modelado de sistemas 3D, se lo puede probar en 1D e infinitoD para ver si cumplen con los teoremas demostrados por los matemáticos. Si lo hacen uno gana en confianza sobre lo que estos cálculos numéricos podrían decirnos en casos 3D.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Reseña de la IV Reunión Binacional de Ecología</title>
      <link>https://ciencianet.com.ar/post/resena-de-la-iv-reunion-binacional-de-ecologia/</link>
      <pubDate>Wed, 29 Sep 2010 20:39:43 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/resena-de-la-iv-reunion-binacional-de-ecologia/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Irina Izaguirre.&lt;/strong&gt; Departamento de Ecología, Genética y Evolución (FCEN, UBA).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/09/eco1.jpg&#34; alt=&#34;:left&#34; title=&#34;eco2&#34;&gt; Entre los días 8 y 13 de agosto de 2010 se llevó a cabo en Facultad de Ciencias Exactas y Naturales de la Universidad de Buenos Aires la IV Reunión Binacional de Ecología (Argentina – Chile), una reunión científica que es organizada cada tres años por la Asociación Argentina de Ecología (AsAE) y la Sociedad de Ecología de Chile (SOCECOL). Este evento también constituyó la XXIV Reunión Argentina de Ecología y la XVII Reunión de la Sociedad de Ecología de Chile, y contó con el auspicio de diversas instituciones y fundaciones de ambos países. En Argentina las reuniones de ecología tuvieron su inicio en 1972 en Vaquerías, Córdoba, y a lo largo de más de tres décadas se organizaron en distintas ciudades del país, contribuyendo a la difusión del conocimiento ecológico y a la interacción entre los ecólogos argentinos. La IV Reunión Binacional de Ecología fue planteada bajo el lema &lt;strong&gt;&amp;quot;Interacción, Espacio, Tiempo&amp;quot;&lt;/strong&gt; con los siguientes objetivos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;establecer un foro de discusión sobre los avances conceptuales y empíricos, locales y universales, de la disciplina ecológica&lt;/li&gt;
&lt;li&gt;desarrollar conferencias y simposios sobre tópicos ecológicos actuales&lt;/li&gt;
&lt;li&gt;promover un ámbito de encuentro de ecólogos con intereses comunes de investigación, facilitando su participación en proyectos conjuntos&lt;/li&gt;
&lt;li&gt;promover la participación activa de estudiantes de grado y posgrado, así como su interacción con investigadores y profesionales de reconocida trayectoria.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;La organización de la IV Reunión Binacional en la Facultad de Ciencias Exactas y Naturales resultó un desafío para la institución, pues asistieron alrededor de 1000 participantes, entre ellos unos 300 de Chile, y también numerosos de Brasil y de otros países latinoamericanos. Dado que las charlas fueron abiertas, también pudieron asistir a ellas libremente todos los integrantes de la comunidad universitaria (docentes, alumnos e investigadores) Se contó con la colaboración de más de 60 estudiantes de la facultad en tareas organizativas.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/09/eco11.jpg&#34; alt=&#34;:left&#34; title=&#34;eco1&#34;&gt; Se dictaron 11 conferencias plenarias a cargo de investigadores argentinos y extranjeros de primer nivel. También se organizaron 10 simposios sobre temas de actualidad y 5 mesas redondas. Los tópicos abordados que cubrieron un amplio abanico de temáticas ecológicas, mencionando a modo de ejemplos la ecología de invasiones biológicas, la desertificación, el manejo de ambientes por pueblos originarios, la ecología de poblaciones y problemáticas de los agroecosistemas.&lt;/p&gt;
&lt;p&gt;La reunión contó con la participación de alumnos de grado y de postgrado en diversas actividades. Se dictaron 4 cursos de postgrado a los que asistieron en total alrededor de 120 alumnos. Por otro lado, se entregaron premios a las mejores presentaciones de murales de alumnos de grado y de doctorado. La reunión fue el encuentro científico sobre ecología más importante de Sudamérica, resultando inspirador de una posible reunión trinacional a realizarse en el futuro, integrando también a una de las asociaciones de ecología de Brasil. Para más información y detalles sobre este encuentro, pueden consultar la página web de la reunión: &lt;a href=&#34;http://www.ege.fcen.uba.ar/rbe2010/&#34;&gt;http://www.ege.fcen.uba.ar/rbe2010/&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Escuela Internacional de Gravedad Cuántica</title>
      <link>https://ciencianet.com.ar/post/escuela-internacional-de-gravedad-cuantica/</link>
      <pubDate>Sat, 04 Sep 2010 20:42:43 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/escuela-internacional-de-gravedad-cuantica/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Nicolás Grandi.&lt;/strong&gt; Instituto de Física La Plata.&lt;/p&gt;
&lt;p&gt;Entre el 19 y el 27 de julio de 2010, en el Departamento de Física de la Universidad Nacional de La Plata, tuvo lugar la &lt;em&gt;International School in Quantum Gravity,&lt;/em&gt; que reunió a un centenar de asistentes entre estudiantes e investigadores, provenientes de prestigiosos institutos de más de diez países. La escuela consistió en una serie de &lt;em&gt;lectures&lt;/em&gt; concentradas en uno de los problemas abiertos más intrigantes de la física teórica contemporánea: el comportamiento cuántico de la gravedad. Se abordaron temas relacionados con la teoría de cuerdas, la gravedad cuántica de lazos, y la conjetura de Maldacena, entre otros.&lt;/p&gt;
&lt;p&gt;El objetivo de la escuela fue poner al alcance de los estudiantes de doctorado argentinos y latinoamericanos los avances más recientes en estas áreas, de modo de facilitar el desarrollo de investigación de frontera en nuestros países. Las exposiciones estuvieron a cargo de científicos de primer nivel internacional, entre los que se contaron Joseph Polchinski (Santa Barbara, USA), Martín Kruczenski (Purdue University, USA), Carlos Núñez (Swansea University, UK), Eva Silverstein (Santa Barbara y Stanford, USA), George Thompson (Abdus Salam ICTP, Italia), Sean Hartnoll (Harvard, USA), Rodolfo Gambini (UdelaR, Uruguay), Jorge Russo (ICREA Barcelona, España), Nathan Berkovits (IFT, Brasil) y Jorge Zanelli (CECS, Chile).&lt;/p&gt;
&lt;p&gt;La escuela estuvo dedicada a la memoria de Carlos G. Bollini, uno de los científicos más destacados que dio nuestro país, quien en colaboración con J.J. Giambiagi publicó el artículo más citado de la física argentina. En dicho trabajo se propuso un forma novedosa de controlar los infinitos que parecían plagar la teoría cuántica de campos, la llamada “regularización dimensional”. Una propuesta similar fué aplicada por los físicos holandeces G. &#39;t Hooft y M. Veltman a la teoría de las interacciones fuertes, por lo que recibieron el premio Nobel de Física en 1999. Durante la escuela se realizó un emotivo homenaje a Bollini, en el marco del cual lo recordaron sus colegas y discipulos Daniel Bes, Hector Vucetich y Carlos García Canal, y su viuda Susana Lévy de Bollini.&lt;/p&gt;
&lt;p&gt;La escuela fue organizada por la red argentina Strings@ar, que nuclea a más de cincuenta investigadores argentinos que trabajan en institutos del país y del exterior en temas de teoría de cuerdas, teoría de campos, cosmología y gravitación. Fue financiada parcialmente con aportes del &lt;em&gt;Abdus Salam International Centre for Theoretical Physics&lt;/em&gt; dependiente de la UNESCO, del Centro Latinoamericano de Física, del Instituto de Física de La Plata, del Instituto Balseiro, del programa R@aices dependiente del Ministerio de Ciencia y Técnica, y de la empresa Micro Automación. Al finalizar la escuela, entre el 28 y el 30 de julio, tuvo lugar en la Biblioteca Nacional la conferencia &lt;em&gt;Quantum Gravity in the Southern Cone V&lt;/em&gt;, donde se discutieron los avances más recientes en el área.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Los científicos ante las pseudociencias</title>
      <link>https://ciencianet.com.ar/post/los-cientificos-ante-las-pseudociencias/</link>
      <pubDate>Tue, 31 Aug 2010 20:43:45 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/los-cientificos-ante-las-pseudociencias/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Celso M. Aldao.&lt;/strong&gt; Universidad Nacional de Mar del Plata - CONICET.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/08/aldao-foto.post_.jpeg&#34; alt=&#34;Celso Aldao:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Es realmente asombroso que en pleno siglo XXI, cuando el desarrollo científico y tecnológico alcanza niveles que no dejan de sorprendernos, las pseudociencias crezcan en popularidad. Aunque varias causas pueden ser responsables, los medios masivos de comunicación parecen haber desempeñado un rol protagónico.&lt;/p&gt;
&lt;p&gt;La pseudociencia abunda también porque hace afirmaciones extravagantes que excitan la imaginación y es mucho más fácil de aprender y practicar que la ciencia. Desafortunadamente, muchos científicos consideran inocuas las pseudociencias e inclusive adecuadas para las masas. Por mi parte, creo que tenemos el deber de presentar el punto de vista de la razón, ya que la proliferación de creencias infundadas se ha convertido en un gran negocio en el que se explota la credulidad pública, al mismo tiempo que en muchos casos se pone en peligro la salud de la población.&lt;/p&gt;
&lt;p&gt;Por otro lado, las pseudociencias alientan una actitud de descrédito hacia la ciencia, actitud que se extiende a la sociedad toda. Sin duda, el mejor lugar para el debate es la escuela y la universidad, donde estos temas deberían ser presentados y discutidos regularmente. La imagen pública de la ciencia comenzó a caer a fines de la década del sesenta. Y si bien se reconocen los éxitos tecnológicos que de ella derivan, muchos la ven hoy como la causa de los problemas que nos aquejan. No son pocos los que afirman que la cultura &amp;quot;oficial&amp;quot; es superficial o incompleta, que la ciencia es responsable de los armamentos modernos y de la degradación ambiental, actitud que se extiende a la propia academia. Se sostiene que no hay verdad objetiva, que las teorías científicas son modas, que no hay criterios definidos para la aceptación de una u otra teoría; en síntesis, que no hay diferencia entre ciencia y pseudociencia. Y los científicos somos percibidos por el público como cerrados, negativos, inflexibles, fríos, y faltos de imaginación.&lt;/p&gt;
&lt;p&gt;Creo entonces que no sólo es necesario presentar el punto de vista científico, sino demostrar los beneficios de la actitud científica y la idea de que los científicos no son los aguafiestas. Los beneficios que surgen de no ser embaucados por charlatanes son fácilmente comprendidos, pero también se deben destacar los beneficios de llegar a una percepción de la realidad más efectiva, más sutil, más compleja, y más rica que la ciencia conlleva. Se necesita que se informe al público sobre un número enorme de extrañas afirmaciones sobre las que muchos desean conocer una opinión responsable.&lt;/p&gt;
&lt;p&gt;Creo que los científicos y otros expertos bien informados deberían involucrarse en este aspecto de la educación pública. Al tratar con los defensores de las pseudociencias, uno encuentra actitudes destructivas, reglas oscuras, ataques personales, digresiones interminables totalmente fuera de contexto y, fundamentalmente, una ignorancia e irracionalidad capaces de enfurecernos.&lt;/p&gt;
&lt;p&gt;De vez en cuando, algún científico se presenta en un debate público con, por ejemplo, un parapsicólogo que afirma que los fenómenos paranormales son una realidad y la parapsicología una disciplina respetable. A uno le parecería que el científico debería vencer fácilmente en el debate. Sin embargo, generalmente es el parapsicólogo quien parece ganar y el científico se ve regularmente reducido a una defensa estéril. Los científicos, por lo regular, hemos pasado nuestra vida profesional en debates con otros científicos. En estas confrontaciones se esgrimen como armas los resultados de los experimentos y la consistencia de la argumentación. Se pueden mantener puntos de vista opuestos, las discusiones pueden hacerse acaloradas, pero los participantes seguimos las reglas de la ciencia. No es importante ser un buen orador; lo que cuenta es el contenido. Por el contrario, los parapsicólogos, astrólogos y ufólogos son a menudo personas del espectáculo y casi siempre buenos oradores. No tienen ningún interés por los datos científicos o la racionalidad de sus argumentaciones, únicamente están en el escenario para ganar puntos con el público. Y así hacen un mejor papel que los científicos.&lt;/p&gt;
&lt;p&gt;Por otro lado, por nuestra formación, nosotros estamos condicionados para admitir incertidumbre e ignorancia, actitudes que forman parte esencial en nuestro trabajo. Los charlatanes saben aprovechar esto y atacan en esa dirección. ¿Qué podemos hacer? A partir de mi experiencia, creo que los que carecemos de talento para estos enfrentamientos debemos rechazar el debate; no estamos preparados para las reglas del mundo del espectáculo. Para aquellos que tienen este tipo de talento, creo que no deben limitarse a defender la ciencia sino obligar a su adversario a presentar las pruebas que disponga y demolerlas. Y la razón no basta, creo que los científicos debemos aprender a expresar nuestros sentimientos al comunicarnos, hacer conocer la pasión que nos mueve.&lt;/p&gt;
&lt;p&gt;Por último, es necesario destacar que un alto grado de responsabilidad por la aceptación pública de las pseudociencias le corresponde al sistema educativo. Los jóvenes no son correctamente introducidos en los fundamentos del pensamiento científico. La ciencia es enseñada como una serie de ejercicios académicos, no como una herramienta cognoscitiva, una manera de entender lo que nos rodea. La ciencia es entonces percibida como un conjunto de reglas extrañas expresadas en un lenguaje incomprensible y, por esta razón, es más temida que respetada. Sin embargo, he notado en estos años que donde tengo mejor recepción es en los colegios. Los jóvenes mantienen una frescura y una racionalidad que los años parecieran opacar, de modo que considero especialmente recomendable acercarse a las escuelas.&lt;/p&gt;
&lt;p&gt;Creo que es muy importante que los científicos nos involucremos en la divulgación científica como herramienta para mejorar la educación pública. Más aún, tenemos la obligación moral de hacerlo.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; Celso Aldao (E-mail: &lt;a href=&#34;mailto:cmaldao@mdp.edu.ar&#34;&gt;cmaldao@mdp.edu.ar&lt;/a&gt; )&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Celdas solares con nanohilos</title>
      <link>https://ciencianet.com.ar/post/celdas-solares-con-nanohilos/</link>
      <pubDate>Tue, 17 Aug 2010 20:47:16 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/celdas-solares-con-nanohilos/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Marcelo Cappelletti&lt;/strong&gt;. Facultad de ingeniería. Universidad Nacional de La Plata.&lt;/p&gt;
&lt;p&gt;El avance a pasos agigantados de la nanotecnología brinda la posibilidad de nuevas investigaciones y desarrollos con aplicaciones en diversos campos tales como la informática, medicina o fuentes de energía alternativas, que prometen mejorar la calidad de vida de la humanidad y el medio ambiente en general.&lt;/p&gt;
&lt;p&gt;En este sentido, en el campo de almacenamiento, producción y conversión de energía, los dispositivos fotovoltaicos (dispositivos capaces de convertir energía solar en energía eléctrica) de unión &lt;a href=&#34;http://es.wikipedia.org/wiki/Uni%C3%B3n_PN&#34;&gt;p-n&lt;/a&gt; que emplean nanohilos semiconductores, presentan una clara ventaja respecto de los dispositivos convencionales de película delgada, en cuanto a tener una mayor eficiencia en la conversión de energía y un menor costo.&lt;/p&gt;
&lt;p&gt;Sin embargo, actualmente, los dispositivos que emplean nanohilos basados en semiconductores compuestos III-V (semiconductores formados por dos elementos de los grupos III y V de la tabla periódica), presentan una pobre característica de rectificación y una eficiencia de conversión de energía solar muy inferior a lo que predice la teoría. Esto puede ser debido a los &lt;a href=&#34;http://es.wikipedia.org/wiki/Portador_de_carga&#34;&gt;portadores de carga&lt;/a&gt; atrapados en las superficies de los nanohilos, a las elevadas resistencias de los contactos y a un control insuficiente del &lt;a href=&#34;http://es.wikipedia.org/wiki/Dopaje_%28semiconductores%29&#34;&gt;dopaje&lt;/a&gt; a través de los nanohilos.&lt;/p&gt;
&lt;p&gt;Jorge Caram, Claudia Sandoval, Mónica Tirado y David Comedi de la Universidad Nacional de Tucumán, junto a Josef Czaban, David Thompson y Ray LaPierre de la Universidad de McMaster (Canadá) han estudiado este problema a partir de dispositivos fotovoltaicos de unión p-n que emplean nanohilos de un semiconductor compuesto, el arseniuro de galio (GaAs).&lt;/p&gt;
&lt;p&gt;Un método típico para evaluar el rendimiento de los dispositivos fotovoltaicos consiste en realizar mediciones de la característica Tensión-Corriente continua. Sin embargo, en este trabajo, los autores proponen una caracterización del dispositivo mucho más completa, mediante un estudio de &lt;a href=&#34;http://es.wikipedia.org/wiki/Espectroscopia_de_impedancia&#34;&gt;espectroscopía de impedancia&lt;/a&gt; de estructuras de nanohilos coraza(shell)-corazón(core) p-n de GaAs, el cual es un método que analiza la dispersión de las propiedades eléctricas en un intervalo amplio de frecuencias (usando corriente alterna).&lt;/p&gt;
&lt;p&gt;Los dispositivos de nanohilos para aplicaciones fotovoltaicas que han sido investigados, fueron fabricados por el grupo de investigación LaPierre de la Universidad de McMaster, y consisten de coraza-corazón p-n alineados de manera perpendicular al GaAs. El material empleado para el dopaje tipo n fue el telurio (Te) en lugar de silicio (Si), debido al comportamiento &lt;a href=&#34;http://es.wikipedia.org/wiki/Anf%C3%B3tero&#34;&gt;anfótero&lt;/a&gt; que el Si puede presentar en nanohilos de GaAs.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/08/cappelletti.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Se diseñaron dos clases bien diferentes de dispositivos dependiendo del grado de superposición espacial, coraza-corazón p-n, a lo largo del eje del nanohilo. Estas diferentes distribuciones espaciales resultaron ser un punto trascendente para la obtención de importantes conclusiones en cuanto a las características eléctricas en el rango de frecuencias 103–107 Hz.&lt;/p&gt;
&lt;p&gt;Los resultados obtenidos indican que para frecuencias menores a 104 Hz, cuando se invierte la tensión de polarización desde 1.5 V hasta -1.5 V, la impedancia medida de los dispositivos con mayor superposición coraza-corazón (muestra D en la figura), se comporta de acuerdo a lo esperado para una unión p-n.&lt;/p&gt;
&lt;p&gt;Por el contrario, los dispositivos con pequeña superposición (muestra A en la figura) no siguen este comportamiento. Además, la respuesta del dispositivo decae abruptamente para frecuencias mayores de 104 Hz. Los autores atribuyeron este efecto a los portadores capturados y liberados desde los niveles de energía profundos de la &lt;a href=&#34;http://es.wikipedia.org/wiki/Banda_prohibida&#34;&gt;banda prohibida&lt;/a&gt; del semiconductor.&lt;/p&gt;
&lt;p&gt;Este estudio permitió conseguir una mejor comprensión de los fenómenos físicos que actualmente limitan la eficiencia de estos dispositivos. Los autores creen que sus hallazgos permitirán mejorar la eficiencia en la conversión de energía solar a través de dispositivos fotovoltaicos basados en nanohilos.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; &lt;a href=&#34;http://iopscience.iop.org/0957-4484/21/13/134007/&#34;&gt;Electrical characteristics of core–shell p–n GaAs nanowire structures with Te as the n-dopant&lt;/a&gt; , Nanotechnology, 21, 134007 (2010)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; David Comedi (E-mail: &lt;a href=&#34;mailto:dcomedi@herrera.unt.edu.ar&#34;&gt;dcomedi@herrera.unt.edu.ar&lt;/a&gt; ) &lt;a href=&#34;http://www.herrera.unt.edu.ar/nano&#34;&gt;http://www.herrera.unt.edu.ar/nano&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Reciente avance en física de medios granulares</title>
      <link>https://ciencianet.com.ar/post/reciente-avance-en-fisica-de-medios-granulares/</link>
      <pubDate>Thu, 05 Aug 2010 20:48:31 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/reciente-avance-en-fisica-de-medios-granulares/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Andrés D. Medus.&lt;/strong&gt; Estudiante de doctorado del Departamento de Física, Facultad de Ciencias Exactas y Naturales, Universidad de Buenos Aires.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Este artículo ha sido financiado por el proyecto INVOFI de la Asociación Física Argentina.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Investigadores del Grupo de Medios Porosos de la Facultad de Ingeniería de la UBA en colaboración con pares de la Universidad de la Plata y de la Universidad de Lyon (Francia), demostraron experimentalmente que el flujo de material granular a través de un orificio resulta independiente de la presión en la base del recipiente contenedor, derrumbando de este modo un antiguo postulado de la Física de Medios Granulares.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Por el ápice abierto el cono inverso &lt;br&gt;
Deja caer la cautelosa arena,  &lt;br&gt;
Oro gradual que se desprende y llena &lt;br&gt;
El cóncavo cristal de su universo. &lt;br&gt;&lt;/p&gt;
&lt;p&gt;-- &lt;em&gt;El reloj de arena&lt;/em&gt; de Jorge Luis Borges.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Muchos de nosotros, en algún momento de nuestras vidas, hemos sido hipnotizados por el constante fluir de los granos de arena a través del orificio que une los bulbos en un reloj de arena. Este fluir casi imperturbable no se daría si de un líquido se tratase. En ese caso la velocidad de salida del líquido sería dependiente de la presión en el orificio, que a su vez depende de la altura de la columna de líquido restante en el bulbo superior. Sin embargo, para el caso de medios granulares como la arena, se da un extraño fenómeno denominado “efecto Janssen”, por el cual la presión ejercida por los granos que se encuentran encima del orificio, se transmite por fricción hacia los laterales del recipiente contenedor.&lt;/p&gt;
&lt;p&gt;Como resultado, cuando la altura de la columna de arena es superior al doble del tamaño de la apertura de salida, la presión ejercida en la región próxima al orificio es casi constante e independiente de la cantidad restante. Luego, el flujo de arena a través del orificio será también aproximadamente constante e independiente de la cantidad que reste en el bulbo superior.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/08/cinta-transportadora-CN.png&#34; alt=&#34;Modelo experimental armado.&#34; title=&#34;cinta-transportadora-CN&#34;&gt;&lt;/p&gt;
&lt;p&gt;De lo dicho hasta aquí se desprende un importante postulado de la Física de los Medios Granulares: si la presión en el fondo de un silo es constante, entonces también lo será el flujo de material granular a través del orificio. “&lt;em&gt;En los últimos 50 años numerosos trabajos de diversas disciplinas utilizan este efecto (presión constante en la base del recipiente) para justificar que el caudal permanece constante durante la descarga. Es decir, presión constante, implica caudal constante. Hasta tal punto está generalizada esta idea, que esta explicación ha sido sostenida aún cuando la columna de granos no es suficiente para alcanzar una presión constante en la base&lt;/em&gt;” nos cuenta la Dra. María Alejandra Aguirre, integrante del equipo que ha demostrado recientemente que este postulado es falso.&lt;/p&gt;
&lt;p&gt;Para ello, construyeron el dispositivo que puede observarse en la figura, consistente en una caja contenedora de Plexiglas con una abertura de tamaño variable, conteniendo discos del mismo material que son impulsados por una cinta móvil de velocidad regulable. A diferencia de lo que ocurre para un reloj de arena, en este caso el dispositivo está dispuesto en forma horizontal y los discos son impulsados por una cinta móvil y no por la gravedad. Las experiencias realizadas muestran que “&lt;em&gt;el caudal permanece constante aunque la presión en la base varíe y que sólo puede cambiarse si se modifica la velocidad con que los granos atraviesan la apertura&lt;/em&gt;” concluye la Dra. Aguirre, refutando aquel antiguo postulado.&lt;/p&gt;
&lt;p&gt;Consultada acerca de las posibles aplicaciones de esta investigación, la Dra. Aguirre indica que “&lt;em&gt;puede servir de fundamento para buscar diferentes maneras y/o mecanismos que aumenten la velocidad de salida de granos a fin de incrementar el caudal de descarga en aplicaciones industriales&lt;/em&gt;”.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo Original:&lt;/strong&gt; M. A. Aguirre, J. G. Grande, A. Calvo, L. A. Pugnaloni, J.-C Géminard; Phys. Rev. Lett. 104, 238002 (2010) (&lt;a href=&#34;http://prl.aps.org/abstract/PRL/v104/i23/e238002&#34;&gt;http://prl.aps.org/abstract/PRL/v104/i23/e238002&lt;/a&gt;, &lt;a href=&#34;http://arxiv.org/abs/1005.2884&#34;&gt;PDF&lt;/a&gt;).&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Cinemática: de los números a las representaciones algebraicas</title>
      <link>https://ciencianet.com.ar/post/cinematica-de-los-numeros-a-las-representaciones-algebraicas/</link>
      <pubDate>Thu, 22 Jul 2010 20:51:47 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/cinematica-de-los-numeros-a-las-representaciones-algebraicas/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero&lt;/strong&gt;. Facultad de Ciencias Exactas (UNLP).&lt;/p&gt;
&lt;p&gt;Trabajos previos (Redish; 2005, 2009) muestran que los matemáticos hacen una lectura diferente de las ecuaciones respecto de la que hacen los físicos, quienes incluyen cuestiones como las unidades, la diferencia entre variables, condiciones iniciales y constantes y las cuestiones de notación. El conocimiento de matemática parece entonces necesario pero no suficiente para lograr un aprendizaje en física.&lt;/p&gt;
&lt;p&gt;Con el objetivo de describir e interpretar los mecanismos de los estudiantes al resolver problemas de física las investigadoras tomaron un cuestionario a alumnos universitarios. En él se pedía, en primer lugar, la resolución de problemas sencillos cinemática (cálculo de velocidades medias, distancias recorridas y tiempos de llegada). Luego se les presentaba la expresión algebraica para la velocidad media y se les pedía que la usaran resolver problemas. En ambos casos se les solicitaba que dijeran cuál era la cuenta que empleaban para responder. Finalmente, se les pedía “despejar” una variable de una fórmula algebraica, fuera de todo planteo físico.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/07/profesor-300x225.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Entre los resultados reportados sorprende la variedad de procedimientos realizados por los alumnos en situaciones tan sencillas, aunque casi la mitad de los encuestados resolvieron la primera parte con la popular “regla de tres”. Pero además este estudio señala cierta independencia entre la capacidad de los alumnos para resolver problemas algebraicos cuando no se asigna ningún sentido físico a las variables y cuando las fórmulas representan un problema físico particular.&lt;/p&gt;
&lt;p&gt;Si se trata de resolver un problema concreto de física, los alumnos muestran dificultades para trabajar con expresiones algebraicas sin reemplazar todas las cantidades por valores numéricos. Pero paradójicamente pueden operar con estas expresiones “sin datos”, si se encuentran fuera del contexto de un problema.&lt;/p&gt;
&lt;p&gt;Esta línea de trabajo parece indicar entonces que las dificultades en el uso de la matemática para resolver problemas de física no pueden “patearse fuera de la cancha” adjudicándoselas a deficiencias de bagaje matemático con que los alumnos llegan a los cursos, sino que son propias del uso particular que hace la física de ese lenguaje y no pueden atribuirse únicamente a problemas conceptuales o algebraicos.&lt;/p&gt;
&lt;p&gt;Se espera que los aportes de esta novedosa línea de trabajo puedan ayudar a la elaboración de recomendaciones didácticas para los profesores de física. Por lo pronto, dan indicios del por qué del fracaso de aquellos cursos de ingreso o de apoyo que buscan mejorar la enseñanza de ciencias cubriendo supuestos baches con clases de matemáticas.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; Cinemática: de los números a las representaciones algebraicas Revista Enseñanza de las ciencias, VIII Congreso internacional sobre Investigación en Didáctica de las Ciencias 2009 - Silvia Pérez y Celia Dibar Ure.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instituciones:&lt;/strong&gt; CeFIEC - FCEN - UBA&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; &lt;a href=&#34;mailto:silvia.m.perez@gmail.com&#34;&gt;silvia.m.perez@gmail.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referencias:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Redish, E. F. (2005) &lt;a href=&#34;http://www.physics.umd.edu/perg/papers/redish/IndiaMath.pdf&#34;&gt;Problem Solving and the Use of Math in Physics Courses&lt;/a&gt;, &lt;em&gt;Proceedings of the Conference, World View on Physics Education in 2005: Focusing on Change&lt;/em&gt;, Delhi. &lt;a href=&#34;http://umdperg.pbworks.com/Joe-Redish%253A-Selected-Publications&#34;&gt;http://umdperg.pbworks.com/Joe-Redish%253A-Selected-Publications&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Redish, E.F. and Gupta, A. (2009) &lt;a href=&#34;http://www2.physics.umd.edu/%7Eredish/Papers/GIREP2009_16_Redish.pdf&#34;&gt;Making Meaning with Math in Physics: A Semantic Analysis&lt;/a&gt;. &lt;em&gt;GIREP Conference Proceedings&lt;/em&gt;, Leicester, UK. &lt;a href=&#34;http://umdperg.pbworks.com/Joe-Redish%253A-Selected-Publications&#34;&gt;http://umdperg.pbworks.com/Joe-Redish%253A-Selected-Publications&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

        
      </description>
    </item>
    
    <item>
      <title>Sobre el movimiento democrático a escala molecular en líquidos a baja temperatura</title>
      <link>https://ciencianet.com.ar/post/sobre-el-movimiento-democratico-a-escala-molecular-en-liquidos-a-baja-temperatura/</link>
      <pubDate>Tue, 23 Mar 2010 20:53:28 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/sobre-el-movimiento-democratico-a-escala-molecular-en-liquidos-a-baja-temperatura/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;J. Ariel Rodriguez Fris.&lt;/strong&gt; Universidad Nacional del Sur.&lt;/p&gt;
&lt;p&gt;El movimiento de los átomos, moléculas e iones que conforman un líquido cuando, en promedio, se desplazan el equivalente a su tamaño a partir de su posición inicial se la denomina relajación estructural. La dinámica de relajación estructural de líquidos a baja temperatura es heterogénea en tiempo y espacio. Es decir que dicha dinámica es muy diferente de una región a otra y además cambia en el tiempo. Cabe aclarar que estas heterogeneidades se hacen más conspicuas con la disminución de la temperatura.&lt;/p&gt;
&lt;p&gt;El hallazgo de heterogeneidades dinámicas, tanto teóricas como experimentales, en líquidos a baja temperatura brindó una clave para comprender el mecanismo por el cual relajan dichos sistemas. En esta tesis demostramos, mediante simulación por computadoras, que dicha relajación, al menos para los tres líquidos estudiados, se debe a la ocurrencia de eventos que denominamos &lt;em&gt;d-clusters&lt;/em&gt; (por &lt;em&gt;democratic clusters&lt;/em&gt;, o agregados democráticos), los cuales son una clase de heterogeneidades dinámicas no estudiadas hasta el momento.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/03/fris.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Para caracterizar la dinámica de relajación utilizamos tres modelos de líquidos: uno arquetípico de constituyentes que interaccionan como los gases nobles (como el Argón), otro que emula agua sobreenfriada y, por último, uno que emula dióxido de silicio. Al seguir la dinámica de pequeñas porciones de estos sistemas en función del tiempo, descubrimos que los constituyentes pasan largos períodos de &amp;quot;inactividad&amp;quot;, sin moverse demasiado en promedio (por lo que en estos tiempos el sistema no relaja ya que se halla aproximadamente inmóvil o congelado). Se analizan pequeñas porciones del líquido ya que cada región se comporta diferente debido a las heterogeneidades dinámicas.&lt;/p&gt;
&lt;p&gt;Por otro lado, vimos que a ciertos tiempos sobreviene una rápida reorganización de aproximadamente el 35 % de los constituyentes formando una agrupación (ó cluster) relativamente compacta. Posteriormente reaparece el período inmóvil y así sucesivamente, alternan. A este tipo de movimiento colectivo, el cual es una nueva clase de heterogeneidades dinámicas, que dura sólo una fracción respecto del gran período inmóvil, lo denominamos &lt;em&gt;d-cluster&lt;/em&gt;. La d por democrático ya que es una importante fracción del total del sistema que se reorganiza.&lt;/p&gt;
&lt;p&gt;Se destaca el hecho que nuestro trabajo permitió a otros autores, estudiando un polímero a baja temperatura y comparando simulaciones con experimentos, dar soporte experimental a la ocurrencia de estos eventos. En conclusión, nuestro trabajo ayudó a mejorar la comprensión del mecanismo de relajación estructural a baja temperatura de tres líquidos de muy distinta naturaleza, lo cual presupone que dicho mecanismo podría ser entonces común a todos los líquidos.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;CienciaNet&lt;/strong&gt; consultó a Ezequiel Albano, de la Universidad de La Plata, quien fue jurado evaluador de la tesis. Ezequiel comenta que &amp;quot;La tesis de Ariel constituye un aporte teórico fundamental para el entendimiento de los complejos procesos de relajación dinámica que tienen lugar durante la formación de vidrios. Asimismo, los trabajos surgidos de la tesis han logrado una gran repecución internacional y motivado la realización de experimentos con el objeto de corroborar sus predicciones.&amp;quot;&lt;/p&gt;
&lt;p&gt;Esta tesis recibió el premio J.J. Giambiagi 2009 a la mejor tesis de física teórica, otorgado por la Asociación Física Argentina.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo Original:&lt;/strong&gt; &amp;quot;Dinámica de relajación de líquidos formadores de vidrios&amp;quot; presentada por Ariel Rodriguez Fris para optar por el grado de Doctor en la Universidad Nacional del Sur, bajo la dirección de Gustavo Appignanesi.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; Ariel Rodriguez Fris (E-mail: &lt;a href=&#34;rodriguezfris@plapiqui.edu.ar&#34;&gt;rodriguezfris@plapiqui.edu.ar&lt;/a&gt;).&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>¿Qué nota escuchamos durante un vibrato?</title>
      <link>https://ciencianet.com.ar/post/que-nota-escuchamos-durante-un-vibrato/</link>
      <pubDate>Tue, 09 Mar 2010 20:54:39 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/que-nota-escuchamos-durante-un-vibrato/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Damián H. Zanette.&lt;/strong&gt; Centro Atómico Bariloche.&lt;/p&gt;
&lt;p&gt;El vibrato es un recurso expresivo usado por cantantes y ejecutantes de instrumentos de cuerda para agregar textura sonora a la música. Consiste en variar levemente y muy rápido (varias veces por segundo) la altura de la nota que se canta o se toca –es decir, la frecuencia del sonido– produciendo una vibración característica.&lt;/p&gt;
&lt;p&gt;Hay quien dice que el vibrato sirve para disimular notas desafinadas, pero los experimentos muestran que el oyente percibe, además de la vibración, una nota de altura constante y bien definida. Si la frecuencia del sonido está variando, ¿qué altura tiene la nota que escuchamos en un vibrato? La relación entre las propiedades físicas de un estímulo sensorial –por ejemplo, una señal acústica– y la sensación que nos produce es uno de los problemas más apasionantes del campo interdisciplinario de las neurociencias.&lt;/p&gt;
&lt;p&gt;Recientemente, Bruno Mesz y Manuel Eguia, del Laboratorio de Acústica y Percepción Sonora de la Universidad Nacional de Quilmes, han investigado cómo el sistema auditivo convierte al vibrato en una nota única. En primer lugar, realizaron experimentos para comparar las notas percibidas durante vibratos producidos por diferentes perfiles de variación de la frecuencia del sonido. Investigaciones anteriores sugerían que, cuando la variación es simétrica alrededor de un valor dado, la frecuencia percibida es la media geométrica de esa variación. Para variaciones asimétricas, no inesperadamente, Mesz y Eguia encontraron que la nota que oímos es más aguda cuando la frecuencia pasa más tiempo en valores relativamente altos, y viceversa.&lt;/p&gt;
&lt;p&gt;La contribución clave de estos investigadores es haber propuesto un modelo matemático que describe fenomenológicamente el proceso mediante el cual los canales auditivos del sistema nervioso integran la señal de frecuencia variable para generar una nota bien definida. Este modelo se basa en analizar una representación del sonido introducida en 2005, en otros, por el físico argentino Marcelo Magnasco, y describe correctamente los resultados experimentales.&lt;/p&gt;
&lt;p&gt;Uno de los aspectos más interesantes del mecanismo propuesto es que tiene en cuenta la información contenida en la distribución de fases de la señal sonora. En el oído, podría estar implementado mediante la detección del intervalo de tiempo entre las señales nerviosas provenientes de diferentes fibras auditivas. El modelo combina dos ingredientes de la señal acústica que muchas veces se han considerado como elementos contrapuestos en el proceso auditivo: la codificación temporal dada por las fases de las ondas, y la distribución de sus frecuencias y amplitudes. Bruno Mesz es matemático y, además, es un destacado pianista especializado en música contemporánea. Manuel Eguia, profesor de la Universidad de Quilmes, es físico.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original&lt;/strong&gt;: B. A. Mesz and M. C. Eguia, The Pitch of Vibrato Tones. &lt;em&gt;&lt;a href=&#34;https://books.google.com.ar/books?hl=en&amp;amp;lr=&amp;amp;id=w_sVnZjUsPoC&amp;amp;oi=fnd&amp;amp;pg=PA126&amp;amp;dq=B.+A.+Mesz+and+M.+C.+Eguia,+The+Pitch+of+Vibrato+Tones.+A+Model+Based+on+Instantaneous+Frequency+Decomposition,+Ann.+N.+Y.+Acad.+Sci.+1169,+126-130+(2009).&amp;amp;ots=z1dxn9jgLe&amp;amp;sig=ECS7YpkuWiMPG5Umyl_jdERRPok#v=onepage&amp;amp;q&amp;amp;f=false&#34;&gt;A Model Based on Instantaneous Frequency Decomposition&lt;/a&gt;&lt;/em&gt;, Ann. N. Y. Acad. Sci. 1169, 126-130 (2009).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instituciones&lt;/strong&gt;: &lt;a href=&#34;http://www.unq.edu.ar/&#34;&gt;Universidad Nacional de Quilmes&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto&lt;/strong&gt;: Manuel Eguia (E-mail: &lt;a href=&#34;mailto:meguia@unq.edu.ar&#34;&gt;meguia@unq.edu.ar&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Más información:&lt;/strong&gt; &lt;a href=&#34;http://es.wikipedia.org/wiki/Vibrato&#34;&gt;Vibrato&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>La evolución de los lenguajes y la física contemporánea</title>
      <link>https://ciencianet.com.ar/post/la-evolucion-de-los-lenguajes-y-la-fisica-contemporanea/</link>
      <pubDate>Tue, 16 Feb 2010 20:55:51 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/la-evolucion-de-los-lenguajes-y-la-fisica-contemporanea/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Gabriel Baglietto.&lt;/strong&gt; Instituto de investigaciones Fisicoquímicas Teóricas y Aplicadas (CONICET, UNLP).&lt;/p&gt;
&lt;p&gt;La amplitud de los temas tratados por la física hoy en día es sorprendente. Además de la temática tradicionalmente asociada a esta disciplina, existe una gran variedad de artículos en revistas de física sobre economía, música, ecología, inteligencia artificial, demografía, redes sociales, etc. ¿Qué tipo de aporte puede realizar un físico en estos campos? En este artículo comentamos el trabajo realizado por un físico del Instituto Balseiro para entender la evolución de los idiomas.&lt;/p&gt;
&lt;p&gt;En 1889 el matemático francés Henry Poincaré descubrió y demostró que un sistema tan simple como el constituido por tres partículas que interactúan gravitatoriamente no podía resolverse exactamente. A partir de ese momento, muchos físicos comenzaron a interesarse en lo que hoy se denominan &lt;strong&gt;sistemas complejos&lt;/strong&gt;, sistemas en los que las interacciones entre sus partes hacen difícil, sino imposible, su resolución exacta.&lt;/p&gt;
&lt;p&gt;Así comenzó a generarse una gran cantidad de herramientas matemáticas y conceptuales para extraer información útil de problemas complicados. Ahora estas herramientas se están utilizando en muchos campos del conocimiento. Uno de ellos es el de la lingüística evolutiva, que se encarga del estudio de los orígenes y desarrollos de los diferentes idiomas. Una de las preguntas que trata de responder esta ciencia es mediante qué mecanismo se alcanzó la distribución de idiomas que existe en la actualidad. Se sabe que hay pocos idiomas hablados por muchas personas y muchos hablados por muy pocas. Más precisamente, la relación entre cuántos idiomas son hablados por una dada cantidad de personas sigue una &lt;a href=&#34;http://es.wikipedia.org/wiki/Distribuci%C3%B3n_log-normal&#34;&gt;distribución log-normal&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/02/evolucion-lenguas.preview-300x144.png&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;La mayoría de los modelos que se han propuesto para tratar de explicar por qué los idiomas se distribuyen de acuerdo con esta ley hacen mucho hincapié en los efectos de las mutaciones de varias características lingüísticas que dan origen al surgimiento de nuevos lenguajes y además consideran la posibilidad de la extinción de los lenguajes.&lt;/p&gt;
&lt;p&gt;Todo esto parece muy razonable. Sin embargo, los modelos propuestos hasta el presente no lograban reproducir fielmente la distribución real de lenguajes. Damián Zanette del Instituto Balseiro se dio cuenta de que “estos modelos no consideraban el hecho de que durante períodos de tiempo que son cortos comparados con las escalas típicas de la evolución de los lenguajes, los hablantes de una dada lengua pueden variar sustancialmente en número simplemente por los efectos de su propia dinámica poblacional” antes de que por interacciones o alteraciones en la lengua misma. “Por ejemplo, en los últimos cinco siglos –un período que incluye la devastadora invasión cultural de Europa al resto del globo- quizás el 50% de los lenguajes del mundo se extinguieron (entre ellos, dos tercios de los 2000 lenguajes nativos preexistentes en América) o cambiaron drásticamente.&lt;/p&gt;
&lt;p&gt;En el mismo período, sin embargo, la población del mundo creció doce veces o más.” O sea que si bien en este período resultó considerable la disminución en el número de lenguas debida a la interacción entre culturas, el efecto en el número de hablantes en las diferentes lenguas producido simplemente por el crecimiento poblacional fue mucho mayor.&lt;/p&gt;
&lt;p&gt;Sobre la base de esta observación construyó un modelo que logra explicar la distribución de lenguas existente tomando en cuenta sólo los efectos demográficos. Esto no significa que los otros factores no deban ser tenidos en cuenta, pero parece ser que hasta el momento, los modelos existentes habían menospreciado un factor determinante.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo Original&lt;/strong&gt;: D. H. Zanette, &amp;quot;Demographic growth and the distribution of language sizes&amp;quot;, International Journal of Modern Physics C, vol. 19, pp. 237–247 (2008). &lt;a href=&#34;http://arxiv.org/pdf/0710.1511&#34;&gt;http://arxiv.org/pdf/0710.1511&lt;/a&gt; Instituciones: Instituto Balseiro &lt;a href=&#34;http://www.ib.edu.ar/&#34;&gt;http://www.ib.edu.ar/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; Damián Zanette (E-mail: &lt;a href=&#34;mailto:zanette@cab.cnea.gov.ar&#34;&gt;zanette@cab.cnea.gov.ar&lt;/a&gt;)&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>“¿Es realmente posible la divulgación científica?” Fragmentos de una entrevista a Pierre Thuillier</title>
      <link>https://ciencianet.com.ar/post/es-realmente-posible-la-divulgacion-cientifica-fragmentos-de-una-entrevista-a-pierre-thuillier/</link>
      <pubDate>Thu, 04 Feb 2010 20:57:19 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/es-realmente-posible-la-divulgacion-cientifica-fragmentos-de-una-entrevista-a-pierre-thuillier/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero&lt;/strong&gt;. Facultad de Ciencias Exactas (UNLP).&lt;/p&gt;
&lt;p&gt;En este artículo reproducimos parte de una entrevista realizada al filósofo francés Pierre Thuillier (1927-1998), quien se ha dedicado a la historia de la ciencia, la epistemología y al estudio de las relaciones entre Ciencia, Tecnología y Sociedad. Además, fue editor de la revista de divulgación &amp;quot;&lt;em&gt;La recherche&lt;/em&gt;&amp;quot; desde su fundación en 1970 hasta 1992. La entrevista –publicada en la revista de divulgación Ciencia hoy- fue hecha en 1989 por Paulo César Abrantes, Ildeu Castro de Moreira y Alicia Ivanissevich en su visita a Río de Janeiro con motivo de un Seminario conmemorativo de los 350 años de la publicación de los Diálogos acerca de dos nuevas ciencias, de Galileo. En ella Thuillier responde sobre Galileo y el impacto de su obra en la ciencia, pero también sobre cuestiones como la divulgación científica y los problemas éticos que conllevan algunas áreas de la ciencia contemporánea.&lt;/p&gt;
&lt;p&gt;Sobre la divulgación científica se suele discutir si es una tarea que corresponde a los científicos, quienes deberían informar a la sociedad de sus resultados, o si debe quedar en manos de periodistas y comunicadores especializados. En sus declaraciones, Pierre Thuillier va más allá en sus cuestionamientos, los cuales permanecen vigentes a pesar de los más de 20 años trascurridos.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/02/ciencia_hoy_thuillier_0.jpg&#34; alt=&#34;:left&#34; title=&#34;Revista Ciencia Hoy&#34;&gt;&lt;/p&gt;
&lt;p&gt;Invitado a reflexionar sobre la cuestión de la divulgación científica, Thuillier expresa:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A este respecto haría una primera pregunta: ¿es realmente posible la divulgación científica? Hay quienes creen que ella promueve el aprendizaje de la ciencia. Son como los &amp;quot;internalistas&amp;quot; de la historia de la ciencia, que sólo consideran los textos y creen que, leyéndolos, se aprende todo. El caso extremo es el de la persona no muy instruida, que compra un montón de revistas científicas, lee todo y queda convencida de que conoce la ciencia. En fin, ¿se puede aprender ciencia a través de la mera lectura de textos? Ya comentamos que la ciencia no está hecha de textos, es una práctica... Otro problema es saber si la divulgación científica forma de hecho a las personas y cultiva el espíritu crítico, como en general se afirma. Lo que la experiencia muestra es que se trata, en muchos casos, de una literatura completamente mistificadora. Cuanto más sofisticado es un artículo, cuando más abstrusa es la cuestión que aborda, más disparates puede contener. Un ejemplo: cuando La Recherche cumplió un año, resolvimos hacer un &amp;quot;día de los inocentes&amp;quot;. Preparamos un artículo que relacionaba la configuración de ciertas estrellas con la nariz de un animal inventado por un biólogo francés para burlarse de los evolucionistas, que caminaba sobre su propia nariz. El texto tenía dos páginas de consideraciones sobre tales estrellas y tal animal, citando periódicos inexistentes. ¿Sabe que mucha gente lo tomó en serio? Hasta un famoso periodista científico de la televisión francesa, autor de libros de divulgación que se venden por millares, creyó ingenuamente en una cosa tan disparatada. Nunca más repetimos la broma: mucha gente cree cualquier cosa si está dicha por una revista científica. El problema es serio. ¡Ningún absurdo parecerá más espantoso que el de la mecánica cuántica! En su afán de enseñar ciencia las obras o revistas de divulgación van al encuentro de personas incapaces de crítica. (...) ¿No sería el verdadero saber aquél que las personas son capaces de dominar? Si el público no domina el saber ni es capaz de criticarlo, estamos exhibiendo una especie de &amp;quot;vidriera de la ciencia&amp;quot; y un saber que solamente puede ser contemplado, no tocado.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/02/thuillier3.jpg&#34; alt=&#34;Revista La Recherche:left&#34; title=&#34;Revista La Recherche&#34;&gt;&lt;/p&gt;
&lt;p&gt;Ante la pregunta de cómo evitar estos efectos, Thuillier puntea:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;El verdadero problema sería distinguir niveles de vulgarización (Nota: utilizado como sinónimo de popularización), y ser concientes de que ella no es enseñanza.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Es necesario también diferenciar vulgarización científica y cultura científica. Esta última consiste en el saber que se puede tener sobre la ciencia. No se trata de estar a la par de los últimos resultados de la cosmología relativista, de ésta o de aquélla novedad; la cultura científica se ocupa, sobre todo, de las informaciones sobre el progreso, el avance del conocimiento.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;En el plano del conocimiento hay dos cosas fundamentales: primero, mostrar bien el significado de lo que se hace en ciencia, lo que en general no ocurre. Son comunes los grandes titulares que anuncian descubrimientos que, de hecho, no cambiaron gran cosa en un determinado campo. El segundo punto tiene que ver con las lagunas. No deberían divulgarse sólo los triunfos de la ciencia, también es fundamental mostrar lo que no se conoce. Tomemos por ejemplo la teoría de la evolución. Permanentemente se anuncian pequeños perfeccionamientos logrados en este terreno, lo que sugiere que ya existe un cuerpo completo de conocimientos que va recibiendo retoques finales. Esto no es verdad: la teoría de la evolución tiene enormes lagunas. Es difícil mostrarlo porque las revistas, los científicos y los mismos lectores quieren progresos, pasos decisivos. Pero el efecto de la práctica usual es desastroso: hay personas que después de leer veinte artículos sobre la teoría de la evolución adquieren al respecto ideas claras y precisas, en tanto los verdaderos científicos están llenos de dudas. De esta manera no se contribuye, por lo tanto, a la formación de un espíritu crítico. Un artículo importante tal vez debería estar acompañado de un texto crítico, escrito por otros científicos, con sus evaluaciones respectivas.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;También me gustaría señalar que existe una contradicción permanente en la divulgación científica. Para el investigador los problemas se presentan complicados; cuando el asunto se enseña en las facultades, se lo empieza a simplificar; en la escuela primaria o en la vulgarización todo se vuelve aún más simple. Queriendo &amp;quot;facilitar&amp;quot;, la vulgarización simplifica y dogmatiza. (...)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Finalmente, creo que debe hacerse un esfuerzo para mostrar el saber de un modo crítico. Se debe presentar una imagen realista de lo que es una institución científica. Mostrar las presiones a las que están sujetos los investigadores, mostrar la cultura política de la ciencia, cómo está financiada, cómo es su organigrama administrativo. Se debe saber que el progreso en un área dada resulta no sólo de las presiones sociales sino también de las dotaciones presupuestarias. En la divulgación, por lo tanto, lo esencial no es sólo revelar los últimos detalles técnicos, los descubrimientos, los resultados, sino mostrar el significado de los resultados y la forma en que se produce ciencia: el funcionamiento de la institución científica, las academias, los árbitros, las dificultades del científico joven, el financiamiento, las diversas interpretaciones existentes, etc. De todo esto depende lo que llamo la verdadera cultura del ciudadano. Es esto lo que hará posible el control colectivo y democrático de la ciencia.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Por último, ofrece una propuesta:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Para los problemas importantes del hombre no hay solución técnica neutra: los problemas son siempre más complicados que el modelo científico. La salida tal vez consista no en pretender la objetividad y la neutralidad, sino en hacer que varias personas, con puntos de vista variados, hablen sobre un mismo asunto. De esta forma el lector podrá formarse un juicio. El verdadero problema detrás de todo esto es el de transformar al lector pasivo (en la divulgación tradicional, el lector es pasivo, los descubrimientos llegan hasta él, pero él no puede siquiera evaluarlos) llevándolo a ejercitar su espíritu crítico. Esto es muy importante, precisamente porque la ciencia está en el corazón del sistema. Si enseñamos a las personas a respetar por demás a la ciencia, estaremos socavando la posibilidad de criticar a la tecnocracia.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;&amp;quot;Pierre Thuillier : El Contexto Cultural de la Ciencia&amp;quot;, de Paulo Cesar Abrantes, Ildeu de Castro Moreira y Alicia Ivanissevich, publicado en Ciencia Hoy, Volumen 1, Nº 3, abril-mayo 1989, pp. 18-24&lt;/strong&gt;.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>El Taller de Enseñanza de Física de la UNLP como innovación: diseño, desarrollo y evaluación</title>
      <link>https://ciencianet.com.ar/post/el-taller-de-ensenanza-de-fisica-de-la-unlp-como-innovacion-diseno-desarrollo-y-evaluacion/</link>
      <pubDate>Wed, 16 Dec 2009 20:58:34 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/el-taller-de-ensenanza-de-fisica-de-la-unlp-como-innovacion-diseno-desarrollo-y-evaluacion/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Diego Petrucci&lt;/strong&gt;. Instituto de Desarrollo Humano Universidad, Universidad Nacional General Sarmiento.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/12/Laboratoriogrande.post_.jpg&#34; alt=&#34;Laboratorio: Espacio utilizado por el TEF.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Esta Tesis presenta una descripción curricular sistemática y un análisis detallado de un curso universitario de física básica llamado Taller de Enseñanza de Física (TEF). El TEF representa un modelo de enseñanza de la Física compatible con las recomendaciones de especialistas en Didáctica de las Ciencias, subsistiendo más de 25 años en un contexto desfavorable, con una infraestructura deficiente y sin haber recibido nunca subsidios.&lt;/p&gt;
&lt;p&gt;A lo largo de su existencia ha contribuido a la formación de más de 2000 estudiantes y 50 docentes y se han generado numerosos recursos didácticos valiosos. En este estudio se tratan específicamente la creación, evolución y características del TEF. Resulta de interés conocer sus características particulares, ya que si bien existen infinidad de ejemplos de cómo no se debe enseñar Física, las prácticas concretas de buena enseñanza son menos habituales. Por otra parte, se aborda la cuestión de la enseñanza de Física a estudiantes de Ciencias Naturales, habitualmente dificultosa. La experiencia del TEF se estudia en diversos aspectos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Descripciones del contexto institucional, del contexto social y del curso antes de la creación del TEF. Relato del proceso de surgimiento y evolución del TEF.&lt;/li&gt;
&lt;li&gt;Descripciones de las innovaciones didácticas desarrolladas en el currículo. Análisis de los problemas, las actividades, las estrategias y el sistema de evaluación.&lt;/li&gt;
&lt;li&gt;Evaluación del currículo. Se elaboró la noción de Innovación Sistemática como un modelo de desarrollo de innovaciones didácticas.&lt;/li&gt;
&lt;li&gt;Se estudió si el enfoque metodológico del TEF afecta a la forma en que los estudiantes conciben la naturaleza de la ciencia.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Entre los resultados se destacan:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Los docentes que iniciaron las innovaciones se centraron en aquello que podían modificar. Cambiaron su concepción de alumno (habitualmente definido por sus carencias) por la de un estudiante interesado en su carrera. Se permitió a los estudiantes participar de la tarea docente, ocupando los espacios que les fueron abiertos. De este modo aportaron su vivencia, enriqueciendo los análisis docentes sobre la marcha del curso. Además se les brindó la posibilidad de elaborar y llevar a cabo sus ideas. Todo ello se dio en un contexto social y político de renovación y cambio.&lt;/li&gt;
&lt;li&gt;Los cambios iniciales denotan rechazo por la enseñanza tradicional y valoración de la dimensión afectiva. Esta distinción entre propuestas tradicionales o innovadoras fue perdiendo importancia con los años, mientras que como consecuencia de presiones externas la dimensión afectiva se fue desdibujando a medida que ganaba peso la dimensión conceptual.&lt;/li&gt;
&lt;li&gt;Los criterios de selección, secuenciación y jerarquización de los contenidos que se priorizaron fueron los pedagógicos y los disciplinares. Los contenidos fueron orientados hacia la formación en Ciencias Naturales. Se explicitaron aspectos metodológicos que definen una visión de la Física como descripción de los estados que asume un cierto objeto de estudio.&lt;/li&gt;
&lt;li&gt;Se modificaron las estrategias de enseñanza, integrando teoría y práctica, evidenciándose. Se evidenció una enseñanza centrada en el estudiante.&lt;/li&gt;
&lt;li&gt;Se favorecieron en los estudiantes mecanismos de autorregulación de su aprendizaje. El sistema de evaluación propuesto por el TEF tiende a generar compromiso. Al decidir la acreditación buscando el consenso entre los docentes y cada estudiante, el poder queda repartido. Los alumnos encuentran espacio para llevar a cabo iniciativas propias como la elaboración de trabajos de investigación, de aplicación o de extensión.&lt;/li&gt;
&lt;li&gt;Las clases del TEF, más pautadas que las tradicionales, con los docentes como coordinadores, consideraban la organización espacial del aula, ubicando a los estudiantes en el centro de la escena. Para cada contenido se partía de un lenguaje común. Estas modificaciones tienen por objeto democratizar el poder. Se valoraban los consensos tanto en el trabajo disciplinar como en el didáctico, respetando la diversidad y dándole prioridad a la argumentación sobre la autoridad.&lt;/li&gt;
&lt;li&gt;En esta propuesta, los estudiantes aprenden una gran cantidad de aspectos metodológicos y actitudinales de los cuales los cursos convencionales no se ocupan.&lt;/li&gt;
&lt;li&gt;El conocimiento declarativo sobre aspectos generales de la filosofía de la ciencia no parece modificarse durante un ciclo lectivo, aun cursando según diferentes modalidades de enseñanza. Como conclusión se sugiere que un abordaje constructivista del proceso de enseñanza no necesariamente produce en los estudiantes una visión constructivista de la ciencia.&lt;/li&gt;
&lt;li&gt;El TEF pudo perdurar porque su propuesta resulta, en la práctica, exitosa. Es un ejemplo de que es posible trabajar en cursos numerosos con una modalidad constructivista mediante grupos. Las características del TEF exceden al aula ya que ha sido un generador constante de propuestas innovadoras, dentro y fuera de la Universidad, algunas desarrolladas autónomamente. Muchos de quienes se formaron allí han participado de propuestas de docencia, investigación, extensión conformando equipos que reproducían características y mecanismos propios del curso.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;CienciaNet&lt;/strong&gt; consultó a María Rut Jiménez Liso de la Universidad de Almería, España, quien fue uno de los Jurados del trabajo de Diego Petrucci. Sobre el mismo, Rut comenta:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Es una investigación de una innovación en el nivel universitario, con lo que investigación e innovación van de la mano. No muchas tesis de Didáctica de las Ciencias Experimentales pueden decir esto. Además la tesis doctoral supone un reconocimiento o puesta en valor a la innovación de aula. El trabajo muestra una metodología etnográfica para el análisis de &amp;quot;innovaciones singulares&amp;quot; basada en entrevistas a los participantes y en los documentos producidos. Al tratarse de una tesis que se puede mirar con la perspectiva de los años, creo que ha sido bastante innovador el análisis sobre la naturaleza de la ciencia y los resultados (exitosos o no) pues en su momento, en el ámbito iberoamericano, había poca producción sobre ello.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; Este estudio forma parte de la tesis defendida por Diego Petrucci el 14 de Septiembre de 2009 para optar al título de Doctor en Didáctica de las Ciencias Experimentales por la Universidad de Granada, España. Título: &lt;em&gt;&amp;quot;El Taller de Enseñanza de Física de la UNLP como innovación: diseño, desarrollo y evaluación.&amp;quot;&lt;/em&gt; Directores: Francisco Javier Perales Palacios, (UNGr, España) y María Celia Dibar (UBA).&lt;/p&gt;
&lt;p&gt;Copias de la tesis pueden solicitarse directamente al autor por &lt;a href=&#34;mailto:diegope@gmail.com&#34;&gt;email&lt;/a&gt;, o descargar de &lt;a href=&#34;http://www.fcnym.unlp.edu.ar/catedras/fisica_taller/TesisPetrucci/&#34;&gt;aquí&lt;/a&gt;.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Líquido que desafía las reglas</title>
      <link>https://ciencianet.com.ar/post/liquido-que-desafia-las-reglas/</link>
      <pubDate>Mon, 30 Nov 2009 21:01:34 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/liquido-que-desafia-las-reglas/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro&lt;/strong&gt;. Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP) y Universidad Tecnológica Nacional.&lt;/p&gt;
&lt;p&gt;Físicos franceses descubrieron un líquido que se &amp;quot;congela&amp;quot; cuando se calienta. Marie Plazanet y colegas de la &lt;a href=&#34;http://www.ujf-grenoble.fr/36392593/0/fiche___pagelibre_accueil/&#34;&gt;Université Joseph Fourier&lt;/a&gt; y del &lt;a href=&#34;http://www.ill.eu/&#34;&gt;Institut Laue-Langevin&lt;/a&gt;, ubicados en Grenoble, encontraron un solución simple formada por dos compuestos orgánicos que se convierte en sólido cuando se calienta a temperaturas entre 45 y 75°C, y se convierte en líquido cuando se enfría nuevamente. El equipo sostiene que los responsables de este comportamiento novedoso son los &lt;a href=&#34;http://es.wikipedia.org/wiki/Puentes_de_hidr%C3%B3geno&#34;&gt;puentes de hidrógeno&lt;/a&gt; .&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/11/puentes.post_.jpg&#34; alt=&#34;Estructuras moleculares de αCD: Estructura de energía mínima con 12 puentes de hidrógeno intramolecular (izquierda) y otra con 4 puentes rotos (derecha).:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Por lo general, un sólido se funde cuando se calienta, y los líquidos se transforman en gas, aunque existen algunas excepciones cuando al calentar se producen algunas reacciones químicas que no son reversibles, tal como la &lt;a href=&#34;http://es.wikipedia.org/wiki/Polimerizaci%C3%B3n&#34;&gt;polimerización&lt;/a&gt;. Sin embargo, una transición reversible en la cual un líquido se solidifica al calentarse nunca había sido observada hasta ahora.&lt;/p&gt;
&lt;p&gt;Plazanet y sus colegas prepararon una solución líquida conteniendo α-ciclodextrina (αCD), agua y 4-metilpiridina (4MP). Las &lt;a href=&#34;http://es.wikipedia.org/wiki/Ciclodextrina&#34;&gt;ciclodextrinas&lt;/a&gt; son estructuras cíclicas que contienen grupos terminales hidroxilos que pueden formar puentes de hidrógeno con 4MP o moléculas de agua. A temperatura ambiente, hasta 300 gramos de αCD pueden disolverse en un litro de 4MP. La solución resultante es homogénea y transparente, pero se transforma en un sólido blanco lechoso cuando se calienta. La temperatura a la que cambia de fase disminuye a medida que la concentración de αCD aumenta.&lt;/p&gt;
&lt;p&gt;Estudios de dispersión de neutrones revelaron que la fase sólida es un sistema &amp;quot;sol-gel&amp;quot; en el que la formación de puentes de hidrógeno entre αCD y 4MP producen una estructura rígida ordenada. A menores temperaturas, sin embargo, los puentes de hidrógeno tienden a romperse lo que resulta en que la solución vuelve a la fase líquida. Simulaciones de &lt;a href=&#34;http://es.wikipedia.org/wiki/Din%C3%A1mica_molecular&#34;&gt;dinámica molecular&lt;/a&gt; realizadas por Plazenet y sus colaboradores confirmaron que los anillos de ciclodextrinas se distorsionan a medida que la temperatura se aproxima al punto de solidificación. Los puentes de hidrógeno internos en αCD se rompen y los grupos hidroxilos rotan hacia afuera, lo que permite la formación de una red de uniones entre diferentes moléculas. El equipo ha encontrado que un número de sistemas ciclodextrinas/piridinas también se solidifican al calentarse, y ahora se encuentra estudiando con más detalle el sistema sol-gel para entender mejor el mecanismo de solidificación.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; &lt;a href=&#34;http://www.ncbi.nlm.nih.gov/pubmed/15352791&#34;&gt;M Plazanet et al. 2004 J. Chem. Phys., 121 5031.&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Uno de los derechos humanos es conocer la composición de nuestros alimentos</title>
      <link>https://ciencianet.com.ar/post/uno-de-los-derechos-humanos-es-conocer-la-composicion-de-nuestros-alimentos/</link>
      <pubDate>Fri, 09 Oct 2009 21:03:55 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/uno-de-los-derechos-humanos-es-conocer-la-composicion-de-nuestros-alimentos/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Guillermo Bibiloni.&lt;/strong&gt; Departamento de Física, Facultad de Ciencias Exactas, UNLP.&lt;/p&gt;
&lt;p&gt;La radiación ambiental se origina a partir de un gran número de fuentes naturales y antropogénicas o creadas por el hombre. Los elementos radiactivos naturales que se hallan en el aire, el agua y el suelo y pueden clasificarse en: primordiales, creados antes de la formación de la Tierra, y cosmogénicos, formados como resultados de reacciones entre los núcleos de los gases que constituyen la atmósfera y los rayos cósmicos que inciden sobre ella. Ejemplos del primer tipo son el 40K, el 238U, el 235U y el 232Th y sus hijos, productos de los procesos de decaimiento. Por otro lado, el 7Be, 14C y el 3H, son ejemplos de radioisótopos de origen cosmogénico.&lt;/p&gt;
&lt;h3 id=&#34;vivimos-en-un-mundo-radioactivo-y-también-lo-son-nuestros-alimentos&#34;&gt;Vivimos en un mundo radioactivo y también lo son nuestros alimentos&lt;/h3&gt;
&lt;p&gt;La radioactividad de la corteza terrestre es principalmente debida a la presencia de 40K y a los que se suman los radionucleídos de las tres series naturales (238U, 235U y 232Th). Estos nucleidos están en todo tipo de suelos y aguas. Los niveles específicos de radiación terrestre ambiental están relacionados con la composición geológica de cada área separada litológicamente y al contenido natural de radionucleídos en rocas a partir de los cuales los suelos fueron originados. De modo que, diferencias sobresalientes en la radioactividad natural de muestras ambientales, tales como agua y suelos, existen en relación al origen geológico de los suelos.&lt;/p&gt;
&lt;p&gt;La distribución de equilibrio de los isótopos naturales puede ser perturbada cuando materiales naturales que contienen isótopos radioactivos son utilizados como materia prima por diversas industrias convencionales (concentración o refinamiento de minerales, petróleo y agua para aplicaciones, centrales de carbón, etc.), o nucleares. De esta manera, los elementos radioactivos presentes, tales como Ra, U y Th pueden ser concentrados en los productos, subproductos, residuos y efluentes debido a la actividad humana. Así, los efluentes y residuos modificarían el nivel de actividad por sobre los valores naturales.&lt;/p&gt;
&lt;p&gt;El advenimiento de la era nuclear, a mediados del siglo pasado, trajo aparejada la producción de nuevos radioisótopos, ahora creados por el hombre, como por ejemplo el 137Cs y el 90Sr. Estos radioisótopos antropogénicos agregaron globalmente cantidades pequeñas al inventario de radiactividad. Cuando tienen lugar ensayos de armas nucleares o accidentes vinculados a la industria nuclear, enormes cantidades de partículas radiactivas son liberadas en la atmósfera, las cuales, arrastradas por los vientos, recorren largas distancias para luego precipitar y diseminarse sobre grandes extensiones de la Tierra.&lt;/p&gt;
&lt;p&gt;Este fenómeno, denominado lluvia radioactiva, produce el depósito de elementos radiactivos sobre los pastos, las hojas de las plantas, los suelos, las aguas y los sedimentos. Esta contaminación fue objeto de interés internacional, lo que devino en una variedad de mediciones de los niveles de radiactividad depositados sobre la superficie de la Tierra. Vivimos entonces en un mundo radioactivo en el cual los niveles de radiación natural son diferentes, en general, en cada región del planeta. Los seres vivos también son radioactivos ya que el aire que respiran y los alimentos que consumen contienen radionucleídos los cuales quedan alojados en los tejidos.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/10/vaca.jpg&#34; alt=&#34;:left&#34; title=&#34;vaca&#34;&gt; &lt;strong&gt;Nuestros alimentos son radioactivos pero, ¿están contaminados radioactivamente?&lt;/strong&gt; ¿Cómo saber, entonces, si un alimento, hoy y aquí, tiene niveles de nucleídos por encima de los niveles naturales o si contiene algún isótopo antropogénico? Lo ideal es determinar los constituyentes radiactivos en condiciones “normales” para determinar la línea de base y disponer de “un observatorio de control” capaz de comparar estos resultados. En el Grupo de Investigación y Servicios de Radioactividad Natural y Medio Ambiente, Departamento de Física-Facultad de Ciencias Exactas-UNLP, IFLP-CONICET (GISDRAMA) se llevan adelante un conjunto de proyectos destinados a la construcción de estas líneas de base. En particular, los autores del trabajo que aquí comento, &lt;em&gt;Activity levels of gamma-emitters in Argentinean cow milk&lt;/em&gt; (&lt;em&gt;Journal of Food Composition and Analysis, Volume 22, Issue 3, May 2009, Pages 250-253&lt;/em&gt;), pretende justamente esto.&lt;/p&gt;
&lt;p&gt;Los autores, integrantes del GISDRAMA, hacen un relevamiento de la radioactividad presente en leche vacuna de las diferentes cuencas lecheras de la Argentina y los registran para establecer niveles de base. Como dato interesante estudian también muestras de leche de nuestros vecinos, Chile y Uruguay. Para ello determinaron sistemáticamente la concentración de núcleos emisores de radiación electromagnética (radiación gamma) en más de treinta muestras de leche tomadas entre los años 2000 y 2007 de diferentes cuencas lecheras. Estos experimentos se llevaron a cabo utilizando un espectrómetro de radiación gamma de alta resolución.&lt;/p&gt;
&lt;p&gt;Los resultados mostraron que en condiciones normales, las leches argentinas contienen en promedio 60 Bq/l del nucleído natural 40K. La dosis efectiva comprometida, determinada considerando el consumo anual de leche en Argentina (219 l por persona) junto con el coeficiente de dosis y la pirámide poblacional, indican que los valores de dosis efectiva es cercana a los valores recomendados por el Comité Internacional de Radioprotección y la Comité Científico sobre los Efectos de Radiación Atómica de las Naciones Unidas. Por otra parte, las actividades de los elementos de los nucleídos pertenecientes a las cadenas naturales del U y del Th están por debajo del límite de detección (2 Bq/l y 1 Bq/l, respectivamente).&lt;/p&gt;
&lt;p&gt;Con referencia a los nucleídos antropogénicos, sólo se detectaron niveles de actividad de 137Cs por encima del límite de detección (0.06 Bq/l) en la muestra de leche chilena. Estos valores son compatibles con la lluvia radioactiva proveniente de los ensayos nucleares realizados en el Pacífico Sur durante décadas pasadas.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;¿Cómo seguir adelante y detectar eventuales contaminaciones radioactivas?&lt;/strong&gt; Pero como siempre, detrás de una publicación hay una idea rectora. Y que mejor que transcribir palabras de la propia Judith Desimoni y sus colaboradores: “La radioactividad no se detecta por ninguno de los cinco sentidos humanos pero es omnipresente. Sin embargo, la sola mención de la palabra radioactividad genera temor en la población debido a la mala prensa que se ha ganado por los accidentes en reactores nucleares y a las explosiones de Nagasaki e Hiroshima, por ejemplo&amp;quot;.&lt;/p&gt;
&lt;p&gt;&amp;quot;Entonces, cómo convivir sin temor con la producción de energía nucleoeléctrica, las aplicaciones industriales y médicas de los nucleídos y el Plan Estratégico Nuclear de nuestro país? Por un lado, alfabetizando científicamente a la población y por otro haciendo controles medioambientales y alimenticios sistemáticos y periódicos. El primer punto es un trabajo que conecta a los docentes-investigadores y la sociedad mediante el mejoramiento del conocimiento de la población de los efectos de la exposición a bajas dosis de radiaciones ionizantes. En efecto, nuestro laboratorio es un &lt;strong&gt;&#39;laboratorio de puertas abiertas&#39;&lt;/strong&gt; dedicado a la recepción de alumnos y docentes de escuelas, y de las distintas carreras de grado y postgrado de la UNLP, organizaciones barriales y no gubernamentales. Estos actores son utilizados como multiplicadores de la información. En lo que respecta al monitoreo, tratamos de contribuir mediante el desarrollo de programas de control gerenciados por laboratorios constituidos en universidades nacionales, tal como en Europa. Nos enorgullece afirmar que somos el primer laboratorio universitario con estos objetivos del país! Pero obviamente somos conscientes de que nuestra contribución es sólo un granito de arena en una playa.&amp;quot;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Artículo original:&lt;/strong&gt; J. Desimoni, F. Sives, L. Errico, G. Mastrantonio, M.A. Taylor, &lt;a href=&#34;https://doi.org/10.1016/j.jfca.2008.10.024&#34;&gt;Activity levels of gamma-emitters in Argentinean cow milk&lt;/a&gt;, Journal of Food Composition and Analysis, vol 22, pp 250-253&lt;/p&gt;
&lt;p&gt;**Instituciones:**Departamento de Física, Facultad de Ciencias Exactas, Universidad Nacional de La Plata, Instituto de Física La Plata (CONICET), LaSeISiC, Facultad de Ciencias Exactas, Universidad Nacional de La Plata (CIC–CONICET), Toxicología de Alimentos, Departamento de Química, Universidad Nacional de La Pampa.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; Leo Errico (E-mail: &lt;a href=&#34;mailto:errico@fisica.unlp.edu.ar&#34;&gt;errico@fisica.unlp.edu.ar&lt;/a&gt;), Guillermo Bibiloni (E-mail: &lt;a href=&#34;mailto:bibiloni@fisica.unlp.edu.ar&#34;&gt;bibiloni@fisica.unlp.edu.ar&lt;/a&gt;)&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Monopolos magnéticos y cuerdas de Dirac en un material</title>
      <link>https://ciencianet.com.ar/post/monopolos-magneticos-y-cuerdas-de-dirac-en-un-material/</link>
      <pubDate>Sun, 13 Sep 2009 21:05:17 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/monopolos-magneticos-y-cuerdas-de-dirac-en-un-material/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro&lt;/strong&gt;, Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP) y Universidad Tecnológica Nacional.&lt;/p&gt;
&lt;p&gt;Cualquiera que ha jugado con un imán sabe que tiene dos polos, norte y sur, y que no importa en cuantos pedazos lo rompa, cada uno de ellos también tendrá un polo norte y uno sur. Todos los imanes que conocemos –todas las formas de magnetismo que conocemos- están basadas en dipolos magnéticos, es decir, en componentes elementales que tienen los dos polos. Sin embargo, sabemos que para el caso de la electricidad hay cargas positivas y negativas (protones y electrones por ejemplo) que podemos encontrar en forma completamente separada.&lt;/p&gt;
&lt;p&gt;Esta asimetría –la aparente inexistencia de cargas magnéticas separadas, “norte” o “sur” (mono-polos en lugar de di-polos)- ha sido un interrogante para la física moderna. Varias teorías predicen partículas elementales con carga magnética única, los monopolos magnéticos. En 1931, el físico Paul Dirac llego a la conclusión de que los monopolos magnéticos elementales deben existir, y los describió asociados a una líneas solenoidales, las cuerdas de Dirac, que llevan flujo magnético. Estas cuerdas se extienden al infinito, o conectan dos monopolos de carga opuesta.&lt;/p&gt;
&lt;p&gt;En forma más reciente, en un trabajo teórico liderado por Roderich Moessner, de Dresden (Alemania), se propuso la aparición de monopolos como excitaciones colectivas en una clase de sistemas magnéticos frustrados, los llamados hielos de spin. Estos monopolos “emergentes” difieren de los elementales de Dirac en que no tienen existencia fuera del material magnético, en que sus cargas son menores que las de Dirac y en que sus cuerdas asociadas son observables. Hasta ahora no existía ninguna evidencia experimental reproducible de la existencia de monopolos, ni en forma elemental, ni como partículas emergentes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/09/monopolosM.jpg&#34; alt=&#34;:left&#34;&gt;
El experimento liderado por Tennant y Grigera se basa en el estudio por distintas técnicas de un material, el titanato de disprosio, que a temperaturas por debajo de 1 K (aproximadamente -272 °C) se comporta como un hielo de spin. Usando la técnica de difracción de neutrones, se determinó que los momentos magnéticos dipolares de este material se reorganizan formando una especie de spaghetti magnético (ver figura). El nombre viene de la manera en la que los dipolos se ordenan formando tubos contorsionados por las que se transporta flujo magnético (las cuerdas de Dirac).&lt;/p&gt;
&lt;p&gt;Los neutrones tienen momento magnético, y por lo tanto sufren la influencia de campos magnéticos externos. Si un haz de neutrones atraviesa un material magnético, los neutrones sufren distintas deflexiones dependiendo de los campos magnéticos que encuentran en su camino. De las características del haz resultante, y mirando varios haces apuntados en distintas direcciones, se puede reconstruir la distribución de campo magnético dentro del material (ver figura). De esta manera, analizando en pantallas detectoras los neutrones inyectados a través de titanato de disprosio a temperaturas debajo de 1 K, este grupo encontró evidencia de la existencia de las cuerdas de campo magnético. La aplicación de un campo magnético externo permitió “peinar” estas cuerdas, estirándolas en una dirección. De esta manera es posible reducir su densidad y promover la disociación de los monopolos que existen en sus puntas. Como resultado, fue posible observar monopolos magnéticos unidos por cuerdas de flujo.&lt;/p&gt;
&lt;p&gt;Otras características de este gas de monopolos fueron observadas con medidas de magnetización y calor específico. Estas proveyeron confirmación de la existencia de los monopolos y mostraron que interactúan en una manera similar a las cargas eléctricas (un equivalente a la ley de Coulomb). Hay aspectos del problema que van más allá de la ciencia básica. Como explica Jonathan Morris, uno de los miembros del grupo de Berlin, y primer autor en el trabajo: &amp;quot;Estamos escribiendo sobre propiedades nuevas y fundamentales de la materia. Estas propiedades son válidas en general para materiales de la misma topología, es decir, para momentos magnéticos en una red de pirocloro. Esto puede tener implicancias importantes en el desarrollo de tecnologías magnéticas. Sobre todo, para nosotros, significa que por primera vez hemos observado fraccionalización en tres dimensiones&amp;quot;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; &lt;a href=&#34;https://doi.org/10.1126/science.1178868&#34;&gt;Science&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; S. A. Grigera, &lt;a href=&#34;mailto:sag@iflysib.unlp.edu.ar&#34;&gt;sag@iflysib.unlp.edu.ar&lt;/a&gt; Instituto de Física de Líquidos y Sistemas Biológicos, La Plata, +54 (0)221 4233283.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>La música de las esferas</title>
      <link>https://ciencianet.com.ar/post/la-musica-de-las-esferas/</link>
      <pubDate>Fri, 28 Aug 2009 21:08:39 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/la-musica-de-las-esferas/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Nicolás Grandi:&lt;/strong&gt; Instituto de Física La Plata.&lt;/p&gt;
&lt;p&gt;Texto publicado en el libro &lt;strong&gt;&amp;quot;Cero absoluto- Curiosidades de Física&amp;quot;&lt;/strong&gt;, escrito por docentes del Museo de Física UNLP (Editorial IFLP - Conicet, 2005). Publicado con autorización de la Editorial.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/08/MUSICA_LLORET-355x500.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Es posible que el origen del pensamiento científico se encuentre en la visión del cielo nocturno. La perturbación causada por el movimiento de los planetas a través de la de otro modo inmutable belleza de la Vía Láctea, provocó en los antiguos observadores la necesidad de encontrar alguna regularidad o “ley” que ordenara esos movimientos. Esto llevo a &lt;a href=&#34;http://es.wikipedia.org/wiki/Pit%C3%A1goras&#34;&gt;Pitágoras&lt;/a&gt; y sus seguidores en la antigua Grecia a la idea de la “Música de las Esferas”, una visión del mundo que equiparaba al Universo con una melodía tocada por los cuerpos celestes, a cuyas “notas” obedecía todo lo existente, desde las estrellas y los planetas hasta la más humilde de las gotas de lluvia.&lt;/p&gt;
&lt;p&gt;Podríamos decir que los Pitagóricos buscaron en los cielos las reglas que regían el mundo cotidiano. Sin embargo, la posterior comprensión de las leyes del movimiento de los objetos, desarrollada por &lt;a href=&#34;http://es.wikipedia.org/wiki/Galileo_Galilei&#34;&gt;Galileo&lt;/a&gt; y luego por &lt;a href=&#34;http://es.wikipedia.org/wiki/Isaac_Newton&#34;&gt;Newton&lt;/a&gt;, y su aplicación a los cuerpos celestes, dejaron la idea de Pitágoras en el olvido. Así surgió una visión completamente opuesta, donde son los constituyentes elementales o “más pequeños” los que siguen determinadas leyes sencillas, cuyas consecuencias afectan al universo todo. En esta visión, el Universo se parece a un edificio, a cuyos “ladrillos” llamamos partículas elementales y cuya “arquitectura” sigue reglas bien determinadas.&lt;/p&gt;
&lt;p&gt;De esta manera, la explicación de la complejidad de la química mediante constituyentes en movimiento dio sustento a la antigua idea de átomo. Más adelante, la comprensión de la estructura y variedad de los átomos mismos llevó a la introducción de los neutrones, protones y electrones, los cuales combinados en distintas formas y números forman los diversos elementos. Dando “un paso más”, la estructura de protones y neutrones se explicó en términos de partículas aún más elementales: quarks, gluones, etc.&lt;/p&gt;
&lt;p&gt;La riqueza de este “edificio” es intrigante. La pregunta de por qué existen tantos tipos diferentes de “ladrillos” y qué es finalmente lo que determina las reglas adecuadas para “apilarlos” no parece tener respuesta. Es por esto que en las últimas décadas una idea innovadora ha ganado lugar en el pensamiento científico. Según esta nueva visión, los constituyentes últimos o “más elementales” del universo, serían microscópicas “cuerdas” idénticas, y la única manera de construir cosas con ellas sería “cortarlas y unirlas por sus extremos”.&lt;/p&gt;
&lt;p&gt;Sorprendentemente, en contra de lo que se podría imaginar, este esquema tan simple permite explicar y comprender en profundidad una gran cantidad de fenómenos, desde las interacciones nucleares hasta la gravedad. Este escenario plantea la siguiente pregunta: si nos hemos convencido de que el mundo está hecho de un enorme zoológico de partículas elementales diferentes, ¿Cómo podríamos construirlas con un solo tipo de cuerdas? La respuesta es tan simple como sugestiva: cada tipo de partícula elemental no sería más que cuerdas idénticas a las otras, pero vibrando en un tono diferente. De este modo cada “nota” corresponde a una de las especies de partículas que constituyen el mundo, desde los electrones y quarks, pasando por los fotones que constituyen la luz, hasta los gravitones que guían el movimiento de la Vía Láctea.&lt;/p&gt;
&lt;p&gt;De este modo, la idea Pitagórica de Música de las Esferas parece haber retornado de manera inesperada.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Más vale MAV en compu que cien volando</title>
      <link>https://ciencianet.com.ar/post/mas-vale-mav-en-compu-que-cien-volando/</link>
      <pubDate>Mon, 06 Jul 2009 21:25:28 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/mas-vale-mav-en-compu-que-cien-volando/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Ramiro Irastorza:&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP).&lt;/p&gt;
&lt;p&gt;Desde la década pasada existe un gran interés por los micro-vehículos aéreos autónomos (MAVs). La meta de los investigadores es desarrollar una nueva generación de MAVs que posean una dimensión máxima de 15 cm, un peso límite de 100 gr, y una autonomía máxima de mas de 90 minutos (los MAVs existentes no se superan los 30 minutos).&lt;/p&gt;
&lt;p&gt;Recientes investigaciones, inspiradas en el vuelo de insectos, muestran que el uso de alas flexibles, la correcta cinemática, y la elección de la frecuencia de aleteo son factores importantes para optimizar el consumo de energía. Sergio Preidikman, Julio Massa, y Bruno Roccia de la Universidad Nacional de Córdoba, junto a Balakumar Balachandran, Elias Balaras, Marcos Vanella, Timothy Fitzgerald, y Marcelo Valdez de la Universidad de Maryland (USA) han atacado este problema observando la naturaleza; particularmente algunos insectos voladores y ciertas aves pequeñas.&lt;/p&gt;
&lt;p&gt;La razón para inspirarse en la naturaleza parece evidente: los insectos son pequeños, livianos y al parecer muy eficientes a la hora de usar su energía. Aunque los biólogos han estudiado mucho el tema, existen varios aspectos no resueltos en el vuelo con alas batientes, particularmente, el conocido como “hovering” que corresponde a mantenerse suspendido volando en el mismo lugar. En este trabajo, los investigadores intentan explicar mediante simulaciones numéricas realizadas con la ayuda de computadoras la influencia del complejo flujo de aire alrededor de las alas, y de la flexibilidad de las mismas en el rendimiento aerodinámico de cada ciclo de aleteo.&lt;/p&gt;
&lt;p&gt;En su evolución natural, los insectos voladores y los pájaros siguieron dos caminos diferentes. Si bien ambos vuelos se basan en alas batientes, la mayoría de las aves no pueden ejecutar ciertos tipos de vuelo como el “hovering”. Un punto curioso y complejo es que en la mayoría de los insectos las alas carecen de músculos y por consiguiente no poseen “actuadores” que permitan el control interno de la misma. En esta dirección apunta la pregunta que se proponen responder los investigadores: ¿Cómo afecta la performance aerodinámica de un ciclo de aleteo la flexibilidad estructural del ala y cuál es el efecto del número de Reynolds del flujo de aire?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/07/ramiro.png&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;El &lt;a href=&#34;http://es.wikipedia.org/wiki/N%C3%BAmero_de_Reynolds&#34;&gt;número de Reynolds&lt;/a&gt; (Re) es un número adimensional que caracteriza el flujo: un Re grande corresponde a flujos donde dominan las fuerzas de inercia (proporcionales a la velocidad y a la densidad del flujo), mientras que un número Re bajo es típico de flujos donde dominan las fuerzas disipativas (proporcionales a la viscosidad del flujo). Para comenzar a entender el fenómeno aeroelástico asociado a un ala flexible, los investigadores desarrollaron, en una primera etapa, simulaciones numéricas en dos dimensiones. El modelo estructural bidimensional del ala consiste de dos barras rígidas &lt;em&gt;A&lt;/em&gt; y &lt;em&gt;B&lt;/em&gt; unidas en un extremo por un resorte torsional (&lt;em&gt;kt&lt;/em&gt;) con comportamiento lineal (ver Figura).&lt;/p&gt;
&lt;p&gt;La flexibilidad se encuentra concentrada en un solo punto del sistema, esto es, la rotula elástica donde se unen las dos barras rígidas &lt;em&gt;A&lt;/em&gt; y &lt;em&gt;B&lt;/em&gt;. El flujo del fluido circundante se considera no lineal e inestacionario, y se estudia para diferentes números de Reynolds bajos y moderados (&lt;em&gt;Re&lt;/em&gt; = 75, 250 y 1000).&lt;/p&gt;
&lt;p&gt;El movimiento del aleteo de este sistema dinámico está gobernado por un conjunto de ecuaciones que tienen en cuenta la estructura mecánica del sistema y el fluido circundante. En los ciclos de aleteo simulados, se imprimen movimientos traslacionales al brazo B, forzándolo a una oscilación de una frecuencia particular, y se deja libre al brazo &lt;em&gt;A&lt;/em&gt;. La dinámica del sistema puede describirse utilizando sólo una coordenada generalizada: el ángulo de deflexión &lt;em&gt;α&lt;/em&gt;(&lt;em&gt;t&lt;/em&gt;) (ver Figura). También se puede demostrar que &lt;em&gt;α&lt;/em&gt;(&lt;em&gt;t&lt;/em&gt;) está gobernado por una ecuación equivalente a la que gobierna las evolución temporal de un &lt;a href=&#34;http://es.wikipedia.org/wiki/Doble_p%C3%A9ndulo&#34;&gt;péndulo doble&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Para evaluar los resultados de este modelo, los autores “miden” en la simulación algunos parámetros aerodinámicos (tales como el coeficiente de sustentación en función de la frecuencia) y los comparan con los obtenidos a partir de una ala rígida (usando solo una barra &lt;em&gt;B&lt;/em&gt;). Ha habido especulaciones de que muchos insectos aletean a frecuencias cercanas a la frecuencia natural de la estructura del ala. La frecuencia natural de una estructura es la frecuencia principal con que queda vibrando si se le da un golpe. Esto sugiere que los insectos tomarían ventaja de una &lt;a href=&#34;http://es.wikipedia.org/wiki/Resonancia_(mec%C3%A1nica)&#34;&gt;resonancia&lt;/a&gt; (amplificación del movimiento que se da al oscilar una estructura con una frecuencia igual a su frecuencia natural) del ala para reducir el consumo de energía y aumentar la performance aerodinámica. A pesar de que se probaron varios números de Reynolds, las simulaciones se desestabilizan al operar a frecuencias cercanas a la de resonancia. Esto es posible que se deba a que se concentra la flexibilidad de la estructura en un solo punto (los autores intuyen que se mejoraría con el reemplazo de las barras rígidas por unas barras elásticas).&lt;/p&gt;
&lt;p&gt;Como resultado notorio, en este trabajo se encontró que en el rango de frecuencias por debajo de la primer frecuencia natural, la mejor performance se alcanza cuando el ala vibra a una frecuencia cercana a una de las resonancias no-lineales del sistema; en este caso se trata de la resonancia superarmónica de orden tres. Esto es, cuando la frecuencia de oscilación del brazo B (movimiento forzado) llega a un tercio del valor de la frecuencia natural del ala. La flexibilidad entonces puede ser beneficiosa en términos de incrementar la performance aerodinámica. Este comportamiento es común a todos los números de Reynolds investigados.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; &lt;em&gt;Influence of flexibility on the aerodynamic performance of a hovering wing&lt;/em&gt;, &lt;a href=&#34;https://doi.org/10.1242/jeb.016428&#34;&gt;Journal of Experimental Biology&lt;/a&gt;, 212, pp. 95-105 (2009). &lt;em&gt;De la biología a los insectos robots: Desarrollo de un código computacional interactivo para estudiar la cinemática de alas batientes&lt;/em&gt;, Mecánica Computacional, 27, pp. 3041-3058 (2008).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instituciones:&lt;/strong&gt; Departamento de Estructuras FAMAF, UNC (Argentina). Departament of Mechanical Engineering, University of Maryland (EEUU).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; Dr. Sergio Preidikman (E-mail: &lt;a href=&#34;mailto:spreidik@umd.edu&#34;&gt;spreidik@umd.edu&lt;/a&gt;)&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>El ciclo de vida de los plaguicidas organofosforados en los Agronegocios para la producción sustentable en la Argentina</title>
      <link>https://ciencianet.com.ar/post/el-ciclo-de-vida-de-los-plaguicidas-organofosforados-en-los-agronegocios-para-la-produccion-sustentable-en-la-argentina/</link>
      <pubDate>Fri, 03 Jul 2009 21:27:23 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/el-ciclo-de-vida-de-los-plaguicidas-organofosforados-en-los-agronegocios-para-la-produccion-sustentable-en-la-argentina/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Javier Quagliano&lt;/strong&gt;. Instituto de Investigaciones y Desarrollo del Ministerio de Defensa.&lt;/p&gt;
&lt;p&gt;La Argentina es un país productor de alimentos y agroexportador por excelencia. La cosecha actual de productos agropecuarios de origen vegetal llegó en 2007 a 100 millones de toneladas. Argentina es el octavo país productor de alimentos del mundo, los productos agroalimentarios representan más de la mitad de las ventas por exportaciones de nuestro país. Es por ello que el uso de plaguicidas es una necesidad para la protección de sus cultivos. Se ha estimado que las pérdidas producidas por acción de las plagas llegan hasta el 30% de la producción, en algunos casos.&lt;/p&gt;
&lt;p&gt;Actualmente, el mercado internacional de los plaguicidas está concentrado en 10 compañías, que representan el 80% del mercado mundial de agroquímicos. Esto representa más de US$ 80.000 millones, de los cuales el 25% de las ventas se realizan en países en desarrollo. El uso de plaguicidas en nuestro país está en continuo aumento. Aún a pesar de la reducción en las importaciones de plaguicidas de nueva generación como consecuencia de la crisis de 2001, el uso de plaguicidas aumenta, en parte a expensas de la producción nacional que ha aumentado desde entonces, en un claro ejemplo de sustitución de importaciones. Entre los cuatro primeros de mayor uso, dos de ellos son organofosforados: el glifosato y el clorpirifós.&lt;/p&gt;
&lt;h3 id=&#34;los-organofosforados&#34;&gt;Los Organofosforados&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/07/392px-Soybeans.post_.jpg&#34; alt=&#34;Fuente: Wikimedia Commons.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;El conocimiento de los compuestos organofosforados data ya de 1820 a partir de los trabajos de Lassaigne quien utilizó ácido ortofosfórico y alcoholes para obtener los primeros. En 1854 Clermont sintetiza el tetraetilpirofosfato (TEPP), aunque recién sus propiedades insecticidas fueron advertidas 80 años después. Sólo llegado el siglo XX comenzaron las investigaciones serias sobre compuestos organofosforados. Las primeras investigaciones fueron llevadas a cabo por Saunders en Inglaterra y por Schrader en Alemania. En 1934 se le asignó al Profesor Schrader, de la IG Farben, la tarea de desarrollar un pesticida. Dos años más tarde, un compuesto de fósforo de extremadamente alta toxicidad fue producido por primera vez. De acuerdo con las regulaciones militares de aquella época, todo compuesto químico producido de muy alta toxicidad debía ser reportado a las autoridades. Esto fue hecho por Schrader, quien llamó a este compuesto “Tabun”, que fue la primera de las sustancias conocidas luego como agentes de guerra química o agentes nerviosos.&lt;/p&gt;
&lt;p&gt;Esta tecnología bélica fue luego aplicada en áreas civiles al campo de protección de cultivos, utilizándose compuestos similares aunque mucho menos tóxicos como plaguicidas. Posteriormente, desde los cuarenta a los sesenta, se desarrollaron y utilizaron compuestos organoclorados, de los cuales el más emblemático fue el dicloro difenil tricloretano o DDT, utilizado para el control de la malaria en zonas tropicales y en conflictos bélicos por su gran efectividad. Con el tiempo se observó que los pesticidas organoclorados tienen gran persistencia y se acumulan en los tejidos grasos de animales y el hombre. Afectan en forma crónica al sistema nervioso central (depresión, narcosis), así como al hígado y al riñón. Es así que, a pesar de su toxicidad aguda en animales, se introdujeron los fosforados para el control de los insectos en los cultivos, debido a su baja persistencia en el medio ambiente (días hasta varias semanas) y a sus beneficios para el control de insectos.&lt;/p&gt;
&lt;p&gt;Décadas atrás, el paratión fue el plaguicida más utilizado en la agricultura. Sin embargo, por su elevada toxicidad su uso está totalmente restringido (junto con el metil-paratión) y sujeto al procedimiento de consentimiento fundamentado previo del &lt;a href=&#34;http://es.wikipedia.org/wiki/Convenio_de_R%C3%B3tterdam&#34;&gt;Convenio de Rotterdam&lt;/a&gt; (que regula los movimientos internacionales de algunos químicos peligrosos). Otros organofosforados, algo menos tóxicos se siguen usando, como el malatión, dimetoato y clorpirifós, pero lentamente son reemplazados por otras nuevas moléculas, como los piretroides o bipiridinilos.&lt;/p&gt;
&lt;p&gt;Dado que los plaguicidas organofosforados tienen alta toxicidad (muchos están prohibidos en países del Primer Mundo) es prioritario balancear los beneficios respecto de sus efectos negativos, de modo que la producción nacional de agroalimentos sea sustentable. Por ello es de interés minimizar los efectos tóxicos derivados del uso de este tipo de plaguicidas. Como se adelantó, su mecanismo de acción es el de inhibir la enzima acetilcolinesterasa, encargada de la degradación del exceso de acetilcolina en la sinapsis entre células nerviosas, por lo que son tóxicos agudos para los animales y seres humanos. Numerosos reportes indican que plaguicidas prohibidos o restringidos son exportados a países en desarrollo en forma irregular, por lo que es menester estudiarlos también desde el punto de vista legal y sanitario.&lt;/p&gt;
&lt;p&gt;Adicionalmente, las impurezas producidas como subproductos de manufactura deficiente o del almacenamiento de los plaguicidas en condiciones adversas o prolongadas tienen alta toxicidad, aún en muy bajas concentraciones. Es por ello que no deben superar los límites de concentración debidamente establecidos por las reglamentaciones vigentes. Al respecto el Servicio Nacional de Sanidad y Seguridad Agroalimentaria (&lt;a href=&#34;http://www.senasa.gov.ar/&#34;&gt;SENASA&lt;/a&gt;) es el ente encargado de verificar el cumplimiento de la resolución 350/99 que fija los límites máximos de las impurezas más críticas que pueden acompañar a los plaguicidas.&lt;/p&gt;
&lt;p&gt;Varios organismos estudian el efecto de los pesticidas en general en el medio ambiente, como la Cámara de Sanidad Agropecuaria y Fertilizantes (Casafe), la Cámara Ciara, Universidades y empresas. Desde su rol de Instituto de Investigaciones y Desarrollo del Ministerio de Defensa, el CITEDef (ex CITEFA) tiene una larga experiencia de décadas en el estudio de compuestos tóxicos, particularmente pesticidas piretroides y también compuestos organofosforados. Estos últimos tienen diversos usos, entre ellos algunos con implicancias para la seguridad nacional e internacional como ser la de su uso como armas químicas, que son químicamente muy similares a los pesticidas organofosforados.&lt;/p&gt;
&lt;p&gt;Desde 1997 la Organización para la Prohibición de Armas Químicas (&lt;a href=&#34;http://www.opcw.org/sp/&#34;&gt;OPCW&lt;/a&gt;) viene destruyendo el stock de las mismas en todo el mundo, para finalizar la destrucción total aproximadamente para el año 2012. Respecto de la evaluación de los riesgos para la salud humana por el uso y liberación de plaguicidas, deben seguirse una serie de lineamientos que aseguren un conocimiento apropiado de la situación particular. De acuerdo con la Organización Panamericana de la Salud (OPS), debe caracterizarse el lugar donde ocurre la exposición, las poblaciones potencialmente expuestas, las posibles rutas de exposición, debe cuantificarse la exposición, identificar las incertidumbres y finalmente evaluar todo el conjunto de la información.&lt;/p&gt;
&lt;h3 id=&#34;uso-de-organofosforados-en-la-argentina&#34;&gt;Uso de organofosforados en la Argentina&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/07/800px-Sembrado_de_soja_en_argentina-768x576.jpg&#34; alt=&#34;Sembrado de soja. Fuente: Wikimedia Commons.&#34;&gt;&lt;/p&gt;
&lt;p&gt;El metil azinfós es un fosforado muy usado en el Alto Valle del Río Negro en plantaciones frutales. El clorpirifós es más usado en cereales, ocupa el cuarto lugar entre los pesticidas utilizados en nuestro país, luego del glifosato, 2,4-D y la atrazina. El metil azinfós es uno de los más tóxicos pesticidas organofosforados, la administración repetitiva de 1,5 mg/día lleva a la muerte en humanos. Su uso está restringido por el SENASA.&lt;/p&gt;
&lt;p&gt;Los organofosforados son plaguicidas de baja persistencia en el medio ambiente, su persistencia es de días o semanas. Se hidrolizan a pH alto o bajo y también son degradados por la luz de sol. Su Ciclo de Vida comienza con la fabricación llegando hasta su uso final en las cosechas. Cuando se utilizan productos vencidos, hay riesgo de introducir impurezas que son mucho más tóxicas que el pesticida de origen (por ejemplo, los llamados derivados “oxon” del pesticida original). Adicionalmente, luego del uso, los envases en los que se comercializan deben ser idealmente destruídos o bien sometidos a un proceso de triple lavado, para evitar intoxicaciones por el reciclado de los bidones. Estos suelen ser de construcción muy sólida y son muy atractivos para su reutilización. Casafe viene bregando por la implementación de este sistema junto con el programa Agrolimpio, organizando cursos y seminarios de difusión en todo el país.&lt;/p&gt;
&lt;p&gt;En las últimas décadas se afianzó en la Argentina el sistema de cultivo denominado siembra directa, que no requiere roturación previa del suelo. Este sistema de cultivo está intimamente asociado al herbicida glifosato, ya que requiere la aplicación del mismo para controlar las malezas. El glifosato es químicamente una fosfonometil glicina, es clasificado como de toxicidad relativamente baja, en la clase III por la Environmental Protection Agency (&lt;a href=&#34;http://www.epa.gov/espanol/&#34;&gt;EPA&lt;/a&gt;) de EUA. Tiene baja toxicidad aguda, estudios en ratones indican que debe administrarse una dosis de 5.6 g por kilo de peso del animal para matar a la mitad de la población de ratones ensayada, lo cual indica baja toxicidad. No se observaron efectos luego de administrarlo a ratones y perros durante 2 años seguidos. Su toxicidad para mamíferos y peces es considerada baja.&lt;/p&gt;
&lt;p&gt;Sin embargo, un estudio reciente reportado en la revista científica “Chemical Research in Toxicology”, señaló que formulaciones del producto comercial &amp;quot;Round Up&amp;quot; de la firma Monsanto y productos del metabolismo del mismo produjeron la muerte de células humanas de placenta, embriones y umbilicales &lt;a href=&#34;#1&#34; title=&#34;1&#34;&gt;[1]&lt;/a&gt;. Los efectos no eran proporcionales a la concentración de glifosato en el producto sino a la de los coadyuvantes en el mismo, que son tensioactivos que se agregan para facilitar la penetración del herbicida en las malezas.&lt;/p&gt;
&lt;p&gt;En nuestro país se anunció recientemente en la prensa sobre un estudio realizado en el Centro de Embriología Aplicada de la Facultad de Medicina de la Universidad de Buenos Aires, que indicaría que el glifosato es tóxico para los anfibios en su etapa embrionaria. Aunque estos resultados todavía no han sido publicados, existen algunas referencias al respecto en la bibliografía internacional &lt;a href=&#34;#2&#34; title=&#34;2&#34;&gt;[2]&lt;/a&gt;. También de que el efecto depende del estadío de desarrollo larval, de la dosis aplicada, y que la naturaleza el efecto tóxico puede deberse a múltiples “estresores” &lt;a href=&#34;#3&#34; title=&#34;3&#34;&gt;[3]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;La situación es que el uso de glifosato en la Argentina y países limítrofes es masivo, por lo que es necesario regular su uso. Por ejemplo, la &lt;em&gt;Environmental Protection Agency&lt;/em&gt; de los Estados Unidos fijó en 1974 un máximo de 0.7 mg/litro para la concentración máxima admisible de glifosato en aguas &lt;a href=&#34;#4&#34; title=&#34;4&#34;&gt;[4]&lt;/a&gt;. Lamentablemente, no existen estudios que midan en nuestro país o países limítrofes residuos de glifosato en aguas. Varios investigadores en Córdoba, Entre Ríos y Rosario están realizando estudios desde distintos enfoques (toxicológico, sanitario) del efecto del uso masivo o de la utilización de malas prácticas en la aplicación de plaguicidas. En las zonas tabacaleras el uso de agroquímicos es muy intenso, en particular de endosulfán, un organoclorado clasificado por la OMS como moderadamente tóxico (clase II) y como muy tóxico por la EPA (clase I). Actualmente está prohibido en sesenta países &lt;a href=&#34;#5&#34; title=&#34;5&#34;&gt;[5]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Otros investigadores han sugerido que el uso intensivo de glifosato produjo una baja en la población de anfibios que a su vez hizo incrementar la de mosquitos, en asociación con las mayores temperaturas promedio que se vienen registrando desde 2008.&lt;/p&gt;
&lt;h3 id=&#34;conclusiones&#34;&gt;Conclusiones&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/07/800px-Sojail.jpg&#34; alt=&#34;Fuente: Wikimedia Commons&#34;&gt;&lt;/p&gt;
&lt;p&gt;Si estos estudios se profundizan y confirman, deberán definirse los límites para el uso de estos productos del agro, en un compromiso o &lt;em&gt;trade-off&lt;/em&gt; entre obtener mayores cosechas con los consiguientes recursos fiscales a corto plazo y los perjuicios al medio ambiente a mediano y largo plazo, que a la larga traerán también una declinación en los rindes y en los recursos fiscales. Es aquí donde se plantean la discusión de crecer como país agrícola siguiendo o no los principios del desarrollo sustentable, de manera de cuidar el sistema productivo para que no afecte a las generaciones futuras. Esta claro que si se siguieran los principios de Buenas Prácticas Agrícolas (BPA) para la aplicación de pesticidas, estos productos no causarían intoxicaciones como las que se vienen registrando, independientemente de su mayor o menor toxicidad.&lt;/p&gt;
&lt;p&gt;Varios son los factores que inciden sobre la eficiencia de la producción agraria: naturales como sequías, suelos, etc. pero también problemas de índole política. Los Agronegocios son un sistema muy amplio y su éxito requiere que sea viable desde el punto de vista tecnológico como político. En todos los casos, se requieren consensos, diálogo y estudios desde muchos sectores, por las implicancias económicas y sociales en juego.&lt;/p&gt;
&lt;h3 id=&#34;referencias&#34;&gt;Referencias&lt;/h3&gt;
&lt;p&gt;&lt;a id=&#34;1&#34;&gt;1.&lt;/a&gt; Benachour Nora, Gilles-Eric Séralini &lt;em&gt;“Glyphosate formulations induce apoptosis and necrosis in human umbilical, embryonic and placental cells”&lt;/em&gt; Chemical Research in Toxicology, Diciembre 2008. Disponible en: &lt;a href=&#34;http://pubs.acs.org/doi/abs/10.1021/tx800218n&#34;&gt;http://pubs.acs.org/doi/abs/10.1021/tx800218n&lt;/a&gt; .&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;2&#34;&gt;2.&lt;/a&gt; Relya R. (2005) &lt;em&gt;&amp;quot;Impact of insecticides and herbicides on the biodiversity of aquatic communities. Ecological Applications&amp;quot;&lt;/em&gt;, 15: 618-627. Solomon, K.R ., Thompson, D.G. 2002. &lt;em&gt;&amp;quot;Ecological risk assessment for aquatic organisms from over-water uses of glyphosate&amp;quot;&lt;/em&gt;. Journal of Toxic. and Environ. Health.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;3&#34;&gt;3.&lt;/a&gt; &lt;a href=&#34;http://www.hc-sc.gc.ca/sr-sr/finance/tsri-irst/proj/persist-org/tsri-121-eng.php&#34;&gt;http://www.hc-sc.gc.ca/sr-sr/finance/tsri-irst/proj/persist-org/tsri-121-eng.php&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;4&#34;&gt;4.&lt;/a&gt; &lt;a href=&#34;http://www.epa.gov/ogwdw000/contaminants/dw_contamfs/glyphosa.html&#34;&gt;http://www.epa.gov/ogwdw000/contaminants/dw_contamfs/glyphosa.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;5&#34;&gt;5.&lt;/a&gt; &lt;em&gt;“El endosulfán y sus alternativas en America Latina. Resumen”&lt;/em&gt; International POP´s Elimination Network, IPEN. 2009.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; Javier Quagliano &lt;a href=&#34;mailto:jquagliano@citefa.gov.ar&#34;&gt;jquagliano@citefa.gov.ar&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>¿Qué sucede dentro un líquido cuando se transforma en un vidrio?</title>
      <link>https://ciencianet.com.ar/post/que-sucede-dentro-un-liquido-cuando-se-transforma-en-un-vidrio/</link>
      <pubDate>Mon, 29 Jun 2009 21:29:16 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/que-sucede-dentro-un-liquido-cuando-se-transforma-en-un-vidrio/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Rodolfo Borzi.&lt;/strong&gt; Instituto de Investigaciones Fisicoquímicas Teóricas y Aplicadas (INIFTA).&lt;/p&gt;
&lt;p&gt;Las computadoras nos permiten simular y explorar microscópicamente líquidos y sólidos, y también materiales que parecen tener similitudes con ambos: los llamamos vidrios estructurales. Los vidrios son rígidos, como los cristales, pero con una estructura atómica que parece desordenada, como en los líquidos. Pese a ello, resultados recientes muestran que una cierta forma de orden comienza a establecerse en un líquido antes de vitrificarse.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Una foto del problema&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/06/vidrios.png&#34; alt=&#34;Representación esquemática (en dos dimensiones) de configuraciones típicas de un arreglo cristalino, un gas, un líquido y un vidrio. En el caso del líquido hemos señalado una esfera de radio R y coloreado en celeste los átomos cuyo centro es interior a la misma (ver la sección &#39;El trabajo, en detalle&#39;). El estado de un material dado puede especificarse dando un número (enorme) de estas fotos o configuraciones.:right&#34;&gt;&lt;/p&gt;
&lt;p&gt;Supongamos que tuviéramos una cámara que nos permite sacar fotos microscópicas, con un aumento tal que podemos ver cómo se arreglan los átomos que conforman un material. La temperatura tiende a desordenar a los átomos del sistema (más en los gases, menos en los sólidos cristalinos), de manera que dos fotos tomadas a un mismo material en las mismas condiciones no serán necesariamente iguales. ¿Podríamos conocer las propiedades del material a partir de la información contenida en un conjunto de estas fotos?&lt;/p&gt;
&lt;p&gt;La respuesta es –si conocemos cómo interactúan entre sí los átomos, y la temperatura— que generalmente sí podemos hacerlo. Esta posibilidad se relaciona con un procedimiento que solemos utilizar para entender cómo se comporta un material: instruimos a una computadora para que genere un conjunto grande de estas fotografías o &lt;em&gt;configuraciones&lt;/em&gt; según una receta. El conjunto de configuraciones constituyen un &lt;em&gt;estado&lt;/em&gt; del sistema. Los estados que mejor conocemos son los más simples de todos: aquellos en los que el paso del tiempo no juega ningún rol. La receta, entonces, suele generar este tipo de estados, que llamamos &lt;em&gt;estados en equilibrio termodinámico&lt;/em&gt;. Sin embargo, este punto de vista es problemático para describir algunos materiales, entre ellos los llamados &lt;em&gt;vidrios estructurales&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Estos vidrios son materiales sólidos que (como el vidrio común, cuyo nombre toman prestado) se obtienen a partir de un líquido enfriado en condiciones tales que no llega a formarse un cristal. El problema que surge puede plantearse de la siguiente forma. Si tomáramos un conjunto de fotos microscópicas de un líquido, veríamos que estas tienen un aspecto muy similar a las de los vidrios (ver recuadro). En ambos materiales podrían verse a los átomos con casi igual grado de apiñamiento (mucho mayor que para un gas). También el grado de desorden atómico sería similar en las fotos del líquido que en las del vidrio. Y sin embargo, el líquido puede fluir, mientras el vidrio es –a efectos prácticos– rígido, como un cristal. A todas luces, aunque sus fotografías no parecen ser muy distintas, las propiedades físicas sí lo son.&lt;/p&gt;
&lt;h3 id=&#34;la-controversia-en-torno-al-vidrio&#34;&gt;La controversia en torno al vidrio&lt;/h3&gt;
&lt;p&gt;Debido a este contraste en sus propiedades, aparentemente irreconciliable con la similitud entre las fotos de ambos sistemas, muchos científicos sostienen que la transformación que tiene lugar a la temperatura &lt;em&gt;T&lt;sub&gt;v&lt;/sub&gt;&lt;/em&gt;, cuando el líquido pasa a ser un vidrio, es puramente &lt;em&gt;dinámica&lt;/em&gt;. Esto es: las diferencias entre un líquido y vidrio no están en las fotos en sí, sino solamente en el &lt;em&gt;tiempo&lt;/em&gt; que tenemos que esperar en uno y otro caso para que en dos fotografías tomadas consecutivamente los átomos aparezcan ocupando posiciones distintas.&lt;/p&gt;
&lt;p&gt;En los vidrios, este tiempo se hace absurdamente grande. Otros científicos, en cambio, siguen sosteniendo que la &lt;em&gt;termodinámica&lt;/em&gt; (que como dijimos, estudia sistemas en los que el tiempo no cumple un rol importante) no ha dicho aún su última palabra sobre estos materiales. Dicho de otra manera, ellos afirman que debería ser posible entender las diferencias entre el líquido y el vidrio sin tener en cuenta al tiempo, mirando solamente ambos conjuntos de fotografías.&lt;/p&gt;
&lt;p&gt;Lo relevante de los resultados que vamos a describir radica en que, por primera vez, Tomás S. Grigera, de la Universidad Nacional de la Plata y el CONICET, en colaboración con G. Biroli, J.-P Bouchaud, A. Cavagna, y P. Verrocchio, han encontrado evidencias convincentes de que la óptica termodinámica es efectivamente adecuada.&lt;/p&gt;
&lt;h3 id=&#34;enfocando-el-mismo-problema-en-un-único-escenario&#34;&gt;Enfocando el mismo problema en un único escenario&lt;/h3&gt;
&lt;p&gt;El problema que hemos descripto tiene el atractivo de que puede formularse enteramente dentro de la fase líquida. Cerca de la temperatura de vitrificación, variaciones en la temperatura del orden del 10% pueden implicar cambios en la viscosidad del líquido en un factor tan grande como 1.000.000.000. Cualquiera esperaría ver también un cambio en la forma en que se ordenan los átomos en las fotografías del líquido tomadas a esas dos temperaturas, reflejando ese cambio gigantesco en la viscosidad. Y sin embargo, hasta el trabajo de T. S. Grigera y colaboradores no se había encontrado una medida adecuada que muestre una variación sustancial entre las configuraciones estáticas del mismo líquido a distintas temperaturas.&lt;/p&gt;
&lt;p&gt;La importancia de poder plantear el problema sin salir de la fase líquida es que –a diferencia de los vidrios– los líquidos se encuentran en una suerte de estado de equilibrio. Esto posibilitó que, mediante algoritmos más o menos sofisticados, los autores pudieran instruir a una computadora para genere las configuraciones que lo describen. El paso más complicado y novedoso fue encontrar una manera de estudiar al líquido que hiciera evidentes las diferencias entre las fotos tomadas a temperaturas distintas.&lt;/p&gt;
&lt;h3 id=&#34;revelando-un-secreto-oculto-en-las-fotos&#34;&gt;Revelando un secreto oculto en las fotos&lt;/h3&gt;
&lt;p&gt;Analizando las configuraciones generadas por la computadora, los autores encontraron, por primera vez, un parámetro –que es estático, es decir, depende solamente de las fotografías y no del tiempo transcurrido entre ellas– que podría explicar los cambios de viscosidad con la temperatura. Se trata de una distancia, que mide cómo crece el grado de cierto tipo de “orden” (no convencional, es decir, distinto al del cristal) cuando baja la temperatura. El hecho de que exista una longitud que crece junto a un tiempo de respuesta al aproximarse a &lt;em&gt;T&lt;sub&gt;v&lt;/sub&gt;&lt;/em&gt; es relevante por otro motivo: nos hace pensar, por analogía, que un cambio de fase auténtico –y no sólo dinámico– como el que ocurre por ejemplo cuando el agua se cristaliza, puede estar teniendo lugar en un entorno de &lt;em&gt;T&lt;sub&gt;v&lt;/sub&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Además de estos resultados (que son llamados numéricos, porque son obtenidos a partir de las cuentas hechas por una computadora), el artículo está sustentado por cálculos teóricos que explican la forma particular de las curvas obtenidas. Estos hallazgos se enmarcan en una teoría anterior, construida en base a uno de los modelos utilizados para describir sistemas desordenados _ magnéticos_ (los llamados &lt;em&gt;vidrios de spin&lt;/em&gt;), dándole soporte a la idea de que la termodinámica, tanto como la dinámica, tienen muchas cosas para decir sobre estos tipos de materiales.&lt;/p&gt;
&lt;h3 id=&#34;el-trabajo-en-detalle&#34;&gt;El trabajo, en detalle.&lt;/h3&gt;
&lt;p&gt;El acierto en el acercamiento de este grupo conformado por científicos argentinos, franceses e italianos, estriba escencialmente en combinar dos procedimientos: (a) Se impone –arbitrariamente– una condición sobre una parte del líquido. Esta condición introduce una nueva longitud &lt;em&gt;R&lt;/em&gt; en el sistema. (b) Definen una función mide qué tan distintas son dos fotografías, teniendo en cuenta la posición de los átomos. Esta función dependerá de la longitud &lt;em&gt;R&lt;/em&gt; impuesta por la condición mencionada en (a). Además de ello los investigadores encontraron que esta función era sensible a los cambios en la temperatura. Recordemos que el sistema tiene la ventaja que puede ser simulado por medio de una computadora, con lo que hay mucha más libertad para imponer condiciones (y medir propiedades) que en los materiales reales.&lt;/p&gt;
&lt;p&gt;La condición (a) puede describirse así: dentro de una esfera de radio &lt;em&gt;R&lt;/em&gt; los átomos que conforman al líquido (pintados de celeste en la figura) intentan, por acción de la temperatura, cambiar su posición. No obstante ello, su grado de empaquetamiento es tan grande (el correspondiente a un líquido a una temperatura muy baja) que una dada configuración cambia muy lentamente con el tiempo. Los átomos fuera de esa esfera (pintados de verde en la figura) se encuentran a la misma temperatura que los anteriores, y encuentran las mismas dificultades para desplazarse; sin embargo, se permite que estos átomos exteriores intenten desplazarse _ mucho menos frecuentemente que los interiores_, de manera que desde el punto de vista de los átomos celestes ellos están congelados.&lt;/p&gt;
&lt;p&gt;Si intentáramos seguir la evolución temporal mirando las configuraciones, encontraríamos que esta se compone de conjuntos de muchas fotografías consecutivas con los átomos exteriores a la esfera en exactamente las mismas posiciones, pero en algunas de ellas habría cambios en la configuración de los átomos interiores. En forma efectiva, la condición consiste en transformar a los átomos exteriores en una suerte de cárcel de tamaño &lt;em&gt;R&lt;/em&gt;, que influye sobre las configuraciones de los átomos internos a la esfera. Esperaríamos que el efecto de este confinamiento sea menor cuanto mayor es el radio &lt;em&gt;R.&lt;/em&gt; Si fijamos _ T_, existirá un radio &lt;em&gt;R =&lt;/em&gt; &lt;em&gt;x(T)&lt;/em&gt; en el que el confinamiento deja de ser efectivo, y los átomos interiores llegarán a verse (aún cuando demoren mucho tiempo) en una configuración que será muy distinta de la fotografía inicial. La función a que nos referíamos en el punto b) les permitió a los investigadores identificar cuantitativamente el valor del radio &lt;em&gt;x(T)&lt;/em&gt;. Es justamente el incremento de la longitud &lt;em&gt;x(T)&lt;/em&gt; al bajar la temperatura el mayor hallazgo de este trabajo, que muestra por primera vez un cambio significativo en un parámetro termodinámico cuando un líquido en equilibrio se enfría hacia el congelamiento vítreo.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; &lt;a href=&#34;https://doi.org/10.1038/nphys1050&#34;&gt;Thermodynamic signature of growing amorphous order in glass-forming liquids&lt;/a&gt;, G. Biroli, J.P. Bouchard, A. Cavagna, T.S. Grigera y P. Verrocchio, &lt;em&gt;Nature Physics&lt;/em&gt;, Vol. 4, Nro. 10 (octubre 2008), págs. 771-775.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; T.S. Grigera (&lt;a href=&#34;mailto:tgrigera@inifta.unlp.edu.ar&#34;&gt;tgrigera@inifta.unlp.edu.ar&lt;/a&gt;)&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>¿Ciencia? ¿Qué ciencia?</title>
      <link>https://ciencianet.com.ar/post/ciencia-que-ciencia/</link>
      <pubDate>Fri, 19 Jun 2009 21:30:21 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/ciencia-que-ciencia/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;T. S. Grigera:&lt;/strong&gt; Instituto de Investigaciones Fisicoquímicas Teóricas y Aplicadas (CONICET La Plata, UNLP, CIC)&lt;/p&gt;
&lt;p&gt;En Argentina, como en otros países del sur, ocurre un fenómeno llamado fuga de cerebros: jóvenes universitarios de distintas disciplinas científicas, formados mayoritariamente en la Universidad pública, emigran y desarrollan su carrera contribuyendo al desarrollo del conocimiento científico o técnico en algún país del norte.&lt;/p&gt;
&lt;p&gt;¿Por qué se van? La respuesta parece evidente: en Argentina los científicos no pueden trabajar razonablemente. Aquí &amp;quot;no se apoya a la ciencia&amp;quot;. Los científicos &amp;quot;no tienen campo&amp;quot;. A &amp;quot;los políticos&amp;quot;, la ciencia no les importa y por lo tanto no hay presupuesto suficiente para investigar. Los que son buenos se van &amp;quot;porque no tienen más remedio&amp;quot;.&lt;/p&gt;
&lt;p&gt;El lector habrá escuchado más de una vez frases de este tipo, pronunciadas muchas veces por científicos. Sin embargo, dichas así, no pasan de lugares comunes. Porque, ¿qué significa &amp;quot;apoyar a la ciencia&amp;quot;? Si tomamos al azar un laboratorio, por ejemplo, en La Plata (cualquier ciudad argentina daría igual), y lo comparamos con uno equivalente en, digamos, Ginebra, casi seguramente lo encontraremos inferior a este último en múltiples aspectos. Pero lo mismo sucederá si comparamos los trenes que parten de La Plata con los que parten de Ginebra, los hospitales, o las viviendas (especialmente entre los extremos más pobres de ambas ciudades).&lt;/p&gt;
&lt;p&gt;Independientemente de que entre las causas de las diferencias en laboratorios, trenes, hospitales y viviendas haya probablemente varios puntos en común, es evidente que pretender un laboratorio ginebrino en una ciudad como La Plata no sólo es poco realista: sobre todo, es poco ético. Esto no quiere decir que tengamos que renunciar a condiciones apropiadas de trabajo. Pero sí que la definición de &amp;quot;apropiadas&amp;quot; requiere de una gran dosis de responsabilidad.&lt;/p&gt;
&lt;p&gt;&amp;quot;Hay que apoyar a la ciencia&amp;quot; ciertamente no puede querer decir que el estado tiene que garantizarme, como científico, medios y salarios similares a los del norte para trabajar en investigación en el tema que a mí se me ocurra, por más &amp;quot;nivel internacional&amp;quot; que tenga mi curriculum o mi plan de trabajo. Pero entonces, ¿qué quiere decir &amp;quot;apoyar a la ciencia&amp;quot;? ¿Y por qué hay que apoyarla? ¿Se puede hacer ciencia en Argentina? ¿Tiene sentido intentarlo? ¿No será la ciencia un lujo que Argentina no puede darse? ¿No habrá que esperar a alcanzar cierto grado de desarrollo antes de dedicar parte de nuestros recursos a la investigación científica? ¿Qué rol cabe a los científicos que no se van, a los que vuelven (porque también hay de esos)?&lt;/p&gt;
&lt;p&gt;Podríamos responder con las palabras de Jawaharlal Nehru:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;quot;Sólo la ciencia puede resolver los problemas del hambre y la pobreza, de la insalubridad y el analfabetismo [...] ¿Quién puede darse el lujo de ignorar a la ciencia en estos tiempos?&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Y sería una excelente respuesta, siempre y cuando supiéramos qué significa, y, sobre todo, estuviéramos dispuestos a trabajar muy duro para construir un sistema de ciencia y técnica digno de esa respuesta. Porque tenemos que admitir que la cadena causal entre la creación de buenos grupos de investigación en, digamos, física de sistemas desordenados o neurotransmisores, y el descenso de las tasas de mortalidad infantil o desnutrición no es tan obvia como para que cualquiera la perciba a primera vista.&lt;/p&gt;
&lt;p&gt;En realidad, la mera existencia de grupos de investigación científica, aunque sean de &amp;quot;nivel internacional&amp;quot;, no garantiza que el país tomará la senda del desarrollo socioeconómico, cualquiera sea el sentido que se le dé a este último. Nehru continúa diciendo:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;quot;A cada paso tenemos que recurrir a su ayuda [de la ciencia]&amp;quot;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Esto es: la ciencia genera conocimiento, pero a ese conocimiento hay que usarlo. En palabras de Marcelino Cereijido: la cuestión no es &lt;em&gt;apoyar a&lt;/em&gt; la ciencia, sino &lt;em&gt;apoyarse en&lt;/em&gt; la ciencia. Argentina, entonces, ¿recurre a la ayuda de la ciencia? Si lo hace, ¿se vale de ciencia propia o compra ciencia ajena? ¿Nos dirigen preguntas a los científicos argentinos?&lt;/p&gt;
&lt;p&gt;Es cierto que alguna vez nos mandaron a lavar los platos, pero ¿y si no lo hicieran? Si nos formulan preguntas pertinentes, ¿sabemos responder? ¿Puede Argentina apoyarse en su sistema científico-tecnológico para crecer? Los científicos argentinos, ¿producimos el conocimiento que Argentina necesita? Pero, ¿qué conocimiento necesita Argentina? ¿Qué ciencia tiene Argentina y qué ciencia necesita Argentina? ¿Cómo hay que hacer para cerrar el circuito retroalimentado entre la producción y el uso del conocimiento?&lt;/p&gt;
&lt;p&gt;No tengo respuestas a esas preguntas; o más bien tengo respuestas parciales, más o menos discutibles. Pero el objetivo de esta nota no es proponer respuestas, sino señalar que esas preguntas deben ser formuladas y respondidas, que responderlas es una necesidad urgente, y que una parte importante de las respuestas ha de provenir de los propios científicos.&lt;/p&gt;
&lt;p&gt;Por si no queda claro: contestar esas preguntas equivale a formular una política científica. Y esa política no está desvinculada de otras cuestiones nada sencillas, tales como qué clase de desarrollo queremos, para quiénes y para cuántos. Porque la política científica no es sólo una cuestión técnica que habrá de ocupar a especialistas, sino ante todo y justamente, una cuestión política, esto es, una cuestión de poder, de su ejercicio y distribución. Luego, todos son llamados a opinar, y en particular los científicos, actores y ejecutores últimos de las políticas resultantes. Dedicar parte de nuestro tiempo a discutir estas cuestiones es parte de nuestra responsabilidad, tanto o más que publicar el próximo paper o escribir el próximo pedido de subsidio.&lt;/p&gt;
&lt;p&gt;Y lo mismo podemos decir de la construcción de un sistema de ciencia y tecnología consecuente con las respuestas que demos. La formación de un científico es larga y difícil. Requiere de cierto talento y de mucho esfuerzo y perseverancia. Hemos invertido mucho, y han invertido mucho en nosotros. Seguir adelante automáticamente, sin una idea acerca de cómo contestar esas preguntas es tirar a la basura esfuerzo propio y ajeno.&lt;/p&gt;
&lt;p&gt;Claro que es más fácil evitar las preguntas, porque entonces podemos seguir tranquilamente el camino seguro para desarrollar una carrera &amp;quot;exitosa&amp;quot;. Se trata de un itinerario que exige cierto esfuerzo, pero que está trazado de antemano: publicar mucho (no importa de qué calidad), asistir a congresos, obtener subsidios, ocupar cargos docentes (aunque no nos interese dar clases), dirigir alguna tesis. Haciendo todo eso prolijamente durante algunos años lograremos ascensos, ganaremos nuevos concursos, obtendremos más subsidios, quizás algún premio. Llegaremos a ocupar puestos en comisiones asesoras y cargos directivos. Y desde allí podremos controlar que nuestros sucesores hagan lo mismo que nosotros, para recompensarlos del mismo modo.&lt;/p&gt;
&lt;p&gt;Pero cuando empezamos a estudiar ciencia (hace tanto y tan poco), seguro que era para otra cosa...&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Universidad y Sociedad: Extensión Universitaria en Física en la UNLP</title>
      <link>https://ciencianet.com.ar/post/universidad-y-sociedad-extension-universitaria-en-fisica-en-la-unlp/</link>
      <pubDate>Thu, 21 May 2009 21:33:41 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/universidad-y-sociedad-extension-universitaria-en-fisica-en-la-unlp/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero.&lt;/strong&gt; Departamento de Física, Facultad de Ciencias Exactas y Naturales, Universidad de Buenos Aires.&lt;/p&gt;
&lt;p&gt;Históricamente, las diversas formas de participación de las universidades en la comunidad y su impacto en su zona de influencia, así como las políticas que guían las actividades de extensión, no han sido uniformes en las distintas instituciones del país. Algunas universidades consideran la extensión como una forma de transferencia de conocimiento al sector privado, otras como una oferta educativa de posgrado, otras intentan dar respuestas a problemáticas sociales y culturales de la región. Pero estas distintas concepciones no son sólo heterogéneas espacialmente, sino que la idea que una institución educativa universitaria tiene de la extensión también varía a lo largo de los años y las gestiones.&lt;/p&gt;
&lt;p&gt;En este artículo se describe la evolución de las actividades de extensión del Instituto de Física de la UNLP en los años 1905 –año de creación de la universidad- y 1930 –interrupción institucional debido al golpe de estado-, período atravesado por la Reforma Universitaria del 18, intentando aportar claridad a partir de un análisis histórico. En el artículo se referencia a varios pensadores de la época, como Ortega y Gasset y Durkheim y otros más recientes como Freire y García Guadilla, evitando explícitamente la tentación de “juzgar el pasado con criterios del presente”.&lt;/p&gt;
&lt;p&gt;La idea fundamental de este trabajo es que las actividades desarrolladas en cada momento estaban modeladas por un lado, por la concepción vigente sobre cómo debía plantearse la relación entre la universidad y la comunidad, y por otro lado, por el perfil científico dominante. Esta idea es sostenida con abundantes ejemplos; así, por ejemplo, durante la gestión de Joaquín V. González, primer presidente de la UNLP, el positivismo fundacional dirigió la extensión a divulgar la ciencia y la cultura entre sectores populares mediante charlas, cursos nocturnos y libres y el acceso facilitado al museo y biblioteca.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/05/ANFITEATRO-s.jpg&#34; alt=&#34;Anfiteatro de Física: Ver descripción al final del artículo.&#34;&gt;&lt;/p&gt;
&lt;p&gt;En cuanto a la propia disciplina, las actividades realizadas por los primeros físicos que trabajaron en La Plata fueron recopiladas, documentando la intensidad y continuidad de los vínculos entre el Instituto y la comunidad. La primera etapa del Instituto, hasta 1909, estuvo bajo la dirección del Ingeniero uruguayo Tebaldo Ricaldoni, cuyo perfil se define como de inventor. Durante su gestión se dieron charlas demostrativas para “vulgarizar los conocimientos” y se desarrollaron aplicaciones destinadas a la industria nacional y al estado: un receptor de telegrafía sin hilos, un reductor de voltaje, una boya de salvataje, un desvía torpedos, un submarino y un panoramoscopio, aunque no todas estas producciones las realizó desde el Instituto.&lt;/p&gt;
&lt;p&gt;En la segunda (hasta 1911), que estuvo redefinida por el estándar europeo de la investigación científica de la mano del físico alemán Emil Bose, las charlas continuaron y se generaron publicaciones. Se intentaron colaboraciones con Jorge Newbery sobre meteorología y como aplicaciones relevantes cuentan un estudio espectroscópico sobre la yerba mate para controlar su calidad, y estudios de peritaje sobre documentación jurisprudencia y filatelia.&lt;/p&gt;
&lt;p&gt;La tercera, hasta 1925, bajo al dirección del físico alemán Richard Gans, hizo hincapié en el apoyo a los docentes de bachilleratos, ya que a los profesores de la época -al igual que a los actuales- les preocupaba la escasa preparación de los ingresantes en física y matemáticas. Entre otras aplicaciones interesantes, se hicieron calibraciones de instrumentos para entidades públicas y privadas, y varias contribuciones sanitarias mediante el uso de técnicas experimentales.&lt;/p&gt;
&lt;p&gt;Como cierre de esta breve reseña, tomamos del artículo una frase de Joaquín V. González, que es rescatada en su espíritu por quienes hacemos extensión “&lt;em&gt;Una universidad moderna que no toma en cuenta el problema social es una universidad exótica, y sus fuerzas se perderán en el vacío si no la dirige a procurar la armonía suprema sobre la que se asienta la humana convivencia&lt;/em&gt;”.&lt;/p&gt;
&lt;p&gt;El artículo original puede encontrarse en: &lt;a href=&#34;http://www.mast.br/arquivos_sbhc/35.pdf&#34;&gt;http://www.mast.br/arquivos_sbhc/35.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Imagen&lt;/strong&gt;: Foto del anfiteatro de Física, revelada por Damián Gulich y Rafaela Paladini a partir de un negativo de vidrio. En Marzo de 1911 Emil Bose dictó allí una conferencia con demostraciones. Tuvo una gran repercusión en los medios y según J. V. González fue relevante en su gestión ante la Cámara de Diputados para obtener la renovación sin recortes del presupuesto de la UNLP.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Hacer el vacío en cirujía</title>
      <link>https://ciencianet.com.ar/post/hacer-el-vacio-en-cirujia/</link>
      <pubDate>Wed, 15 Apr 2009 21:36:54 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/hacer-el-vacio-en-cirujia/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Daniel E. Wainstein:&lt;/strong&gt; Hospital General de Agudos “Enrique Tornu” del Gobierno de la Ciudad de Buenos Aires.&lt;/p&gt;
&lt;p&gt;La fístula enterocutánea es una comunicación anormal entre el tubo digestivo y la piel originada, en la mayoría de los casos, como complicación de una cirugía abdominal. Esta comunicación provoca la pérdida de líquido intestinal y, consecuentemente, desencadena graves trastornos como deshidratación, desnutrición e infección, los que, de no ser tratados rápida y adecuadamente, causarían la muerte. Se trata de una complicación grave que conlleva una mortalidad del 20 al 30% y no existía hasta el momento de iniciada esta tesis un método terapéutico con resultados plenamente satisfactorios. El objetivo de esta tesis fue evaluar los alcances terapéuticos de un método de cerrado de la fístula mediante un sistema de vacío y compactación (SIVACO).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/04/fist2-post.jpg&#34; alt=&#34;Descripción las sucesivas etapas del proceso.&#34; title=&#34;Imagen&#34;&gt;&lt;/p&gt;
&lt;p&gt;Durante la tesis se empleó una adaptación del método de SIVACO, desarrollado, durante la década del ‘80 por el médico argentino, Dr Ernesto Fernandez. Este método se basa en generar una cámara de compactación conectada a una fuente de vacío (ver Figura A). Dicha bomba es capaz de generar una caída de la presión de hasta 600 mm Hg respecto de la presión atmosférica (que es aproximadamente de 760 mm Hg). La cámara de compactación se genera sobre la superficie de la piel del paciente en la región abdominal donde se encuentra un extremo de la fístula (el otro extremo es la luz del instestino).&lt;/p&gt;
&lt;p&gt;La cámara se dispone de acuerdo a las características anatómicas de la lesión comenzando con una profunda limpieza de la herida y protección de la piel con pasta protectora (Figura B). Luego se coloca un manto de fibras poliméricas y, en su espesor, se introduce una tubuladura de una longitud apropiada para permitir la movilización del paciente conectada al sistema de vacío. Por último, se adhiere una lámina de polietileno cubriendo todos los elementos (Figura C). Al activar la aspiración, se genera un sistema de baja presión que, al compactar el polímero dentro de la cámara de compactación, cierra el orificio de salida de la fístula (Figura D).&lt;/p&gt;
&lt;p&gt;Interesantemente, este método mostró curación sin necesidad de cirugía en un amplio porcentaje de casos. A su vez, en los pacientes que requirieron cirugía, el SIVACO permitió optimizar la condición clínica y nutricional para encarar el tratamiento quirúrgico en el momento oportuno. El método, incluso, pudo implementarse en el domicilio del paciente, lo cual facilitó su tratamiento. Las conclusiones de este trabajo son que el SIVACO demostró ser un método eficaz para controlar el flujo en las fístulas enterocutáneas posoperatorias.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;CienciaNet&lt;/strong&gt; consultó al Dr. Mariano Gimenez del Hospital de Clínicas &amp;quot;José de San Martín&amp;quot; (Buenos Aires), quien comenta: &amp;quot;En este estudio se ha realizado una actualización de los conceptos referentes al manejo de una patología sumamente compleja. El doctorando aporta su visión, basándose para ello, en un estudio profundo del tema y en su experiencia personal sobre una amplia serie de casos tratados en el Hospital “E. Tornú” y en la práctica extrahospitalaria. Pero fundamentalmente, se ha realizado una evaluación científica de los alcances terapéuticos de un método desarrollado por el médico argentino Ernesto Fernandez: el sistema de vacío y compactación (SIVACO).&amp;quot;&lt;/p&gt;
&lt;p&gt;&amp;quot;Considero que los resultados de esta investigación constituyen un aporte original que, indudablemente, será de gran utilidad para aquellos que deban afrontar el tratamiento de esta grave complicación.&amp;quot;&lt;/p&gt;
&lt;p&gt;El Dr. Mariano Gimenez fue uno de los jurados del trabajo de tesis de Daniel E. Wainstein.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; Este estudio forma parte de la tesis defendida por el autor de esta nota el 5 de Noviembre de 2008 en la Facultad de Medicina (UBA), para optar al título de Doctor de la Universidad de Buenos Aires en el área de Cirugía. Título: &amp;quot;Fístulas enterocutáneas postoperatorias de alto flujo. Tratamiento local con presión subatmosférica.&amp;quot;, Director: Prof. Vicente P. Gutierrez. Copias de la tesis pueden buscarse en la página web del autor (&lt;a href=&#34;http://www.dr-dw.com/&#34;&gt;http://www.dr-dw.com/&lt;/a&gt;) o solicitándolas directamente al mismo (Email: &lt;a href=&#34;mailto:dwainstein@telered.com.ar&#34;&gt;dwainstein@telered.com.ar&lt;/a&gt;).&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;comentarios&#34;&gt;Comentarios&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;cris-arias@live.com.ar&#34;&gt;Cristina&lt;/a&gt; - &lt;time datetime=&#34;2019-03-08 17:09:45&#34;&gt;Mar 5, 2019&lt;/time&gt;&lt;/p&gt;
&lt;p&gt;Yo necesito armar para mi madre este sistema Sivaco en el domicilio. Existe algún lugar donde acudir para el aspirador de secreción? Los que consulte son portátil , mi madre lo necesita de manera continua, permanente, sin desconectar hasta lograr cicatrización. Dejo mi mail: Gracias&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;m.carlevaro@gmail.com&#34;&gt;Manuel&lt;/a&gt; - &lt;time datetime=&#34;2019-03-15 12:01:58&#34;&gt;Mar 5, 2019&lt;/time&gt;&lt;/p&gt;
&lt;p&gt;Hola Cristina. Deberías ponerte en contacto con Daniel Wainstein en el Hospital Tornú, aunque esta nota es de 2009 y no sabemos si está aún trabajando allí. Lamentablemente en CienciaNet no podemos responder tu pregunta pues solo difundimos los trabajos, pero no somos los que los hacemos. Suerte. M.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>El discurso periodístico sobre la actuación de los nuevos movimientos sociales</title>
      <link>https://ciencianet.com.ar/post/el-discurso-periodistico-sobre-la-actuacion-de-los-nuevos-movimientos-sociales/</link>
      <pubDate>Wed, 01 Apr 2009 21:38:01 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/el-discurso-periodistico-sobre-la-actuacion-de-los-nuevos-movimientos-sociales/</guid>
      <description>
        
          &lt;p&gt;Enviado por &lt;strong&gt;Jorgelina Bustos Arlin&lt;/strong&gt;, Facultad de Ciencias Políticas y Sociales. Universidad Nacional de Cuyo. Mendoza.&lt;/p&gt;
&lt;p&gt;Una propuesta teórico-metodológica aplicada la gráfica a partir de la relación entre cognición social y discursos.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/04/mendoza.jpg&#34; alt=&#34;Movilización en Mendoza. Fuente: ANRed con licencia CC BY-SA&#34;&gt;&lt;/p&gt;
&lt;p&gt;Un estudio detallado del tratamiento periodístico sobre recientes movimientos sociales destaca la incapacidad de los medios de comunicación para analizar las nuevas manifestaciones de protesta desde sus motivaciones. La prensa parece focalizarce más en las acciones de protesta originales y coloridas que en las raíces del conflicto y la organización de movimiento que las lleva adelante.&lt;/p&gt;
&lt;p&gt;La preocupación sobre los ‘marcos interpretativos’ desde los cuales determinado discurso periodístico configura la ‘realidad’ social, focalizado particularmente en la actuación de los llamados ‘nuevos movimientos sociales’, fue el punto de partida de un trabajo de naturaleza compleja. Esta pretensión, demandó inevitablemente; definir, caracterizar, explicar y analizar, cada uno de los elementos puestos en juego: ‘Nuevos movimientos sociales; discursos sociales/discurso periodístico; contexto social/contexto de producción; y cognición social/marcos interpretativos.&lt;/p&gt;
&lt;p&gt;La noción de ‘marco interpretativo’ es el punto de encuentro que nos permite desentrañar el vínculo entre la realidad social, y los rasgos que el discurso periodístico selecciona de ella. Por lo tanto ingresamos en nociones tales como: sistema de creencias, representaciones sociales, poder, ideología, etc., cuyos referentes operan jerárquicamente en determinados modelos de funcionamientos colectivos. Estas relaciones se basan en la propuesta que Teun van Dijk realiza sobre la ideología que resume ‘en un triángulo formado por los conceptos Cognición, Sociedad y Discurso’.&lt;/p&gt;
&lt;p&gt;Apoyados epistemológicamente en supuestos teóricos que permiten trabajar en la complejidad, propusimos un modelo de análisis –una propuesta metodológica- con el objeto de abordar este campo discursivo puntual: las crónicas en la gráfica sobre la actuación de movimientos sociales contemporáneos en la provincia de Mendoza. Así, a partir de la problemática de la cognición desarrollamos el análisis y la explicación acerca de la jerarquía que tiene la prensa masiva en la formación de esquemas cognitivos por su fuerte ascendiente en la constitución de un conjunto de creencias sobre la realidad pública y social: se trata de los dos diarios de venta mayor en la provincia de Mendoza (Argentina).&lt;/p&gt;
&lt;p&gt;Las conclusiones son numerosas y variadas luego de cruzar los elementos referidos, sin embargo la propuesta metodológica para el análisis de las dos publicaciones de mayor tiraje durante un año, permite reconocer –mediante ‘marcas’ o ‘huellas’ inscriptas en la materialidad significante-, la falta de conocimiento sobre las características de estos nuevos movimientos en ambos diarios, puesto que esperan de ellos comportamientos análogos a los de gremios y/o sindicatos propios de la sociedad industrial y de estructuras organizativas piramidales.&lt;/p&gt;
&lt;p&gt;A partir de esa ubicación es difícil encontrar información referida a nuevos movimientos sociales específicamente. Al no configurarse dentro de una problemática de orden sociopolítico-económico, no advierten las diferencias entre formas alternativas/nuevas de organización en la protesta. Raramente se reconocen y explican, las nuevas formas para interrogar al poder. El tratamiento mediático de la cuestión social, se emparenta frecuentemente sólo con la pobreza, la cual es descripta mediante generalizaciones y apreciaciones cristalizadas en el ‘sentido común’. Se destacan los rasgos de mayor visibilidad, y se jerarquizan actuaciones llamativas por el colorido, la provocación o la originalidad sin reflexionar sobre las motivaciones de dichas acciones (se destacan relevamientos de encuentros feministas, piqueteros, acciones contra la pobreza).&lt;/p&gt;
&lt;p&gt;Finalmente, los aspectos ‘no dichos’, o ‘dichos’ desde un abordaje cognitivo esclerotizado, en el marco del actual proceso de dinámica social en crisis o en cambio, pueden resultar significativamente disuasorios para el pensamiento innovador y la consecuente actitud fáctica de los lectores/actores sociales, que configuran -en parte- su universo de representaciones desde las crónicas diarias.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;CienciaNet&lt;/strong&gt; consultó a Roberto Follari (Univ. Nacional de Cuyo) respecto de este trabajo, quien comenta:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;quot;Se trata de un trabajo trascendente, en tanto muestra con claridad la no-transparencia de toda información, su construcción acorde a marcos interpretativos determinados; y, por ello, su imposible neutralidad. En el caso de los nuevos movimientos sociales, ello se hace más patente por lo que han tenido de originales y disruptivos&amp;quot;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Roberto Follari fue el director de este trabajo.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo Original:&lt;/strong&gt; &amp;quot;El discurso periodístico sobre la actuación de los nuevos movimientos sociales: una propuesta teórico metodológica a partir de la relación entre cognición social y discursos. Análisis de campo discursivo en los diarios Los Andes y Uno de la provincia de Mendoza.&amp;quot; presentada por Jorgelina Alcira Bustos Arlin para optar por el grado de Magister de Ciencia Política y Sociología de la Facultad Latinoamericana de Ciencias Sociales (2005), bajo la dirección del Prof. Roberto Agustín Follari.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; &lt;a href=&#34;mailto:jorgelinabustos@ciudad.com.ar&#34;&gt;Jorgelina Alcira Bustos Arlin&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Todo lo que Ud. siempre quiso saber sobre el Dengue y no se animaron a contarle</title>
      <link>https://ciencianet.com.ar/post/todo-lo-que-ud-siempre-quiso-saber-sobre-el-dengue-y-no-se-animaron-a-contarle/</link>
      <pubDate>Tue, 31 Mar 2009 21:40:00 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/todo-lo-que-ud-siempre-quiso-saber-sobre-el-dengue-y-no-se-animaron-a-contarle/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero:&lt;/strong&gt; Departamento de Física, Facultad de Ciencias Exactas y Naturales, Universidad de Buenos Aires.&lt;/p&gt;
&lt;p&gt;El físico Hernán Solari, investigador del Conicet en el Departamento de Física de la Facultad de Ciencias Exactas y Naturales (UBA) y autor de varios artículos sobre epidemiología matemática -en particular sobre modelos de propagación del mosquito &lt;em&gt;Aedes aegypti&lt;/em&gt; y de fiebre amarilla y dengue, elaborados junto al Grupo de Estudio de Mosquitos-, responde aquí una serie de preguntas al respecto.&lt;/p&gt;
&lt;h3 id=&#34;sobre-el-mosquito&#34;&gt;Sobre el mosquito&lt;/h3&gt;
&lt;p&gt;- &lt;strong&gt;¿Quién es el transmisor del Dengue?&lt;/strong&gt; - &lt;strong&gt;H.S.&lt;/strong&gt;: El dengue pasa del hombre al mosquito; en este último el virus que causa la enfermedad se reproduce sin afectarlo. Pasado el período de reproducción el mosquito puede infectar a los humanos que pica. Esto es lo que diferencia a un transmisor mecánico -como podría ser cualquier mosquito no portador o un tábano o una jeringa- de un vector, donde el virus se reproduce. El resultado es que solo unos pocos mosquitos tienen esta capacidad. De entre ellos, &lt;em&gt;Aedes aegypti&lt;/em&gt;, por su carácter urbano es el vector más eficiente. En la misma familia &lt;em&gt;Aedes albopictus&lt;/em&gt; también transmite la enfermedad y está presente en paisajes más agrestes. Y ya está invadiendo el norte del país. Al menos en Misiones se lo ha detectado.&lt;/p&gt;
&lt;p&gt;- &lt;strong&gt;¿Qué hábitos tiene el vector? ¿Dónde vive? ¿Cuándo pica?&lt;/strong&gt; - &lt;strong&gt;H.S.&lt;/strong&gt;: &lt;em&gt;Aedes aegypti&lt;/em&gt;, como todo mosquito, pasa por tres fases que se desarrollan en la cercanía del agua y una fase de adulto en la que vuela. Los huevos son puestos en las paredes de receptáculos con agua clara, particularmente donde hay levaduras producto de la descomposición de vegetales. Estas levaduras suelen dar un tinte amarillento al agua. Por sobre el nivel de agua se depositan los huevos y cuando estos se sumergen eclosionan las larvas. Más tarde se forma la pupa o crisálida y finalmente emerge el adulto. Todo el ciclo de vida gira alrededor de este ambiente.&lt;/p&gt;
&lt;p&gt;El adulto no resiste altas temperaturas por lo que se refugia del sol a la sombra de las plantas y prefiere lugares húmedos. Suele vivir en el interior de las casas. En épocas de grandes epidemias los pobladores comentaban que al abrir cajones salían nubes de mosquitos. Su característica es picar al amanecer y al atardecer. Los pulmones de manzana suelen ser los mejores lugares para buscarlos, pero un clásico es encontrarlos compartiendo el hábitat con plantas de potus mantenidas en agua.&lt;/p&gt;
&lt;p&gt;- &lt;strong&gt;¿Cómo es posible reconocerlo?&lt;/strong&gt; - &lt;strong&gt;H.S.&lt;/strong&gt;: Si uno tiene buena vista, puede observar las rayas blancas en las patas del adulto que son características. También tiene escamas plateadas en su costado. Las larvas son muy nerviosas y se esconden en el fondo del recipiente al detectar nuestra presencia, aunque finalmente tienen que subir a respirar. Como mencionamos, sus movimientos son característicamente nerviosos y su tamaño no es muy grande, digamos de 3 milímetros. En la zona de Ensenada (Punta Lara) suelen verse mosquitos con marcas blancas en sus patas y de gran tamaño, pero pertenecen a otra especie.&lt;/p&gt;
&lt;p&gt;- &lt;strong&gt;¿Cuán infalible es la transmisión de la enfermedad?&lt;/strong&gt; - &lt;strong&gt;H.S.&lt;/strong&gt;: Hay muchos factores a tener en cuenta. Tres de cada cuatro mosquitos que pican a un humano en período de viremia se infectan, y tres de cada cuatro veces que un mosquito con virus ya maduro pica a un adulto le transmite la enfermedad. El mosquito sigue siendo contagioso y puede picar a otra persona, aunque también se puede morir antes de ser contagioso. Todo es falible, en todo hay elementos que podríamos considerar azarosos. ¿Cómo se explica entonces que con la abundancia que actualmente existe del mosquito lleguen casos de dengue importados y no se inicien epidemias? El problema consiste en que los síntomas clínicos se dan al final del período contagioso. Durante unos días (por dar un valor, digamos 5) luego de ser infectada la persona, el virus se multiplica en el huésped (humano), y a este período le siguen unos 3 días donde se da la mayor probabilidad de contagiar a un mosquito, apareciendo los síntomas clínicos sobre el final.&lt;/p&gt;
&lt;p&gt;Hay personas que tienen solo molestias leves, son casos subclínicos. Un 60% o más de las infecciones son subclínicas, no se reportan. Cuando un caso evoluciona clínicamente a formas graves de la enfermedad, esas formas graves se dan después del período en que es contagioso. Ocurre exactamente igual con la fiebre amarilla. El virus de la fiebre amarilla es transmitido por los mismos mosquitos y pertenece a la misma familia -flavivirus- que el del dengue. Lo que históricamente se ha reconocido como fiebre amarilla, son solo el 15% de los casos. El resultado es que cuando nos alarmamos y tomamos medidas, la epidemia ya está en marcha silenciosamente.&lt;/p&gt;
&lt;p&gt;Por el momento no existe una transmisión vertical del virus en el mosquito (de madres a hijos -huevos-), de existir, de producirse una mutación del virus en ese sentido, entonces se estaría en condiciones de catástrofe: cada hembra pone unos 60 huevos con un período que en verano puede llegar a ser tan corto como 4 días. Este es el peligro global del dengue, con la expansión permanente de los vectores y detrás de ellos la enfermedad.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/03/DENGUE_03-20-03-2007-768x622.jpg&#34; alt=&#34;Prevención: Folleto del Gobierno de la Provincia de Corrientes.&#34;&gt;&lt;/p&gt;
&lt;p&gt;- &lt;strong&gt;¿Es útil la fumigación?&lt;/strong&gt; - &lt;strong&gt;H.S.&lt;/strong&gt;: La fumigación es un recurso extremadamente limitado en la lucha contra este mosquito. Hay muchos problemas, como por ejemplo acceder a donde está el adulto (interior de las casas y debajo de las plantas) ya que se encuentra protegido de la niebla del fumigador. El tamaño de la gota es crítico para la efectividad del veneno y los aparatos de fumigación deben estar calibrados. Se elimina solo parte de los adultos, pero cada hembra deposita unos 60 huevos por oviposición, lo que le da un potencial reproductivo formidable. Si uno observa la evolución de la epidemia de Tartagal (2004), donde se fumigaba 400 metros alrededor de cada caso de dengue detectado, se ve que la política de fumigación no pudo impedir la formación de focos, es decir, la propagación de la epidemia por el mosquito.&lt;/p&gt;
&lt;p&gt;- &lt;strong&gt;¿Cómo controlarlos?&lt;/strong&gt; - &lt;strong&gt;H.S.&lt;/strong&gt;: Lo más sensato es revisar la casa a conciencia, buscando recipientes abandonados que acumulen agua donde se puedan criar los mosquitos. Cubiertas de auto en desuso están entre sus favoritos, acumulaciones de agua en los desagües pluviales, frascos con agua, aun en la cocina o interior de la casa, bebederos de los animales y un larguísimo etcétera. La ventaja adicional del método es que cuando, después de tomar sangre, la hembra se dispone a poner los huevos, busca sitios de cría y al no encontrarlos, finalmente emigra. Por eso es que la eliminación de criaderos debe realizarse preventivamente, antes de que se produzca un foco epidémico en el lugar. Comenzar a eliminar criaderos donde se da el foco es una medida controvertida, ya que promueve que la enfermedad se traslade a zonas vecinas.&lt;/p&gt;
&lt;h3 id=&#34;sobre-la-enfermedad&#34;&gt;Sobre la enfermedad&lt;/h3&gt;
&lt;p&gt;- &lt;strong&gt;¿Qué tipos de Dengue hay?&lt;/strong&gt; - &lt;strong&gt;H.S.&lt;/strong&gt;: Se suele hablar de dengue hemorrágico y dengue común. Ocurre que hay cuatro cepas virales de dengue, nombradas DEN1 a DEN4. Si nunca tuvimos dengue, cualquiera de estas cepas nos produce la enfermedad. Si tuvimos dengue, tendremos anticuerpos para esa cepa, digamos DEN1, pero la infección de otra cepa nos producirá un cuadro caracterizados por hemorragias. Este cuadro, cuando no es detectado a tiempo o no es atendido adecuadamente, es el que produce casi toda la mortalidad. En el año 2008, alguna cepa circulante por Paraguay era capaz de producir hemorragias y cuadros severos en la primera infección.&lt;/p&gt;
&lt;p&gt;- &lt;strong&gt;¿Cuál es el diagnóstico diferencial? ¿Cómo distinguir entre dengue y gripe?&lt;/strong&gt; - &lt;strong&gt;H.S.&lt;/strong&gt;: No soy médico, pero sí he tenido oportunidad de escuchar a los expertos del Hospital Muñiz explicándoselo a médicos de Capital Federal. En rasgos generales la cuestión es así: Los síntomas iniciales son similares a los de la gripe, como dolor de cabeza, fiebre alta y náuseas. Suele aparecer un dolor característico “detrás” del ojo. El dolor en las articulaciones suele ser intenso, por lo que a la enfermedad se la conoce como “quebranta huesos”.&lt;/p&gt;
&lt;p&gt;La mayor diferencia consiste en que el dengue no produce desarrollo de mucosidad. Por eso, se recomienda a los médicos, que ante un cuadro compatible con dengue, se indique al paciente que regrese a la consulta para un control. En caso de confirmarse los síntomas de dengue, así se puede actuar a tiempo. A los fines de controlar la epidemia, yo agrego que todo caso tipo gripe debe ser tratado como un caso de dengue. Es decir, debemos extremar las medidas como el uso de tules y mosquiteros (práctica hospitalaria) y repelentes (espirales, vaporizadores eléctricos, etcétera) para evitar que el mosquito sano pique al enfermo de dengue. De esta forma se corta el ciclo de la epidemia. Esperar a la confirmación del caso es actuar tarde. No sirve.&lt;/p&gt;
&lt;p&gt;- &lt;strong&gt;¿Y entre dengue y fiebre amarilla?&lt;/strong&gt; - &lt;strong&gt;H.S.&lt;/strong&gt;: Como ya dije, dengue y fiebre amarilla son extremadamente parecidos en su forma de transmisión (mismos vectores, mismo sistema) y los virus pertenecen a la misma familia. La evolución clínica inicial es similar en ambos casos, los tests de laboratorios más accesibles como el llamado ELISA (realizados ya por centros especializados), solo confirman flavivirus y no distinguen entre ellos. Hay tests específicos que solo se realizan en el Instituto Maiztegui de Pergamino, que ante una epidemia como la presente esencialmente colapsa.&lt;/p&gt;
&lt;p&gt;- &lt;strong&gt;¿Existen vacunas?&lt;/strong&gt; - &lt;strong&gt;H.S.&lt;/strong&gt;: Para el dengue no hay vacunas, uno enfrenta un virus complicado para el sistema inmune. El dengue hemorrágico es una clara manifestación de que el pensamiento más simplista respecto de generar anticuerpos al tener la enfermedad, no es adecuado.&lt;/p&gt;
&lt;p&gt;- &lt;strong&gt;¿Qué hacer ante la sospecha de la enfermedad?&lt;/strong&gt; - &lt;strong&gt;H.S.&lt;/strong&gt;: Ante un cuadro clínico compatible se debe ir al médico. No se debe tomar aspirina, pues es anticoagulante y puede agravar el cuadro, otros antipiréticos no tienen este problema. Concurrir al médico obviamente. El médico deberá hacer un seguimiento del cuadro para poder distinguirlo de otras enfermedades de igual comienzo, no solo cuadros gripales, es común confundirlo con la leptospirosis, por ejemplo. El caso del paciente fallecido de fiebre amarilla en Misiones (2008) se trató de un diagnóstico equivocado de leptospirosis. El paciente fue dado de alta y en su casa, sin asistencia médica, se desarrolló el cuadro tóxico de la fiebre amarilla. La hija del muerto, también diagnosticada con leptospirosis fue re-examinada y se confirmó fiebre amarilla. El caso no solo ilustra el problema del diagnóstico, sino cómo el contagio se suele dar en núcleos familiares que “comparten mosquitos”.&lt;/p&gt;
&lt;p&gt;- &lt;strong&gt;¿Cuál es el tratamiento?&lt;/strong&gt; - &lt;strong&gt;H.S.&lt;/strong&gt;: No hay más tratamiento que el control de los síntomas, la estabilización del paciente (en caso de que no esté estabilizado) y, en el caso de dengue hemorrágico, la internación y la hidratación. Los médicos del Muñiz a cargo de enfermedades tropicales enfatizaban que la mortalidad en dengue hemorrágico se controla manteniendo al paciente hidratado, entiendo que por vía intravenosa. Habría que preguntarle a los expertos.&lt;/p&gt;
&lt;p&gt;- &lt;strong&gt;¿Cuál es la mortalidad?&lt;/strong&gt; - &lt;strong&gt;H.S.&lt;/strong&gt;: 5 por 1.000 de los casos de dengue diagnosticados clínicamente se convierten en cuadros graves, y aproximadamente la mitad de los graves en mortales. Pero estadísticas como estas no son confiables, por lo general existen problemas metodológicos al realizarlas. La estadística es un arte muy difícil y suele haber mucho de arbitrario y antojadizo en la interpretación, aun en publicaciones reputadas como científicas.&lt;/p&gt;
&lt;p&gt;- &lt;strong&gt;¿Quedan secuelas?&lt;/strong&gt; - &lt;strong&gt;H.S.&lt;/strong&gt;: No, que yo sepa, tanto de fiebre amarilla como de dengue, la recuperación es total.&lt;/p&gt;
&lt;p&gt;- &lt;strong&gt;¿Es el dengue una enfermedad de los pobres?&lt;/strong&gt; - &lt;strong&gt;H.S.&lt;/strong&gt;: Este es un concepto peligroso. La falta de medios suele exponernos más a todas las enfermedades, ¿qué no decir de una mala alimentación o un sistema inmunológico deprimido? Las condiciones de vida que impone la falta de recursos siempre son negativas. Pero el dengue no es una enfermedad exclusiva de los pobres. Imaginemos un escenario posible. Juan y su familia viven en una casa quinta o en un barrio privado. Este año decidieron tomar las vacaciones en Brasil, donde existe una importante epidemia de dengue que involucra la ciudad costera que ellos visitarán, “naturalmente” nadie les avisó. Para el final de la estadía Juan es picado por un mosquito infectado y regresa a su casa incubando el dengue. En la casa, Juan tiene unos hermosos potus con agua, unos desagües pluviales excelentes, etc. Todo esta pulcro y cuidado. Hay algunos mosquitos, pero ¿quién puede librarse de ellos? Molestan al amanecer y al atardecer, pero nada del otro mundo ¿quién no soporta entre 3 y 5 picaduras por día?&lt;/p&gt;
&lt;p&gt;El no imagina que tiene una probabilidad mayor al 98% de infectar esos mosquitos y una probabilidad cercana al 90% de que alguien se contagie dengue en su casa. Sin embargo, en esos potus, en esos desagües construidos con criterio hidráulico pero sin criterio epidemiológico Juan cría &lt;em&gt;Aedes aegypti&lt;/em&gt;, en número suficiente como para iniciar una epidemia o un brote desde su propio hogar. Juan no hizo nada por librar su casa de criaderos de mosquitos, porque al final, el dengue es una enfermedad de los pobres, ¿no? Y Juan sabe bien que él no es pobre.&lt;/p&gt;
&lt;p&gt;- &lt;strong&gt;¿Qué condiciones deben darse para que se desencadene una epidemia?&lt;/strong&gt; - &lt;strong&gt;H.S.&lt;/strong&gt;: Las condiciones básicas son la presencia de mosquitos &lt;em&gt;Aedes aegypti&lt;/em&gt; en forma adulta (durante el invierno no están más que en la forma de huevos), el arribo de un enfermo contagioso que sea picado por ellos y un poco de mala suerte. Cuando hay una epidemia cercana, como ocurría en el norte del país con el dengue circulando por Bolivia, Paraguay y Brasil, donde el mosquito se encuentra establecido, la única barrera que quedaba era la suerte. El dengue prueba y prueba. ¿Hoy no prendió? Mala suerte, mañana mandamos otro infectado y así hasta que prende. Las comunicaciones internas, la rapidez de los viajes en estos tiempos, la presencia del mosquito en la zona de La Plata, y el Gran Buenos Aires, por mencionar solo dos regiones, la extensión de la temporada cálida, el aparente fin de la sequía, son todos factores que hacen probable o más probable un brote de dengue en la región.&lt;/p&gt;
&lt;p&gt;A favor de la salud juega en general el invierno, ya que al bajar las temperaturas disminuye la velocidad de propagación de la enfermedad y la presencia de mosquitos adultos. Como la evolución de la epidemia toma tiempo, las probabilidades de una importante epidemia en la zona este año son bajas. Lo que hagamos nosotros destruyendo sitios de cría ayudará a que bajen aún más y se mantengan así el próximo verano. Donde el acceso al agua es difícil (no hay agua corriente) se deben modificar las costumbres de almacenar agua de lluvia, pues los depósitos suelen ser enormes criaderos de estos mosquitos.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contra el dengue, lo único que funciona es la prevención con participación comunitaria. El mosquito que me transmitirá el mal, si eso ocurre, casi con seguridad se crió en mi casa o en la de mis vecinos.&lt;/strong&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>¿Es la Química una rama de la Física?</title>
      <link>https://ciencianet.com.ar/post/es-la-quimica-una-rama-de-la-fisica/</link>
      <pubDate>Mon, 30 Mar 2009 21:43:25 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/es-la-quimica-una-rama-de-la-fisica/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Osvaldo Cappannini&lt;/strong&gt;. Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP)&lt;/p&gt;
&lt;p&gt;El título resulta ya elocuente en cuanto a la postura de los autores reflejada cabalmente en el artículo. La pretensión del positivismo lógico a principios del siglo XX era, tal cual lo dicen Martín Labarca y Olimpia Lombardi, la unidad de las ciencias a partir de considerar, desde una óptica absolutamente reduccionista, que todas las disciplinas científicas podían ser explicadas por medio de las leyes de la Física.&lt;/p&gt;
&lt;p&gt;En este marco, los autores abordan la relación entre la Química y la Física, en la que la primera fue relegada a una posición inferior considerándola obtenible de la cuántica. ¿Puede el mundo químico en su totalidad ser reconstruido a partir de las leyes que la Física ha generado para su modelo atómico de la materia? El artículo de Labarca y Lombardi recorre algunos de los argumentos planteados en cuanto a lo epistemológico, tanto desde la perspectiva reduccionista (la Química concebida como una rama de la Física y reducida a una mera disciplina “fenomenológica”) como la no reduccionista (la Química como una disciplina autónoma, con leyes y nociones no derivables de ninguna otra disciplina) y también en cuanto a lo ontológico (desde la postura reduccionista se considera indiscutible la dependencia ontológica del mundo químico respecto del mundo físico mientras que el punto de vista no reduccionista propone que sólo tenemos acceso a la realidad a través de nuestras teorías y que, entonces, cada teoría constituye una ontología relativa desde la cual construimos nociones).&lt;/p&gt;
&lt;p&gt;A través de la discusión de algunos de estos argumentos, el artículo induce a profundizar en un debate abierto y necesario que influye poderosamente en la formulación no sólo de propuestas didácticas para las ciencias sino, además, en la actividad científica de las disciplinas citadas (extensible a la Biología y a las demás Ciencias Naturales) y a un severo cuestionamiento de presupuestos muy poco discutidos durante la formación y la actividad en ciencias.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo Original:&lt;/strong&gt; Martín Labarca, Olimpia Lombardi, The end of the dream of unity, &lt;a href=&#34;https://d1wqtxts1xzle7.cloudfront.net/3468375/438.pdf?response-content-disposition=inline%3B+filename%3DThe_end_of_the_dream_of_unity.pdf&amp;amp;Expires=1608001017&amp;amp;Signature=EGiPc4yhjDDAEjhZYCfWOjXrdDwoyw7~4MM5nr50SjOTg61A1j94RmHiytJgY3ozU4g02J9Ie2F~xHVwlux6EMJ4KP6ynIWmpNZuC7y~CUaf-UIdf27ULyKrMWdVfTiZKWmkCBNz0OQT618fpz6OSrao2EYqTar71Bs6~0V7pxKJbfBW5qn58rp4vJGBZqgpxqXOztkG2J4sgooG2vyDyDdz4diuQRwN9VwFuZ338w7q9qEUmCelxylmAYN4MLcHYcIgo4ybK6-nJP63mq-nrWAtTG5WoxBeKR9uSEZus8qMK5ehxJmZ9PZiSs-7sqJRB1lH-fa3TD9InwLMaoIs3Q__&amp;amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA&#34;&gt;Current Science&lt;/a&gt; , vol. 94, pag. 438 (2008).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; Martín Labarca (E-mail: &lt;a href=&#34;mailto:mglabarca@unq.edu.ar&#34;&gt;mglabarca@unq.edu.ar&lt;/a&gt;)&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Una mirada láser sobre el pasado: instrumentos arqueológicos de roca estudiados mediante microscopía confocal de barrido</title>
      <link>https://ciencianet.com.ar/post/una-mirada-laser-sobre-el-pasado-instrumentos-arqueologicos-de-roca-estudiados-mediante-microscopia-confocal-de-barrido/</link>
      <pubDate>Sun, 15 Mar 2009 21:49:21 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/una-mirada-laser-sobre-el-pasado-instrumentos-arqueologicos-de-roca-estudiados-mediante-microscopia-confocal-de-barrido/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero.&lt;/strong&gt; Departamento de Física, Facultad de Ciencias Exactas y Naturales, Universidad de Buenos Aires.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/03/Philos1.jpg&#34; alt=&#34;Imagen del equipo utilizado.:left&#34;&gt; En general, en el mundo de la arqueología se acepta que pueden diferenciarse las superficies de piedra de objetos arqueológicos cortantes que hayan sido pulidas por su uso sobre distintos materiales (madera, hueso, cuero). Sin embargo, las técnicas tradicionales de identificación microscópicas se consideran actualmente subjetivas y sujetas al sesgo de la observación humana.&lt;/p&gt;
&lt;p&gt;A partir de este problema, en las últimas décadas se han desarrollado diversas técnicas más o menos exitosas. En este contexto, en el Laboratorio de Procesamiento Láser del CIOP, con participación del Departamento de Física, la Facultad de Informática de la Universidad Nacional La Plata y el Museo de Antropología de la Universidad Nacional de Córdoba, se desarrolló un conjunto de programas de análisis de imágenes llamado Philos.&lt;/p&gt;
&lt;p&gt;Este paquete fue diseñado por Damián Gulich, Roxana Cattaneo, Nahuel Lofeudo, Pablo Meilán y Mario Garavaglia. Philos fue creado específicamente para el estudio y la caracterización funcional de instrumentos arqueológicos de roca, como hachas y cuchillos, permitiendo determinar el uso particular que tuvieron. Mediante el escaneo de los filos de los instrumentos con un Microscopio Láser Confocal de Barrido (CLSM) [&lt;a href=&#34;#1&#34;&gt;1&lt;/a&gt;] y el posterior análisis con Philos de las imágenes obtenidas es posible entonces estimar automáticamente si un instrumento de roca fue empleado por ejemplo, para cortar carne seca o para tallar madera. ¿Cómo se realiza la identificación? Comparando la imagen obtenida con un conjunto de imágenes de referencia provenientes de una colección experimental con patrones de uso conocidos, hecha para tal propósito.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/03/Philos2.jpg&#34; alt=&#34;Algunos resultados del análisis arqueológico.&#34;&gt;&lt;/p&gt;
&lt;p&gt;Los autores estudiaron también los requerimientos mínimos de magnificación de la imagen, así como el tiempo de uso de la pieza que resultan necesarios para poder establecer un análisis con valor diagnóstico de la pieza estudiada, información que resulta muy relevante en Arqueología. El uso del CLSM y el paquete Philos presentan ventajas en cuanto a la fiabilidad con respecto a los métodos usados tradicionalmente para el estudio de instrumentos líticos.&lt;/p&gt;
&lt;p&gt;Los investigadores han aplicado con éxito el software Philos a materiales líticos provenientes de sitios arqueológicos de la Patagonia argentina, donde las estimaciones de uso del paquete de programas fueron confirmadas por la técnica de espectrofotometría infrarroja [&lt;a href=&#34;#2&#34;&gt;2&lt;/a&gt;] de los residuos de materiales pegados a las piezas.&lt;/p&gt;
&lt;p&gt;Para obtener más información sobre Philos puede consultarse a &lt;a href=&#34;mailto:dgulich@ciop.unlp.edu.ar&#34;&gt;dgulich@ciop.unlp.edu.ar&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referencias:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;1&#34;&gt; 1. &lt;/a&gt; Es una técnica que permite obtener imágenes de mejor calidad (nitidez, contraste y resolución) que la microscopía tradicional. Además, permite un estudio tridimensional de la muestra. El microscopio confocal permite estudiar una superficie de modo que el detector reciba sólo la luz procedente de un dado plano focal. El uso de láser como fuente de luz requiere la realización de un barrido, ya que el haz láser ilumina una pequeña sección de la superficie por vez.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;2&#34;&gt; 2. &lt;/a&gt; Es una técnica óptica que permite identificar sustancias a partir de su absorción y emisión de radiación ultravioleta.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>BNCT: Neutrones, Aceleradores y Oncología</title>
      <link>https://ciencianet.com.ar/post/bnct-neutrones-aceleradores-y-oncologia/</link>
      <pubDate>Sun, 15 Mar 2009 21:44:21 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/bnct-neutrones-aceleradores-y-oncologia/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Diego Vallejo&lt;/strong&gt;: Facultad de Ingeniería - Universidad Nacional de La Plata. En colaboración con &lt;strong&gt;Sandra Maguid&lt;/strong&gt;, Universidad Nacional de Quilmes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Este artículo ha sido financiado por el proyecto INVOFI de la Asociación Física Argentina.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cáncer&lt;/strong&gt;: un conjunto de enfermedades que desequilibran el control celular. Aparecen células que se multiplican en exceso y adquieren carácter maligno. Al cáncer lo sufren los que lo padecen y los que están cerca de los que lo padecen. Google devuelve casi 300 millones de páginas relacionadas con esa palabra. El cáncer es siempre noticia, a veces es susurro y otras veces es política: en 1971 Richard Nixon (entonces presidente de los Estados Unidos) se tomó un momento para declararle “norteamericanamente” la guerra al cáncer, con las siguientes palabras:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;“Ha llegado el momento en Norteamérica en que el mismo tipo de esfuerzo mancomunado que logró dividir el átomo, y llevar al Hombre a la Luna, debe dirigirse a la conquista de esta terrible enfermedad”&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/03/DV-fig1-300x200.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Pero aquí, en Argentina, en la Comisión Nacional de Energía Atómica, lejos de las declaraciones rimbombantes, el Dr. Andrés Kreiner, Jefe del Grupo de Espectroscopía Nuclear, trabaja junto con otros científicos en una terapia alternativa para la enfermedad. Nos recibe amablemente en su laboratorio del Centro Atómico Constituyentes y nos introduce en un sector amplio, con mobiliario electrónico, multitud de llaves, botones y controles que deslizan sobre escalas graduadas. Y todo ocurre casi tranquilamente, en medio de un trabajo que no se detiene.&lt;/p&gt;
&lt;h3 id=&#34;terapias-y-límites&#34;&gt;Terapias y límites&lt;/h3&gt;
&lt;p&gt;¿Cuál es el ideal de un tratamiento oncológico? Sin duda, destruir o detener todas las células malignas sin causar daño a las sanas. Las terapias usuales por radiación gama funcionan con tumores localizados. Sin embargo, en el caso de los melanomas y los glioblastomas, el éxito de las técnicas convencionales es limitado. En palabras de Kreiner: –&lt;em&gt;Hay cánceres geométricamente difusos, hay células &amp;quot;guerrilleras&amp;quot; que infiltran el tejido normal circundante. El volumen afectado no está bien definido. Si el tumor es infiltrante y difuso, esa estrategia [la de los rayos gama enfocados] fracasa&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/03/DV-fig2-300x193.jpg&#34; alt=&#34;:right&#34;&gt;&lt;/p&gt;
&lt;p&gt;Un tratamiento alternativo en el que trabaja el equipo de Kreiner es la Terapia por Captura Neutrónica en Boro (BNCT). –&lt;em&gt;La idea fue propuesta por Gordon Locher en 1936, poquito tiempo después del descubrimiento del neutrón, que fue en 1932&lt;/em&gt; –refiere. Hay dos etapas en el BNCT: primero se inyecta al paciente una sustancia, que contiene boro 10, un elemento no tóxico, no radiactivo. Esta sustancia debe ser afín a las células cancerosas. En el caso ideal, el boro es absorbido por todas las células tumorales y no por las células sanas.&lt;/p&gt;
&lt;p&gt;En la segunda etapa se irradia con neutrones, los que son capturados con alta eficiencia por el boro. Entonces ocurre una reacción nuclear y se emiten dos partículas: un núcleo de litio y una partícula alfa. Al recorrer su camino dentro del tejido vivo y frenarse el litio y la particula alfa producen un gran daño biológico. Nos dice Kreiner: –&lt;em&gt;Esa es la ventaja del BNCT: que toda la radiación queda localizada en la célula que tiene boro. [...] Los rangos [donde queda la radiación] son entre 5 y 10 micrones, del tamaño típico de una célula.&lt;/em&gt; Y agrega: –&lt;em&gt;Uno busca producir daño al ADN para impedir que prolifere, para esterilizar [la célula cancerosa]. Estas partículas producen un daño muy complejo. Para ejemplificarlo, en vez de romper una cadena, rompe las dos del ADN. Y el tejido vivo [que incluye a las células malignas] no tiene medios eficientes para reparar este daño. Y así se destruyen o dejan de proliferar las células malignas, que es lo que uno quiere.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;–¿Qué es lo que demora tanto la implementación del BNCT? –&lt;em&gt;La idea es genial, pero la implementación práctica está llevando muchísimo tiempo. Por muchos motivos. Uno fue obtener una droga suficientemente selectiva [la sustancia que transporta al boro]. Y además se irradiaba a los pacientes con haces de neutrones térmicos, que se absorben desde la superficie, en el hidrógeno y el nitrógeno, entonces todo el tejido anterior recibía mucha más dosis que el propio tumor. Eso hizo que esta técnica fracasase. Después de unos cuantos años la gente dijo &amp;quot;no usemos neutrones térmicos sino epitérmicos&amp;quot;, que tienen 10 keV y eso hace que pueda penetrar el haz sin ser absorbido, que se termalice en el propio tejido y llegue térmico al tumor donde está el boro. Son varias cosas que la gente trabajosamente fue aprendiendo.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;entender-el-efecto&#34;&gt;Entender el efecto&lt;/h3&gt;
&lt;p&gt;¿De qué depende el éxito del BNCT? ¿Cuáles son las condiciones para que sea efectivo? –Cuéntenos cómo se evalúa el efecto específico del litio y de la particula alfa. –&lt;em&gt;Irradiamos cultivos celulares con haces de litio, cuya energía podemos fijar. Podemos fijar la dosis, podemos fijar claramente las condiciones de trabajo. Y medir la eficacia biológica relativa, que es el parámetro que sirve para comparar el efecto de diferentes iones relativamente a la radiación gama, que es la radiación de referencia porque se usa en radioterapia convencional.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;–¿Y respecto a la distribución de boro? ¿La sustancia que lo transporta logra llevarlo a las células cancerosas? –&lt;em&gt;Hemos utilizado el microhaz de iones pesados del Laboratorio, y hemos estudiado dentro del tejido cómo se microlocaliza la droga.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/03/DV-fig3-300x272.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Andrés Kreiner hace una pausa. Escucha los comentarios de otros investigadores, se excusa brevemente y se reúne con su equipo. Argumenta, evalúa la marcha de la experiencia que está ocurriendo. Luego vuelve y continúa explicándonos. –&lt;em&gt;La muestra es un corte muy delgado de un tejido tumoral de un hámster previamente inyectado con el compuesto de boro cuya microdistribución queremos estudiar. Con un haz de oxígeno uno barre la muestra. Esto produce rayos X característicos, que se generan en el cobre del compuesto químico que contiene al boro. Esto me permite detectar trazas y saber dónde está y cuánto boro hay en cada punto de la muestra. Son intentos de ir entendiendo el problema. Tratar de predecir el efecto radiobiológico del tratamiento.&lt;/em&gt; Una vez que los especialistas pueden evaluar y entender el efecto en tejidos de animales de laboratorio, disponen de modos específicos de trasladar ese conocimiento a los efectos en tejidos humanos.&lt;/p&gt;
&lt;h3 id=&#34;neutrones-y-aceleradores&#34;&gt;Neutrones y Aceleradores&lt;/h3&gt;
&lt;p&gt;–Háblenos del problema de los neutrones. –&lt;em&gt;Hasta ahora el BNCT se hace sólo en reactores porque en nuestra civilización es la única fuente de neutrones de que disponemos&lt;/em&gt; –explica Kreiner y agrega: –&lt;em&gt;Llevar pacientes a un reactor no es lo ideal... y tampoco lo es llevar reactores a un hospital. Entonces lo que nosotros estamos tratando de hacer es desarrollar un relativamente pequeño acelerador capaz de ser instalado en un hospital y que sea generador de neutrones. Un haz de protones incidiendo sobre un blanco conveniente produce una reacción nuclear que genera neutrones. Y esos neutrones son los que queremos utilizar para el BNCT.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;–Está el tema del costo, no es lo mismo hacer un reactor que un acelerador, ¿no? –&lt;em&gt;Sí, y muy a favor del acelerador. Un reactor nuclear es un aparato enormemente complejo, con un costo de inversión muy grande, con problemas de seguridad radiológica importantes, con uranio... y un acelerador es un aparato que uno desenchufa y se acabó la radiación.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/03/DV-fig4-300x208.jpg&#34; alt=&#34;:right&#34;&gt;&lt;/p&gt;
&lt;p&gt;–¿Y si uno quisiera comprarlo, de qué valor estamos hablando? –&lt;em&gt;&amp;quot;Cuando&amp;quot; se desarrollen... porque hoy en día no se puede comprar. No hay ningún acelerador en ningún lugar del mundo que produzca la cantidad y la calidad de neutrones para BNCT. Hay gente que está trabajando hace tiempo en esto. Por ejemplo en Japón, en Italia, en Rusia&lt;/em&gt; –y agrega con entusiasmo– &lt;em&gt;y nosotros estamos corriendo esa carrera&lt;/em&gt;. Kreiner nos explica que últimamente hay cambios en el paradigma del BNCT. Tradicionalmente se pensaba que el boro debía dopar todas las células malignas. Sin embargo se está evaluando que si el boro se localiza en los vasos que irrigan al tejido tumoral, que tienen elevada radiosensibilidad, también podría prevenirse su proliferación. Este conocimiento fue adquirido en nuestro país por la Dra. A. Schwint, colega de la CNEA.&lt;/p&gt;
&lt;h3 id=&#34;aplicación-exitosa&#34;&gt;Aplicación exitosa&lt;/h3&gt;
&lt;p&gt;–¿En algún caso se aplicó BNCT exitosamente? –&lt;em&gt;Está en desarrollo. La droga está en el límite de selectividad aceptable. En Japón [...] algunos pacientes se han tratado con efectos impresionantes. He visto fotografías de personas con tumores de cabeza y cuello inoperables. Recibieron una sola aplicación de menos de una hora, frente a las repetidas aplicaciones en radiación gama. Después de unos meses la cara estaba totalmente limpia.&lt;/em&gt; Pero agrega con cautela: –&lt;em&gt;Tiene una efectividad limitada, porque la droga no es óptima, también en cada paciente el efecto es diferente, la concentración en tejido no es la misma. Hay una variabilidad muy grande propia de cada organismo en particular. Acá en la Argentina se está aplicando con reactores, sobre melanomas de extremidades, brazos y piernas, que no implica riesgo para el paciente, y el control local de los nódulos melanóticos es muy bueno. Lo que pasa es que a los melanomas, una vez que proliferan, no hay manera de pararlos. Es decir, el control local es muy bueno, pero la enfermedad reaparece en otros lugares.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/03/DV-fig6-300x141.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;–&lt;em&gt;En síntesis&lt;/em&gt; –concluye Kreiner– &lt;em&gt;el juego es: primero cargar todas las células tumorales con boro y lo menos posible el tejido sano, y luego irradiar con neutrones. Es una terapia binaria. El tema es: utilizar una droga que tenga una selectividad particular en relación al tumor, y después una máquina chiquita, instalable en un hospital, disponible las 24 horas para poder desarrollar BNCT en el medio idóneo, que es una institución dedicada al cáncer, donde están los médicos y la infraestructura médica adecuada. Trabajamos en relación con el Centro Médico Vidt y el Instituto Roffo.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Para mayor información:&lt;/strong&gt; &lt;a href=&#34;http://www.tandar.cnea.gov.ar/&#34;&gt;http://www.tandar.cnea.gov.ar/&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Historia evolutiva de las proteínas</title>
      <link>https://ciencianet.com.ar/post/historia-evolutiva-de-las-proteinas/</link>
      <pubDate>Wed, 11 Mar 2009 21:52:35 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/historia-evolutiva-de-las-proteinas/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Bruno De Angelis:&lt;/strong&gt; Departamento de Materiales Didácticos, Editorial de la Universidad Nacional de Quilmes.&lt;/p&gt;
&lt;p&gt;Científicos de la Universidad Nacional de Quilmes y La Plata reproducen mediante computadoras la historia evolutiva de familias de proteínas.&lt;/p&gt;
&lt;h3 id=&#34;la-evolución-en-movimiento&#34;&gt;La evolución en movimiento&lt;/h3&gt;
&lt;p&gt;Puestos a comparar, sin mucho rigor por cierto, el hombre (y las proteínas) evolucionaron a lo largo del tiempo adaptándose a los cambios exigidos por el medio ambiente: la postura erecta y el bipedalismo permitieron a los homínidos hacer determinadas tareas que de otra manera no habrían podido realizar. Las proteínas, por su parte, evolucionaron y se adaptaron a nuevas circunstancias también.&lt;/p&gt;
&lt;p&gt;Es posible seguir los pasos de esa evolución relacionando la manera en que se mueven sus átomos con la función biológica que realiza cada una de ellas. Pues bien, a esa específica tarea de conjugar movimiento y función de proteínas a lo largo del tiempo se dedican Sandra Maguid, Sebastian Fernandez-Alberti, Leticia Ferrelli y Julian Echave de la Universidad Nacional de Quilmes (UNQ) y La Plata (UNLP). La idea es explorar el movimiento global de las proteínas que tienen una misma estructura pero distintas funciones y secuencias en su formación.&lt;/p&gt;
&lt;h3 id=&#34;cuestión-de-familia&#34;&gt;Cuestión de familia&lt;/h3&gt;
&lt;p&gt;La evolución –en un sentido amplio– comprende los cambios que se producen en las características de los organismos en períodos largos de tiempo, esto incluye tanto la dimensión temporal como la noción de cambio. Ahora bien, si en lugar de evolución en general se habla de evolución molecular, se está hablando entonces del desarrollo y cambio de moléculas y más concretamente de los ácidos nucleicos (el ADN y el ARN que constituyen el material hereditario, y por tanto los genes) y de las proteínas (que son el producto primario de la expresión de estos genes).&lt;/p&gt;
&lt;p&gt;El tema es así: las proteínas son moléculas que desempeñan una amplia gama de actividades vitales en las células. Están compuestas por cadenas (secuencias) de moléculas más pequeñas, los aminoácidos, que son las unidades de construcción con que se arman las proteínas y que ofician de “bloques” con los que el organismo reconstituye permanentemente las proteínas que se “gastan” en la diaria y ardua tarea de vivir. Existen veinte aminoácidos diferentes, ensamblados en combinaciones distintas para formar proteínas, también diferentes. Por ejemplo, el cabello está constituido por una proteína específica, elaborada con una secuencia determinada de aminoácidos; los músculos del brazo pueden estar compuestos por los mismos veinte aminoácidos, pero se agrupan en una secuencia distinta para producir músculo en vez de cabello.&lt;/p&gt;
&lt;p&gt;De esta manera, el cuerpo fabrica una serie de proteínas diferentes con diversas funciones, usando los mismos veinte bloques de construcción, pero en combinaciones distintas. Las proteínas son cruciales para el funcionamiento y la existencia misma del organismo, desde las enzimas que ayudan a digerir los alimentos a la hemoglobina que transporta el oxígeno en la sangre.&lt;/p&gt;
&lt;p&gt;Una de las formas de optimizar el análisis de la evolución es clasificando las proteínas con algún criterio. Hay muchos criterios de clasificación de proteínas y uno de ellos es hacerlo por familias, por ejemplo las globinas (la más conocida es la hemoglobina) que comparten algunas funciones básicas en todos los organismos. En este caso, la captación de oxígeno.&lt;/p&gt;
&lt;p&gt;Después de comparar cadenas de aminoácidos que constituyen la base molecular de las proteínas de diferentes organismos se concluyó que son proteínas con una función tan necesaria que sobrevivieron al proceso evolutivo (naturalmente, con mutaciones y cambios). Comparando estructuras se pudo elaborar un “árbol genealógico” de proteínas que las relaciona a todas por parentesco, y se logró identificar muchos de los cambios sufridos en su evolución desde una proteína ancestral común.&lt;/p&gt;
&lt;h3 id=&#34;y-sin-embargo-se-mueve&#34;&gt;Y sin embargo se mueve&lt;/h3&gt;
&lt;p&gt;Ahora bien, ¿cómo se llega a estos resultados? Pues estudiando la evolución de las proteínas. De las muchas formas de hacerlo los investigadores de la UNQ y UNLP eligieron partir del movimiento que realizan los átomos en una proteína, y de las muchas formas que éstos tienen de moverse eligieron los movimientos vibracionales.&lt;/p&gt;
&lt;p&gt;Evolutivamente las proteínas que se originaron en un mismo período conservan ciertas características en su estructura, las diferentes mutaciones no cambiaron determinados tipos de movimientos que realizan y que tienen que ver (o no) con su función. Como dijimos, la hemoglobina tiene la función de transportar oxígeno. El trabajo que realiza para cumplir su tarea es múltiple: tiene que moverse de determinada manera para dejar pasar la molécula de oxígeno, debe “atrapar” esa molécula, luego “transportarla” y después “soltarla”.&lt;/p&gt;
&lt;p&gt;Todas estas funciones y muchísimas otras tienen estrecha relación con los movimientos que esa proteína efectúa. Detectar qué movimientos se mantuvieron invariantes a través de millones de años y decir qué es lo que no cambió permite categorizar las proteínas y detectar a qué responden algunas características del proceso evolutivo.&lt;/p&gt;
&lt;h3 id=&#34;en-armonía&#34;&gt;En armonía&lt;/h3&gt;
&lt;p&gt;Siguiendo con las comparaciones, el movimiento de la cadena de aminoácidos de una proteína se asemeja al de una cuerda. Es decir, para cumplir su función, la proteína necesita vibrar. Como cualquier vibración (por ejemplo, la de una cuerda de guitarra o violín) hay un modo fundamental (por ejemplo el do) y muchos armónicos, de frecuencias crecientes. El equipo de investigadores toma los primeros armónicos, a los que llama “modo normal” y estudia ese modo en distintas familias de plegamientos (estructura de las cadenas de aminoácidos en la proteína).&lt;/p&gt;
&lt;p&gt;Como los modos normales de baja frecuencia describen movimientos colectivos (es decir, de todos los átomos de las proteínas) y están estrechamente vinculados a la función biológica, la obtención de los mismos permite identificar los movimientos que se mantuvieron invariantes durante la evolución de cada familia. Una vez determinados esos invariantes en familias de proteínas, es posible desarrollar modelos computacionales de simulación.&lt;/p&gt;
&lt;h3 id=&#34;un-método-para-simular&#34;&gt;Un método para simular&lt;/h3&gt;
&lt;p&gt;Con toda esa información analizada detenidamente, recopilada de diferentes bases de datos y publicaciones disponibles en investigaciones anteriores se procede a calcular nuevos resultados. A partir de un método se crea un algoritmo, es decir, una forma de cálculo matemático que luego será traducido a lenguaje de programación. Finalmente se puede seguir a través de la pantalla de la computadora el proceso evolutivo de la proteína en cuestión. En fin, el nivel de especificidad de la investigación científica parece no tener límites y el proyecto de investigación dinámica parece confirmarlo.¡Tiemblen átomos!, la evolución y su investigación continuarán.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Más información:&lt;/strong&gt; Sandra Maguid, Sebastian Fernandez-Alberti, Leticia Ferrelli, Julian Echave, &lt;a href=&#34;https://doi.org/10.1529/biophysj.104.053041&#34;&gt;Exploring the common dynamics of homologous proteins. Application to the globin family&lt;/a&gt;. Biophysical Journal, vol. 89, pags. 3-13 (2005). Sandra Maguid, Sebastian Fernandez-Alberti, Julian Echave, &lt;a href=&#34;https://doi.org/10.1016/j.gene.2008.06.002&#34;&gt;Evolutionary conservation of protein vibrational dynamics&lt;/a&gt;. Gene, vol. 422, pags. 7-13 (2008).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instituciones:&lt;/strong&gt; Centro de Estudios e Investigaciones (UNQ), &lt;a href=&#34;http://www.inifta.unlp.edu.ar/&#34;&gt;Instituto Nacional de Investigaciones Fisicoquímicas Teóricas y Aplicadas&lt;/a&gt; (UNLP).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; Sandra Maguid (E-mail: &lt;a href=&#34;mailto:smaguid@unq.edu.ar&#34;&gt;smaguid@unq.edu.ar&lt;/a&gt;).&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>La interdisciplina revisitada</title>
      <link>https://ciencianet.com.ar/post/la-interdisciplina-revisitada/</link>
      <pubDate>Mon, 09 Mar 2009 21:54:15 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/la-interdisciplina-revisitada/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Bertha Orozco Fuentes.&lt;/strong&gt; Investigadora del Instituto de Investigaciones sobre la Universidad y la Educación (IISUE) de la Universidad Nacional Autónoma de México.&lt;/p&gt;
&lt;p&gt;Considero un acierto el que Roberto Follari, en su artículo &amp;quot;La interdisciplina revisitada&amp;quot;, retome el debate sobre la problemática de la interdisciplina, desde una perspectiva latinoamericana y en un contexto de cambio y de crisis de todo orden; particularmente Follari señala la crisis de las ciencias sociales y la ausencia de un debate en serio y a fondo sobre el tema.&lt;/p&gt;
&lt;p&gt;Esto no es cuestión menor, sobre todo ante la despolitización del conocimiento hoy en día. Esto es lo que quiero enfatizar de la reflexión que comento, es importante poner en la mira la despolitización de los discursos sobre lo social, y más allá aún, si tomamos en cuenta la escasez de argumentos en relación al conocimiento de lo social frente a la embestida de la denominada sociedad del conocimiento (en singular) que pone el énfasis en el conocimiento aplicativo instrumental para avanzar en la economía del conocimiento que pretende hegemonizar los discursos de las ciencias sociales.&lt;/p&gt;
&lt;p&gt;El autor hace un recuento histórico del tema desde los años ochenta a la fecha (que no resumo, pero invito a los lectores a recorrer los detalles narrados), para mostrar cómo en sus comienzos la discusión en relación a la interdisciplina no dejó de lado las dimensiones política, social, epistemológica y económica; dimensiones que se han borrado o invisibilizado, o replegado para dar paso a una argumentación que prioriza la dimensión económica del asunto, pero no de la economía política, sino de la versión de una economía que favorece al capitalismo actual, voraz y peregrino financieramente hablando (la economía del conocimiento), en donde el conocimiento equivale a lo útil por su valor de cambio en el mercado, porque el conocimiento se ha convertido en mercancía, alejándose del conocimiento o saber socialmente útil, necesario y equitativo.&lt;/p&gt;
&lt;p&gt;Follari apela a recuperar estas dimensiones excluidas y a reconocer los alcances y los límites en donde quedó el debate hace tres décadas aproximadamente y sobre el cual hay que volver a debatir sus límites, y al hacerlo recupera una categoría epistémica central para potenciar un debate, por ahora ausente, la categoría de totalidad, la cual también puede fundamentarse desde diversas perspectivas disciplinarias por cierto.&lt;/p&gt;
&lt;p&gt;Este ángulo del debate me parece significativo y potenciador. Él por su parte, retoma la tradición del pensamiento marxista, entre otras, en donde el conocimiento de lo social justamente articuló y no fragmentó las ópticas disciplinarias, principalmente la política, la social y la económica. Clama por recuperar la discusión de lo social como una unidad, como un todo relación de dimensiones de realidad que no se pueden dispersar en pedazos de estudios disciplinares.&lt;/p&gt;
&lt;p&gt;Este artículo de Follari, en síntesis, es una invitación y una provocación a retomar la interdisciplina con argumentos y con responsabilidad política, en el ámbito de la discusión académica y de la investigación, para pensar de otro modo el asunto. De un modo en que no impere la despolitización del conocimiento, y de asumir el debate seriamente, desde una perspectiva político social que articule las dimensiones canceladas o desplazadas en su trayecto histórico.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo Original:&lt;/strong&gt; Roberto Follari, &amp;quot;&lt;a href=&#34;http://www.uacm.edu.mx/andamios/anteriores02.html&#34;&gt;La interdisciplina revisitada&lt;/a&gt; &amp;quot;, Andamios: revista de investigación social, Nro. 2, pags. 7-18 (2005)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; Roberto Follari (E-mail: &lt;a href=&#34;mailto:robfollari@ciudad.com.ar&#34;&gt;robfollari@ciudad.com.ar&lt;/a&gt;)&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>¿Es posible controlar la luz difractada por una red de difracción?</title>
      <link>https://ciencianet.com.ar/post/es-posible-controlar-la-luz-difractada-por-una-red-de-difraccion/</link>
      <pubDate>Fri, 27 Feb 2009 21:55:10 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/es-posible-controlar-la-luz-difractada-por-una-red-de-difraccion/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro&lt;/strong&gt;. Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Este artículo ha sido financiado por el proyecto INVOFI de la Asociación Física Argentina.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;¡Sí!...y se logra empleando elementos dispersores distribuidos de forma determinada. Científicos argentinos han estudiado en forma numérica este fenómeno y lograron dicho &lt;em&gt;dominio&lt;/em&gt; utilizando un conjunto de cilindros de sección circular. Lo que es aún más sorprendente es que la predicción de las direcciones en que se intensificarán la luz reflejada y la luz transmitida es independiente del material de los cilindros y de la polarización de la luz incidente.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/02/interf-300x116.jpg&#34; alt=&#34;Esquema de la estructura dispersora que forma una red finita de período d.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Este control es posible a partir de un efecto meramente geométrico. La estructura en estudio posee lo que los investigadores denominan “doble período”, la cual está formada por un número determinado de cilindros circulares de igual radio uno al lado del otro, cuya distancia entre centros se mantiene constante. Este sub-arreglo se repite un cierto número de veces y conforma una especie de “red de cables” inmersa en aire como se observa en la figura. La presencia de esta geometría en particular, modifica tanto la respuesta de la intensidad de la luz reflejada como la de la luz transmitida. Este trabajo de investigación no sólo es innovador desde el punto de vista teórico sino que también tiene un acercamiento “realista” pues la estructura con la que se trabaja tiene dimensiones finitas y puede estar formada tanto de un material aislante como de un metal.&lt;/p&gt;
&lt;p&gt;El radio de los cilindros modelados fue de 50 nm (un nanómetro, nm, es la milmillonésima parte de un metro) y se los consideró formados tanto de un material metálico, plata, como de un material aislante, sílice. Para un cierto número de cilindros en cada sub-arreglo, iluminando con luz infrarroja y para distintos ángulos de observación, se puede lograr la &lt;strong&gt;cancelación&lt;/strong&gt; o bien la &lt;strong&gt;minimización&lt;/strong&gt; de algunos de los órdenes de la luz difractada en la reflexión y en la transmisión; y si se modifica la cantidad de cilindros en el sub-arreglo es posible lograr la &lt;strong&gt;intensificación&lt;/strong&gt; de otros órdenes, lo cual no ocurre para el caso de una red simple (formada por un único cilindro en cada sub-arreglo). Por lo tanto, la geometría de la estructura dispersora es la causante de, por ejemplo, obtener en ciertas direcciones que la intensidad de la luz reflejada sea despreciable; y lo que es más importante es que estas características se mantienen para cualquier ángulo de incidencia.&lt;/p&gt;
&lt;p&gt;El gran interés en el estudio de la respuesta electromagnética de este tipo de estructuras complejas radica en las posibles y prometedoras aplicaciones; algunas de ellas son el diseño de polarizadores y de filtros o bien de superficies que permitan saber de antemano las características de la luz difractada; entre otras.&lt;/p&gt;
&lt;p&gt;Este estudio también es aplicable a otras regiones del espectro electromagnético, como por ejemplo las microondas o las ondas milimétricas. Además, en este trabajo se verificó que la obtención de una respuesta específica de cierta clase de estructuras dispersoras es independiente del material con el cual fueron construidas, hecho que facilitaría notablemente su diseño y manufactura.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo Original:&lt;/strong&gt; Marcelo Lester, Diana C. Skigin, y Ricardo A. Depine. &lt;a href=&#34;https://doi.org/10.1364/AO.47.001711&#34;&gt;Control of the diffracted response of wire arrays with double periods&lt;/a&gt;. Appl. Opt. 47, 1711-1717 (2008).&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Prevención de infecciones perinatales por Streptococcus beta-hemolítico grupo B en Argentina</title>
      <link>https://ciencianet.com.ar/post/prevencion-de-infecciones-perinatales-por-streptococcus-beta-hemolitico-grupo-b-en-argentina/</link>
      <pubDate>Mon, 16 Feb 2009 21:58:07 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/prevencion-de-infecciones-perinatales-por-streptococcus-beta-hemolitico-grupo-b-en-argentina/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero.&lt;/strong&gt; Departamento de Física, Facultad de Ciencias Exactas y Naturales, Universidad de Buenos Aires.&lt;/p&gt;
&lt;p&gt;El &lt;em&gt;streptococcus agalactiae&lt;/em&gt; o estreptococo beta-hemolítico grupo B (EGB) es una bacteria que suele habitar el tracto digestivo de los seres humanos y puede colonizar además las vías genitales, rectal y urinaria. Si bien es generalmente inofensiva y no presenta síntomas, la presencia de EGB reviste seriedad en el caso de los bebés nacidos expuestos durante el parto vaginal de madres infectadas (entre el 5 y el 30% de las gestantes). Si bien la mayoría de los bebés no desarrollan enfermedad alguna, entre el 1 y 2% contraen la infección y pueden desarrollar complicaciones tempranas serias e incluso mortales como infección en la sangre (sepsis), neumonía, meningitis, artritis, infección de piel, etc. En nuestro país, según la Sociedad Argentina de Pediatría [&lt;a href=&#34;#1&#34;&gt;1&lt;/a&gt;], la incidencia de la enfermedad en recién nacidos era en 1999 de 0.6 a 1 por mil nacidos vivos, reportándose además un aumento de los casos en los últimos 20 años en todo el mundo.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/02/Streptococcus_agalactiae-300x204.jpg&#34; alt=&#34;Streptococcus agalactiae: Fuente: MicrobeLibrary.org:left&#34; title=&#34;Streptococcus agalactiae&#34;&gt;&lt;/p&gt;
&lt;p&gt;Por este motivo, la detección del EGB en las vías rectal y vaginal de las embarazadas es un análisis de rutina en la mayoría de los países, que de resultar positivo es tratado con antibióticos. Como la bacteria tiende a recolonizar fácilmente luego de ser combatida, el examen se realiza entre las semanas 35 a 37 de gestación y el tratamiento para prevenir la infección del recién nacido se aplica a la madre inmediatamente antes y durante el parto.&lt;/p&gt;
&lt;p&gt;En Argentina, la Cámara de Senadores aprobó en Abril de 2008 el proyecto de ley que establece la obligatoriedad de la realización del cultivo para la detectar el EGB en todas las embarazadas del país. El Consenso de la SAP indicaba un tratamiento único con antibióticos (penicilina como primera opción, ampicilina como alternativa y clindamicina en el caso de alergia a los anteriores). Sin embargo, el uso extendido de estos antibióticos en condiciones clínicas diversas ha favorecido la emergencia de la resistencia antibiótica de la bacteria EGB, y por lo tanto una reducción en la eficacia de el tratamiento tradicional.&lt;/p&gt;
&lt;p&gt;Un estudio reciente de la Universidad Nacional de Misiones [&lt;a href=&#34;#2&#34;&gt;2&lt;/a&gt;] sobre la tasa de colonización y la susceptibilidad a los antibióticos en mujeres gestantes realizado en el Hospital Dr. Ramón Madariaga de Posadas sugiere que tanto la incidencia de la colonización como el antibiótico a emplear a emplear en la profilaxis intraparto dependería de variaciones geográficas y de las características de la población estudiada. Por ejemplo, la colonización materna es del 1.4% en Córdoba, 3.2 % en Rosario y 18.2% en el Hospital José de San Martín en Capital Federal.&lt;/p&gt;
&lt;p&gt;En cuanto al tratamiento, el trabajo citado analiza la susceptibilidad de las cepas de EGB a diversos antibióticos a fin de recomendar elecciones alternativas a las utilizadas hasta ahora. La investigación sugiere que la resistencia a la clindamicina y eritromicina (drogas de elección para la profilaxis de las gestantes alérgicas a la penicilina) es diversa a lo largo del país, y por lo tanto la profilaxis óptima para esta población debería ser guiada por los patrones de resistencia antibiótica de cada región.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referencias&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;1&#34;&gt; 1. &lt;/a&gt; Consenso de Infecciones Perinatales 1999, Sociedad Argentina de Pediatría.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;2&#34;&gt; 2. &lt;/a&gt;&lt;em&gt;Antibiotic susceptibility patterns and prevalence of Group B streptococcus isolated from pregnant women in Misiones, Argentina&lt;/em&gt;. M. Quiroga, E. Pegels, E. Peryra, M. Vergara. Brazilian Journal of Microbiology (2008) 39: 245-250.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>¿Puede la fricción generar rayos X?</title>
      <link>https://ciencianet.com.ar/post/puede-la-friccion-generar-rayos-x/</link>
      <pubDate>Tue, 13 Jan 2009 21:59:08 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/puede-la-friccion-generar-rayos-x/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Diego Maza.&lt;/strong&gt; Grupo de Medios Granulares. Universidad de Navarra.&lt;/p&gt;
&lt;p&gt;La tribología es una rama de la ciencia poco conocida y escasamente explorada por la comunidad científica internacional. Su objeto de estudio son los procesos de fricción y desgaste que aparecen entre diferentes cuerpos cuando estos interactúan entre sí mediante fuerzas de contacto.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/01/RX.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Un caso típico de esa interacción viene dado por la separación de dos superficies que interaccionan mediante un medio adhesivo como ocurre comúnmente con las cintas tipo Scotch que todos tenemos en casa. Durante este proceso, comúnmente conocido como “&lt;em&gt;peeling&lt;/em&gt;”- su traducción literal al castellano sería “decapado”- la variedad de procesos físico-químicos que tienen lugar pueden dar lugar a sorprendentes respuestas como la generación de rayos X (RX).&lt;/p&gt;
&lt;p&gt;Tal extremo había sido reportado por científicos rusos durante la época soviética en idioma ruso, razón por la cual, su existencia era prácticamente ignorada por el resto de la comunidad científica. Recientemente sin embargo, este fenómeno ha sido “redescubierto” por científicos del Departamento de Física y Astronomía de la Universidad de California, quienes han demostrado la generación de fotones de altas energías cuando desenrollamos un trozo de cinta Scotch (¡el resultado no depende del fabricante!) de su carrete.&lt;/p&gt;
&lt;p&gt;Este estudio [&lt;a href=&#34;#1&#34;&gt;1&lt;/a&gt;], ha puesto de manifiesto que aún a velocidades moderadas de separación de la cinta del rollo, se pueden generan RX -además de fotones de un amplio espectro de energía- si el proceso tiene lugar en un vacío moderado (1 mTorr). El mecanismo de generación de esta radiación no se conoce aún con precisión, existiendo de hecho, un par de teorías contrapuestas que intentan dar una explicación del mismo.&lt;/p&gt;
&lt;p&gt;Sin entrar en detalles, resulta claro que la generación de rayos X requiere de altas energías. Así, generar “triboelectrones” -como se conoce a los electrones generados mediante un proceso de fricción- lo suficiente energéticos como para generar radiación de frenado deben ser producidos por una diferencia de potencial muy alta. Esta diferencia de potencial podría aparecer - especulan los autores- entre el material de la cinta (de base acrílica y cargado positivamente) y el rollo de polietileno y cargado negativamente. Así las cosas, no sería de extrañar que en un país tan dado a los extremos legales como los EE UU, pronto veamos en las etiquetas de cinta Scotch “Advertencia: riesgo de exposición a radiación...”.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referencia:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;1&#34;&gt; 1. &lt;a href=&#34;http://www.nature.com/nature/journal/v455/n7216/full/nature07378.html&#34;&gt;Correlation between nanosecond X-ray flashes and stick-slip friction in peeling tape&lt;/a&gt; . C.G. Camara, J.V. Escobar, J. R. Hird &amp;amp; S.J. Putterman. Nature, vol 455.  &lt;/p&gt;
&lt;p&gt;En la dirección: &lt;a href=&#34;http://www.nature.com/nature/videoarchive/x-rays/&#34;&gt;http://www.nature.com/nature/videoarchive/x-rays/&lt;/a&gt; Puede encontrarse un video donde se muestra como bajo condiciones de vacío moderado esta radiación puede ser tan intensa como para radiografiar un dedo humano.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>La naturaleza nos invita a jugar con la luz</title>
      <link>https://ciencianet.com.ar/post/la-naturaleza-nos-invita-a-jugar-con-la-luz/</link>
      <pubDate>Tue, 02 Dec 2008 22:00:45 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/la-naturaleza-nos-invita-a-jugar-con-la-luz/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro.&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP) y Universidad Tecnológica Nacional.&lt;/p&gt;
&lt;p&gt;Científicos de la Universidad de Utah lograron desentrañar la estructura fotónica tridimensional de las escamas presentes en el caparazón de un escarabajo autóctono de Brasil.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/12/aluna-fig1-216x300.png&#34; alt=&#34;Lamprocyphus augustus.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Su color verde iridiscente es consecuencia de la estructura periódica de cada una de las escamas que conforman su caparazón y no de su pigmentación. Cuando la luz incide sobre él se producen interferencias múltiples dando como resultado sólo la reflexión del color verde. Cada una de sus escamas actúa como un cristal fotónico natural.&lt;/p&gt;
&lt;p&gt;Pero ¿qué es un cristal fotónico? La comunidad científica ha denominado con este nombre a los materiales compuestos por varios elementos periódicamente distribuidos que dispersan la luz. Hoy en día hay un gran interés en el estudio de estos nuevos materiales debido a la búsqueda del control de las propiedades ópticas, es decir de la manipulación de la luz y del potencial y amplio espectro de sus aplicaciones, como por ejemplo en nuevas tecnologías de comunicación puramente ópticas o en computadoras ópticas ultrarrápidas diseñadas con circuitos integrados ópticos o chips que funcionen con luz y no con electricidad.&lt;/p&gt;
&lt;p&gt;La especie que inspiró a Galusha y a Bartl para modelar un cristal fotónico artificial fue el escarabajo Lamprocyphus augustus, cuya característica particular es que su coloración es independiente del ángulo de observación. Este hecho se explica analizando y entendiendo la geometría y composición de una de sus escamas, que está formada por 200 piezas de quitina y cada una de ellas ésta orientada en diferentes direcciones haciendo que la luz que incide con distintos ángulos sobre el caparazón del escarabajo dé como resultado que la luz reflejada sea de un color verde nítido y brillante.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/12/aluna-fig2-300x126.png&#34; alt=&#34;Imagen de un corte transversal de la escama.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;El siguiente paso consistió en determinar con gran precisión la estructura tridimensional de la escama. Para ello se usó un microscopio electrónico de barrido con el fin de adquirir y procesar imágenes, y por otro lado, se recurrió a la focalización de un haz de iones de galio para retirar capas extremadamente delgadas de la escama. Repitiendo sucesivas veces este procedimiento se obtuvieron 150 imágenes de distintas secciones de una misma escama que sirvieron para reproducir, en forma teórica, su estructura 3D.&lt;/p&gt;
&lt;p&gt;La compleja estructura fotónica tridimensional, que opera en el rango de longitudes de onda visibles, no se obtuvo a través de un problema “pensado” sino emulando la arquitectura de los sistemas biológicos presentes en la naturaleza y tal hallazgo abre las puertas a la creación de nuevos dispositivos ópticos que podrían revolucionar nuestra tecnología actual.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo Original:&lt;/strong&gt; Jeremy W. Galusha, Lauren R. Richey, John S. Gardner, Jennifer N. Cha, and Michael H. Bartl. &lt;a href=&#34;https://doi.org/10.1103/PhysRevE.77.050904&#34;&gt;Discovery of diamond-based photonic crystal structure in beetle scales&lt;/a&gt;. Physycal Review E 77, 050904(R) (2008).&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>El inteligente chimango</title>
      <link>https://ciencianet.com.ar/post/el-inteligente-chimango/</link>
      <pubDate>Thu, 20 Nov 2008 22:04:37 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/el-inteligente-chimango/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Luis Pugnaloni&lt;/strong&gt;. Instituto de Física de Líquidos y Sistemas Biológicos (IFLySiB).&lt;/p&gt;
&lt;p&gt;Quienes transitan las rutas argentinas están acostumbrados a observar esos pequeños halcones que habitan nuestras pampas: los chimangos. El saber popular recomienda no gastar pólvora en ellos. ¿Será porque su carne no es sabrosa o porque, como encontraron investigadores de la Universidad Nacional de Mar del Plata, estas aves son inteligentes y las consideramos dignas de un respeto especial?&lt;/p&gt;
&lt;p&gt;Laura Marina Biondi, Maria Susana Bó y Aldo Iván Vassallo, publicaron recientemente un artículo donde describen el comportamiento de un grupo de chimangos puestos a resolver problemas de ingenio. No,no jugaron al ajedrez. Simplemente tuvieron que sacar la comida de una caja de acrílico con puertas que se abren de diferentes formas.&lt;/p&gt;
&lt;p&gt;Durante la observación del comportamiento de los chimangos ante el problema de extraer la comida de un compartimento los investigadores registraron el tiempo que les toma decidirse a intentar abrir la caja, el tiempo total que les toma abrirla, y el número de intentos necesarios hasta conseguir el objetivo. El estudio muestra que los chimangos son capaces de resolver nuevas situaciones. Más aún, la siguiente vez que son puestos ante el mismo desafío, parecen haber aprendido de su experiencia anterior y lo resuelven más rápido.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; &lt;em&gt;Experimental assessment of problem solving by Milvago chimango (Aves: Falconiformes)&lt;/em&gt;, [Journal of Ethology]((&lt;a href=&#34;https://doi.org/10.1007/s10164-007-0035-2&#34;&gt;https://doi.org/10.1007/s10164-007-0035-2&lt;/a&gt;) (2008) 26:113–118.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instituciones:&lt;/strong&gt; &lt;a href=&#34;http://www.mdp.edu.ar/exactas/&#34;&gt;Facultad de Ciencias Exactas y Naturales&lt;/a&gt; (UNMDP).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; Laura Marina Biondi (Email: &lt;a href=&#34;mailto:lmbiondi@mdp.edu.ar&#34;&gt;lmbiondi@mdp.edu.ar&lt;/a&gt; )&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Percepción pública de la nanotecnología: Un viaje al nanomundo</title>
      <link>https://ciencianet.com.ar/post/percepcion-publica-de-la-nanotecnologia-un-viaje-al-nanomundo/</link>
      <pubDate>Tue, 28 Oct 2008 22:06:39 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/percepcion-publica-de-la-nanotecnologia-un-viaje-al-nanomundo/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Astrid Bengtsson&lt;/strong&gt;. Instituto Balseiro, Universidad Nacional de Cuyo. &lt;a href=&#34;mailto:astrid@cab.cnea.gov.ar&#34;&gt;astrid@cab.cnea.gov.ar&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Algunas áreas de la ciencia comienzan a resonar más que otras en los medios de comunicación y en la ficción, causando cierta fascinación por parte de la población a pesar de que se desconozcan sus alcances. Una de estas áreas es la Nanociencia y Nanotecnología (N&amp;amp;N). Es por ello que una investigación llevada a cabo en Brasil y Argentina a cargo de los Dres. Marcelo Knobel y Sandra Murriello (Unicamp, Brasil), analiza las imágenes que se van conformando sobre este nuevo campo y las dificultades conceptuales para comprenderlo.&lt;/p&gt;
&lt;p&gt;Después de analizar otras investigaciones en curso sobre la comunicación de N&amp;amp;N llegaron a la conclusión acerca de la necesidad de lograr ciertos conceptos clave para entender de qué se trata este nuevo campo científico-tecnológico, entre ellos se encuentra la escala. En efecto, una ciencia y una tecnología que se maneja en dimensiones por debajo de una millonésima parte del metro no es sencillo de imaginar. Otra cuestión es profundizar en lo que se piensa sobre la materia: cómo está conformado todo lo que nos rodea puede parecer obvio, ¡pero no lo es! Una tercera cuestión es poder entender que, en esa escala, la materia presenta propiedades absolutamente diferentes de las conocidas en otras escalas. Y un cuarto desafío es comprender cómo se puede manipular la materia en escala nanométrica. En la medida en que se puedan abordar estos asuntos se podrá comunicar al gran público de que se tratan las N&amp;amp;N que son hoy objeto de inversiones millonarias por parte de estados y empresas.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/10/games_nano.jpg&#34; alt=&#34;Exposición Nanoaventura: Juegos interactivos (gentileza Museu Exploratório de Ciências - UNICAMP).:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;El objetivo de esta investigación fue indagar lo que al sociedad piensa sobre las N&amp;amp;N. Desde el año 2006 se realizaron más de 2000 cuestionarios y cerca de 100 entrevistas con niños, adolescentes y adultos en Bariloche, Argentina, y en las ciudades brasileras de Campinas, San Pablo y Porto Alegre [&lt;a href=&#34;#1&#34;&gt;1&lt;/a&gt;]. La investigación se llevó a cabo en contextos escolares y durante la visita a la “NanoAventura”, una exposición brasilera sobre el tema.&lt;/p&gt;
&lt;p&gt;Si bien se registró la asociación de las N&amp;amp;N con algo diminuto, más en Argentina que en Brasil, posiblemente por una cuestión terminológica, cabe cuestionarse cuán pequeño es lo que alguien se puede imaginar ya que los resultados muestran una gran dificultad en identificar con precisión conceptual estructuras menores a la escala micrométrica. Se observa que la utilización de términos científicos como átomo, molécula o célula no conlleva a la comprensión de sus significados.&lt;/p&gt;
&lt;p&gt;También se encontró que es posible pensar en los componentes básicos de la materia al referirse a una sustancia conocida y estudiada en la escuela en términos químicos como el agua, pero que cuesta pensar en otras sustancias en términos de composición atómica. Estas ideas interfieren en la comprensión de propiedades de la materia y más aún de sus posibilidades de manipulación. “Estos resultados nos enfrentan con la necesidad de abordar la comunicación pública y la enseñanza de las N&amp;amp;N desde las bases más elementales. Sólo por ese camino se podrá contar con una población informada y crítica sobre un tópico crucial en el actual desarrollo tecnológico”, opina Murriello.&lt;/p&gt;
&lt;p&gt;Actualmente se está dando continuidad a este estudio con una muestra de población de estudiantes secundarios en Bariloche con miras a poder generar materiales de difusión apropiados sobre este tópico [&lt;a href=&#34;#2&#34;&gt;2&lt;/a&gt;]. Esta investigación se realizó con una beca de posdoctorado de la Fundaçao de Amparo à Pesquisa do Estado de Sao Paulo (FAPESP), Brasil. Proyecto Percepção Pública da Nanotecnologia, desarrollada en el Laboratório de Estudos Avançados em Jornalismo,Universidade Estadual de Campinas (2006-2008) y en el Centro Atómico Bariloche (Oct-Dic, 2007).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referencias&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;1&#34;&gt; 1. &lt;/a&gt; MURRIELLO, S.E, CONTIER,D., KNOBEL,M “Desafios de uma exposição sobre nanociência e nanotecnologia” / &amp;quot;Challanges of an exhibition on nanoscience and nanotechnology&amp;quot;. Journal of Science Communication (JCOM), v.5, n.4. Dec.2006. &lt;a href=&#34;http://jcom.sissa.it/archive/05/04/Jcom0504%282006%29A01&#34;&gt;http://jcom.sissa.it/archive/05/04/Jcom0504%282006%29A01&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;2&#34;&gt; 2. &lt;/a&gt; MURRIELLO,S.,KNOBEL,M.,VOGT,C. Nanotecnologia, uma nova tecnologia para o público novo. VII Congreso Iberoamericano de Indicadores de Ciência y Tecnologia. São Paulo, Brasil. &lt;a href=&#34;http://www2.ricyt.org/docs/VII_Congreso/DIA_24/SALA_B/17_00/Murriello_Knobel_Vogt.pdf&#34;&gt;http://www2.ricyt.org/docs/VII_Congreso/DIA_24/SALA_B/17_00/Murriello_Knobel_Vogt.pdf&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Formación de opinión: Líderes vs medios de comunicación</title>
      <link>https://ciencianet.com.ar/post/formacion-de-opinion-lideres-vs-medios-de-comunicacion/</link>
      <pubDate>Thu, 16 Oct 2008 22:08:44 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/formacion-de-opinion-lideres-vs-medios-de-comunicacion/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Norma Canosa:&lt;/strong&gt; Instituto de Física de La Plata (UNLP-CONICET).&lt;/p&gt;
&lt;p&gt;En este trabajo se investiga un modelo para la formación de opinión, basado en la teoría del impacto social, mediante métodos estadísticos que permiten estudiar el comportamiento dinámico de un grupo social bajo la influencia de un líder fuerte y de los medios masivos de comunicación. La denominada teoría del impacto social es un modelo matemático desarrollado por el psicólogo estadounidense Bibb Latané en 1981, que describe la forma en que una persona responde a la influencia social de un grupo de individuos en base a factores tales como: (a) Las relaciones de fuerza, es decir que tan importante es el grupo de individuos que influye sobre un único individuo; (b) El grado de proximidad social del individuo al grupo; (c) El número de integrantes del grupo.&lt;/p&gt;
&lt;p&gt;Resulta más probable que la opinión de un individuo coincida con las opiniones del grupo cuando aumenta su proximidad social al grupo, o cuando la importancia del grupo crece. La teoría original estudiaba únicamente el impacto de los grupos sobre los individuos pero luego fue generalizada por el mismo Latané a un modelo estadístico para poder describir la evolución dinámica del impacto de los grupos sociales.&lt;/p&gt;
&lt;p&gt;Esta teoría ha servido de base para desarrollar los llamados &lt;em&gt;modelos para la formación de opinión&lt;/em&gt;, los cuales han despertado un gran interés entre los físicos debido a que exhiben una rica variedad de fenómenos similares a los que se dan en diversos sistemas físicos muy estudiados en mecánica estadística o materia condensada. Uno de estos fenómenos ocurre cuando la competencia entre un líder fuerte y un medio de comunicación masivo provoca una transición brusca entre dos estados de opinión distintos; la opinión del líder y la del medio.&lt;/p&gt;
&lt;p&gt;En un sistema físico esto corresponde a una transición de fase abrupta (conocida como de primer orden) que se produce cuando, debido a la variación de uno o varios parámetros de control, el sistema pasa abruptamente de un estado a otro. Tanto estos como otros interesantes aspectos se pueden investigar mediante métodos estadísticos para lo cual se recurre a representar a los &lt;em&gt;modelos para la formación de opinión&lt;/em&gt; como sistemas físicos.&lt;/p&gt;
&lt;p&gt;Uno de los más simples consiste de una cadena cíclica de partículas interactuantes, las que representan a individuos, donde cada partícula posee dos posibles orientaciones, hacia arriba o hacia abajo, que representan las opiniones. Cada individuo del grupo experimenta una fuerza promedio similar y puede cambiar de opinión. El líder está representado por una partícula con un parámetro de fuerza mucho mayor que la fuerza promedio, indicando que su impacto sobre la sociedad será mayor que el promedio y cuya orientación (opinión) puede no cambiar. El efecto de la proximidad entre los integrantes del grupo está cuantificado por un factor de peso, que modifica la fuerza promedio y que se mide en base a la inversa de la distancia entre las partículas que interactúan entre sí.&lt;/p&gt;
&lt;p&gt;Este sistema se puede colocar en un campo externo variable que estará representando la influencia de un medio masivo de comunicación sobre la opinión de los individuos. En este trabajo se emplea un &lt;em&gt;modelo para la formación de opinión&lt;/em&gt; investigando el comportamiento dinámico de un grupo social en relación con la ocurrencia de una transición de fase dinámica, como las que se presentan en sistemas magnéticos cuando un parámetro de control, es decir el campo magnético, oscila. En el caso investigado, el grupo está sometido a la influencia de un líder fuerte y a la de los medios masivos de comunicación en dos posibles escenarios:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(1) Solo el líder está actuando y cambia periódicamente su opinión.&lt;/strong&gt; En este caso la influencia del líder conduce a una transición de fase dinámica entre dos estados. Uno de ellos corresponde a una interacción débil entre el líder y el grupo, de modo que el grupo no sigue la opinión del líder y el otro a un estado dinámico donde el grupo sigue la opinión del líder. Esta transición de fase es abrupta, y se asemeja a una transición de fase de primer orden. Estas transiciones abruptas ocurren cuando el grupo posee un número muy grande de integrantes. A medida que el número de integrantes disminuye se pueden observar efectos de tamaño finito que dan cuenta de los cambios en el comportamiento de comunidades más reducidas.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(2) Los medios masivos cambian periódicamente su mensaje y deben competir con el líder, quien no cambia de opinión.&lt;/strong&gt; En este caso se observa que los medios masivos requieren superar cierto umbral a fin de imponerse sobre la opinión del líder y llevar al grupo social a un estado dinámicamente desordenado. Estos resultados muestran la utilidad de emplear métodos de la física estadística para describir el comportamiento dinámico de un grupo social bajo la influencia de distintos factores que compiten entre sí.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; &lt;a href=&#34;https://doi.org/10.1103/PhysRevE.76.061125&#34;&gt;Dynamic behavior of a social model for opinion formation&lt;/a&gt; , Clelia M. Bordogna, Ezequiel V. Albano, &lt;em&gt;Physical Review E&lt;/em&gt; 76, 061125 (2007).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instituciones:&lt;/strong&gt; &lt;a href=&#34;http://www.ing.unlp.edu.ar/fismat/imapec/imapec2/index.html&#34;&gt;IMApEC&lt;/a&gt; (Fac. Ing., UNLP), &lt;a href=&#34;http://www.inifta.unlp.edu.ar/&#34;&gt;INIFTA&lt;/a&gt; (UNLP-CONICET).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; C. Bordogna (E-mail: &lt;a href=&#34;mailto:cleliabordoga@yahoo.com.ar&#34;&gt;cleliabordoga@yahoo.com.ar&lt;/a&gt; ), E. Albano (E-mail: &lt;a href=&#34;mailto:ezequielalb@yahoo.com.ar&#34;&gt;ezequielalb@yahoo.com.ar&lt;/a&gt; )&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>El trabajo científico en los grandes emprendimientos</title>
      <link>https://ciencianet.com.ar/post/el-trabajo-cientifico-en-los-grandes-emprendimientos/</link>
      <pubDate>Mon, 29 Sep 2008 22:09:49 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/el-trabajo-cientifico-en-los-grandes-emprendimientos/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Luis Pugnaloni&lt;/strong&gt;. Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP).&lt;/p&gt;
&lt;p&gt;Valeria Perez Reale hizo su licenciatura en Fisica en la Universidad Nacional de La Pampa y luego de realizar su tesis doctoral en la Universidad de Berna se incorporo al proyecto ATLAS que es parte del gran colisionador de protones (LHC, por sus siglas en ingles). En esta entrevista no disimula su entusiasmo por su trabajo en Ginebra que ha alcanzado niveles insospechados de repercusion mundial.&lt;/p&gt;
&lt;p&gt;-&lt;strong&gt;¿Cuántas entrevistas te han hecho en los últimos días? ¿Esperabas esta reacción del público al lanzamiento del LHC?&lt;/strong&gt; Muchas, no me imaginaba la repercusion que genero la noticia del primer haz de proton injectado en el LHC el 10 de septiembre, me dio mucha alegria! Yo estaba en la sala de control de la experiencia ATLAS y nunca vi tantas camaras de television y reporteros siguiendo tambien con emocion este momento historico. Como Pampeana la nota que me emociono mas y me lleno de orgullo fue la del Diario La Arena. Y ahora me dicen que tambien estaremos en la revista Gente! de chiquita nunca me imaginaria tal magnitud de repercusion! Estos es gracias a la WWW (que fue inventada en el CERN) )y la globalizacion de la informacion.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/09/valeria-290x300.jpg&#34; alt=&#34;Valeria Perez Reale: En su oficina en el CERN (Ginebra).:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;-&lt;strong&gt;¿Cuál es tu trabajo específico en el colisionador?&lt;/strong&gt; El LHC (gran colisionador de protones) es la maquina mas potente del mundo- se encuentra 100 m bajo tierra en un tunel de 27 km de circunferencia, esta compuesto de casi 2000 imanes superconductores enfriados a las temp mas baja del universo: 2 K. Los protones viajaran casi a la velocidad de la luz a energias jamas alcanzadas (comparados a dos tren bala) en sentidos opuestos donde colisionaran en 4 puntos donde se encuentran los detectores. Uno de los detectores- que uno se debe imaginar como un gran microscopio que &amp;quot;ve&amp;quot; lo que ocurre en cada colision- es ATLAS. La experiencia ATLAS para la cual participo hace casi 7 años desde el 2002, es una colaboracion internacional demas de 2500 scientificos de 38 paises del mundo.&lt;/p&gt;
&lt;p&gt;En la experiencia ATLAS yo trabaje 6 años para la seleccion de datos. En antiguos experimentos toda colision era registrada en disco, en el LHC existen mil millones de colisiones por segundo (casi 200 CDs de informacion que se producen por segundo en ATLAS), necesitamos un sistema de flitro llamado &amp;quot;&lt;em&gt;trigger&lt;/em&gt;&amp;quot; que solo seleccione eventos interesantes de la colision. Yo fui la coordinadora de un grupo de 30 personas de distintos lugares del mundo que seleccionaba electrones y photones. Desde este año llevo el sombrero &amp;quot;Pixel&amp;quot; el subdetector mas cerca del haz y mas preciso, mide con tecnologia semiconductora la dirección de trazas de particulas electricas, en cada colision se producen casi 100.000 trazas por evento, y tenemos que buscar la interesante. Es casi como buscar un aguja en un pajar...&lt;/p&gt;
&lt;p&gt;-&lt;strong&gt;Después de todos estos años en Ginebra, contanos un punto débil y un punto fuerte que hayas descubierto de tu formación en la Universidad Nacional de La Pampa.&lt;/strong&gt; La Universidad de La Pampa me dio una educación de fisica basica al mismo nivel que cualquier otra Universidad argentina. La educacion de La pampa ha sido de alto nivel.&lt;/p&gt;
&lt;p&gt;-&lt;strong&gt;Entonces no hay nada que mejorar en la educacion universitaria? Ningun punto debil?&lt;/strong&gt; Quizas un punto de mejoria seria de introducir en el ultimo año materias mas prácticas y enfatizar en la programacion. El systema de educacion mas corta &amp;quot;sistema de Bologna&amp;quot; que se adopto en Europa sigue un poco estas lineas, UK en cuatro años gradúa físicos con un conocimiento global teórico pero tambien práctico. Esto facilita la inserción al PhD o laboral.&lt;/p&gt;
&lt;p&gt;-&lt;strong&gt;Creo adivinar la respuesta pero... ¿Regresarías a vivir y trabajar en Argentina? ¿Por qué?&lt;/strong&gt; Nunca descarté la posibilidad de volver a trabajar a la Argentina. Si me dan la posibilidad de dirigir un grupo de investigacion en física de altas energías y ATLAS lo tendría que pensar seriamente, tambien en la vida está el aspecto personal y familiar que pesa mucho en la decisión de donde uno vive y trabaja.&lt;/p&gt;
&lt;p&gt;-&lt;strong&gt;Digamos que te ofrecen armar de cero un grupo en física de altas energias en Argetnina. Te dan un puesto en una univerisdad o CONICET en la ciudad de tu elección y los derechos de cualquier investigador argentino a solicitar subsidios y dirigir estudiantes. ¿Volverias?&lt;/strong&gt; Todo depende del grupo, proyecto y lugar/ciudad de trabajo, igual aplicaría a Europa, USA o Asia.&lt;/p&gt;
&lt;p&gt;-&lt;strong&gt;La mayoría de los científicos están acostumbrados a trabajar en soledad o en pequeños grupos de investigación, ¿qué cualidades se necesitan para trabajar en un proyecto con miles de científicos involucrados?&lt;/strong&gt; La ciencia del futuro es de investigacion en grupo, se dió en la física de altas energias hace años y se está dando en otras ramas como astronomía, medicina, etc. Los proyectos grandes son como una gran empresa, se necesita saber trabajar en grupo, tener una visión global del experimento y ser muy práctico en las decisiones y prioridades del proyecto.&lt;/p&gt;
&lt;p&gt;-&lt;strong&gt;¿Entonces en el futuro los pequenios grupos (menos de 10 investigadores) no seran muy bien considerados? ¿o no tendran chances de hacer aportes importantes a la ciencia?&lt;/strong&gt; No fue mi respuesta, a lo que me refiero es que la globalizacion tambien afectó a la ciencia y que cada vez más tendrá que haber colaboraciones o mas grandes o mas estrechas entre grupos.&lt;/p&gt;
&lt;p&gt;-&lt;strong&gt;El reconocimiento por un descubrimiento es un factor importante para todo científico, ¿de qué modo te llega el reconocimiento personal cuando el descubrimiento lo hacés junto a tantos colegas?&lt;/strong&gt; El reconocimiento es una satisfaccion personal, cuando hay mas de 200 nombres en un paper o casi mil para los que vendrán del LHC, el reconocimiento será dentro del grupo de trabajo directo.&lt;/p&gt;
&lt;p&gt;-&lt;strong&gt;¿Qué pasa si no encuentran el bosón de Higgs? ¿Le echarás las culpas a tus programas o al modelo estándar?&lt;/strong&gt; Si el boson de Higgs existe el LHC lo tendrá que ver. El problema será que debe haber al menos dos años de toma de datos a la luminosidad deseada y la búsqueda de muchos canales complementarios por parte de las dos experiencias ATLAS y CMS. Si no descubrimos el Higgs siempre tendremos nueva física para ver SUSY, exotica o algo imaginado, asi que siempre será interesante y siempre estaremos ocupados!&lt;/p&gt;
&lt;p&gt;-&lt;strong&gt;Intuyo que no te seria fácil descartar el modelo estandar solo porque con el LHC no puedan &amp;quot;probar&amp;quot; la existencia de partículas aún no observadas experimentalemnte.&lt;/strong&gt; El LHC tiene un rango de energia y luminosidad que podrá ver el SM Higgs o nueva física (varios higgs,etc). Espero que la naturaleza nos revele alguna pista para saber que camino seguir (en este momento son los teóricos que proponen varios caminos y los experimentales debemos seguir).&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Cuando investigadores en física piensan en la educación y la divulgación de la ciencia</title>
      <link>https://ciencianet.com.ar/post/cuando-investigadores-en-fisica-piensan-en-la-educacion-y-la-divulgacion-de-la-ciencia/</link>
      <pubDate>Fri, 26 Sep 2008 22:12:05 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/cuando-investigadores-en-fisica-piensan-en-la-educacion-y-la-divulgacion-de-la-ciencia/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Sandra Murriello:&lt;/strong&gt; LABJOR, Universidade Estadual de Campinas (Brasil).&lt;/p&gt;
&lt;p&gt;En las últimas décadas las actividades de divulgación científica revelan un creciente auge y muchas veces son los mismos centros de investigación quienes las llevan a cabo. De este modo los investigadores, con gran experiencia en la composición escrita de artículos científicos, se transforman en productores de textos de divulgación sobre el tema en que ellos son expertos.&lt;/p&gt;
&lt;p&gt;Esta investigación se propone entender las formas en que científicos del área de Física conciben y, eventualmente, asumen la actividad de composición de textos de divulgación científica o transferencia de conocimientos a una comunidad más amplia y heterogénea, en lo demográfico, evolutivo, cultural y social.&lt;/p&gt;
&lt;p&gt;Basadas en el modelo de teorías implícitas del aprendizaje, Astrid Bengtsson del Instituto Balseiro, Nora Scheuer de la Universidad Nacional del Comahue y Mar Mateos Sanz de la Universidad Autónoma de Madrid diseñaron y aplicaron en la comunidad de físicos argentinos un cuestionario que sondea concepciones sobre divulgación y educación científica.&lt;/p&gt;
&lt;p&gt;Este modelo sostiene que las actividades de producción y comprensión realizadas por las personas están mediadas por concepciones de carácter relativamente implícito que esta investigación busca explicitar. Se conocen como teorías implícitas del aprendizaje: la teoría directa, la interpretativa y la constructiva. La directa establece una relación lineal entre condiciones y resultados del aprendizaje, sin atender a los procesos. Asume que el conocimiento, considerado una copia del modelo, se alcanza por exposición a una fuente autorizada. La teoría interpretativa es una evolución de la anterior pero incorpora al sujeto y sus condiciones personales o las condiciones del ambiente como factores que influyen en el resultado. Por su parte, la teoría constructiva implica un salto cualitativo, considerando el aprendizaje como una redescripción de los propios conocimientos del aprendiz. Jerarquiza las situaciones de aprendizaje que tienen en cuenta los contextos de producción del conocimiento. Considera los procesos representacionales de los aprendices y promueve explicitar los distintos puntos de vista, así como la integración de saberes.&lt;/p&gt;
&lt;p&gt;Siendo que esta investigación está orientada a analizar particularmente la divulgación de la física, el cuestionario fue enviado por correo electrónico a los asociados a la Asociación Física Argentina de los cuales respondieron 71. En esta muestra se analizaron las posibles asociaciones entre las opciones de respuesta seleccionadas, preguntas y datos de caracterización de los participantes por medio de diversos cálculos estadísticos.&lt;/p&gt;
&lt;p&gt;Los resultados muestran que los científicos que contestaron el cuestionario tienen preferencia por las opciones de la teoría constructiva en divulgación, sin embargo, para educación científica aumentan las elecciones de opciones correspondientes a la teoría directa. La combinación de teorías predominante es la dupla constructiva-interpretativa.&lt;/p&gt;
&lt;p&gt;Sólo un grupo reducido (18) mostró gran consistencia en la elección de opciones constructivas y parecería estar mostrando tener una teoría respecto a la adquisición y transmisión de conocimientos científicos. Los demás investigadores (53) presentan diversos abordajes, definida en este marco como &amp;quot;baja consistencia&amp;quot;, respecto a cómo debe ser transmitido el conocimiento científico a personas no expertas.&lt;/p&gt;
&lt;p&gt;&amp;quot;Estos resultados nos permitirían sostener la importancia de la inclusión de estas cuestiones en la formación de los científicos, en este caso los físicos, para que puedan desempeñar en forma más potente y efectiva su participación, cada vez más necesaria, en actividades de comunicación científica a comunidades más amplias y heterogéneas&amp;quot;, sostiene Bengtsson. Desde este punto de vista la promoción de la divulgación científica podría pensarse como una de las nuevas alfabetizaciones necesarias para el siglo XXI, no sólo en lo que hace a sus destinatarios, sino también a sus propios agentes, quienes podrían beneficiarse de oportunidades para extender y revisar sus formas de componer textos para destinatarios más alejados de la comunidad científica.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; A. Bengtsson, &lt;em&gt;Concepciones en investigadores del área de la física sobre divulgación y educación científicas a través de textos&lt;/em&gt;. &lt;a href=&#34;http://www.fceia.unr.edu.ar/fceia/sief9/PAGINA_WEB/index.htm&#34;&gt;Noveno Simposio de Investigación en Educación en Física&lt;/a&gt; (2008). A. Bengtsson, &lt;em&gt;Concepciones sobre divulgación y aprendizaje de la ciencia en autores de textos de divulgación científica&lt;/em&gt;, Diploma de Estudios Avanzados, Facultad de Psicología, Universidad Autónoma de Madrid, (2004)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instituciones:&lt;/strong&gt; &lt;a href=&#34;http://www.ib.edu.ar/&#34;&gt;Instituto Balseiro&lt;/a&gt; (Universidad Nacional de Cuyo), &lt;a href=&#34;http://www.uncoma.edu.ar/&#34;&gt;Universidad Nacional del Comahue&lt;/a&gt;, &lt;a href=&#34;http://www.uam.es/&#34;&gt;Universidad Autónoma de Madrid&lt;/a&gt; (España)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; Astrid Bengtsson (E-mail: &lt;a href=&#34;mailto:astrid@cab.cnea.gov.ar&#34;&gt;astrid@cab.cnea.gov.ar&lt;/a&gt;)&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>El LHC ¿la maquina del fin del mundo?</title>
      <link>https://ciencianet.com.ar/post/el-lhc-la-maquina-del-fin-del-mundo/</link>
      <pubDate>Sun, 14 Sep 2008 22:12:49 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/el-lhc-la-maquina-del-fin-del-mundo/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Nicolás Grandi&lt;/strong&gt;. Instituto de Física La Plata&lt;/p&gt;
&lt;p&gt;Empecemos por el principio ¿Qué es el LHC? El LHC o “&lt;em&gt;Large Hadron Collider&lt;/em&gt;” (“Gran Colisionador de Hadrones”) es un acelerador de partículas. Una máquina que toma partículas del tipo de las que se encuentran en el núcleo atómico (hadrones) y las pone a girar en un círculo de 27 kilómetros de longitud. Con la ayuda de enormes electroimanes, acelera ese movimiento de modo que las partículas terminen moviéndose a gran velocidad a lo largo del círculo. En un dado momento se toman dos haces de partículas que giran en sentidos contrarios y se los desvía haciéndolos chocar de frente.&lt;/p&gt;
&lt;p&gt;¿Por qué producir tal choque? Desde Demócrito, una de las ideas que ha guiado el desarrollo de la física, es que la complejidad del mundo visible puede ser explicada en términos de leyes simples que rigen el funcionamiento de sus componentes elementales. Los primeros químicos llevaron la idea atomista a su etapa de madurez: la infinidad de substancias diferentes que constituyen nuestra experiencia inmediata, pudo ser explicada en términos de sólo un centenar de tipos diferentes de átomos, que se agrupan siguiendo ciertas reglas para formar las moléculas de cada una de las substancias. El catálogo de todas esas reglas y su estudio constituyeron la base de la ciencia que hoy llamamos Química.&lt;/p&gt;
&lt;p&gt;Esa asombrosa conclusión se alcanzó mediante la técnica de “romper” las moléculas en sus átomos componentes y luego permitir que tales átomos vuelvan a asociarse en nuevas moléculas. Para producir esta “rotura” los investigadores calentaban sus muestras, es decir daban energía a las moléculas encerradas en sus recipientes de modo de que se movieran muy rápidamente y chocaran, destrozándose en sus átomos componentes, que al volver a chocar se asociaban en nuevas moléculas. El éxito de este programa de investigación motivó su aplicación subsiguiente a los átomos mismos. Si golpeando y rompiendo las moléculas edificamos la Química ¿Qué podríamos lograr golpeando y rompiendo los átomos? De nuevo, el método se mostró exitoso. Golpeando los átomos con partículas de luz (o fotones) y con partículas de electricidad (o electrones), se descubrió que están compuestos por dos componentes básicos: un pequeñísimo núcleo que se esconde en el interior, y una nube difusa de electrones que forma su cubierta exterior.&lt;/p&gt;
&lt;p&gt;El estudio de la forma de esta nube exterior de electrones y de su interacción con los fotones y los electrones incidentes, permitió desarrollar un enfoque nuevo y revolucionario de la física: la Mecánica Cuántica. Como premio a este esfuerzo, las reglas de la Mecánica Cuántica explican completamente las leyes de la Química, haciéndolas consecuencia del modo en que los electrones se acomodan en la cubierta exterior de los átomos. Es decir que ahora comprendemos más cosas (la forma en que los electrones rodean al núcleo para formar los átomos y la manera en que los diferentes átomos se agrupan en moléculas) con menos reglas (las de la Mecánica Cuántica, la Química siendo sólo una consecuencia lógica de ellas).&lt;/p&gt;
&lt;p&gt;Durante todo el siglo XX, la Física persistió con ese programa, aplicándolo a escalas cada vez más pequeñas. El siguiente escalón fue la investigación del núcleo atómico, bombardeándolo con electrones o protones, o colisionándolo con otros núcleos, para así romperlo en sus componentes elementales y comprender las reglas que rigen su interacción. Descubrimos que el mundo subnuclear es increíblemente rico, existiendo un enorme zoológico de partículas elementales descripto por lo que se conoce como Modelo Standard de las Interacciones Fundamentales. Durante este proceso, en más de una ocasión se infirió la existencia de un componente aún desconocido a una dada escala, a partir de una aparente inconsistencia de las reglas de interacción a esa escala.&lt;/p&gt;
&lt;p&gt;Un posterior experimento, siempre del tipo de “romper e investigar los pedazos”, confirmó la existencia del componente propuesto. Por ejemplo, la existencia de “huecos” en la tabla original de Mendeleev, sugirió que debía existir un nuevo tipo de átomo con las propiedades necesarias para llenar cada hueco. La posterior investigación permitió aislar esos elementos confirmando tal hipótesis. Similar es la historia de los neutrinos, propuestos por Fermi como explicación a la aparente falla en la regla de “conservación de la energía” en una reacción conocida como “decaimiento beta”. También la de algunos quarks, cuya existencia se sabía necesaria antes de su observación, como modo de evitar una inconsistencia conocida como “anomalía” en el Modelo Standard.&lt;/p&gt;
&lt;p&gt;Sin embargo, una de tales partículas hipotéticas ha resultado elusiva, no dejándose observar hasta el presente. Es el llamado “bosón de Higgs”, responsable de dar masa a las partículas intervinientes en la interacción nuclear, asegurando que las fuerzas involucradas sean de corto alcance. Una de las razones, tal vez la principal, para la construcción del LHC es la búsqueda del bosón de Higgs.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/09/1-768x500.jpg&#34; alt=&#34;Instalación del calorímetro del experimento ATLAS. Fuente: http://atlas.ch&#34;&gt;&lt;/p&gt;
&lt;p&gt;Pero hay muchas otras cosas interesantes que pueden suceder en el LHC. Por ejemplo, se ha propuesto que, además de las tres dimensiones que observamos forman el mundo (a saber: alto, ancho y espesor, o bien arriba-abajo, izquierda-derecha, adelante-atras), podrían existir otras dimensiones hasta ahora inobservadas. La pregunta inmediata de ¿por qué no las vemos? se puede responder diciendo que las direcciones adicionales son compactas y pequeñas. Una dirección compacta es una que, si caminamos sin volvernos a lo largo de ella, terminamos en el punto de partida. Para fijar ideas, pensemos en una hoja de papel extendida sobre la mesa. Esta hoja tiene sólo dos dimensiones, las cuales no son compactas. Una hormiga puesta sobre la hoja puede caminar en dos direcciones perpendiculares entre sí y, si no se vuelve, no retornará jamás a su punto de partida. Si ahora enrollamos la hoja de papel para formar un cilindro, una de las dimensiones, aquella que se extiende a lo largo del cilindro, sigue sin ser compacta, pero en cambio la otra, la que se extiende alrededor del cilindro, se vuelve compacta. Una hormiga que caminara en esa dirección llegaría al cabo de un tiempo, después de dar una vuelta completa alrededor del cilindro, a su punto de partida. Si el cilindro fuese muy pequeño, la hormiga podría no notar la existencia de esa dirección, ya que al caminar a lo largo de ella retorna casi inmediatamente al punto de partida.&lt;/p&gt;
&lt;p&gt;Una explicación alternativa a por qué no vemos las dimensiones adicionales, es que puede existir algún tipo de fuerza que nos impida movernos en esas direcciones. Por ejemplo si tomamos la hoja de papel y en lugar de enrollarla la doblamos formando una zanja profunda, nuestra hormiga podría quedar atrapada en dicha zanja, siéndole imposible trepar por las paredes. Para ella, es sólo posible moverse a lo largo de la zanja y no en la dirección transversal, es decir que puede resultarle natural asumir que dicha dirección no existe. Existe la posibilidad de que algunas de las partículas resultantes de una colisión en el LHC simplemente desaparezcan. Si tal cosa sucede, es natural inferir que dichas partículas se movieron en alguna de las direcciones adicionales que no podemos ver. En otras palabras, el LHC puede ser la maquina que descubra las dimensiones extra.&lt;/p&gt;
&lt;p&gt;Finalmente, una de las posibilidades más interesantes del LHC es la creación de microscópicos agujeros negros. Un agujero negro es un punto del espacio donde la materia está tan apretada que no puede salir. Para entender la idea, recodemos que todos los cuerpos se atraen entre sí por medio de la gravedad. Eso es también cierto para las partículas elementales. Por lo tanto, cuando una partícula se aleja de otra debe, al igual que un cohete que se aleja de la tierra, alcanzar una cierta “velocidad de escape” que le permita deshacerse del efecto atractivo del campo gravitatorio de la otra partícula. Cuanto más cerca estén las partículas, más intenso es el campo gravitatorio y más grande será la velocidad de escape necesaria.&lt;/p&gt;
&lt;p&gt;En algún punto, para partículas muy cercanas, la velocidad de escape se hace mayor que la de la luz. Como nada puede moverse mas rápido que la luz, dichas partículas están definitivamente ligadas por la gravedad, no pudiendo jamás separarse. Mas aun, si alguna de dichas partículas emitiera fotones, ni siquiera ellos podrían escapar del campo gravitatorio de la partícula emisora. Por eso los agujeros negros son negros, ¡ni siquiera la luz puede escapar de ellos!&lt;/p&gt;
&lt;p&gt;Una consecuencia natural de lo expuesto es que cualquier partícula adicional que sea atraída por el campo gravitatorio del agujero negro y se acerque demasiado, caerá en él y no podrá jamás salir. Es decir que los agujeros negros son objetos voraces: tragan todo lo que tienen alrededor.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/09/9-768x658.jpg&#34; alt=&#34;Simulación de la creación de un agujero negro. Fuente: http://atlas.ch&#34;&gt;&lt;/p&gt;
&lt;p&gt;Existe la posibilidad de que durante las colisiones producidas en el LHC, las partículas lleguen tan cerca unas de otras que produzcan un agujero negro. Esta es ciertamente una de las posibilidades más excitantes del LHC, y una de las razones por las que ha estado en los medios en estos días. Como con casi todo lo que dicen, los medios deforman, desinforman y, cuando pueden, mienten, respecto de los peligros de un evento de esa naturaleza. Dicen que un agujero negro comenzaría inmediatamente a tragar todo lo que haya a su alrededor (cierto) y que por lo tanto engulliría la ciudad de Ginebra, los Alpes suizos y finalmente la Tierra, en pocos segundos (falso).&lt;/p&gt;
&lt;p&gt;Casi sin excepción dicen que para hacer tales afirmaciones se han asesorado por científicos (cierto tal vez en algún caso) que son expertos en el área (falso con seguridad en todos los casos). Mas allá de la intención sensacionalista de estos informes, también se esconde en ellos el ya omnipresente complejo de Frankenstein (el miedo del hombre a su propia creación), y el estereotipo post segunda guerra del científico amoral a quien solo le importa el conocimiento y no se preocupa por los efectos de sus descubrimientos. Un estereotipo bastante oscurantista, que propaga solapadamente la máxima “mejor no saber ciertas cosas”. Una imagen que es además irresponsable, porque pone el énfasis de la bomba atómica no en los políticos que la construyeron, el pueblo que los votó y los militares que la arrojaron, sino en los científicos cuyos descubrimientos la hicieron posible.&lt;/p&gt;
&lt;p&gt;Pero ¿es realmente peligroso el LHC? La respuesta es: simple, total y absolutamente NO. Los agujeros negros tienen una propiedad adicional, que fue descubierta por Hawking en el trabajo que constituyó la base de su fama, y que los vuelve completamente inofensivos: ¡se evaporan! En efecto, los agujeros negros tienen una temperatura que depende inversamente de su tamaño. Un agujero negro gigante, como el que se supone que existe en el centro galáctico, está relativamente frío. En cambio un agujero negro pequeño, como los que podrían llegar a producirse en el LHC, está extremadamente caliente. Como todo objeto caliente, los agujeros negros emiten calor en forma de radiación, brillan como brilla una pieza de metal al ser calentada. En otras palabras ¡no son tan negros después de todo!&lt;/p&gt;
&lt;p&gt;Junto con la radiación, el agujero negro pierde energía, y dado que la energía es lo mismo que la masa, el agujero negro pierde masa mientras brilla, haciéndose cada vez más pequeño. Los agujeros negros grandes, fríos, emiten muy poca radiación y por lo tanto pierden masa muy lentamente, mientras que a la vez atraen fuertemente y engullen todo lo que los rodea, por lo que ganan masa muy rápidamente. El efecto resultante de esta competencia es que los agujeros negros grandes crecen. En cambio los agujeros negros pequeños, calientes, emiten muchísima radiación por lo que pierden masa muy rápidamente, y mientras tanto atraen y engullen los objetos de su entorno muy lentamente, por lo que ganan muy poca masa. Es decir que el efecto resultante es que se evaporan muy rápidamente. De hecho en unos pocos microsegundos. En tan corto tiempo no son capaces de absorber ni siquiera el átomo más cercano.&lt;/p&gt;
&lt;p&gt;En conclusión, aquéllos quienes nos dedicamos a la Física de las Interacciones Fundamentales vivimos el momento más estimulante de la última década. Todas las puertas están por abrirse, lo que sea que encontremos del otro lado será desconocido y maravilloso. Casi cualquier escenario posible de descubrimiento en el LHC es extremadamente interesante. La posibilidad más aburrida posible es que encontremos sólo el bosón de Higgs ¡y eso no tiene nada de aburrido! Significaría que el Modelo Standard que hemos desarrollado a lo largo de los últimos 30 años es esencialmente correcto. Pero es probable que encontremos además muchas nuevas partículas elementales, dimensiones extra y hasta efímeros agujeros negros. Incluso si contra todas las expectativas el bosón de Higgs no apareciera, las implicaciones de tal ausencia serían enormes y el proceso ulterior de reformulación del Modelo Standard sería de lo más estimulante. La cantidad de trabajo inminente recuerda la antigua maldición china “ojala vivas en tiempos interesantes”.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Capilaridad, diferencias de presión, transpiración, y.... Nuevo mecanismo para el transporte de agua en plantas vasculares</title>
      <link>https://ciencianet.com.ar/post/capilaridad-diferencias-de-presion-transpiracion-y-nuevo-mecanismo-para-el-transporte-de-agua-en-plantas-vasculares/</link>
      <pubDate>Wed, 10 Sep 2008 22:15:12 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/capilaridad-diferencias-de-presion-transpiracion-y-nuevo-mecanismo-para-el-transporte-de-agua-en-plantas-vasculares/</guid>
      <description>
        
          &lt;h6 id=&#34;este-artículo-ha-sido-financiado-por-el-proyecto-invofi-de-la-asociación-física-argentina&#34;&gt;Este artículo ha sido financiado por el proyecto INVOFI de la Asociación Física Argentina.&lt;/h6&gt;
&lt;p&gt;Mediante un modelo matemático simple, científicos argentinos proponen un mecanismo basado en consideraciones geométricas, que acercaría un poco más de luz sobre el aún incompleto conocimiento del fenómeno de transporte de agua en plantas vasculares.&lt;/p&gt;
&lt;p&gt;El transporte de agua y otras sustancias desde las raíces hasta la hoja es un problema de la fisiología vegetal que aún hoy en día deja más dudas que certezas a la hora de establecer los mecanismos que intervienen en este complejo e interesante proceso. Algunos de estos mecanismos han sido estudiados, pero con el estado actual del conocimiento en esta materia no puede explicarse acabadamente este fenómeno que permite la vida y el crecimiento de estos organismos.&lt;/p&gt;
&lt;p&gt;El sistema vascular xilemático (capilares micrométricos interconectados) es el encargado del transporte de agua y nutrientes de la raíz hasta las hojas. Estos capilares están interconectados, lateral y axialmente, por orificios denominados pits. Estos orificios poseen en su interior una membrana conformada por una red de micro fibras elásticas que actúan como una válvula capilar. Debido a la alta evaporación, la presión dentro de los tubos desciende hasta valores negativos (tensión) y, por esto, a través de la membrana del pit, se generan burbujas (proceso denominado &lt;em&gt;air-seeding&lt;/em&gt;), obstruyéndose así el flujo de la savia ascendente.&lt;/p&gt;
&lt;p&gt;No esta claro si la formación de burbujas en los capilares (cavitación) perjudica al árbol, evitando el flujo de agua, o beneficia su estructura eliminando las tensiones que podrían romper las paredes de los tubos. El proceso es factible cuando un tubo lleno de savia y otro de aire son adyacentes. Debido a la elevada tensión en el tubo con savia, la interfase liquido-aire se desplaza hasta que una burbuja vence la fuerza capilar y entra en el sistema disparando la cavitación. Esa burbuja se expande relajando el líquido y deteniendo el flujo de agua en ese tubo. Cuando la transpiración disminuye, durante la noche, ese tubo se rellena y el sistema recupera así su conductividad inicial.&lt;/p&gt;
&lt;p&gt;En el artículo publicado en la revista &lt;em&gt;Tree Physiology&lt;/em&gt;, Ariel Meyra, Victor Kuz y Guillermo Zarragoicoechea proveen una posible explicación de este mecanismo de &lt;em&gt;air-seeding&lt;/em&gt;, propuesto por Martin Zimmermann, y de cómo funcionan las membranas que dan origen al mismo. También afirman que la la posibilidad de variar la curvatura de las válvulas no sólo es importante para producir el &lt;em&gt;air-seeding&lt;/em&gt;, sino también en el control del transporte de savia, al producir un exceso de presión capilar.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo Original:&lt;/strong&gt; &lt;a href=&#34;https://academic.oup.com/treephys/article/27/10/1401/1659188&#34;&gt;Meyra, Ariel G; Kuz, Victor A; Zarragoicoechea, Guillermo J. &lt;em&gt;Geometrical and physicochemical considerations of the pit membrane in relation to air seeding: the pit membrane as a capillary valve&lt;/em&gt;. Tree Physiology 27, 1401-1405 (2007).&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instituciones:&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (IFLySiB - CCT CONICET La Plata), UNLP, CICPBA.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; &lt;a href=&#34;mailto:vasco@iflysib.unlp.edu.ar&#34;&gt;vasco@iflysib.unlp.edu.ar&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Peso y gravedad: dificultad en la construcción de conceptos</title>
      <link>https://ciencianet.com.ar/post/peso-y-gravedad-dificultad-en-la-construccion-de-conceptos/</link>
      <pubDate>Mon, 08 Sep 2008 22:17:50 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/peso-y-gravedad-dificultad-en-la-construccion-de-conceptos/</guid>
      <description>
        
          &lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/09/david-goliat.png&#34; alt=&#34;David y Goliat.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Dante tira por quinta vez su sonajero al piso. Está confirmando lo que cualquier bebé de 6 meses descubre: que todo objeto que cae continúa cayendo hasta encontrar una superficie. Al final de su primer año habrá aprendido también la coordinación necesaria para mantener una postura erguida y un andar bípedo sin caerse. Cuando llegue a los 6 ó 7 años podrá además ponerle nombre al responsable de las caídas: el “peso propio” de los objetos. Sabrá incluso atajar una pelota, deslizarse sentado en el tobogán y otros varios movimientos en un mundo con gravedad. En este trabajo Celia Dibar Ure y Silvia M. Pérez analizan numerosas nociones alternativas y representaciones implícitas asociadas a la cotidiana pero invisible gravedad, que resultan de interés tanto teórico como aplicado al aprendizaje de física.&lt;/p&gt;
&lt;p&gt;Según describen las autoras, el estudio del complejo aprendizaje sensoriomotor relacionado con la gravedad puede estudiarse a partir de filmaciones de bebés y niños e incluso realizando experiencias en cápsulas espaciales en órbita. Las observaciones reportadas indican que las representaciones motoras y los aprendizajes sobre el mundo físico no se relacionan con el aprendizaje conceptual de un modo continuo, llegando incluso a ser divergentes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/09/principia-260x300.png&#34; alt=&#34;Ilustración: Principios Matemáticos de la Filosofía Natural, de I. Newton.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;A modo de ejemplo: aunque la destreza física adquirida por David con la honda le permitió derrotar al gigante Goliat, hubiera tenido serias dificultades para responder en qué punto de la trayectoria debe soltar la cuerda para que la piedra dé en el blanco. Desde un punto de vista teórico, las autoras han abordado el trabajo empleando la perspectiva neuroconstructivista, que propone el desarrollo cognitivo como “un proceso emergente que se va construyendo a través de una interacción dinámica y compleja entre cerebro, mente y ambiente”.&lt;/p&gt;
&lt;p&gt;La imposibilidad de acción a distancia entre objetos es uno de los principios implícitos que guían la percepción del mundo físico en niños y adultos. Es probable que la “violación” de este principio sea la responsable de la atracción que producen los imanes, pero también de la principal dificultad en el aprendizaje de la gravedad en el marco de la mecánica newtoniana. Como suele suceder, los árboles no permiten ver el bosque: la omnipresencia y el tamaño de la propia Tierra ocultan la interacción gravitatoria entre ella y los demás objetos. Otros ingredientes como la modelización de la Tierra como una partícula en situaciones de enseñanza, o la asimilación del magnetismo terrestre con la atracción gravitatoria, no hacen sino dificultar aún más la construcción del concepto. Basadas en estas consideraciones, las autoras proponen algunas ideas para presentar la gravedad en la escuela:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dimensionalización del problema, abordando la Tierra como cuerpo cósmico y comparando su tamaño con el de otros objetos. Construcción de un modelo mental previo de acción a distancia. Por ejemplo, mediante el uso de imanes en situaciones diversas, como el movimiento de objetos pequeños a través de superficies de separación.&lt;/li&gt;
&lt;li&gt;Experimentación involucrando el cuerpo y sus sensaciones. Por ejemplo, la introducción al tema de acción y reacción a partir de la vivencia de dos personas empujándose, como manera de tomar conciencia del par de fuerzas involucrado.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Detalles sobre el marco teórico utilizado así como numerosos ejemplos pueden encontrarse en el artículo original: Revista de Enseñanza de la Física, APFA, Volumen 20 (2007) pp 33-39.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; M. Celia Dibar Ure (&lt;a href=&#34;mailto:celiadibar@yahoo.com&#34;&gt;celiadibar@yahoo.com&lt;/a&gt;) y Silvia Margarita Pérez (&lt;a href=&#34;mailto:silviamargaritaperez@yahoo.com.ar&#34;&gt;silviamargaritaperez@yahoo.com.ar&lt;/a&gt;) CEFIEC (Centro de Formación e Investigación en Enseñanza de las Ciencias) Facultad de Ciencias Exactas y Naturales, Universidad de Buenos Aires e IDH UNGS.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Frentes térmicos en el borde de la plataforma continental patagónica</title>
      <link>https://ciencianet.com.ar/post/frentes-termicos-en-el-borde-de-la-plataforma-continental-patagonica/</link>
      <pubDate>Wed, 13 Aug 2008 22:20:43 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/frentes-termicos-en-el-borde-de-la-plataforma-continental-patagonica/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Luis Pugnaloni.&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP).&lt;/p&gt;
&lt;p&gt;Recientemente, un grupo de investigadores encontraron que en la vecindad del borde de la plataforma continental patagónica existen frentes térmicos claramente distintos generados por múltiples ramas de agua fría, lo que modifica la visión de estudios anteriores que describen la Corriente de Malvinas como una única rama fría y que genera un único frente sobre el talud continental.&lt;/p&gt;
&lt;p&gt;El estudio de Bárbara Franco, Alberto Piola, Andrés Rivas, Ana Baldoni y Juan Pisoni, investigadores pertenecientes a diversas instituciones científicas de Argentina, se basa en el análisis de gradientes de datos satelitales de temperatura superficial del mar obtenidos y publicados por el proyecto Pathfinder (NOAA/NASA).&lt;/p&gt;
&lt;p&gt;Dieciocho años (1985-2002) de datos satelitales fueron usados para el cálculo de las variaciones estacionales de los frentes térmicos. Las ramas frías y frentes térmicos son prácticamente paralelos al borde exterior de la plataforma continental argentina entre la altura de Bahía Blanca y Rawson. Este borde de plataforma presenta una abrupta caída del fondo marino, lo que es conocido como talud continental, y es lo que delimita al Mar Argentino del océano Atlántico.&lt;/p&gt;
&lt;p&gt;Sobre este borde se observaron cambios relativamente intensos de la temperatura superficial del agua conforme se lo cruza mar adentro. Esta zona es conocida por su riqueza ictícola y la concentración de diversos organismos marinos y las temperaturas del agua y los frentes térmicos son determinantes para el ecosistema y ciclo de vida de estas especies.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CienciaNet&lt;/strong&gt; consultó a Bárbara Franco quien comenta: &amp;quot;Este trabajo destaca la importancia del empleo de diferentes técnicas para la determinación de frentes térmicos en regiones como el borde de plataforma patagónica, donde aparecen múltiples ramas frías en una escala espacial de distancia relativamente pequeña. Además, el estudio despierta nuevas curiosidades en cómo esta compleja dinámica frontal esta relacionada a los diversos organismos marinos que tienen sus ciclos de vida asociados a estas zonas frontales.&amp;quot;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo Original:&lt;/strong&gt; Barbara C. Franco, Alberto R. Piola, Andrés L. Rivas, Ana Baldoni, Juan P. Pisoni, &lt;a href=&#34;https://doi.org/10.1029/2007GL032066&#34;&gt;Multiple thermal fronts near the Patagonian shelf break&lt;/a&gt;, &lt;em&gt;Geophysical Research Letters,&lt;/em&gt; vol. 35, pag. L02607 (2008).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instituciones:&lt;/strong&gt; Departamento Oceanografía (&lt;a href=&#34;http://www.hidro.gov.ar/&#34;&gt;Servicio de Hidrografía Naval&lt;/a&gt; ), &lt;a href=&#34;http://www-atmo.at.fcen.uba.ar/&#34;&gt;Departamento de Ciencias de la Atmosfera y los Oceanos&lt;/a&gt; (UBA), &lt;a href=&#34;http://www.cenpat.edu.ar/&#34;&gt;Centro Nacional Patagonico&lt;/a&gt; (CONICET), Facultad de Ciencias Naturales (Universidad Nacional de la Patagonia), &lt;a href=&#34;http://www.inidep.edu.ar/&#34;&gt;Instituto Nacional de Investigacion y Desarrollo Pesquero&lt;/a&gt; .&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; Bárbara Franco (Email:&lt;a href=&#34;mailto:ocebcf@furg.br&#34;&gt;ocebcf@furg.br&lt;/a&gt; )&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Más información:&lt;/strong&gt; &lt;a href=&#34;http://es.wikipedia.org/wiki/Plataforma_continental&#34;&gt;Plataforma continental&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Jóvenes Científicos Precarizados</title>
      <link>https://ciencianet.com.ar/post/jovenes-cientificos-precarizados/</link>
      <pubDate>Tue, 12 Aug 2008 23:22:16 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/jovenes-cientificos-precarizados/</guid>
      <description>
        
          &lt;p&gt;Vea también artículo de opinión &lt;a href=&#34;https://ciencianet.com.ar/post/becarios/&#34;&gt;Becarios&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;La presente nota fue escrita por &lt;a href=&#34;http://www.precarizados.com.ar/&#34;&gt;&amp;quot;Jóvenes Científicos Precarizados&amp;quot;&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Los actuales becarios de investigación somos jóvenes investigadores que durante un promedio de 5 años desarrollamos trabajos tendientes a la producción de nuevos conocimientos y tecnologías, con una dedicación exclusiva. Actualmente no se nos considera trabajadores, obteniendo una remuneración que consiste en un estipendio de beca y que se encuentra muy por debajo de la canasta básica familiar. Además, carecemos de derechos a una obra social, aportes jubilatorios, aguinaldo, vacaciones reglamentarias, licencias por maternidad y enfermedad y a los beneficios de la antigüedad por el trabajo realizado, entre otras cosas. &amp;quot;Es decir, no gozamos de ningún derecho laboral.&amp;quot;&lt;/p&gt;
&lt;p&gt;En Jóvenes Científicos Precarizados nos hemos agrupado becarios de todo el país para reclamar por estos derechos.&lt;/p&gt;
&lt;p&gt;Sostenemos que las tareas intelectuales, prácticas, metodológicas, de producción científica y tecnológica, de extensión, difusión y divulgación del conocimiento que realizamos los becarios, son un tipo particular de actividad laboral. Es decir, que mejor sería llamarnos Investigadores en Formación, ya que además de estar formándonos académicamente, somos parte de la base del conjunto de los trabajadores del sistema de Ciencia y Técnica (CyT).&lt;/p&gt;
&lt;p&gt;En el 2007 la Diputada Norma Morandini presentó en la Cámara de Diputados de la Nación el Proyecto de Ley número 4908-D-07 que busca reglamentar los derechos laborales de los actuales becarios de investigación.Este proyecto intenta brindar un marco legal a la labor que hoy en día desarrollamos los becarios, reconociéndola como un trabajo con los derechos y obligaciones que esto representa.Entre otras cosas, esta ley garantizaría:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;acceso a una obra social&lt;/li&gt;
&lt;li&gt;aportes jubilatorios&lt;/li&gt;
&lt;li&gt;aguinaldo&lt;/li&gt;
&lt;li&gt;licencias por maternidad, paternidad y enfermedad&lt;/li&gt;
&lt;li&gt;vacaciones pagas&lt;/li&gt;
&lt;li&gt;ART&lt;/li&gt;
&lt;li&gt;derecho a la indemnización por despido injustificado&lt;/li&gt;
&lt;li&gt;seguridad e higiene laboral&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Vea también artículo de opinión &lt;a href=&#34;https://ciencianet.com.ar/post/becarios/&#34;&gt;Becarios&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CienciaNet&lt;/strong&gt; pretende publicar solamente artículos de opinión firmados por personas físicas. Dada la relevancia del caso, en esta ocación se reproduce material firmado por una organización al solo efecto de promover la discusión de esta problemática.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Becarios</title>
      <link>https://ciencianet.com.ar/post/becarios/</link>
      <pubDate>Thu, 07 Aug 2008 23:24:23 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/becarios/</guid>
      <description>
        
          &lt;p&gt;La presente nota fue publicada como Editorial en el número 105 de la revista Ciencia Hoy y se reproduce con autorización de la Asociación Civil Ciencia Hoy.&lt;/p&gt;
&lt;p&gt;En los últimos tiempos se han difundido diversos reclamos de naturaleza política y gremial de un grupo de becarios que adoptó el nombre de &amp;quot;jóvenes científicos precarizados&amp;quot;. Sus reivindicaciones son principalmente de dos clases. Unas (que no comentaremos en lo que sigue) se refieren a la política oficial de ciencia y tecnología; por ejemplo, que las acciones derivadas de esa política no se financien con recursos de la banca internacional de fomento, como el BID o el Banco Mundial, o que los becarios participen en la dirección de las instituciones científicas. Las otras reivindicaciones, de las que trata el presente editorial, apuntan a las condiciones de desempeño de los becarios en el sistema formal de investigación científica y tecnológica de la Argentina.&lt;/p&gt;
&lt;p&gt;El grupo reclamante está principalmente integrado por becarios de CONICET y de otros organismos, es decir, por estudiantes de postgrado que preparan su doctorado con la ayuda de una beca de esas entidades, y por aquellos que, habiéndolo alcanzado, accedieron a una beca postdoctoral para completar su formación científica. Ambas etapas -la doctoral y la postdoctoral- son parte tradicional y aceptada de los inicios de una carrera en la ciencia, realizada habitualmente en el medio académico, aunque también, con menos frecuencia, fuera de él.&lt;/p&gt;
&lt;p&gt;Esto ha sido y es así tanto en Argentina como en la mayoría de los países con un sistema científico institucionalizado. Entre los principales reclamos que nos interesa comentar se cuenta un cambio radical en el concepto de becario. Buscan los integrantes del grupo que dicho concepto sea substituido por el de &#39;investigador en formación&#39;, lo que significaría abandonar la figura habitual del estudiante de postgrado subsidiado y cambiaría por la de un trabajador sujeto, con ciertas restricciones, a la legislación laboral.&lt;/p&gt;
&lt;p&gt;Así, los portavoces del grupo defienden el otorgamiento a los becarios de &#39;plenos derechos laborales&#39;, incluidos &#39;vacaciones, obra social, aportes jubilatorios, aguinaldo, salario familiar, licencias por enfermedad y maternidad, etcétera&#39;. Afirman que &amp;quot;investigar es trabajar&amp;quot;, frase que toman como uno sus lemas centrales. Sostienen que los becarios realizan &#39;tareas de investigación, lo cual constituye una actividad laboral&#39;, y que por ello son o deberían ser empleados de los entes que asignan becas, aunque aclaran que llevan a cabo esas tareas &#39;mientras completan su formación académica&#39;.&lt;/p&gt;
&lt;p&gt;Estas inquietudes, con los argumentos que las sustentan, fueron recogidas por la diputada nacional Norma Morandini, que hacia fines de 2007 presentó en la Cámara de Diputados un proyecto de ley que las incluye. Las autoridades del nuevo Ministerio de Ciencia y Tecnología acogieron los reclamos con escaso entusiasmo. En particular, se apresuraron a señalar que no existe relación laboral entre los organismos otorgantes de becas o, en ultima instancia, entre el Estado, cuya estructura esos organismos integran, y los becarios de ciencia y tecnología. También manifestaron que aceptar el reclamo conduciría a la reducción del número de becas debido al mayor costo de cada una, por la incidencia de las llamadas cargas sociales. Excluyeron, en consecuencia, la posibilidad de transformar el régimen de becas en un contrato laboral, sin por ello oponerse a realizar modificaciones parciales a dicho régimen.&lt;/p&gt;
&lt;p&gt;Independientemente de la forma en que se realizaron los reclamos, e incluso de las medidas propuestas por los reclamantes y recogidas en el proyecto de la mencionada legisladora, creemos que los hechos comentados ponen en evidencia determinados desajustes que sería oportuno discutir con minuciosidad y procurar corregir. La principal corrección propuesta, sin embargo, de transformar al becario en asalariado, o, si se prefiere, al estudiantes de postgrado en trabajador, va más allá de esos desajustes y constituye una innovación substancial.&lt;/p&gt;
&lt;p&gt;Para analizar esa innovación conviene distinguir lo esencial de los accesorio. Lo esencial es establecer, dejando de lado preferencias semánticas, si quienes emprenden un doctorado, y luego un período de formación postdoctoral, como pasos iniciales del camino que conduce a una carrera en la ciencia (en el medio académico, en organismos técnicos de la administración pública o en empresas), llevan a cabo una tarea laboral o se trata de estudiantes avanzados. Planteada así la cuestión, no tenemos dudas de que nos encontramos ante lo segundo y que, por ende, lo que realizan las personas en cuestión no son actividades laborales sino estudios superiores o, si se prefiere, ejercicios de aprendizaje profesional. Serían, recurriendo a un antiguo concepto aún vigente en ciertos países y profesiones, &amp;quot;aprendices de investigador&amp;quot;, aunque esta afirmación podría relativizarse para los becarios postdoctorales.&lt;/p&gt;
&lt;p&gt;Para adaptar el concepto de aprendiz al contexto académico, hay que tener en cuenta una diferencia sustancial entre la profesión de investigador y otras profesiones. Esa diferencia es la libertad académica. Si objetivo de los becarios, como aprendices de investigador, es adquirir la capacidad de generar conocimiento, es requisito esencial para lograrlo que &amp;quot;su aprendizaje promueva su creatividad y autonomía de criterio y reúna las condiciones básicas para que se ejerciten en el pensamiento independiente&amp;quot;. Los instrumentos utilizados para alcanzar estos propósitos son varios: cursos avanzados, seminarios, participación en tareas de investigadores formados, experimentos propios de laboratorio, trabajos de campo, concurrencia a congresos científicos, entre otros; culminan en la realización de la tesis doctoral que consista en resolver alguna cuestión no resuelta o en responder alguna pregunta no contestada, es decir, sea el resultado de un proceso completo de investigación llevado a cabo en un marco didáctico bajo la guía de un director.&lt;/p&gt;
&lt;p&gt;Si bien muchas veces las tesis exitosas ya constituyen aporte originales al conocimiento, el producto esencial del proceso es formar investigadores que continuarán y, con el tiempo, modificarán las líneas de pensamiento aprendidas de sus maestros y directores. Este mundo se compadece mal con el concepto de actividad laboral, que tiene una orientación práctica y evoca un mundo subordinado a fines que le son fijados por otros.&lt;/p&gt;
&lt;p&gt;Sobre estas bases, creemos que los becarios se hacen un triste favor y no benefician al sistema académico cuando reclaman derechos laborales y eligen el carácter de asalariados. Avanzar por esa vía pierde de vista que la pertenencia al sistema científico carece de sentido si no se preserva a ultranza la independencia de criterio y de pensamiento, es decir, la libertad académica. Además llevaría al sistema científico un trecho más allá por la vía de la burocratización y la pérdida de creatividad, por la que ha avanzado mucho más de lo necesario, con consecuencias sumamente negativas.&lt;/p&gt;
&lt;p&gt;Lo dicho, sin embargo, nada tiene que ver con las condiciones materiales del desempeño de la función académica, a las que es necesario atender adecuadamente para el buen funcionamiento del sistema científico. Nos referimos, entre otras, al monto de los estipendios, al cuidado de la salud, al vínculo con el sistema previsional. Tienen razón los becarios en reclamar que se atiendan mejor sus necesidades personales, cosa que &amp;quot;Ciencia Hoy&amp;quot; apoya sin reservas y que se puede hacer perfectamente sin cambiar la figura de estudiante avanzado por la de trabajador.&lt;/p&gt;
&lt;p&gt;Comprendemos el argumento de que, muchas veces, los becarios terminan convertidos en los proletarios del sistema científico, porque son utilizados como mano de obra barata y sumisa por sus jefes de grupos de investigación. Esas situaciones ponen en evidencia serias irregularidades, que deben ser corregidas eliminando el abuso y no haciendo respetable la explotación por el reconocimiento de derechos laborales a los explotados. Conviene no perder de vista que la formación doctoral bajo la guía de maestros o en el seno de grupos de investigación constituye para los becarios una etapa crucial (además de extraordinariamente estimulante) de su carrera académica.&lt;/p&gt;
&lt;p&gt;Por ello es necesario librarlos de exigencias laborales que tengan propósitos ajenos a su formación y los podrían asimilar ameros asistentes de investigación o de laboratorio (lo que no excluye la realización de tareas rutinarias o incluso manuales, así como las realizadas con la administración de los proyectos, emprendidas con objetivos didácticos, porque dominarlas es tambiénparte deloficio que procuran aprender).&lt;/p&gt;
&lt;p&gt;Tienen también razón las autoridades del Ministerio de Ciencia y Tecnología en rechazar la exigencia de los becarios de transformarse en asalariados. Pero no sería oportuno que dejaran las cosas allí. Esta sería una buena ocasión para hacer reformas mucho más ambiciosas que las reclamadas y, recuperando algo de vitalidad y creatividad que tuvieron los organismos de promoción científica en sus tiempos fundacionales, poner a la producción científica del país en condiciones más acordes con el actual contexto nacional y mundial. Con la energía propia de un nuevo ministerio, es una oportunidad que no sería bueno desaprovechar.&lt;/p&gt;
&lt;p&gt;Un análisis sereno de los distintos sistemas de formación de investigadores en el resto del mundo, en el que participen autoridades y equipos técnicos del ministerio, legisladores, científicos formados y losmismos becarios, quizás arroje luz sobre el tema y conduzca a ponerse de acuerdo sobre soluciones.&lt;/p&gt;
&lt;p&gt;Estamos en momentos de construir nuevos caminos, por lo que sería una pena que el debate se planteara en términos corporativos, como una puja entre intereses de los diferentes sectores, y se centrara en buscar argumentos a favor o en contra de esos intereses. Como en cualquier investigación, analicemos los datos, tomemos distancia, reflexionemos y formulemos con creatividad las soluciones.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>El rol de los trabajos prácticos de laboratorio en la enseñanza de física</title>
      <link>https://ciencianet.com.ar/post/el-rol-de-los-trabajos-practicos-de-laboratorio-en-la-ensenanza-de-fisica/</link>
      <pubDate>Fri, 11 Jul 2008 23:30:45 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/el-rol-de-los-trabajos-practicos-de-laboratorio-en-la-ensenanza-de-fisica/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Luis Pugnaloni&lt;/strong&gt;. Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP).&lt;/p&gt;
&lt;p&gt;El año 2008 ha sido declarado como el Año de la Enseñanza de las Ciencias por los Ministerios de Educación y de Ciencia, Tecnología e Innovación Productiva, con el objetivo de mejorar la enseñanza de las Ciencias Naturales y la Matemática. Entre las preocupaciones de ambos Ministerios se encuentran los pobres resultados obtenidos por los alumnos argentinos en evaluaciones nacionales e internacionales y la disminución de las vocaciones científicas entre los estudiantes que continúan estudios en el nivel universitario, según consta en el &lt;a href=&#34;http://www.educaciencias.gov.ar/archivos/acercade/Documento.pdf&#34;&gt;Plan de Mejoramiento de la Enseñanza de las Ciencias&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;El proyecto de mejora incluye medidas variadas y dispares: la promoción de las ciencias a través de actividades de difusión y divulgación, la creación del portal de internet &lt;a href=&#34;http://www.educaciencias.gov.ar/&#34;&gt;www.educaciencias.gov.ar&lt;/a&gt;, capacitaciones diversas para docentes, la revisión de contenidos y metodologías de enseñanza, y la asignación de dinero para equipamiento de laboratorio y material didáctico para escuelas e institutos de formación docente.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/07/logo_2008_ciencias.gif&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Pero aún en el caso en que todas estas medidas resultaran exitosas y los jóvenes emprendieran con entusiasmo el estudio de carreras científicas y tecnológicas, el problema distaría de estar resuelto. ¿Qué ocurrirá cuando estos estudiantes ingresen a la Universidad? Muchos de ellos quedarán en el camino rápidamente y sólo unos pocos obtendrán su título. Para ilustrar esto podemos citar datos oficiales del total de las universidades nacionales: en el año 2002 hubieron en las Universidades Nacionales 1.243.880 alumnos, de los cuales 305.496 fueron nuevos inscriptos, y egresaron sólo 47.475&lt;a href=&#34;#ref1&#34;&gt;1&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;¿Cuáles son los obstáculos que los hacen desistir? ¿Cómo puede mejorarse la enseñanza universitaria de ciencias? Estas y otras preguntas similares, en particular sobre el aprendizaje de física, son planteadas en la Universidad de General Sarmiento (UNGS) por el Grupo de Investigación en Didáctica de la Física, integrado por el físico José Ure y los profesores en física Diego Petrucci, Silvia M. Pérez, Marisol Montino, Gladys Antúnez, Alejandra Aleman.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/07/Labo_ungs.jpg&#34; alt=&#34;Laboratorio: Integrantes del grupo de investigación en un aula de UNGS.&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;laboratorio&#34;&gt;Laboratorio&lt;/h3&gt;
&lt;p&gt;Integrantes del grupo de investigación en un aula de UNGSLa física es una asignatura presente en las carreras tanto del área de las ciencias exactas y naturales como tecnológicas, e incluye contenidos de mecánica, ondas, termodinámica y fluidos, entre otros. Está planteada en general mediante el dictado de clases teóricas, la resolución de problemas en papel, y la realización de trabajos prácticos de laboratorio (TPL): experimentos con sistemas simples y ya modelizados, diseñados para la observación de fenómenos y verificación de leyes y/o la medición de observables físicos. Por ejemplo, la medición de la aceleración de la gravedad mediante un péndulo, y el cálculo del error cometido.&lt;/p&gt;
&lt;p&gt;En los últimos años el grupo se ha dedicado a estudiar el rol que tienen estos TPL en el aprendizaje de los contenidos, ya que a pesar de ser escasamente discutidos como herramientas, su presencia en los cursos es incuestionable para los docentes y constituyen el caballito de batalla de la mayoría de las acciones –emprendidas tanto desde Ministerios como desde Facultades- destinadas al mejoramiento de la enseñanza.&lt;/p&gt;
&lt;p&gt;Muchas de estas propuestas comienzan (y terminan) en el envío de equipo y el montaje de laboratorios en las escuelas. En esta línea de trabajo, los investigadores han comenzado por reseñar en la bibliografía específica qué cosa se entiende por trabajo práctico de laboratorio. Posteriormente, entrevistaron a docentes y alumnos de cursos de física general sobre diferentes aspectos de los TPL. Exponemos aquí algunos de sus resultados.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Qué dicen los docentes&lt;/strong&gt;. Los objetivos y finalidades de los TPL más mencionados están referidos a la medición de variables, el cálculo de errores y la visualización del fenómeno, y en menor medida la adquisición de habilidades y el desarrollo de la intuición. Afirman que los TPL son importantes por diversos motivos, pero en todos los argumentos presentados parece subyacer una explicación epistemológica: debido a que la física es una ciencia fáctica, ver o comprobar un fenómeno le daría sentido a la teoría.&lt;/p&gt;
&lt;p&gt;Además, si bien los docentes no discuten la importancia de lo que se aprende a través de los TPL, no reconocen estrategias didácticas específicas para lograr el aprendizaje en situación de laboratorio.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Los estudiantes&lt;/strong&gt;. Los estudiantes los valoran como una herramienta mediante la cual “ven” los conceptos explicados en el pizarrón; además, aprovechan la instancia de redacción del informe como el momento de unificar y comprender lo que hicieron en el laboratorio. Sin embargo, tienen dificultades para comprender los objetivos del TPL que los docentes plantean, para darle significado a las tareas que realizan y para desarrollar los TPL en general. En relación con el error en la medición no se registraron opiniones positivas: el cálculo de errores les fue presentado con un grado de dificultad formal que superaba su capacidad de comprensión.&lt;/p&gt;
&lt;p&gt;Mediante observaciones de clases de laboratorio, se detectó además que los estudiantes no utilizan criterios de validación internos, recurriendo a la lógica o al saber disciplinar. Los criterios suelen apelar a una autoridad: el docente, un libro, la guía de trabajos prácticos o un compañero.&lt;/p&gt;
&lt;p&gt;Estas observaciones reflejan que los estudiantes realizan los TPL mecánicamente, sin tomar decisiones en función de los objetivos, probablemente porque no les resultan claros: siguen el procedimiento pero sin saber cuál es la pregunta a contestar.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Propuesta&lt;/strong&gt;. Como conclusión, y en palabras de Diego Petrucci: “Hay mucha distancia entre lo que los docentes proponen y lo que a los estudiantes les resulta útil para su aprendizaje. Por ello pensamos en diseñar laboratorios cuya finalidad es asistir al aprendizaje de aquellos conceptos más difíciles de asimilar.”&lt;/p&gt;
&lt;p&gt;Sobre la base de sus investigaciones, el grupo avanzó en el diseño de TPL con una modalidad no tradicional, en la cual se propone que los estudiantes se involucren con la tarea y que ésta les represente un desafío que puedan resolver experimentalmente. La tarea se constituye en una secuencia de actividades integradas, en la cual los alumnos toman decisiones de modo no arbitrario, avanzando hacia el objetivo y entendiendo lo que hacen.&lt;/p&gt;
&lt;p&gt;Tres TPL diseñados con esta modalidad han sido implementados con resultado satisfactorio. “De este modo, no sólo se favorece el aprendizaje de conceptos, sino que estos laboratorios fomentan una forma de hacer y de concebir a la física que supera la idea de un montón de fórmulas y conocimientos arbitrarios, mostrándola como un campo al que ellos pueden darle sentido y tomar dediciones en función de lo que saben”. Después de todo, quienes enseñamos física mediante experimentos y demostraciones estamos presentando a los alumnos una forma nueva de dialogar con la Naturaleza: hacemos las preguntas mediante el experimento e interpretamos la respuesta que ella nos da a nuestra manera particular de preguntar. Y como sucede con cualquier presentación, las primeras impresiones son importantes, de modo que les debemos a los interlocutores la reflexión sobre qué queremos enseñar y qué es lo que realmente enseñamos con los laboratorios.&lt;/p&gt;
&lt;p&gt;Para contactarse con el grupo, escribir a &lt;a href=&#34;mailto:dpetrucc@ungs.edu.ar&#34;&gt;dpetrucc@ungs.edu.ar&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referencias:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://www.me.gov.ar/spu/guia_tematica/PMSIU/pmsiu____algunos_datos.html&#34;&gt;Programa Mejoramiento del Sistema de Información Universitario&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;“Propuesta de modalidad de trabajo práctico de laboratorio para el nivel universitario básico”. Petrucci Montino, Pérez y Ure. VI ENPEC, Universidade de Santa Catarina, 2007.&lt;/li&gt;
&lt;li&gt;“Concepciones de los docentes universitarios sobre los trabajos prácticos de laboratorio”. Antúnez, Pérez y Petrucci. En prensa en la Revista Brasileira de Pesquisa em Educação em Ciências. 2008.&lt;/li&gt;
&lt;li&gt;&amp;quot;¿Magia o Física? Los estudiantes universitarios y los trabajos prácticos de laboratorio&amp;quot;. Petrucci, Montino y Ure. Memorias del 8vo SIEF 2006.&lt;/li&gt;
&lt;li&gt;&amp;quot;Cómo ven a los trabajos prácticos de laboratorio de física los estudiantes universitarios” Petrucci, Ure, Salomone. Revista de Enseñanza de Física, 19 (1), pp. 7-20. APFA, 2006.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Nota:&lt;/strong&gt; Las referencias 2, 3 y 4 pueden solicitarse en formato PDF a la dirección de e-mail mencionada arriba.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>El estilo de vida sano apaga los genes que causan cáncer</title>
      <link>https://ciencianet.com.ar/post/el-estilo-de-vida-sano-apaga-los-genes-que-causan-cancer/</link>
      <pubDate>Sat, 21 Jun 2008 23:38:24 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/el-estilo-de-vida-sano-apaga-los-genes-que-causan-cancer/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro&lt;/strong&gt;. Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP) y Universidad Tecnológica Nacional.&lt;/p&gt;
&lt;p&gt;No es un secreto que un estilo de vida sano puede hacer más lenta la progresión del cáncer, pero cómo ocurre esto ha sido un misterio. Ahora, nueva evidencia sugiere una respuesta: una dieta saludable y ejercicios pueden activar o desactivar la expresión de genes cruciales. En un estudio piloto con 30 hombres con las primeras etapas de cáncer de próstata, Dean Ornish y colegas de la Universidad de California en San Diego, Estados Unidos, analizaron los efectos del cambio drástico del estilo de vida en la expresión genética en la próstata.&lt;/p&gt;
&lt;p&gt;Este trabajo fue publicado recientemente en &lt;em&gt;Proceedings of the National Academy of Sciences&lt;/em&gt; en junio de 2008 (ver Referencia). El equipo tomó biopsias antes y después de 3 meses de alimentación saludable, ejercicios moderados, administración de estrés y psicoterapia, mostrando un cambio significante en la expresión de cientos de genes. Entre las modificaciones al estilo de vida, se destacan una dieta baja en grasas (solo el 10% de las calorías provenientes de grasas), alimentos vegetales, 60 minutos de administración del estrés por día (yoga, respiración, meditación y relajación progresiva), ejercicios aeróbicos moderados (caminatas de 30 minutos por día, 6 días por semana) y 1 hora de terapia de grupo semanal. La dieta fue suplementada con soja (una porción diaria de tofu más 58 g de una bebida enriquecida con proteína de soja fortificada), aceite de pescado (3 g diarios), vitamina E (100 unidades por día), selenio (200 mg diarios) y vitamina C (2 g por día). Muchos genes, incluyendo varios relacionados con la formación de tumores, fueron menos activos. Otros, incluyendo algunos genes que combaten la enfermedad, fueron más activos.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/06/heatmap.jpg&#34; alt=&#34;Figura: Mapa de color que muestra 48 transcripciones activadas (rojo) y 453 desactivadas (verde) en las muestras post-intervención de tejido prostático normal.&#34;&gt;&lt;/p&gt;
&lt;p&gt;El estudio de 2005 (&lt;a href=&#34;https://www.pnas.org/content/pnas/105/24/8369.full.pdf&#34;&gt;pdf en inglés&lt;/a&gt;) realizado por Ornish y su equipo mostró cómo los cambios en el estilo de vida pueden reducir ciertos marcadores de cáncer de próstata, posiblemente frenando su progresión. &amp;quot;Ahora estamos comenzando a entender algunos de los mecanismos genéticos por los cuales esos cambios pueden ejercer tales efectos&amp;quot;, dice Ornish.&lt;/p&gt;
&lt;h3 id=&#34;implicaciones-más-amplias&#34;&gt;Implicaciones más amplias&lt;/h3&gt;
&lt;p&gt;Debido a que los investigadores analizaron tejido prostático sano -los pacientes tenían tumores muy pequeños, difíciles de analizar con biopsias- los resultados pueden ser importantes para la prevención del cáncer. &amp;quot;Las consecuencias de este estudio pueden ser mucho más amplias, y no sólo limitadas a los hombres para este asunto&amp;quot;, dice Ornish. Dos importantes genes que causan cáncer, denominados RAN y Shoc2, cuyas expresiones fueron suprimidas por los cambios de estilo de vida, se encuentran en la mayoría de los tipos de tumores, incluyendo los cáncer de mama y colon.&lt;/p&gt;
&lt;p&gt;Sin embargo, Meir Stampfer, un epidemiólogo de Harvard Medical School, sostiene que es aún demasiado pronto para extraer conclusiones sobre causas y efectos. Son necesarios estudios de seguimiento de largo plazo para determinar si esos cambios genéticos pueden realmente frenar o prevenir el cáncer, afirma. &amp;quot;Pero es un primer paso muy importante&amp;quot;, agrega. &amp;quot;Esto va a marcar una nueva ola de investigación&amp;quot;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fuente:&lt;/strong&gt; &lt;a href=&#34;http://www.newscientist.com/channel/health/cancer/dn14158-healthy-lifestyle-turns-off-genes-that-cause-cancer.html?feedId=cancer_rss20&#34;&gt;NewScientist&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referencia:&lt;/strong&gt; Dean Ornish, Mark Jesus M. Magbanua, Gerdi Weidner, Vivian Weinberg, Colleen Kemp, Christopher Green, Michael D. Mattie, Ruth Marlin, Jeff Simko, Katsuto Shinohara, Christopher M. Haqq, and Peter R. Carroll. &lt;em&gt;Changes in prostate gene expression in men undergoing an intensive nutrition and lifestyle intervention&lt;/em&gt;. PNAS 2008 105: 8369-8374. (&lt;a href=&#34;https://doi.org/10.1073/pnas.0803080105&#34;&gt;link al artículo&lt;/a&gt;)&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>El futuro de los biocombustibles en Argentina</title>
      <link>https://ciencianet.com.ar/post/el-futuro-de-los-biocombustibles-en-argentina/</link>
      <pubDate>Tue, 17 Jun 2008 23:43:06 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/el-futuro-de-los-biocombustibles-en-argentina/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro&lt;/strong&gt;. Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP) y Universidad Tecnológica Nacional.&lt;/p&gt;
&lt;p&gt;En un trabajo reciente se analiza la situación actual del mercado de biocombustibles en Argentina y su perspectiva a corto (2010) y mediano plazo (2020). Los investigadores sugieren que el biodiesel tiene mayores posibilidades que el bioetanol. En principio el biodiesel será producido con soja, por su bajo precio, pero se alerta sobre la mayor eficiencia de otras fuentes como el girasol.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/06/biodiesel.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Lamersa y K. McCormick de Universidad de Lund (Suecia) junto a J.A. Hilbert del INTA Castelar han publicado un artículo donde se analiza el presente y futuro del mercado de biocombustibles en Argentina. En el mismo se destaca la posición preferencial del biodiesel (sustituto del gasoil) sobre el bioetanol (sustituto de las naftas). Argentina consume preferentemente gasoil (más del 50% del mercado) y las naftas están siendo reemplazadas por el GNC. Por otro lado, los exportadores de aceites vegetales (el material necesario para producir biodiesel) preferirán convertirlo a combustible ya que su exportación implica más del 18% menos en cargas impositivas.&lt;/p&gt;
&lt;p&gt;Otro factor en favor del crecimiento del biodiesel sobre el bioetanol es el relativo bajo costo de la infraestructura necesaria para la conversión. Instalar una planta de biodiesel cuesta alrededor de un tercio del valor de una planta de bioetanol. Se espera que en unos 3 años Argentina no pueda autoabastecerse en gasoil y el uso de biodiesel resultará de suma importancia. Para esto, sugieren los autores, deberá incentivarse el consumo local de biodiesel que actualmente es producido por las empresas con miras a la exportación al mercado europeo.&lt;/p&gt;
&lt;p&gt;El Estado Argentino ha establecido como meta que el 5% del diesel consumido en Argentina tiene que ser biodiesel. Para esto será necesario en 2010 convertir el 12% de la soja que se produzca en combustible. La soja es el principal producto agrícola en Argentina y tiene grandes posibilidades de convertirse en el único soporte para la producción de biodiesel. Sin embargo, los investigadores alertan sobre la mejor eficiencia en la conversión de otros cultivos como el girasol. Por otro lado, la diversificación de fuentes para el biodiesel ayudaría a dar mayor flexibilidad a la producción y a reducir los riesgos del monocultivo.&lt;/p&gt;
&lt;p&gt;Finalmente, los autores del trabajo destacan que en la actualidad la producción de biodiesel está siendo concentrada por grandes empresas. La falta de claros incentivos para las pequeñas y medianas empresas puede llevar a un marcado desbalance en la distribución de los beneficios de este mercado aún incipiente en la sociedad argetina.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CienciaNet&lt;/strong&gt; consultó a Jorge Hilbert quien comentó que el futuro de este nuevo mercado tiene una &amp;quot;fuerte dependencia de precios relativos del petróleo y los insumos para producir biocombustibles&amp;quot;. También destacó &amp;quot;la necesidad de permanecer en la búsqueda de diferentes alternativas complementarias que permitan reducir la competencia con los alimentos&amp;quot;.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; &lt;a href=&#34;https://doi.org/10.1016/j.enpol.2007.12.023&#34;&gt;The emerging liquid biofuel market in Argentina: Implications for domestic demand and international trade&lt;/a&gt; , Energy Policy 36 (2008) 1479–1490.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instituciones:&lt;/strong&gt; &lt;a href=&#34;https://inta.gob.ar/iir&#34;&gt;Instituto de Ingeniería Rural&lt;/a&gt; (INTA Castelar), &lt;a href=&#34;http://www.lu.se/lund-university/&#34;&gt;Universidad de Lund&lt;/a&gt; (Suecia)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; Jorge Antonio Hilbert (E-mail: &lt;a href=&#34;mailto:hilbert@cnia.inta.gov.ar&#34;&gt;hilbert@cnia.inta.gov.ar&lt;/a&gt; )&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Más información:&lt;/strong&gt; &lt;a href=&#34;http://es.wikipedia.org/wiki/Biodi%C3%A9sel&#34;&gt;Biodiesel&lt;/a&gt;, &lt;a href=&#34;http://es.wikipedia.org/wiki/Bioetanol&#34;&gt;Bioetanol&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Experimentos con rayos que hicieron historia</title>
      <link>https://ciencianet.com.ar/post/experimentos-con-rayos-que-hicieron-historia/</link>
      <pubDate>Thu, 22 May 2008 23:46:49 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/experimentos-con-rayos-que-hicieron-historia/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero&lt;/strong&gt;. Instituto de Investigaciones Fisicoquímicas Teóricas y Aplicadas (CONICET-UNLP-CIC).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Relato presentado en el Museo de Física de la Universidad Nacional de La Plata durante la Noche de los Museos 2008.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;En la década de 1870, el británico William Crookes, reconocido químico de la época, creyó que había encontrado un nuevo estado de la materia que se sumaba a los tres ya conocidos (sólido, líquido y gaseoso): la materia radiante.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/05/florence2.jpg&#34; alt=&#34;William Crook con la medium Florence Cook.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Como muchos otros investigadores, tenía diversos intereses: creía que podía estudiar científicamente la “fuerza psíquica” que ejercían los mediums mediante experimentos. Fue uno de los más importantes investigadores de lo que se llama Espiritismo Científico. Llegó incluso a publicar un artículo en la revista &lt;em&gt;Quarterly Journal of Science&lt;/em&gt; –de la cual fue editor–, donde clasificaba los 13 tipos de fenómenos que había observado.&lt;/p&gt;
&lt;p&gt;En aquel entonces los científicos estaban completando la Tabla Periódica, descubriendo nuevos elementos químicos. Crookes era experto en la identificación de sustancias químicas a partir de sus espectros de emisión y había descubierto un nuevo elemento, el Talio. Entre otras cosas, generaba y estudiaba descargas eléctricas en tubos con gases a baja presión. Alrededor de 1875, Crookes mejoró los tubos de vacío inventados por Geissler. Estos tubos tenían dos placas metálicas (ánodo y cátodo) y cuando se conectaban a una fuente eléctrica mostraban zonas luminosas, diferentes según la presión del gas.&lt;/p&gt;
&lt;p&gt;Crookes consiguió alcanzar presiones aún más bajas, obteniendo descargas que se propagaban en línea recta, en forma de rayos. Cuando estos misteriosos “rayos catódicos” impactaban contra las paredes del vidrio generaban un llamativo resplandor verde pálido. Motivado por su descubrimiento, hizo más experimentos. Haciendo girar molinillos de mica dentro de los tubos, se convenció de que estaba observando materia, pero en un nuevo estado, que llamó radiante. Pensaba que en el alto vacío del tubo, el gas llegaba a un inconcebible estado de división, y sus átomos eran rechazados por el cátodo, generando los rayos. Además, los rayos podían producir también efectos térmicos y ser desviados por campos magnéticos, sugiriendo de que se trataba de partículas eléctricamente cargadas emitidas por el cátodo.&lt;/p&gt;
&lt;h3 id=&#34;controversia&#34;&gt;Controversia&lt;/h3&gt;
&lt;p&gt;Otros investigadores se sumaron al estudio del nuevo fenómeno y pronto se generó un debate. El físico alemán Lenard era el principal opositor a la hipótesis de Crookes. Había observado que los rayos catódicos podían atravesar láminas metálicas delgadas sin ser desviados de su trayectoria recta. Sostenía entonces que no podía tratarse de partículas sino de “perturbaciones ondulatorias del éter” (actualmente, ondas electromagnéticas). Muchos ingleses se sumaron al bando de Crookes; entre otros Thomson y FitzGerald. Pero los alemanes, entre quienes se encontraban Hertz y Goldstein, se alineaban detrás de Lenard. No es la materia que viaja –decían- es el éter que vibra. Por otra parte, la crítica a sus investigaciones “del otro mundo” fue unánime. En 1907 recibió el Premio Nobel de Química.&lt;/p&gt;
&lt;h3 id=&#34;consecuencias-de-los-experimentos-de-crookes&#34;&gt;Consecuencias de los experimentos de Crookes&lt;/h3&gt;
&lt;h4 id=&#34;rayos-x&#34;&gt;Rayos X&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/05/Rontgen-en-su-laboratorio.png&#34; alt=&#34;Wilhem Röntgen en su laboratorio.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;En 1895 Wilhem Röntgen se entusiasmó con la fluorescencia observada por Crookes. Se preguntaba si los rayos catódicos atravesaban el vidrio de los tubos, y para comprobarlo, cubrió con cartón uno de los mismos. No observó ningún resplandor, pero sí vio luminiscencia en una pantalla de platinocianuro de bario que tenía en su laboratorio.&lt;/p&gt;
&lt;p&gt;Durante las siguientes semanas, repitió el experimento interponiendo diferentes materiales entre la pantalla y el tubo, notando que sólo el plomo podía impedir la luminiscencia. La conclusión era inevitable: el tubo emitía algún tipo de radiación, invisible pero penetrante en la materia. Cuando intentó en fotografiar este fenómeno encontró otra sorpresa: las placas fotográficas que tenía estaban veladas. Para comprobar el alcance de la radiación en la emulsión, colocó el tubo y la placa fotográfica en distintas habitaciones, obteniendo una imagen de la puerta que las separaba. Obtuvo también imágenes del paso de la radiación a través del cuerpo humano. La primera radiografía fue una imagen de la mano de su esposa Bertha luego de una exposición de 15 minutos.&lt;/p&gt;
&lt;p&gt;Röntgen se convirtió en el científico del momento. Había descubierto los rayos X. Posteriormente a su conferencia de 1896 cosechó múltiples reconocimientos y en 1901 recibió el Nobel de Física. A pesar de las posibles aplicaciones, Röntgen se negó a comercializar o patentar su descubrimiento, argumentando que el beneficio pertenecía a la Humanidad.&lt;/p&gt;
&lt;h4 id=&#34;radiactividad&#34;&gt;Radiactividad&lt;/h4&gt;
&lt;p&gt;Motivado por las investigaciones de Crookes y Röntgen, Henri Becquerel, en 1896, retomó el estudio iniciado por su padre sobre minerales fluorescentes. Como en los tubos de Crookes la emisión de rayos X estaba acompañada de la fluorescencia, Becquerel se preguntó si sus materiales luminosos emitirían también rayos X.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/05/Becquerel-curie--300x189.jpg&#34; alt=&#34;Henri Bequerel juento a Pierre y Marie Curie.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Comenzó a experimentar: exponía una muestra de sal de uranio al sol y luego la depositaba sobre una placa fotográfica cubierta por un grueso envoltorio. Observaba que la placa se velaba. Un día nublado alteró su rutina: sin previa exposición colocó la muestra sobre la placa, y la guardó a la espera de días soleados.&lt;/p&gt;
&lt;p&gt;Poco después, con un presentimiento, reveló la placa y encontró la veladura provocada por la muestra, notando que la radiación se emitía sin necesidad de la exposición a la luz. Becquerel había descubierto la propiedad de ciertas sustancias de emitir por sí mismas radiación penetrante, posteriormente nombrada radiactividad por Mme. Curie. Cuando Becquerel (y el resto de la comunidad) observó que no podía obtener imágenes de huesos como ocurría con los rayos X, se desinteresó del asunto. Más tarde, propuso a la joven estudiante Marie Curie que continuara la investigación. Junto al matrimonio Curie, Becquerel recibió el Premio Nobel de Física en 1903.&lt;/p&gt;
&lt;h4 id=&#34;descubrimiento-del-electrón&#34;&gt;Descubrimiento del electrón&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/05/thomson.jpg&#34; alt=&#34;Joseph Thomson en su laboratorio.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;En la prolongada controversia onda-partícula, Joseph J. Thomson dio la respuesta definitiva, al menos hasta el advenimiento de la Mecánica Cuántica. Diseñó un dispositivo para hacer pasar los rayos por un campo magnético o eléctrico, desviando sus trayectorias. Aplicando un campo electromagnético, y mediante argumentos teóricos, pudo determinar tanto la velocidad de las partículas como el cociente entre su carga eléctrica y su masa.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/05/mano-lp-683x1024.jpg&#34; alt=&#34;Mano humana: una de las primeras imágenes de radiografías obtenidas en Argentina (Instituto de Física La Plata).&#34;&gt;&lt;/p&gt;
&lt;p&gt;En aquel tiempo, las únicas partículas cargadas negativamente que se conocían eran los iones negativos de los átomos. Pero las partículas de los rayos catódicos no podían identificarse con tales iones, pues para ser desviadas tan marcadamente, debían de poseer una carga eléctrica inimaginablemente elevada, o bien tratarse de partículas muy ligeras, mil veces más livianas que el átomo más ligero. Esta última interpretación encajaba mejor, y por otra parte, los físicos habían intuido ya que la corriente eléctrica era transportada por partículas cargadas.&lt;/p&gt;
&lt;p&gt;Los rayos catódicos fueron entonces identificados como las trayectorias de partículas subatómicas, que además eran las unidades elementales de la electricidad, dándoseles el nombre de electrones. Aunque algunos grandes científicos de la época, como Lord Kelvin, menospreciaron el hallazgo, en 1906 Thomson recibió el Nobel de Física.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Museo Interactivo de Ciencias “Puerto Ciencia”</title>
      <link>https://ciencianet.com.ar/post/museo-interactivo-de-ciencias-puerto-ciencia/</link>
      <pubDate>Fri, 25 Apr 2008 23:57:54 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/museo-interactivo-de-ciencias-puerto-ciencia/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero:&lt;/strong&gt; Instituto de Investigaciones Fisicoquímicas Teóricas y Aplicadas (CONICET-UNLP-CIC).&lt;/p&gt;
&lt;p&gt;Los museos expresan sus objetivos, finalidades y el público a quien dedican sus actividades en un brevísimo texto que llaman la misión. En el caso de Puerto Ciencia, la misión es la propuesta por la UNESCO en el año 2000: &lt;em&gt;Promover la popularización y alfabetización científica y tecnológica para todos, haciendo hincapié en la educación no formal, satisfaciendo tanto a los sujetos en particular, como a las comunidades en general, desde el niño al adulto de la tercera edad&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/04/logopc.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Fue creado en 1996 el seno de la Facultad de Ingeniería de la Universidad Nacional de Entre Ríos (UNER), en la ciudad de Paraná, como expresión del trabajo sostenido de diversos proyectos de extensión e investigación universitaria. Luego se convirtió en un Programa permanente de la UNER. Tal vez por haberse originado en una ciudad costera, y casi paradójicamente, Puerto Ciencia levó anclas y durante sus primeros cinco años recorrió diversas provincias como museo itinerante. Finalmente, a través de un convenio con la Municipalidad, hizo base en un viejo galpón que fuera del Ferrocarril, donde actualmente se encuentra montada su Muestra Interactiva.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/04/2ludion.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Allí se encuentran en un único ambiente varias decenas de módulos interactivos, diversos tanto en temas como en complejidad, pero todos ellos de gran impacto visual. Aunque cada módulo cuenta con un texto informativo, los docentes que acompañan la visita son los encargados de motivar e informar a los visitantes. Los mismos docentes y algunos becarios son quienes diseñan, construyen y mantienen los módulos. Cada visitante -mayormente escolares- decide qué experiencia ver, armando libremente un recorrido que puede incluir por ejemplo jugar con espejos, hacer funcionar un globo aerostático o crear una gigantesca burbuja de jabón.&lt;/p&gt;
&lt;p&gt;Desde el Museo se realizan diversos talleres dirigidos a público en general dedicados a temas como la óptica y la construcción de cocinas solares. También se desarrolla equipamiento para la enseñanza de ciencia destinado a escuelas, y brinda asesoramiento a otras entidades educativas. Puerto Ciencia integra la Red de Popularización de la Ciencia, una Red Latinoamericana promovida por la UNESCO de grupos e instituciones que trabajan en Divulgación y Popularización. Puerto Ciencia es un lugar de encuentro, pero también un generador infatigable de actividades que involucran otras instituciones. El Ingeniero Agustín Carpio, director del Museo desde su creación, tomó la iniciativa para la creación de la Asociación Argentina de Centros y Museos de Ciencias y Tecnología (AACeMuCyT), fundada el 11 de Octubre de este año, con la participación de entidades de todo el país.  &lt;/p&gt;
&lt;h3 id=&#34;entrevista-al-ingeniero-gustavo-romero-integrante-de-puerto-ciencia&#34;&gt;Entrevista al Ingeniero Gustavo Romero, integrante de Puerto Ciencia.&lt;/h3&gt;
&lt;p&gt;- Para dar vida a un museo se necesita básicamente un lugar, un patrimonio, y la decisión de crearlo. ¿Cómo es el proceso cuando solo se tiene la decisión?&lt;/p&gt;
&lt;p&gt;- &lt;strong&gt;G.R.&lt;/strong&gt;: Bien, yo creo que lo primordial es eso, la decisión, la determinación; eso debe ser muy fuerte, después de eso el proyecto es un éxito donde lo hagas y con los recursos que consigas. La ventaja de este tipo de actividad es que se puede empezar con bastante poco, no es necesario una gran infraestructura, aunque si la tienes mucho mejor. En nuestro caso y a pesar de la trayectoria, cantidad de proyectos desarrollados, todavía no contamos con muchas cosas básicas como personal no docente, transporte, y ciertas instalaciones; pero seguimos gracias a la determinación, a la demanda que tenemos y al apoyo de la UNER.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/04/explorando-500x375.jpg&#34; alt=&#34;:right&#34;&gt;&lt;/p&gt;
&lt;p&gt;Cuando surge un grupo de Divulgación es importante contar con un respaldo institucional de nivel superior, aunque no es un requisito imprescindible. Este grupo puede constituir un Programa de talleres para docentes de secundaria, puede centrarse en el desarrollo de material y publicaciones, o puede ser que decida armar un museo interactivo, entre otras formas de comunicación.&lt;/p&gt;
&lt;p&gt;- ¿Qué experiencias rescatan de su etapa itinerante?&lt;/p&gt;
&lt;p&gt;- &lt;strong&gt;G.R.&lt;/strong&gt;: Las experiencias que hemos recogido a lo largo de las muestras itinerantes en los pueblos (hemos estado en escuelas, universidades, municipios y ferias regionales) es que al público le fascina el acceso al conocimiento, el estudiante pregunta y se interesa, se le despierta la curiosidad y el deseo de aprender, se motiva. Les parece increíble que la Universidad baje a los pueblos con experimentos que puede tocar, accionar, interactuar y repetir, que explican fenómenos naturales, leyes físicas, matemáticas y tecnología de un modo sencillo, lúdico y accesible a todo público.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/04/mosaicos-500x375.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Debemos pensar que los maestros actúan independientemente y hacen lo que pueden, muchas veces desbordados por múltiples problemas del alumnado, de la escasez de recursos o instalaciones, y en soledad para abordar temas disciplinares, resultando muy difícil que se hagan las practicas de laboratorio o innovación de métodos de enseñanza. Por otra parte en el interior resulta muy difícil acceder al cursos de capacitación en temas disciplinares o metodológicos ya que no hay casi oferta. El arribo del Museo interactivo de Ciencias a un pueblo resulta una novedad absoluta, y muchas veces lo hemos podido comprobar en las Ferias Regionales ha sido lo más importante de la muestra copando al final las portadas de los periódico locales.&lt;/p&gt;
&lt;p&gt;El último año recorrimos localidades del interior de Entre Ríos, las pautas para elegirlas fueron: distancia al museo no menor a 120/130 km; tamaño poblacional de entre 5000 y 30.000 habitantes; población donde nunca hubiese estado antes otra versión móvil del Museo; y asentamiento en distintas zonas de la provincia. El itinerario cumplido alcanzó a 15.420 km, en 44 viajes a 10 localidades.&lt;/p&gt;
&lt;p&gt;- ¿Cuáles son los proyectos que se están llevando adelante actualmente?&lt;/p&gt;
&lt;p&gt;- &lt;strong&gt;G.R.&lt;/strong&gt;: Actualmente estamos finalizado este proyecto denominado “Evaluación del impacto del Museo Itinerante en pequeñas localidades”, mediante el cual hemos recorrido pueblos del interior de la provincia de Entre Ríos, permaneciendo en cada lugar durante varios días con una muestra de unas 15 exhibiciones de diversos temas (matemática, fluidos, electromagnetismo, física mecánica, etc). El transporte, personal y demás gastos se financió con fondos de la Universidad y de la Red Pop. Este año promovimos la formación de la Asociación AACeMuCyT, cuya dirección ejecutiva recae esta primera vez en el Director de Puerto Ciencia, el Ing. Agustín Carpio. El museo ha atendido durante todo el año a visitantes escolares que llegan al museo previa cita acordada, y se atiende a todo público un par de días de la semana por las tardes (viernes y sábados). Para el año próximo aspiramos a fortalecer la muestra fija, mediante la incorporación de algunos nuevos desarrollos, y por parte de la universidad hay interés en que sigamos con la muestra itinerante llegando a los pueblos, así que esperamos crecer en ese aspecto también.&lt;/p&gt;
&lt;h3 id=&#34;información-de-contacto&#34;&gt;Información de contacto&lt;/h3&gt;
&lt;p&gt;Bv. Racedo y Pascual Palma (Predio ex Ferrocarril) – Paraná – Entre Ríos - Argentina.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;e-mail:&lt;/strong&gt; &lt;a href=&#34;mailto:puertociencia@ingenieria.uner.edu.ar&#34;&gt;puertociencia@ingenieria.uner.edu.ar&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;web:&lt;/strong&gt; &lt;a href=&#34;http://ingenieria.uner.edu.ar/puertociencia/&#34;&gt;http://ingenieria.uner.edu.ar/puertociencia/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Público General:&lt;/strong&gt; Abierto de Abril a Noviembre de cada año, los días Viernes y Sábados de 16:30 hs a 19:30 hs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Delegaciones escolares o turísticas:&lt;/strong&gt; Exclusivamente solicitando turno por mail o teléfono. Tel. +54 - 0343-4975100 / 077 / 078 int: 112 - 136.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>CienciaNet ganó un subsidio de la AFA</title>
      <link>https://ciencianet.com.ar/post/ciencianet-gano-un-subsidio-de-la-afa/</link>
      <pubDate>Sun, 20 Apr 2008 00:05:26 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/ciencianet-gano-un-subsidio-de-la-afa/</guid>
      <description>
        
          &lt;p&gt;Nuestro portal de divulgación científica ganó un subsidio de la Asociación Física Argentina (AFA) en el marco de una convocatoria de proyectos para el Incentivo de Vocaciones para el estudio de la Física (INVOFI).&lt;/p&gt;
&lt;p&gt;La convocatoria cerró el día 15 de Octubre de 2007, presentándose 13 proyectos de distintos puntos del país. El Jurado para evaluarlos quedó conformado por: Enrique Coleonis (FAMAF), Laura Butteler (FAMAF) y Julia Salinas (UNT). Durante el mes de diciembre, una comisión integrada por tres socios de la AFA realizó un estudio de las propuestas y escogió 5 de estos proyectos para ser financiados durante 2008.&lt;/p&gt;
&lt;p&gt;El día 18 de diciembre, en su última reunión, la Comisión Directiva de AFA hizo suyo este dictamen y resolvió financiar los siguientes 5 proyectos: 1) CienciaNet: físicos con la gente, 2) Red creativa de ciencia, 3) La radioactividad en el medioambiente y la industria no nuclear, 4) Inicio de las Olimpíadas de Física en San Luis y 5) Taller de experimentación en fluidos geofísicos.&lt;/p&gt;
&lt;p&gt;En palabras de la &lt;a href=&#34;http://www.fisica.org.ar/&#34;&gt;AFA&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Queremos agradecer profundamente el esfuerzo de aquellos que nos prestigiaron con sus presentaciones, todas de excelente calidad e interés, en esta primera convocatoria”.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Tal como se propuso en el plan de &lt;strong&gt;CienciaNet&lt;/strong&gt;, el proyecto incluye dos aspectos: por un lado garantizar una simbólica retribución económica a los autores de artículos de divulgación del área de la física en el curso del presente año (2008), y por otro lado la realización del primer evento presencial de &lt;strong&gt;CienciaNet&lt;/strong&gt;. En esta oportunidad el encuentro estará dirigido a estudiantes y docentes de nivel medio. Físicos en actividad dictarán charlas, no sólo sobre la física en sí, sino sobre sus experiencias de vida como investigadores. Conferencias que incluyan lo anecdótico y que excluyan los tecnicismos del área.&lt;/p&gt;
&lt;p&gt;En este contexto, &lt;strong&gt;CienciaNet&lt;/strong&gt; hace una convocatoria a escritores de notas de divulgación en el área de física. Los autores de artículos que se ajusten a los requerimeintos del portal serán retribuídos económicamente a través del proyecto INVOFI. Luego de las últimas 366 vueltas que la Tierra diera sobre sí misma, nuestro portal se complace en recibir este reconocimiento por parte de AFA. Esperamos esforzarnos aún más en las próximas revoluciones.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Publicaciones más científicas: Reflexiones sobre el flujo de conocimiento, dinero e influencias en la ciencia</title>
      <link>https://ciencianet.com.ar/post/publicaciones-mas-cientificas-reflexiones-sobre-el-flujo-de-conocimiento-dinero-e-influencias-en-la-ciencia/</link>
      <pubDate>Wed, 02 Apr 2008 00:07:32 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/publicaciones-mas-cientificas-reflexiones-sobre-el-flujo-de-conocimiento-dinero-e-influencias-en-la-ciencia/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Luis A. Pugnaloni.&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET-UNLP).&lt;/p&gt;
&lt;p&gt;¿Por qué, para qué y cómo publica un científico los resultados de su trabajo? ¿Quienes ganan, quienes pierden? ¿Se puede mejorar el modelo prevaleciente? ¿Se puede reemplazar? ¿Se puede complementar? En este artículo se expone una descripción simplificada del sistema prevaleciente de publicación de trabajos científicos. Luego de discutir las ventajas y desventajas que este sistema presenta, se propone un sistema complementario de publicación. Finalmente se enumeran los beneficios, como así también de los inconvenientes, del nuevo sistema.&lt;/p&gt;
&lt;h3 id=&#34;la-investigación&#34;&gt;La investigación&lt;/h3&gt;
&lt;p&gt;Un científico (o un grupo de científicos) identifica un problema que le resulta interesante resolver. Encuentra un medio para avanzar en esa dirección, quizá empezando por estudiar algún aspecto particular del problema, y luego de realizar ciertas investigaciones reúne resultados que considera constituyen un avance significativo respecto de lo conocido por él y los demás científicos de su especialidad. El científico decide entonces escribir un artículo al respecto y publicarlo (hacerlo público).&lt;/p&gt;
&lt;p&gt;¿Para qué? Para convertirlo en conocimiento científico. Porque la ciencia es una actividad social, la ciencia no es el conocimiento de un ser humano, sino el conocimiento de muchos y el potencial de que todos puedan acceder a él. Para que el nuevo conocimiento de este científico se convierta en conocimiento científico ha de transmitirse al mayor número de individuos de la comunidad, científica en principio y no científica después.&lt;/p&gt;
&lt;h3 id=&#34;la-publicación&#34;&gt;La publicación&lt;/h3&gt;
&lt;p&gt;Existen muchas formas de hacer público el conocimiento (revistas, conferencias, charlas, cartas a colegas, correos electrónicos, páginas web, panfletos, afiches, boca a boca, etc). Todas son usadas en mayor o menor medida. La mayoría de los científicos hoy escribe un artículo y lo envía a una revista científica a fin de cumplir con su designio de hacer público su nuevo aprendizaje. ¿Para qué? Para tantas cosas... (1) Para obtener la mayor diseminación posible de su trabajo. (2) Para recibir criticas de colegas que ayuden a mejorar el trabajo realizado. (3) Para recibir una certificación de calidad. (4) Para recibir reconocimiento por su labor. (5) Para solicitar más tarde apoyo (económico o no) a su trabajo. (6) Para satisfacer los requerimientos de quienes apoyaron (económicamente o no) su trabajo.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Diseminación: Las revistas científicas hoy están muy profesionalizadas y hacen una labor excelente en pos de publicitar los artículos que en ellas se publican. Claro, algunas tienen más lectores que otras y serán entonces preferidas.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Críticas: Las revistas más reconocidas usan el sistema de calidad conocido como de &amp;quot;revisión por pares&amp;quot; (&lt;em&gt;peer review&lt;/em&gt;). Esto es, la revista se ocupa de que algún (algunos) colega(s) lean un determinado artículo y den su opinión, críticas y recomendaciones. Estás críticas y recomendaciones son útiles para el científico autor del artículo y le ayudan a mejorar su trabajo.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Calidad: Si el artículo es finalmente aceptado y publicado por la revista el autor puede exhibir este hecho como prueba de que su trabajo ha superado los estándares de calidad de una revista dada.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reconocimiento: Los colegas de un científico darán especial crédito a su trabajo si este cuenta con un &amp;quot;certificado de calidad&amp;quot;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Solicitud de apoyo: Mostrar un trabajo con &amp;quot;certificación de calidad&amp;quot; muestra lo que un científico es capaz de hacer y será considerado especialmente para recibir apoyo (económico o no) para sus investigaciones.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Requerimientos: Si alguien apoyó la investigación de un científico querrá ver resultados con &amp;quot;certificación de calidad&amp;quot;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;el-proceso-de-publicación&#34;&gt;El proceso de publicación&lt;/h3&gt;
&lt;p&gt;En líneas generales el proceso de publicación sigue los siguientes pasos:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;El autor del artículo envía el manuscrito al editor de una revista.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;El editor evalúa si el contenido es apropiado para la revista.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Si el contenido es apropiado el editor solicita a uno o más colegas la revisión técnica del artículo (referato).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Si los revisores aceptan la tarea responden al editor con su opinión, críticas y comentarios pertinentes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;El editor envía entonces los informes de los referís al autor indicando si acepta el artículo, si lo rechaza o si espera una versión mejorada antes de tomar una decisión. En todo momento la identidad de los revisores permanece reservada de modo que el autor no conoce quien ha revisado su manuscrito.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(a) Si el artículo es aceptado el autor transfiere los derechos de copia a la revista y el editor ordena que la revista publique el artículo. (b) Si el artículo es rechazado el autor queda en libertad de intentar publicarlo en otra revista. (c) Si el editor solicita una versión mejorada el autor puede reenviar el artículo corregido tomando en cuenta los comentarios de los revisores y respondiendo a las críticas recibidas. Una nueva ronda de revisión es iniciada entonces por el editor.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Este proceso, en general, puede repetirse una o dos veces hasta que el editor toma la decisión final de aceptar o rechazar el artículo.&lt;/p&gt;
&lt;h3 id=&#34;ventajas-y-desventajas-del-proceso-de-publicación&#34;&gt;Ventajas y desventajas del proceso de publicación&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Ventajas:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Al ser anónimos los revisores opinan más libremente sin temer represalias por parte de autores que no aceptan fuertes críticas.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Al tratarse de equipos editoriales profesionales la calidad del artículo final se ve beneficiada de cuidadosos procesos de postedición.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dado que muchas revistas pertenecen a empresas comerciales, estas llevan adelante fuertes campañas publicitarias para aumentar la visibilidad de los artículos por ellas publicados.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Muchas revistas están avaladas por sociedades científicas, otras poseen largas trayectorias, otras incluyen entre sus editores a renombrados científicos de su especialidad.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Todo esto impacta en los científicos y en la comunidad general proyectando una clara imagen de solvencia académica que da validez a la certificación implícita en la aceptación de un manuscrito. Empleadores y agencias de promoción ven simplificada su tarea de evaluación de candidatos al usar la certificación de las revistas.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Desventajas:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Al ser anónimos los revisores no publican sus críticas de modo que el resto de la comunidad disponga abiertamente de la opinión de especialistas sobre un dado artículo. Las ricas discusiones entre autores, editores y revisores quedan ocultas y no se convierten en conocimiento científico.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Por la razón anterior. los revisores no reciben mayor crédito por su labor. Nadie puede valorar su aporte a la ciencia como crítico de trabajos de colegas si no puede publicar sus informes. Además, las revistas no retribuyen económicamente a los revisores por su trabajo.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dado que muchas revistas pertenecen a empresas comerciales existe un ánimo de lucro que implica que los lectores o los autores o ambos deban pagar los costos y ganancias esperadas por estas empresas. Todo esto atenta contra la visibilidad de un artículo dado que si el autor (o lector) o no puede afrontar los costos el artículo no será publicado (leído).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;La mayoría de los autores trabajan en instituciones académicas que financian los gastos (sueldos, equipamiento, insumos) necesarios para realizar las investigaciones. Estas instituciones pagan la publicación de sus empleados en revistas científicas y luego deben pagar nuevamente para leer el artículo donde se cuentan los resultados de la investigación que financió. Esta es una situación paradójica.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Debido al prestigio (alto o bajo) de una dada revista, los artículos allí publicados no son valorados inicialmente por su contenido a la hora de evaluar el trabajo de un científico a fin de darle empleo, promocionarlo o subsidiar sus proyectos. Con el paso del tiempo el artículo pasa a ser valorado por el impacto que ha hecho en la comunidad medido por el número de citas bibliográficas recibidas. Si bien este último método de valoración no es bueno (artículos en revistas prestigiosas suelen ser citados a fin de argumentar la importancia del tema que se quiere presentar), al menos es mejor que el primero. Desafortunadamente puede tomar algunos años para que la comunidad comience a citar un dado artículo por lo que el artículo gozará (o sufrirá) del prestigio heredado de la revista que lo publicó durante su juventud.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Debido al rechazo de artículos muchos autores se ven obligados a repetir el proceso de publicación en más de una revista hasta conseguir publicar su trabajo. En algunos casos las críticas no implican que un trabajo no sea valioso sino que el revisor no lo considera así. En otros las críticas son tales que muestran claramente la existencia de errores en el trabajo, pero los autores consiguen publicar su artículo si dan con otra revista cuyos revisores no detectan el fallo. Eventualmente, buenos artículos tardan mucho en publicarse (o no lo hacen) y malos artículos son publicados sin advertencias sobre potenciales errores.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Comentarios:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Derechos de copia: La mayoría de las revistas científicas exigen a los autores que les transfieran el derecho de copia (copyright) del artículo aceptado para su publicación. Esto significa que los editores de la revista tienen derecho a reproducir cuantas veces quieran el artículo en su versión final y obtener ganancias de su venta. En general el autor conserva el derecho de compartir un número reducido de copias de su artículo con colegas (en general no está permitido distribuir copias en forma indiscriminada). Es importante destacar que casi siempre el autor conserva todos los derechos sobre su manuscrito original ya que los derechos de copia se refieren a la versión definitiva del editor de la revista. En unos pocos casos extremos la revista exige un embargo según el cual el manuscrito original no puede ser distribuido por nadie. Los derechos de copia en general limitan la difusión que puede tener un dado artículo ya que queda atada a la difusión que la revista le de.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Costos: Los costos de publicación son en general elevados dado el elaborado proceso de postedición que llevan adelante las revistas profesionalizadas.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Open access: La necesidad de acceso irrestricto al material científico es un concepto que está cobrando cada vez más fuerza en la comunidad. El European Research Council requiere &amp;quot;que todas las publicaciones hechas con revisión por pares resultantes de proyectos de investigación financiados por ese consejo deben hacerce de acceso libre en Internet dentro de los seis meses siguientes de la publicación&amp;quot;. Sin embargo las revistas convencionales deben poner barreras económicas para financiar sus actividades. En ocasiones, en pos del open access, se impone la barrera económica al autor del trabajo (o a su empleador), quien debe afrontar los costos de otorgar acceso gratuito al contenido de su artículo.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;propuesta-para-un-sistema-complementario&#34;&gt;Propuesta para un sistema complementario&lt;/h3&gt;
&lt;p&gt;El siguiente modelo de publicación podría resolver, a mi juicio, muchas de las desventajas del sistema convencional de publicación. Por supuesto, también introducirá nuevas desventajas y perderá algunas de las ventajas existentes. Aún así, considero que este sistema podría enriquecer al sistema científico, aumentar la diversidad de opciones de publicación y eventualmente contribuir a mejorar considerablemente la ciencia en el sentido más amplio. El proceso de publicación alternativo consistiría en los siguientes pasos:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;El autor del artículo envía el manuscrito a un colega de su elección y le solicita que obre como editor a fin de publicar el artículo.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Si el editor acepta la tarea, solicita a uno o más colegas la revisión técnica del artículo (referato).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Si los revisores aceptan la tarea responden al editor con su opinión, críticas y comentarios pertinentes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;El editor envía entonces los informes de los referís al autor. La identidad de los revisores es revelada al autor.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(a) Si el autor encuentra, gracias a los comentarios de los revisores, que su artículo no es apropiado para ser publicado solicita al editor que no publique el artículo. (b) Si el autor considera que los comentarios de los revisores no ameritan modificar su trabajo ni responder a las críticas solicita al editor que publique el artículo.
El editor entonces compila el manuscrito, los informes de los revisores y quizá sus propios comentarios y los publica en forma electrónica en un repositorio de artículos científicos. La identidad de todos los involucrados es publicada junto a sus escritos. El autor conserva el derecho de copia y lo otorga en forma no exclusiva bajo las condiciones de su elección. Los revisores conservan derecho de copia sobre sus informes. (c) Si el autor considera, en virtud de los comentarios de los revisores, que el trabajo debe ser mejorado y/o que debe dar respuesta a las críticas recibidas, prepara el nuevo material y lo envía al editor para iniciar una nueva fase de revisión.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Estas rondas corrección-revisión pueden repetirse hasta que el autor (quizá a sugerencia del editor) decide que el artículo ya está listo para hacerse público o que no es apropiado publicarlo. En caso de publicación el editor compila el artículo junto a los informes de los revisores y respuestas del autor a las críticas recibidas y los publica en forma electrónica en un repositorio de artículos científicos. La identidad de todos los involucrados es publicada junto a sus escritos. El autor conserva el derecho de copia y lo otorga en forma no exclusiva bajo las condiciones de su elección. Los revisores conservan derecho de copia sobre sus informes.&lt;/p&gt;
&lt;p&gt;Es importante destacar que en este sistema la decisión de publicar o no un artículo recae en el autor y no en el editor. El autor puede publicar aún ante opiniones negativas de los revisores. Si el autor considera que sus respuestas a las críticas son apropiadas y los revisores insisten en sus críticas toda la discusión será publicada y el lector podrá tomar posición al respecto. El trabajo del editor en este sistema es crucial ya que debe moderar una discusión que eventualmente será publicada y ayudar al autor a tomar la decisión final en algún punto del proceso de revisiones.&lt;/p&gt;
&lt;h3 id=&#34;ventajas-y-desventajas-del-sistema-propuesto&#34;&gt;Ventajas y desventajas del sistema propuesto&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Ventajas:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Los lectores de estos artículos tendrán disponible la opinión de otros expertos y la discusión entre revisores y autores sobre puntos particularmente controvertidos. Esto da un valor agregado respecto de las publicaciones convencionales.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dado que los informes de los revisores se publican con sus nombres, un revisor puede recibir crédito por su trabajo. Por ejemplo, incluyendo en su CV la lista de artículos para los que actuó como revisor.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Como el artículo se pone a disposición de la comunidad en forma gratuita en la Internet, la distribución potencial es máxima y por ende, la probabilidad de convertirse en conocimiento compartido también lo es.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Las instituciones que pagaron los costos de las investigaciones que dieron lugar a un artículo, tienen libre acceso a este y a la opinión de expertos, lo que les ayuda a valorar el resultado de su inversión.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dado que un artículo no fue aceptado por una revista o editor (la decisión de publicación es del autor), sólo puede ser valorado por su contenido y el contenido de las discusiones entre autores y revisores. No es posible, entonces, asociar el prestigio de una revista a un artículo. Eventualmente, como todo artículo, el impacto que haga en la comunidad (medido por las citas recibidas, por ejemplo) será tomado como una medida de su valor.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Un autor no necesita reenviar su artículo a diferentes editores ya que el editor no puede negarse a publicar el artículo una vez que aceptó la tarea bajo este esquema de publicación.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Desventajas:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Muchos científicos no están preparados para revelar su nombre al criticar a otros científicos. Esto puede entorpecer la tarea de búsqueda de revisores para este tipo de artículos. En mi opinión, en una sociedad científica madura, un revisor debería ser libre de opinar sin miedo a represalias por parte del autor. Un autor que tomara esta actitud cometería discriminación por causas de opinión. Este concepto no está aún arraigado en la ciencia y se prefiere prevenir estos incidentes en lugar de repudiar los actos de discriminación (Art. 19 de la Declaración Universal de los Derechos Humanos: Todo individuo tiene derecho a la libertad de opinión y de expresión; este derecho incluye el de no ser molestado a causa de sus opiniones, el de investigar y recibir informaciones y opiniones, y el de difundirlas, sin limitación de fronteras, por cualquier medio de expresión).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dado que los artículos son editados por miembros de la comunidad científica sin el soporte técnico de instituciones o empresas, es posible que la calidad de las compilaciones sea inferior al sistema tradicional. Esto sin embargo, sólo aplica al formato y no al contenido.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;En este esquema un editor no está en condiciones de realizar campañas publicitarias para los artículos que él edita. Esto puede disminuir el impacto de los artículos. Sin embargo, los motores de búsqueda en la Internet hacen simple que cualquier artículo sea encontrado y leído. Más aún, los repositorios de artículos suelen usar listas de correos electrónicos para alertar sobre nuevos artículos.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dado que no se puede asociar el prestigio de una dada revista a un artículo es imposible usar rankings estándares para comparar el trabajo de candidatos en un concurso (para un puesto, un ascenso, un subsidio, etc.) a menos que los artículos hayan tenido tiempo de recibir citas para medir su impacto. En virtud de que usualmente sólo el trabajo de los últimos años es tenido en cuenta para evaluaciones de candidatos, estos artículos no gozarán de el método de certificación de calidad brindado por las revistas científicas. Un evaluador debería leer los informes de los revisores y las respuestas del autor para juzgar un dado artículo.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;comentarios-finales&#34;&gt;Comentarios finales&lt;/h3&gt;
&lt;p&gt;En la comunidad científica los problemas del sistema instaurado de publicaciones son bien conocidos. No he enumerado muchos de los problemas de retroalimentación según los cuales quien publica en revistas reconocidas gana posiciones y dinero que le dan ventaja para seguir publicando y ganando mejores posiciones y más dinero. Quienes no publican en estas revistas, muchas veces porque su trabajo no es bueno, pero muchas otras porque no es considerado interesante o valioso por los revisores (lo que es materia de opinión), quedan rezagados en sus carreras profesionales.&lt;/p&gt;
&lt;p&gt;El sistema de publicación propuesto puede dar una alternativa a dos tipos de trabajos: aquellos que resultan controversiales, y aquellos que desarrollan ideas aún incipientes que encuentran resistencia entre los colegas del autor. También beneficiaría a los revisores por cuanto reciben crédito por su trabajo. Por otro lado, sería de gran utilidad a los lectores que dispondrían de acceso libre y gratuito no sólo al artículo sino también a las opiniones de especialistas.&lt;/p&gt;
&lt;p&gt;Finalmente, el sistema ayudaría a potenciales evaluadores que de este modo dispondrían de la opinión de los referís y de la defensa del autor sobre su trabajo. Nótese que un científico debe sentirse bastante seguro de sus resultados y opiniones para aceptar publicar su trabajo junto a las críticas de los referís. Difícilmente se publiquen trabajos de baja calidad ya que un autor preferirá retirar su artículo antes que quedar en evidencia.&lt;/p&gt;
&lt;p&gt;Este sistema, o alguna variante del mismo, podría presentarse como complementario al sistema tradicional de publicaciones con un fuerte potencial para convertirse en el sistema de preferencia de muchos científicos. En este artículo se asume que todas las personas referidas (generalmente bajo una etiqueta que describe su rol en el sistema científico) realizan su función correctamente. Todos las ideas expuestas pueden adaptarse, y valdría la pena hacerlo alguna vez, al caso en que los sujetos realizan acciones negligentes o maliciosas. Por otro lado, en pos de simplificar y abreviar, se han hecho generalizaciones, no por el desconocimiento de la variedad de situaciones existente, sino en la convicción de que hay una moda, en el sentido estadístico, que domina la distribución.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>¿Las Acuaporinas favorecen un aumento del Calcio intracelular?</title>
      <link>https://ciencianet.com.ar/post/las-acuaporinas-favorecen-un-aumento-del-calcio-intracelular/</link>
      <pubDate>Sat, 29 Mar 2008 00:20:13 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/las-acuaporinas-favorecen-un-aumento-del-calcio-intracelular/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Osvaldo Chara&lt;/strong&gt;: Instituto de Física de Líquidos y Sistemas Biológicos (CONICET-UNLP-CIC)&lt;/p&gt;
&lt;p&gt;Luciano Galizia y colaboradores de la Facultad de Medicina (UBA) demostraron recientemente que las Acuaporinas (proteínas ubicadas en las membranas de las células y que actúan como canales que permiten pasar agua) serían importantes en el aumento de la concentración de Calcio intracelular provocado bajo ciertos procesos osmóticos.&lt;/p&gt;
&lt;p&gt;Cuando las células son bañadas con una solución de osmolaridad menor que su interior celular, estas responden incrementando su volumen, por ósmosis. A continuación, se activan proteínas en la membrana celular que producen salida de solutos, los cuales producen, nuevamente por ósmosis, salida de agua. Esto reduce el volumen de la célula. Dicha respuesta se denomina Regulación de Volumen Decreciente (RVD). El pasaje de agua en este proceso tiene lugar o bien a través de la bicapa fosfolipídica, o alternativamente mediante &lt;a href=&#34;http://es.wikipedia.org/wiki/Acuaporina&#34;&gt;Acuaporinas&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/03/celulas-768x278.jpg&#34; alt=&#34;Figura: células cargadas con un compuesto fluorescente antes (izquierda) y después (derecha) de ser bañadas con medio hipotónico.&#34;&gt;&lt;/p&gt;
&lt;p&gt;Ya se conocían, previamente, los siguientes hechos: 1) una de estas Acuaporinas, la AQP2, era importante en el RVD de células renales, 2) el ingreso de agua por ósmosis provocaba un aumento en la concentración intracelular de Calcio y 3) algunos mecanismos relacionados con el RVD dependían de la concentración de Calcio dentro de la célula. En un intento de conectar estos tres hechos, Galizia y colaboradores, del &lt;a href=&#34;http://www.fmed.uba.ar/depto/fisiologia/biomem.htm&#34;&gt;Laboratorio de Biomembranas&lt;/a&gt; de la Facultad de Medicina (Universidad de Buenos Aires) formularon la siguiente pregunta: ¿Cumple la AQP2 algún papel en el aumento de la concentración intracelular de Calcio durante el ingreso de agua por ósmosis? Los resultados de sus estudios fueron publicados recientemente en &lt;em&gt;American Journal of Physiology&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Trabajando con células con y sin AQP2 cargadas con compuestos fluorescentes los autores pudieron medir el volumen celular y la concentración de Calcio intracelular durante un proceso de ósmosis como el descrito arriba. Interesantemente, los autores observaron que sólo en presencia de AQP2, las células muestran un incremento en la concentración intracelular de Calcio durante la ósmosis. Los autores concluyeron que la AQP2 en la membrana de las células sería crítica para incrementar la concentración de Calcio, lo cual es necesario para activar el RVD.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referencia:&lt;/strong&gt; Galizia L, Flamenco MP, Rivarola V, Capurro C, Ford P. Role of AQP2 in activation of calcium entry by hypotonicity: implications in cell volume regulation. &lt;a href=&#34;http://www.ncbi.nlm.nih.gov/pubmed/18094031?ordinalpos=1&amp;amp;itool=EntrezSystem2.PEntrez.Pubmed.Pubmed_ResultsPanel.Pubmed_RVDocSum&#34;&gt;Am J Physiol Renal Physiol. 2008. 294:F582-F590&lt;/a&gt;.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Conversión de tierras y la deuda de carbono de biocombustibles</title>
      <link>https://ciencianet.com.ar/post/conversion-de-tierras-y-la-deuda-de-carbono-de-biocombustibles/</link>
      <pubDate>Wed, 19 Mar 2008 00:37:43 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/conversion-de-tierras-y-la-deuda-de-carbono-de-biocombustibles/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro:&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET-UNLP)&lt;/p&gt;
&lt;p&gt;El incremento en la demanda energética, el cambio climático y las emisiones de dióxido de carbono (CO2) hacen que la transición al uso de combustibles de bajo contenido de carbono sea prioritario. Los biocombustibles son una fuente potencial de energía con bajo contenido de carbono, pero el ahorro de emisiones depende de cómo son producidos. La conversión de bosques, turberas, sabanas y praderas para producir biocombustibles en Brasil, el sudoeste de Asia y Estados Unidos crea una &amp;quot;deuda de carbono&amp;quot; emitiendo de 17 a 420 veces más CO2 que las reducciones anuales que esos biocombustibles proveen al desplazar combustibles fósiles.&lt;/p&gt;
&lt;p&gt;En un estudio publicado el 7 de febrero de 2008 por &lt;a href=&#34;https://www.sciencemag.org/&#34;&gt;Science&lt;/a&gt;, los investigadores Joseph Fargione, de la organización &lt;a href=&#34;https://www.nature.org/es-us/&#34;&gt;The Nature Conservancy&lt;/a&gt; y David Tilman, Stephen Polasky y Peter Hawthorne, de la Universidad de Minnesota, analizaron las emisiones de carbono en la conversión de hábitat naturales para la producción de biocombustibles a partir de cultivos alimentarios.&lt;/p&gt;
&lt;p&gt;El suelo y biomasa de plantas son los dos reservorios más grandes de carbono terrestre, conteniendo entre ambos aproximadamente 2,7 veces la cantidad de carbono que posee la atmósfera. La conversión de hábitat nativos en cultivos libera CO2 debido a la quema o descomposición microbiana de carbono orgánico contenido en la biomasa o suelo. Luego de la liberación rápida mediante el fuego utilizado en la limpieza de terrenos, o por la descomposición de hojas y raíces finas, existe un período prolongado de liberación de gases de efecto invernadero (GEI) por la descomposición o quema de raíces y ramas gruesas y madera.&lt;/p&gt;
&lt;p&gt;Los autores del trabajo denominan &amp;quot;deuda de carbono&amp;quot; a la cantidad de CO2 liberada durante los primeros 50 años de este proceso de conversión de tierras. En el tiempo, los biocombustibles producidos a partir de tierras convertidas pueden devolver esta deuda si su producción y combustión tiene emisiones de GEI menores que las emisiones del ciclo de vida de los combustibles fósiles que reemplazan. Hasta que la deuda de carbono sea devuelta, los biocombustibles producidas en tierras convertidas tienen un impacto en GEI mayores que el de los combustibles fósiles que desplazan.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/03/Cerrado25.jpg&#34; alt=&#34;Cerrado brasileño: Segunda mayor formación vegetal brasileña, originalmente de aproximadamente 2 millones de kilómetros cuadrados. Fuente: www.unb.br.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;En el estudio, los científicos calculan cuan grande es la deuda de carbono, y cuántos años son necesarios para devolver esta deuda, para seis casos diferentes de conversión de hábitat nativos: el Amazonas brasileño para biodiesel de soja, el Cerrado brasileño para biodiesel de soja y etanol de caña de azúcar, selvas y turberas de Indonesia o Malasia para biodiesel de palma y praderas del centro de Estados Unidos para etanol de maíz.&lt;/p&gt;
&lt;p&gt;Estos casos ilustran algunos de los mayores impactos de biocombustibles en conversión de hábitat. Indonesia y Malasia concentran el 86% de la producción de aceite de palma. La demanda acelerada de aceite de palma contribuye al 1,5% de la tasa anual de deforestación de selvas tropicales en estas naciones. El Cerrado brasileño está siendo convertido para la producción de caña de azúcar y soja, así como el Amazonas brasileño (soja).&lt;/p&gt;
&lt;p&gt;Los precios crecientes de maíz, trigo y soja podrían causar que una porción sustancial de las 1,5 x 10⁷ ha de tierras actualmente protegidas en la pradera central de Estados Unidos sea convertida en tierra para cultivo. La cantidad de años necesarios para recuperar la deuda de carbono en la producción de biodiesel de palma fue estimada en 86 años para el caso de la conversión de selvas tropicales en Indonesia/Malasia, y de 423 años para el producido por la conversión de turberas en esos mismos países. 319 años es el período correspondiente para el biodiesel de soja producido en tierras convertidas a partir de selvas tropicales brasileñas, y 93 años el de etanol de maíz en tierras convertidas a partir de las praderas centrales de Estados Unidos.&lt;/p&gt;
&lt;p&gt;Menor es el impacto del etanol de caña de azúcar de la conversión del Cerrado brasileño (17 años), biodiesel de soja del Cerrado de Brasil (37) y del etanol de maíz generados en zonas de tierras de cultivo abandonadas (48 años). Factores adicionales pueden influir en el impacto de los biocombustibles en la emisión de GEI. Primero, la producción de biocombustibles puede desplazar cultivos o pasturas de las tierras utilizadas actualmente, causando indirectamente la emisión de GEI a través de la conversión de hábitat nativos en tierras utilizables en otra parte. Segundo, mejoramientos en la tecnología de producción de biocombustibles puede reducir los tiempos para devolver la deuda de carbono. Tercero, si la tierra convertida para la producción de biocombustibles acumula carbono (suponiendo tierras en estado estacionario), la deuda de carbono se incrementaría en el futuro por la pérdida de este almacenamiento. En cuarto lugar, una mayor producción de biocombustibles podría disminuir los costos energéticos, lo que aumentaría el consumo y la consiguiente emisión de GEI.&lt;/p&gt;
&lt;p&gt;Los investigadores concluyen que los biocombustibles producidos en tierras convertidas, podrían, por largos períodos de tiempo, ser mucho mayores emisores de GEI que los combustibles fósiles que reemplazan. Al menos en la situación actual de las tecnologías de producción de biocombustibles, cualquier estrategia para reducir emisiones de GEI que cause la conversión de ecosistemas nativos a tierras de cultivo podría ser contraproducente.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fuente:&lt;/strong&gt; &lt;a href=&#34;https://science.sciencemag.org/content/319/5867/1235&#34;&gt;J. Fargione, J. Hill, D. Tilman, S. Polasky y P. Hawthorne. &amp;quot;Land Clearing and the Biofuel Carbon Debt&amp;quot;. Science (2008)&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Nuevos virus para tratar enfermedades bacteriales</title>
      <link>https://ciencianet.com.ar/post/nuevos-virus-para-tratar-enfermedades-bacteriales/</link>
      <pubDate>Wed, 19 Mar 2008 00:34:05 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/nuevos-virus-para-tratar-enfermedades-bacteriales/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro.&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET-UNLP) y Universidad Tecnológica Nacional.&lt;/p&gt;
&lt;p&gt;Virus encontrados en el río Cam, en Cambridge, podrían convertirse en la próxima generación de antibióticos, de acuerdo con lo anunciado en la Universidad de Edinburgo, en el Reino Unido. Con la actual sobre-prescripción de antibióticos para el tratamiento de infecciones bacterianas, y pacientes que no completan propiamente sus tratamientos, muchas bacterias son capaces de conformar genes para resistir a los antibióticos simplemente intercambiando material genético entre si. El MRSA (&lt;em&gt;Staphylococcus aureus&lt;/em&gt; resistente a Meticilina), la cepa resistente a múltiples drogas del Staphylococcus Aureus, y las nuevas cepas emergentes de &lt;em&gt;Clostridium dificile&lt;/em&gt;, han forzado a los investigadores a buscar un abordaje completamente diferente para combatir estas bacterias.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/03/Bacteriofago.jpg&#34; alt=&#34;Esquema de bacteriófago. Fuente: Wikipedia.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Utilizando un virus que solo ataca bacterias, llamado &lt;a href=&#34;http://es.wikipedia.org/wiki/Bacteri%C3%B3fago&#34;&gt;bacteriófago&lt;/a&gt; - y algunos bacteriófagos solo atacan tipos específicos de bacterias - podemos tratar infecciones apuntando a la cepa exacta de la bacteria causante de la enfermedad&amp;quot;, dice Ana Toribio, del &lt;em&gt;Wellcome Trust Sanger Institute&lt;/em&gt; en Hinxton, Cambridgeshire, Reino Unido. &amp;quot;Esto es mucho más específico que la terapia convencional con antibióticos&amp;quot;&lt;/p&gt;
&lt;p&gt;Los científicos utilizaron un pariente cercano de &lt;em&gt;Escherichia coli&lt;/em&gt;, la bacteria que comúnmente causa intoxicación e infecciones gastrointestinales en humanos, llamado &lt;em&gt;Citrobacter rodentium&lt;/em&gt;, que tiene exactamente el mismo efecto gastrointestinal en ratones. Ellos fueron capaces de tratar los ratones infectados con un cóctel de bacteriófagos obtenidos en el río Cam que ataca al &lt;em&gt;Citrobacter rodentium&lt;/em&gt;. Actualmente los investigadores están optimizando la selección de los virus mediante análisis de ADN para utilizar los bacteriófagos con diferentes perfiles.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/03/Trialphage.jpg&#34; alt=&#34;Ataque de bacteriófagos: visualización 3D (fuente: Wikipedia).:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;El uso de bacteriófagos en vez del tradicional tratamiento con antibióticos de amplio espectro, que esencialmente tratan de matar todas las bacterias que se les cruzan, es mucho mejor porque no alteran el balance microbial en el cuerpo&amp;quot;, dice el Dr. Derek Pickard del &lt;em&gt;Wellcome Trust Sanger Institute&lt;/em&gt;. &amp;quot;Todos necesitamos de buenas bacterias para ayudarnos a combatir infecciones, para digerir nuestros alimentos y proveernos de nutrientes esenciales, y los antibióticos convencionales pueden matar también esas bacterias, mientras luchan contra las que provocan la enfermedad&amp;quot;.&lt;/p&gt;
&lt;p&gt;Hasta hace poco, los tratamientos basados en bacteriófagos han sido ignorados en Europa Occidental y Estados Unidos. Los principales reportes clínicos en humanos han sido producidos en Europa Oriental, particularmente en el Instituto Bacteriofago Tbilisi en Georgia, donde los bacteriófagos han sido utilizado con éxito en el tratamiento de infecciones tales como úlceras y llagas en diabéticos. &amp;quot;Cuanto más podamos desarrollar el tratamiento y comprender los obstáculos encontrados en el uso de este método para tratar infecciones intestinales, mayor es la probabilidad de maximizar su posibilidad de éxito en el largo plazo&amp;quot;, dice Ana Toribio. &amp;quot;Hemos encontrado que el uso de una variedad de bacteriófagos (en vez de solo uno) para tratar una enfermedad tiene muchos beneficios en el ataque de una cepa peligrosa de bacteria, sobrepasando cualquier resistencia potencial al bacteriófago mediante mutaciones de la bacteria.&amp;quot;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fuente:&lt;/strong&gt; &lt;a href=&#34;http://www.physorg.com/news108006098.html&#34;&gt;Physorg&lt;/a&gt;.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Bolas de acero hacen salpicaduras en la arena</title>
      <link>https://ciencianet.com.ar/post/bolas-de-acero-hacen-salpicaduras-en-la-arena/</link>
      <pubDate>Wed, 19 Mar 2008 00:30:24 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/bolas-de-acero-hacen-salpicaduras-en-la-arena/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro:&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET-UNLP)&lt;/p&gt;
&lt;p&gt;El aire entre los granos de arena hacen que la misma actúe como un fluido, de acuerdo con físicos holandeses que arrojaron bolas sobre arena para simular impacto de meteoritos. Realizando experimentos en una cámara al vacío, los investigadores descubrieron que las bolas penetraron más profundo en la arena y arrojaban más material a mayores presiones ambientes.&lt;/p&gt;
&lt;p&gt;Estos resultados arrojan nueva luz sobre la relación entre sistemas granulares y fluidos. Dos años atrás, Detlef Lohse y un equipo de físicos de la Universidad de Twente, idearon un mecanismo para la formación de cráteres arrojando bolas metálicas en camas de arena. Encontraron que el impacto arroja arena hacia afuera, en una salpicadura con forma de corona, dejando que la bola penetre más profundamente en la superficie, creando un hoyo. Entonces, la presión de la arena fuerza a los granos a llenar el hoyo, causando un chorro vigoroso de arena que se dispara hacia el aire desde el centro.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/03/pres-sand-235x300.jpg&#34; alt=&#34;Chorro de arena: Fotografías de alta velocidad que muestran el efecto de la presión en la altura del chorro de arena. Fuente: PhysicsWeb.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Los investigadores concluyeron que este mecanismo es muy similar a lo que sucede en un líquido. Esto significa que, bajo ciertas circunstancias, las conocidas ecuaciones de la dinámica de fluidos podrían ser utilizadas para describir los poco comprendidos sistemas granulares como la arena.&lt;/p&gt;
&lt;p&gt;Ahora, Lohse y colaboradores han avanzado un paso más y encontraron que la presión del aire sobre la arena está relacionada con la altura del chorro y la profundidad a la que penetra la bola. En un nuevo conjunto de experimentos, arrojaron bolas de acero de 1,4 cm en una cama de arena de 40 cm de profundidad. Pero esta vez pusieron todo el aparato en un contenedor en el que podían hacer vacío con una bomba de aire y midieron la profundidad a la que penetraron las bolas. A menor presión de aire, encontraron que el chorro es menos vigoroso, y que las bolas no penetran tan profundo.&lt;/p&gt;
&lt;p&gt;Los investigadores afirman que esto se debe a que una menor presión significa que hay menos aire alrededor de la bola y de los granos de arena, incrementando el arrastre y haciendo que la arena se comporte menos como un fluido. El resultado es una menor penetración y un chorro más corto. &amp;quot;Esto revela la importancia del aire en la materia granular fina&amp;quot;, afirma Lohse. Gabriel Caballero, co-autor del trabajo, dijo que el estudio tendría eventuales aplicaciones que van desde el diseño de sondas espaciales que deben aterrizar en otros planetas, al mezclado de polvos químicos en la industria farmacéutica. El equipo se encuentra ahora investigando cómo la salpicadura de arena crea el cráter originado.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fuente:&lt;/strong&gt; &lt;a href=&#34;http://physicsweb.org/articles/news/11/7/13/1?rss=2.0&#34;&gt;PhysicsWeb&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referencia:&lt;/strong&gt; &lt;a href=&#34;https://doi.org/10.1103/PhysRevLett.99.018001&#34;&gt;Physical Review Letters, &lt;strong&gt;99&lt;/strong&gt;, 018001 (2007)&lt;/a&gt;.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Nanotubos contrabandean moléculas anti-VIH dentro de células</title>
      <link>https://ciencianet.com.ar/post/nanotubos-contrabandean-moleculas-anti-vih-dentro-de-celulas/</link>
      <pubDate>Wed, 19 Mar 2008 00:26:39 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/nanotubos-contrabandean-moleculas-anti-vih-dentro-de-celulas/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro:&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET-UNLP) y Universidad Tecnológica Nacional.&lt;/p&gt;
&lt;p&gt;Nanotubos de carbono han sido utilizados para contrabandear moléculas que bloquean el VIH dentro de células humanas. Aunque es un resultado preliminar, el descubrimiento podría conducir a nuevos tratamientos contra el virus. Una característica de la ribointerferencia, las moléculas de &lt;a href=&#34;http://es.wikipedia.org/wiki/ARN_interferente&#34;&gt;ARN interferente pequeño&lt;/a&gt; (ARNip), han sido sugeridas como tratamiento contra el VIH en el pasado. Su capacidad de destrucción del ARN mensajero permite utilizarlas para bloquear la producción de células superficiales receptoras de proteínas de las que depende el VIH para invadir células del sistema inmunológico conocidas como &lt;a href=&#34;http://es.wikipedia.org/wiki/Linfocito_T&#34;&gt;linfocitos T&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2017/12/nanot-vih.jpg&#34; alt=&#34;Nanotubos portadores: Proteinas receptoras en linfocitos T se muestran en rojo (arriba). Los nanotubos equipados con ARNip apagan la expresión de los receptores (abajo).:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;El problema es cómo introducir el ARNip en las células en primer lugar. La utilización de una burbuja de grasa llamada liposoma funciona bien con células de cáncer pero no parece resultar con linfocitos T. Una alternativa es el uso de virus para introducir ADN que codifica la producción de ARNip, pero esta técnica preocupa a algunos científicos porque es difícil controlar con precisión dónde se incorpora el ADN en el genoma, y la ubicación errónea puede causar cáncer.&lt;/p&gt;
&lt;p&gt;Ahora, la química Hongjie Dai y colegas de la Universidad de Stanford, en California, han mostrado que el ARNip puede adjuntarse a nanotubos de carbono. Estos nanotubos penetran las membranas de los linfocitos T y depositan el ARNip dentro de las células sin alterar el ADN. Ellos sugieren que algún día una solución de nanotubos equipados con ARNip podrían combatir el VIH luego de ser inyectados en el flujo sanguíneo del paciente.&lt;/p&gt;
&lt;p&gt;El equipo adjuntó dos tipos de ARNip: uno diseñado para bloquear una proteína receptora llamada CD4 y otra para bloquear un receptor llamado CXCR4. Ambas son utilizados por una variedad del VIH para atacar células. Cuando los nanotubos equipados con ARNip se aplicaron a linfocitos T humanos, el equipo encontró que el 60% de la expresión de CD4 y el 80% de la expresión CXCR4 fueron bloqueadas.&lt;/p&gt;
&lt;p&gt;Zhuang Lui, miembro del equipo, dijo que los nanotubos pueden incluso equiparse con ARNip diseñado para bloquear un tercer receptor llamado CCR5, que es utilizado por otra variedad de VIH. Sin embargo, debe resolverse aún cómo hacer que los nanotubos sean específicos para los linfocitos T y no otros tipos de células. Lui cree que esto puede hacerse sumando un anticuerpo que se adhiera a los receptores superficiales de los linfocitos T.&lt;/p&gt;
&lt;p&gt;Aún no está claro que este abordaje se constituya en un tratamiento efectivo. Un problema es que el ARNip no bloquea completamente la expresión del receptor. El equipo está trabajando en desarrollar una forma más efectiva de bloquear las proteínas ajustando la secuencia de ARNip y alterando la química de los nanotubos, de forma que invadan las células eficientemente. Aún si esto funciona, no se sabe si los nanotubos podrán desplegarse por el flujo sanguíneo y alcanzar al sistema linfático u otros tejidos, donde se localice el VIH. Otra desventaja es que quien reciba el tratamiento necesitaría ser inyectado con los nanotubos equipados con ARNip cada pocos días, lo cual es prohibitivo con los costos actuales.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fuente:&lt;/strong&gt; New Scientist Tech.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referencia:&lt;/strong&gt; Hongjie Dai, siRNA Delivery into Human T Cells and Primary Cells with Carbon-Nanotube Transporters, &lt;a href=&#34;https://doi.org/10.1002/ange.200604295&#34;&gt;Angewandte Chemie International Edition 2007, 46, No. 12, doi: 10.1002/anie.200604295&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Hormigas argentinas forman supercolonias familiares</title>
      <link>https://ciencianet.com.ar/post/hormigas-argentinas-forman-supercolonias-familiares/</link>
      <pubDate>Wed, 19 Mar 2008 00:22:49 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/hormigas-argentinas-forman-supercolonias-familiares/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro.&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET-UNLP) y Universidad Tecnológica Nacional.&lt;/p&gt;
&lt;p&gt;Un estudio realizado por biólogos de la Universidad de California en San Diego (UCSD) muestra que las invasivas hormigas argentinas parecen utilizar diferencias genéticas para distinguir amigas de enemigas. Este hallazgo ayuda a explicar por qué estas hormigas construyen enormes colonias en California. En el número de diciembre de Molecular Ecology, los biólogos brindaron los primeros datos sobre interacciones territoriales entre hormigas argentinas en el campo.&lt;/p&gt;
&lt;p&gt;En California, las hormigas argentinas forman &amp;quot;supercolonias&amp;quot; expansivas que contienen millones de nidos y se extienden por cientos de kilómetros. Los científicos han mostrado desacuerdo en las razones de la pérdida de agresividad entre hormigas de diferentes nidos de la misma colonia. &amp;quot;Algunos ecólogos sostienen la hipótesis que los factores ambientales reducen la agresión entre hormigas argentinas en California&amp;quot;, dice David Holway, profesor asistente de biología en UCSD y autor principal del estudio. &amp;quot;Sin embargo, encontramos que mientras las hormigas de la misma supercolonia no pelean entre si, choques entre hormigas de diferentes supercolonias ocurren frecuentemente a lo largo de los límites territoriales&amp;quot;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/03/hormigasArg.jpg&#34; alt=&#34;Hormigas argentinas invasoras: Mapa mostrando la ubicación de la mayor supercolonia (puntos amarillos), la supercolonia de Lake Hodges (puntos rojos) y sitios de observación (triángulos verdes), al norte de San Diego.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;La distancia entre los nidos no juega ningún papel en el comportamiento territorial de las hormigas. Tampoco hay factores ambientales evidentes que expliquen por qué las hormigas atacan a la misma especie de un nido vecino pero no de otro. Sin embargo, los investigadores encontraron una relación muy estrecha entre comportamiento y genética. Las hormigas que son genéticamente similares mantienen relaciones pacíficas, mientras que las que son genéticamente diferentes se atacan unas a otras.&lt;/p&gt;
&lt;p&gt;Existen cinco supercolonias en el sur de California. La mayor de ellas se extiende aproximadamente por 960 kilómetros sobre la costa de California y linda con tres o cuatro colonias más pequeñas. Sobre los límites territoriales las hormigas de colonias diferentes participan en batallas intensas que resultan en la muerte de un número considerable de trabajadoras. Melissa Thomas (estudiante posdoctoral que trabajó con Holway), estimó que alrededor de una de las coloñas más pequeñas, en Lake Hodges al norte de San Diego, fueron muertas al menos 15 millones de trabajadoras durante los seis meses que duró el estudio.&lt;/p&gt;
&lt;p&gt;Sin embargo, las hormigas no lucharon cuando fueron ubicadas cerca de hormigas provenientes de lugares distantes pero de la misma supercolonia. Christine Payne-Makrsâ y Andrew Suarez, de la Universidad de Illinois y Neil Tsutsui, de U.C. Irvine, encontraron que a lo largo del gran rango geográfico de una supercolonia las hormigas fueron muy similares genéticamente, pero son distintas genéticamente de hormigas de las supercolonias vecinas. Los investigadores sostienen que manteniendo la paz con sus familiares, las hormigas son capaces de dedicar más recursos a alimentarse que a competir.&lt;/p&gt;
&lt;p&gt;En Argentina, las interacciones agresivas entre colonias son mucho más comunes, y las colonias son significantemente más pequeñas. Cuando las hormigas argentinas fueron introducidas en California 100 años atrás, se expandieron ampliamente debido a que no encontraron otras colonias de hormigas argentinas. Los biólogos piensan que las diferentes supercolonias del sur de California provienen de corrientes distintas de hormigas, posiblemente en la tierra de las plantas utilizadas para el desarrollo paisajístico.&lt;/p&gt;
&lt;p&gt;Los científicos esperan que una mejor comprensión de cómo las hormigas distinguen amigos de enemigos, y de los mecanismos que previenen el flujo genético entre colonias, podría conducir a medios efectivos para el control de las hormigas.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fuente:&lt;/strong&gt; &lt;a href=&#34;http://www.physorg.com/news84187768.html&#34;&gt;PhysOrg&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Más información&lt;/strong&gt; &lt;a href=&#34;http://es.wikipedia.org/wiki/Hormiga_Argentina&#34;&gt;http://es.wikipedia.org/wiki/Hormiga_Argentina&lt;/a&gt;.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>El grafeno continua sorprendiendo</title>
      <link>https://ciencianet.com.ar/post/el-grafeno-continua-sorprendiendo/</link>
      <pubDate>Tue, 18 Mar 2008 00:43:03 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/el-grafeno-continua-sorprendiendo/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro:&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET-UNLP) y Universidad Tecnológica Nacional.&lt;/p&gt;
&lt;p&gt;¿Hay algo que el &lt;a href=&#34;http://en.wikipedia.org/wiki/Graphene&#34;&gt;grafeno&lt;/a&gt; -láminas de carbono de solo un átomo de espesor- no pueda hacer? Desde que este material fuera descubierto en 2004, ha mostrado ser un conductor eléctrico extremadamente bueno; un semiconductor que puede ser utilizado para crear transistores; un material muy resistente que puede utilizarse para hacer membranas ultra delgadas. Ahora, investigadores en Estados Unidos han confirmado que el grafeno puede ser también un muy buen conductor del calor.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/03/Graphene_xyz-300x254.jpg&#34; alt=&#34;Grafeno: lámina de carbono de un átomo de espesor. Fuente: Wikipedia.&#34;&gt;&lt;/p&gt;
&lt;p&gt;El equipo, que tuvo que inventar una nueva manera de medir la conductividad térmica para poder estudiar el material, está ahora investigando cómo pueden utilizarse las propiedades térmicas del grafeno para enfriar chips de silicio ultra rápidos. Los físicos sospechaban que el grafeno puede conducir muy bien el calor debido a que nanotubos de carbono, conformados por grafeno enrollado en tubos muy delgados, son muy buenos conductores térmicos. Sin embargo, el grafeno puede ser muy difícil de trabajar, y los investigadores han pugnado por determinar sus propiedades térmicas utilizando las técnicas tradicionales que involucran adherirle calentadores y otros dispositivos al material.&lt;/p&gt;
&lt;h3 id=&#34;dispersión-de-raman&#34;&gt;Dispersión de Raman&lt;/h3&gt;
&lt;p&gt;Alexander Balandin y colegas de la Universidad de California-Riverside han diseñado una nueva técnica de medición que usa un láser para calentar el grafeno y medir su temperatura. El equipo suspendió láminas de grafeno sobre zanjas de micrómetros de ancho excavadas en una superficie de óxido de silicio. Las láminas tenían varios micrometros de longitud y fueron fijadas en ambos extremos por capas de grafito, que actuaron como sumideros de calor.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/03/esquemaGrafeno-300x235.jpg&#34; alt=&#34;Esquema del experimento: Diagrama de la técnica utilizada para medir la conductividad térmica. Fuente: nanotechweb.org.&#34;&gt;&lt;/p&gt;
&lt;p&gt;El centro de la lámina es entonces expuesta al haz de luz láser, que calienta el grafeno y cambia las frecuencias a las cuales vibran los átomos de carbono. Parte de la luz láser cambia su frecuencia debido a que sufre una dispersión Raman, y la magnitud del cambio en la frecuencia es proporcional a la temperatura de la región iluminada.&lt;/p&gt;
&lt;h3 id=&#34;cambio-en-la-frecuencia&#34;&gt;Cambio en la frecuencia&lt;/h3&gt;
&lt;p&gt;Midiendo el cambio en la frecuencia -y por lo tanto la temperatura del grafeno- como función de la potencia del láser, los investigadores pudieron calcular la conductividad térmica del grafeno, cuyo valor resultó ser de 5300 W/(m K) a temperatura ambiente. Este es el valor más alto conocido para un sólido: 50% más alto que el de los nanotubos de carbono y más de 10 veces mayor que el de los metales como cobre y aluminio. Balandin dijo a &lt;a href=&#34;https://physicsworld.com/a/graphene-continues-to-amaze/&#34;&gt;physicsworld.com&lt;/a&gt; que el equipo se sorprendió de encontrar que el grafeno es mucho mejor conductor del calor que los nanotubos de carbono, aún cuando algún trabajo teórico había sugerido que esto era posible.&lt;/p&gt;
&lt;p&gt;La gran conductividad térmica del grafeno es probablemente resultado de la relativa facilidad que tienen los átomos de carbono para moverse en el grafeno, comparada con otros materiales. Balandin y sus colegas están ahora trabajando en una teoría que explique por qué esto es así. Balandin cree que la alta conductividad térmica del grafeno, su forma plana y su capacidad para integrarse con el silicio podrían jugar un papel importante en la disipación de calor de dispositivos electrónicos. El equipo está trabajando actualmente en el diseño de transistores ultra rápidos enfriados por grafeno.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fuente:&lt;/strong&gt; &lt;a href=&#34;https://physicsworld.com/a/graphene-continues-to-amaze/&#34;&gt;physicsworld&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referencia:&lt;/strong&gt; Alexander A. Balandin, Suchismita Ghosh, Wenzhong Bao, Irene Calizo, Desalegne Teweldebrhan, Feng Miao, and Chun Ning Lau. &amp;quot;Superior Thermal Conductivity of Single-Layer Graphene&amp;quot;. &lt;a href=&#34;https://doi.org/10.1021/nl0731872&#34;&gt;ASAP Nano Lett., ASAP Article, 10.1021/nl0731872&lt;/a&gt; , Febrero 20, 2008.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Identificación automática de objetos en imágenes con ruido</title>
      <link>https://ciencianet.com.ar/post/identificacion-automatica-de-objetos-en-imagenes-con-ruido/</link>
      <pubDate>Wed, 12 Mar 2008 00:46:51 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/identificacion-automatica-de-objetos-en-imagenes-con-ruido/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Luis Pugnaloni&lt;/strong&gt;. Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP).&lt;/p&gt;
&lt;p&gt;Hacer que una computadora detecte objetos en una imagen es un problema central. Pensemos en la necesidad de diferenciar y medir los espacios cultivados, forestados y urbanos a partir de imágenes aéreas o satelitales de todo el país. Investigadores de la Universidad de Buenos Aires presentaron un estudio donde comparan varios métodos de detección de bordes de objetos en imágenes con un tipo de ruido llamado &amp;quot;speckle&amp;quot; que aparece, por ejemplo, al usar luz láser.&lt;/p&gt;
&lt;p&gt;El trabajo demuestra que un método basado en el conocimiento del tipo de ruido que afecta a la imagen permite detectar rápida y correctamente las áreas de diferentes partes de la imagen. El trabajo de Juliana Gambini, Marta E. Mejail y Julio Jacobo-Berlles de la Facultad de Ciencias Exactas y Naturales de la Universidad de Buenos Aires en colaboración con Alejandro C. Frery (de la Universidade Federal de Alagoas, Brasil) presenta 5 métodos de detección de bordes en imágenes con ruido &amp;quot;speckle&amp;quot;. Algunos de estos métodos fueron desarrollados por ellos.&lt;/p&gt;
&lt;p&gt;El ruido &amp;quot;speckle&amp;quot; aparece en imágenes iluminadas con fuentes de iluminación coherentes. Tal es el caso del láser, el radar de apertura sintética (SAR), el sonar y el ultrasonido (usado para las ecografías). Las imágenes se ven moteadas a causa de la interferencia de reflejos provenientes de diferentes partes del objeto iluminado y de otros objetos cercanos. Detectar los bordes de un contorno que separa dos regiones de una imagen con tal tipo de ruido es particularmente complejo.&lt;/p&gt;
&lt;p&gt;Gambini y colaboradores usaron un modelo estadístico sobre la distribución aleatoria particular que presenta el ruido &amp;quot;speckle&amp;quot; para así distinguir entre el ruido proveniente de áreas de la imagen con diferente textura. Un algoritmo que les permite detectar los puntos donde el cambio de textura es abrupto les posibilitó identificar en forma eficiente y certera los bordes de cada región de una imagen.&lt;/p&gt;
&lt;p&gt;Puesto a prueba con otros métodos de detección éste resultó ser el más eficaz. Los autores del trabajo muestran las capacidades de este método de detección usando imágenes aéreas tomadas con un radar de apertura sintética. El trabajo podría significar una importante mejora en la evaluación de áreas cultivadas, crecimiento urbano, desastres naturales, etc. que son examinados a través de imágenes que necesariamente llevan ruido &amp;quot;speckle&amp;quot;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; &amp;quot;Accuracy of edge detection methods with local information in speckled imagery&amp;quot;, &lt;a href=&#34;https://doi.org/10.1007/s11222-007-9034-y&#34;&gt;Statistics and Computing&lt;/a&gt; , vol. 18, pp 15 (2008).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instituciones:&lt;/strong&gt; &lt;a href=&#34;http://exactas.uba.ar/&#34;&gt;Facultad de Ciencias Exactas y Naturales&lt;/a&gt; (UBA), &lt;a href=&#34;http://www.ic.ufal.br/&#34;&gt;Instituto de Computación&lt;/a&gt; (Universidade Federal de Alagoas, Brasil)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; Juliana Gambini (E-mail: &lt;a href=&#34;mailto:jgambini@dc.uba.ar&#34;&gt;jgambini@dc.uba.ar&lt;/a&gt; )&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Más información:&lt;/strong&gt; &lt;a href=&#34;http://es.wikipedia.org/wiki/Speckle&#34;&gt;Speckle&lt;/a&gt;, &lt;a href=&#34;http://es.wikipedia.org/wiki/Radar_de_apertura_sint%C3%A9tica&#34;&gt;SAR&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Hielo solar. Una heladera que funciona con el calor del Sol</title>
      <link>https://ciencianet.com.ar/post/hielo-solar-una-heladera-que-funciona-con-el-calor-del-sol/</link>
      <pubDate>Tue, 12 Feb 2008 00:53:29 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/hielo-solar-una-heladera-que-funciona-con-el-calor-del-sol/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero.&lt;/strong&gt; Facultad de Ciencias Exactas (UNLP).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/02/heladera3-198x300.jpg&#34; alt=&#34;:left&#34;&gt;
En tiempos en que la crisis energética es un tema de conversación habitual y la preocupación por el medio ambiente nos desvela, algunos investigadores argentinos dedican tiempo y esfuerzo al estudio de un mejor aprovechamiento de las llamadas energías sustentables. Por otra parte, amplios sectores de la sociedad, por ejemplo los asentamientos de emergencia del conurbano bonaerense, pobladores rurales en la Patagonia o comunidades seminómades como los kollas Tinkunaku de Salta, no tienen acceso a recursos básicos como el gas natural o la electricidad.&lt;/p&gt;
&lt;p&gt;Desde hace varios años y en diversos lugares del mundo se vienen desarrollando prototipos de cocinas solares, y ya existen diversos diseños sencillos y de bajo costo. En este &lt;a href=&#34;http://bioingenieria.edu.ar/grupos/puertociencia/documentos/actualizado/cocinas%20solares.PDF&#34;&gt;link&lt;/a&gt; se puede encontrar un interesante compendio elaborado en la Universidad Nacional de Entre Ríos.&lt;/p&gt;
&lt;p&gt;Pero mucho más innovador resulta el proyecto que se viene desarrollando desde 2003 en la Universidad Nacional de General Sarmiento, en Los Polvorines, para la fabricación de una heladera que funciona con el Sol. Para más sorpresa, esta heladera solar enfría de noche.&lt;/p&gt;
&lt;p&gt;El proyecto es dirigido por el Dr. en Física Rodolfo Echarri, y su equipo está formado por el Lic. Andrés Sartarelli y el Prof. Sergio Vera. Además cuenta con la colaboración del INTEC -Instituto Tecnológico de Santo Domingo, República Dominicana- donde la Ingeniera Inna Samson dirige un equipo de las mismas características.&lt;/p&gt;
&lt;p&gt;Los resultados del proyecto ya están a la vista. El dispositivo creado en la UNGS que funciona mediante la adsorción y desorción de metanol por carbón activado, ya ha llegado a producir 300 gramos de hielo. Suficiente para enfriar una cerveza. El prototipo de heladera está formado por un colector, un condensador, un evaporador y una cámara fría. No tiene enchufe ni motor, es más, no tiene partes móviles.&lt;/p&gt;
&lt;p&gt;El colector es un recipiente que contiene carbón activado (compuesto de carbono, muy poroso y adsorbente), que al principio del ciclo se encuentra “empapado” con metanol (alcohol metílico o alcohol de madera). El condensador convierte en líquido los vapores del metanol. El evaporador recoge este líquido y permite que se evapore nuevamente. En la cámara fría es donde el agua se transforma en hielo. Hilando más finamente, dentro del colector, las moléculas de metanol se encuentran pegadas a la superficie del carbón activado, en un estado que químicamente se define como “adsorbidas”. Para despegar las moléculas de la superficie –es decir, desorberlas- necesitamos entregarle energía al sistema.&lt;/p&gt;
&lt;p&gt;También necesitamos energía para convertir un líquido en vapor. Tanto el proceso de desorción como el de evaporación requieren que el sistema disponga de energía; por ejemplo, aumentando su temperatura. ¿Cómo es el funcionamiento de este dispositivo? Inicialmente, el colector con el carbón activado y el metanol está a temperatura ambiente. Cuando es expuesto a la radiación solar, la temperatura del mismo va aumentando, de modo que el alcohol se va desorbiendo, y pasa al condensador. Allí se acumula en estado gaseoso hasta que alcanza una presión a la cual ocurre la condensación, y vuelve entonces al estado líquido. Se acumula en el evaporador. A medida que el día transcurre y la radiación va disminuyendo, la temperatura desciende.&lt;/p&gt;
&lt;p&gt;Durante la noche el proceso se invierte. El metanol líquido que se encuentra en el evaporador se empieza a evaporar; para este proceso toma energía de la cámara fría haciendo que la temperatura en ella descienda. Esto último fenómeno podemos experimentarlo mojándonos la piel con alcohol: para evaporarse toma energía del cuerpo, haciendo descender la temperatura en la superficie.&lt;/p&gt;
&lt;h3 id=&#34;entrevista-a-rodolfo-echarri-investigador-del-conicet-y-profesor-de-la-ungs&#34;&gt;Entrevista a Rodolfo Echarri, Investigador del Conicet y profesor de la UNGS.&lt;/h3&gt;
&lt;p&gt;-¿Cómo surgió el proyecto de crear una heladera solar? - &lt;strong&gt;R.E.&lt;/strong&gt;: Dadas las características de nuestra Universidad, que intenta una fuerte articulación entre lo académico y el aporte directo al mejoramiento del nivel de vida de la comunidad, decidimos dar un paso en este sentido. El tema de la heladera cumple con ese rol al mismo tiempo que desde el punto de vista académico presenta un desafío mucho mayor que el de un calentador solar, ya que implica el desarrollo de una bomba de calor.&lt;/p&gt;
&lt;p&gt;-¿Cómo está compuesto el equipo de trabajo? - &lt;strong&gt;R.E.&lt;/strong&gt;: el equipo está formado por dos “sub-equipos”. Uno en la UNGS, que cuenta por el momento con tres personas (y otra más a incorporarse en marzo) que tienen una característica muy pronunciada en común: la preocupación por hacer de la física una herramienta de acercamiento a la comunidad. Tanto Andrés como yo, hemos sido formados en la Facultad de Ciencias Exactas y Naturales de la UBA, mientras que Sergio y Ernesto (el próximo integrante) se formaron en esta Universidad.&lt;/p&gt;
&lt;p&gt;La otra parte del equipo, que desarrolla el trabajo en República Dominicana, está formado por la Ingeniera Inna Samson que posee una maestría en Física del Calor y una serie de alumnos de ingeniería que van pasando para desarrollar partes puntuales del proyecto. En total ya han pasado once, gozando de una pequeña beca cada uno.&lt;/p&gt;
&lt;p&gt;-¿Cuáles fueron las dificultades que encontraron en el desarrollo del prototipo? - &lt;strong&gt;R.E.&lt;/strong&gt;: Hubo dificultades propias de cualquier proyecto de investigación (errores y aprendizajes de los mismos), pero también hubo dificultades no tan comunes, como por ejemplo, la falta de un lugar físico donde realizar el armado del prototipo (el primero fue armado en la terraza de mi casa), o la falta de dinero. Sin embargo, dada la buena voluntad de la Universidad, en poco tiempo más contaremos con un lugar más adecuado.&lt;/p&gt;
&lt;p&gt;-¿Cuáles serían las vías para que la heladera solar llegue efectivamente a los potenciales usuarios? - &lt;strong&gt;R.E.&lt;/strong&gt;: Nuestro deseo es lograr un modelo apto para su uso y que logremos obtener financiamiento estatal o de otras organizaciones para producir heladeras a ser colocadas en lugares de alta necesidad. Dicho de otra forma, creemos que el estado tiene que jugar un papel fundamental en la distribución de los bienes.&lt;/p&gt;
&lt;p&gt;-En tu opinión, ¿por qué no hay más proyectos con compromiso social como éste en Argentina? - &lt;strong&gt;R.E.&lt;/strong&gt;: En realidad, creo que la ciencia argentina (y en particular la física) está pensada en un contexto que no mira la realidad de nuestro país. Muchos científicos piensan que la verdadera ciencia es la totalmente aséptica, sin una relación directa con el contexto social, y no tienen en cuenta que el dinero de las investigaciones lo aporta toda la comunidad. Por eso, cuando se trata de un desarrollo tecnológico, a los científicos les da la impresión de estar haciendo “ciencia de segunda”&lt;/p&gt;
&lt;p&gt;-El sistema científico evalúa la producción de los investigadores principalmente contando el número de publicaciones, ¿Qué reconocimientos tiene un proyecto de este tipo? - &lt;strong&gt;R.E.&lt;/strong&gt;: El reconocimiento es el mismo que en otros casos, se cuenta el número de publicaciones, pero no se tienen en cuenta las dificultades adicionales de este tipo de proyectos, donde lo más importante no es publicar sino producir un equipo lo más eficiente posible.  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Más información&lt;/strong&gt; Explicación detallada del funcionamiento del dispositivo: &lt;a href=&#34;http://intec.edu.do/biblioteca/cienciaysociedad/2004/Vol%FAmen%2029-%20N%FAmero%201/Una%20alternativa%20para%20producci%F3n%20de%20fr%EDo%20con%20energ%EDa%20solar%20+%20Inna%20Samson;%20Rodolfo%20Echarri.PDF&#34;&gt;Descargar »&lt;/a&gt; &lt;a href=&#34;http://www.ungs.edu.ar/&#34;&gt;Universidad Nacional de General Sarmiento&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Detección de estilos de aprendizaje utilizando redes bayesianas</title>
      <link>https://ciencianet.com.ar/post/deteccion-de-estilos-de-aprendizaje-utilizando-redes-bayesianas/</link>
      <pubDate>Sun, 03 Feb 2008 00:57:09 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/deteccion-de-estilos-de-aprendizaje-utilizando-redes-bayesianas/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro&lt;/strong&gt;. Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP) y Universidad Tecnológica Nacional.&lt;/p&gt;
&lt;p&gt;Los estudiantes se caracterizan por poseer diversos estilos de aprendizaje, dependiendo de cómo seleccionan la información recibida y su posterior procesamiento en diversas formas. En el diseño de sistemas educativos ví­a web, es deseable que todos los alumnos puedan aprender independientemente de sus diferentes estilos de aprendizaje.&lt;/p&gt;
&lt;p&gt;En un trabajo publicado recientemente en &lt;em&gt;Computers &amp;amp; Education&lt;/em&gt;, los investigadores Patricio Garcí­a, Analía Amandi, Silvia Schiaffino y Marcelo Campo, del Instituto de Sistema Tandil (ISISTAN) pertenenciente a la Universidad Nacional del Centro de la Provincia de Buenos Aires (UNCPBA), modelaron una red bayesiana para detectar los diferentes estilos de aprendizaje de alumnos de un curso vía web de Inteligencia Artificial.&lt;/p&gt;
&lt;p&gt;Un modelo de estilo de aprendizaje clasifica a los estudiantes de acuerdo a cómo se ubican en un número de escalas pertenecientes a las formas en que reciben y procesan la información. Existen diversos modelos propuestos. En este trabajo los autores utilizaron uno propuesto por Felder y Silverman para estudiantes de ingeniería que comprende las siguientes dimensiones: percepción (sensitiva/intuitiva); entrada (visual/verbal); organización (inductiva/deductiva); procesamiento (activo/reflexivo); comprensión (secuencial/global).&lt;/p&gt;
&lt;p&gt;La detección de estos estilos de aprendizaje es importante en el diseño de un curso vía web, ya que el aprendizaje mejora cuando hay cierta concordancia entre el estilo de aprendizaje del alumno y el estilo utilizado para enseñanza, que comprende: el contenido del curso (textos, ejemplos, exámenes y ejercicios propuestos); la forma en que estos contenidos se presentan al alumno; los mecanismos de interacción entre los alumnos y docentes (salas de chat, foros, preguntas frecuentes, listas de e-mail) y el orden en que los contenidos son organizados y presentados en el curso.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/02/redBayesiana-1024x406.png&#34; alt=&#34;Red bayesiana: Modelización del estilo de aprendizaje de un alumno.&#34;&gt;&lt;/p&gt;
&lt;p&gt;Una de las caracterí­sticas más deseables de un sistema educativo ví­a web es que sea adaptativo y personalizado, ya que debe ser utilizado por una amplia variedad de estudiantes con diferentes habilidades y estilos de aprendizaje. Para lograr este objetivo, y reducir la brecha entre los estilos de aprendizaje de los alumnos y el estilo de enseñanza del sistema, es necesario descubrir el estilo de cada alumno y adaptar el curso o asistir al alumno de acuerdo con su estilo.&lt;/p&gt;
&lt;p&gt;En este trabajo, esta caracterización se realiza a través de una red bayesiana, que permite modelar cualitativa y cuantitativamente la información procedente del comportamiento de los alumnos, obtenidos de archivos que registran sus participaciones en actividades tales como salas de chat y foros.&lt;/p&gt;
&lt;p&gt;Los investigadores compararon los resultados obtenidos con la red bayesiana con resultados obtenidos con el test utilizado tradicionalmente para detectar los estilos de aprendizaje, observando que la red bayesiana es capaz de detectar el estilo de percepción con alta precisión, mientras que las dimensiones de comprensión y procesamiento muestran algunas discrepancias. Ellos atribuyen estas discrepancias a la poca experiencia de los alumnos en el uso de la web como herramienta para el abordaje de un curso, y confí­an en que mejoramientos del modelo utilizado en la red bayesiana pueden permitir descubrir los estilos de aprendizajes de alumnos con gran precisión. Nuevos resultados utilizando un modelo mejorado y nuevos experimentos serán publicados próximamente en &lt;em&gt;Journal of Computer Assited Learning&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Consultado por &lt;strong&gt;CienciaNet,&lt;/strong&gt; Patricio García comentó que &amp;quot;este trabajo nos permitió desarrollar un Agente Tutor. Este agente inteligente asiste a los alumnos en los cursos web, basando sus decisiones en este modelo bayesiano. Actualmente este nuevo proyecto se encuentra en la etapa de experimentación que esperamos concluir en la primera parte del año para luego poder publicar los resultados del trabajo.&amp;quot;&lt;/p&gt;
&lt;p&gt;Referencias:&lt;/p&gt;
&lt;p&gt;Patricio Garcí­a, Analí­a Amandi, Silvia Schiaffino, Marcelo Campo, &amp;quot;Evaluating Bayesian networks&#39; precision for detecting students&#39; learning styles&amp;quot;, Computers &amp;amp; Education 49, 794-808 (2007).&lt;/p&gt;
&lt;p&gt;P. García, S. Schiaffino, A. Amandi. &amp;quot;An enhanced Bayesian model to detect students&#39; learning styles in Web-based courses&amp;quot;. Journal of Computer Assisted Learning, Blackwell Publishing - In press.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instituciones:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.exa.unicen.edu.ar/investigacion/isistand.htm&#34;&gt;Instituto de Sistema Tandil (ISISTAN)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.exa.unicen.edu.ar/&#34;&gt;Universidad Nacional del Centro de la Provincia de Buenos Aires (UNCPBA) - Facultad de Ciencias Exactas.&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>La política del teletrabajo</title>
      <link>https://ciencianet.com.ar/post/la-politica-del-teletrabajo/</link>
      <pubDate>Fri, 21 Dec 2007 01:02:54 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/la-politica-del-teletrabajo/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Lenguita&lt;/strong&gt; Centro de Estudios e Investigaciones Laborales - Programa de Investigaciones Económicas sobre Tecnología, Trabajo y Empleo (CONICET).&lt;/p&gt;
&lt;p&gt;Las preocupaciones académicas sobre la dinámica de sujeción laboral en relación a los métodos de producción capitalista es un fenómeno histórico que ha abonado renovadamente los estudios del trabajo. Al interior de ese campo analítico, la Tesis Doctoral titulada “La Política del Teletrabajo. Un estudio comparativo sobre las ideologías y prácticas de control laboral destinadas a los teletrabajadores a domicilio en Argentina” realiza un aporte a la temática del control laboral, centrando la atención en el caso del teletrabajo.&lt;/p&gt;
&lt;p&gt;El teletrabajo es permeable a nuevos mecanismos de organización productiva, y, por ende, el contexto de emergencia de innovaciones disciplinares sobre los trabajadores. Adoptando una definición preliminar, este nuevo modelo organizacional puede comprenderse por la gestión deslocalizada de la fuerza de trabajo, y la posibilidad derivada de desvinculación geográfica y temporal del puesto de trabajo respecto al mando y al colectivo de trabajo.&lt;/p&gt;
&lt;p&gt;Paradójicamente, la fragmentación del colectivo de trabajo no es un obstáculo para la cooperación, ya que en su auxilio intervienen la cada vez más versátiles tecnologías de la comunicación y la información. Ahora bien, cómo es posible controlar a los trabajadores que se encuentran distanciados entre sí y, particularmente, de los puestos de supervisión del trabajo.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2017/12/lenguita.jpg&#34; alt=&#34;Sandra Handley: Secretaria virtual.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Las conclusiones de la tesis han permitido considerar la vigencia de dispositivos disciplinares de control directo, por la vía de los recursos electrónicos de seguimiento y monitoreo de actividades y tiempos de empleo de la computadora. Pero a ello se le anexan ciertas modificaciones en los parámetros tradicionales de control laboral, a las que denominamos formas de control indirecto sobre los teletrabajadores: 1) ligada al sistema de trabajo: la implementación de un mecanismo de aislamiento profesional ligado a la remotización del puesto de trabajo, b) ligada al régimen salarial, la proliferación de un mecanismo de inestabilidad salarial ligado a la adopción de un régimen de subcontratación (si bien características de otros tipos de prácticas laborales, sin dudas, elementos explicativo de la lógica de valorización del trabajo a domicilio).&lt;/p&gt;
&lt;p&gt;En síntesis, las formas de control de los teletrabajadores a domicilio varían en un abanico de prácticas desde formas tradicionales de supervisión electrónica hasta formas más sofisticas de uso y disposición “aislada” de los trabajadores hasta mecanismos, lamentablemente generalizados, de desvalorización del trabajo por la vía de la subcontratación y pérdida de garantías protectorias del trabajador.&lt;/p&gt;
&lt;p&gt;Los elementos negativos para el trabajador no entran en contradicción con una lógica potencialmente “democratizadora” de la estructura reticular de producción. Si bien estas estrategias de organización son flexibles en su organigrama, esta estructuración no alcanza a quebrar la lógica productiva de todo contexto económico de actividad. Es decir, el teletrabajo más allá de los vínculos horizontales de los intercambios electrónicos está inmerso en un contexto de mandos y jerarquías en la toma de decisiones propio de toda esfera productiva del trabajo.&lt;/p&gt;
&lt;p&gt;Se ha avanzado en una lógica menos burocrática de producción, en comparación con modelos antecedentes como los fordistas y tayloristas, sin con ello haber resuelto el problema de control laboral inherente a toda práctica mercantilizada de trabajo. En este caso, las estructuras flexibles de cooperación productiva están al servicio de una disposición de la fuerza de trabajo que la vuelve vulnerable a las arbitrariedades de la contratación y de la demanda empresarial.&lt;/p&gt;
&lt;p&gt;Por lo visto, las relaciones de poder que se recrean en el campo del teletrabajo, caracterizan una tendencia a fragmentar a los teletrabajadores, sin incidencia negativa sobre la dinámica productiva de un esquema reticular de trabajo. Con el agregado que, el aislamiento profesional, así dispuesto, es un escenario que promueve la “invisibilidad” del vínculo contractual, por ende, propicio para ejercer viejos flagelos sobre el trabajo, como es la remuneración a destajo (hoy imbuido de un tinte positivo que lo denomina “trabajo por objetivos”).&lt;/p&gt;
&lt;p&gt;Situación que nos lleva a evaluar una condición consecuente de combinar un sistema atomizado de trabajo y un régimen de subcontratación a destajo: la pérdida del carácter tradicional de disposición del uso/valorización de la fuerza de trabajo, delimitada tradicionalmente por la jornada de trabajo. Sin la consagrada delimitación del “trabajo abstacto”, es decir, la forma mercantilizada en que se transforma el trabajo humano, se observan diversas expresiones en la disponibilidad extensiva y /o intensiva del teletrabajador, con una invasión sobre la práctica cotidiana que alcanza a anular incluso la vida privada.&lt;/p&gt;
&lt;p&gt;Con la pérdida de las garantías institucionalizadas por la regulación laboral, la extensión y/o intensificación del uso/valorización del teletrabajo afecta profundamente la vida extra-laboral de los teletrabajadores. Aún más, la irregularidad descripta se proyecta velozmente, tanto que se ejecuta en regímenes salariales legalmente inexistentes, y, por ende, estrategias empresariales que logran maniatar la capacidad colectiva de resistencia laboral.&lt;/p&gt;
&lt;p&gt;Evidentemente, el teletrabajo es un caso paradigmático de los “nuevos escenarios laborales” en donde se advierten mecanismos perjudiciales para los trabajadores desde el punto de vista de su socialización e interacción, desde el punto de vista de las protecciones y garantías mínimas regidas por la legislación y, sumando ambas, desde el punto de vista de las incapacidad colectivas de sortear el abuso patronal. Tales alegatos parecen volver reaccionario un mundo del trabajo que hoy se altera por la presencia de cada vez más modernos instrumentos para trabajar.&lt;/p&gt;
&lt;p&gt;Por lo visto, las tecnologías informáticas y las redes comunicaciones son el refugio privilegiado para ensayar esquemas de optimización y maximización del rendimiento laboral, gestionando la ansiada ecuación empresaria: volúmenes altos de riqueza con bajo volumen de empleo. Así conjugada, la relación uso/valor de la fuerza de trabajo se ha vuelto el territorio selectivo de los ideólogos de la Sociedad de la Información, y por ello, será, sin dudas, el ámbito de indagación futura del control del teletrabajo; con el objeto de evaluar así cómo el teletrabajo interviene en la relación de poder laboral y modifica un modelo tradicional de gestión y regulación del trabajo.  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; Este estudio forma parte de la tesis defendida por la autora de esta nota el 29 de junio del 2006 en la Facultad de Ciencias Sociales (UBA), para optar al grado de Doctor en Ciencias Sociales. Título: &amp;quot;La Política del Teletrabajo. Un estudio comparativo sobre las ideologías y prácticas de control laboral destinadas a los teletabajadores a domicilio en Argentina&amp;quot;, Directora: Susana Finquelievich, Copias de la tesis pueden obtenerse contactando a la autora: Paula Lenguita (Email: &lt;a href=&#34;mailto:paulalenguita@ciudad.com.ar&#34;&gt;paulalenguita@ciudad.com.ar&lt;/a&gt; )&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Ahorro energético domiciliario: Un balance entre diseño arquitectónico y hábitos de vida</title>
      <link>https://ciencianet.com.ar/post/ahorro-energetico-domiciliario-un-balance-entre-diseno-arquitectonico-y-habitos-de-vida/</link>
      <pubDate>Tue, 18 Dec 2007 01:08:32 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/ahorro-energetico-domiciliario-un-balance-entre-diseno-arquitectonico-y-habitos-de-vida/</guid>
      <description>
        
          &lt;p&gt;Un trabajo de investigarores de CONICET y de la Universidad Nacional de La Pampa analiza el ahorro energético en un edificio construido especialmene para estudiantes de bajos ingresos. El edificio, ubicado en la localidad de General Pico, está diseñado para hacer un uso eficiente de la energía del sol en forma pasiva.&lt;/p&gt;
&lt;p&gt;El estudio muestra que el cuidado de los habitantes en el uso de recursos elementales como apertura y cerrado de ventanas es crucial para la eficiencia. En meses de invierno se consiguieron ahorros del 50% en gastos de calefacción. La Universidad Nacional de La Pampa construyó hace ocho años tres bloques con cuatro departamentos de dos habitaciones cada uno diseñados para hacer un uso eficiente de la energía usando sistemas pasivos como exposición al sol, ventilación natural, y protección solar.&lt;/p&gt;
&lt;p&gt;Los departamentos están habitados por estudiantes de bajos recursos y un sistema de monitoreo permite a los investigadores analizar la temperatura y humedad en algunos departamentos y en el exterior de los edificios. Recientemente C. Filippín y A. Beascochea han publicado un trabajo mostrando los resultados después de 7 años de monitoreo. A pesar de la importante amplitud térmica de la zona (16 ºC) la variación de temperatura en el interior de los departamentos no excedió los 6 ºC.&lt;/p&gt;
&lt;p&gt;Más interesante aún, esto se consiguió con un ahorro de energía del 50% en la calefacción de los departamentos durante los meses de invierno. Los departamentos están diseñados con amplias ventanas de vidrio doble orientadas hacia el norte en las habitaciones y el comedor. Esto permite calefaccionar gracias a la acción del sol en invierno. Las paredes exteriores se construyeron de forma que brinden una buena aislación térmica. Aleros y cortinas opacas permiten mitigar los efectos del sol durante el verano. Árboles en las inmediaciones proveen sombra en verano y dejan pasar los rayos del sol durante el invierno al perder el follaje. La ventilación es cruzada y se realiza simplemente abriendo ventanas.&lt;/p&gt;
&lt;p&gt;Los investigadores advierten que este tipo de diseños dependen mucho del uso adecuado que le dan los habitantes de la propiedad. Ventanas abiertas en horas de calor en verano, por ejemplo, calientan rápidamente las paredes que fueron enfriadas durantes las horas de la noche, lo que va en detrimento del confort u obliga al uso de refrigeración activa. En verano es importante evitar la exposición solar usando cortinas oscuras durante el día y aumentar la ventilación en horas de la noche. En invierno es crucial aumentar la exposición al sol y restringir la ventilación a horarios de mayor temperatura exterior.&lt;/p&gt;
&lt;p&gt;El diseño y los hábitos de vida pueden ayudar a reducir drásticamente el consumo energético domiciliario. Dada la crisis energética mundial estos avances permiten asegurar una buena calidad de vida de los habitantes a un menor costo y por un mayor tiempo.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo Original:&lt;/strong&gt; Energy-efficient housing for low-income students in a highly variable environment of central Argentina, &lt;a href=&#34;http://www.elsevier.com/wps/find/journaldescription.cws_home/969/description#description&#34;&gt;Renewable energy&lt;/a&gt; (2007) vol. 32, pp. 1-20&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instituciones:&lt;/strong&gt; &lt;a href=&#34;http://www.conicet.gov.ar/&#34;&gt;CONICET&lt;/a&gt;, &lt;a href=&#34;http://www.unlpam.edu.ar/&#34;&gt;Universidad Nacional de La Pampa&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; Celina Filipin (Email: &lt;a href=&#34;mailto:c?lippin@cpenet.com.ar&#34;&gt;cflippin@cpenet.com.ar&lt;/a&gt; )&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Más información:&lt;/strong&gt; &lt;a href=&#34;http://es.wikipedia.org/wiki/Ahorro_de_energ%C3%ADa&#34;&gt;Ahorro de energía&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Museo Interactivo de Ciencia, Tecnología y Sociedad  &#34;Imaginario&#34;</title>
      <link>https://ciencianet.com.ar/post/museo-interactivo-de-ciencia-tecnologia-y-sociedad-imaginario/</link>
      <pubDate>Thu, 29 Nov 2007 01:09:47 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/museo-interactivo-de-ciencia-tecnologia-y-sociedad-imaginario/</guid>
      <description>
        
          &lt;p&gt;En una antigua casona de San Miguel donde en algún momento funcionara el Hogar de Niños de una congregación de religiosas de Chile se encuentra hoy el Museo &lt;em&gt;Imaginario&lt;/em&gt;. En la sala donde antes hubiera una capilla hoy funciona una noria gigante donde los chicos caminan para encender luces y motores eléctricos. Quien lo visite puede encontrar desde un modelo del funcionamiento del corazón hasta un remolino gigante, pasando por un mapa mural que se ilumina señalando los puntos más poblados del conurbano bonaerense.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Imaginario&lt;/em&gt; es un museo que depende institucionalmente de la Universidad Nacional de General Sarmiento (UNGS). Está ubicado en la zona noroeste de la provincia de Buenos Aires, en el partido de San Miguel, y su influencia abarca los partidos de Malvinas Argentinas (donde se encuentra la UNGS), José C. Paz y Moreno.&lt;/p&gt;
&lt;p&gt;La iniciativa de generar un nuevo centro de popularización de ciencia la tuvo la doctora en física y profesora Lilia Romanelli, luego de visitar el museo interactivo de Recoleta. Una de las motivaciones para la creación de Imaginario fue ofrecer una propuesta educativa a una de las regiones más pobres del conurbano. El Museo se inauguró en Octubre del año 2003.&lt;/p&gt;
&lt;p&gt;Se trata de un Museo interactivo, es decir, se propone la interacción entre el visitante y los fenómenos como herramienta central. A partir de la experimentación busca generar en el visitante cuestionamientos, inquietudes y preguntas que alimenten el interés por la ciencia y el conocimiento en general. La propuesta contempla el análisis de fenómenos de la vida cotidiana utilizando como herramienta los saberes científicos. La inclusión de las Ciencias Sociales amplia el juego: se discuten además temas como la discriminación o la historia reciente de la Argentina.&lt;/p&gt;
&lt;p&gt;Como sucede en los museos universitarios, el personal está formado por estudiantes, docentes y graduados de la casa, tanto de Ciencias Sociales como Exactas y Naturales. En la UNGS se dictan diversos profesorados de ciencias, y existen además varios proyectos de investigación en enseñanza, para los cuales el Museo ofrece un interesante ámbito de aplicación.&lt;/p&gt;
&lt;p&gt;El Museo está organizado en siete salas de Ciencias Naturales y dos de Ciencias Sociales, donde se ubican módulos interactivos. Las salas son temáticas, y están dedicadas a Mecánica, Oscilaciones y Ondas, Electricidad, Óptica y Percepción, Fluidos, Astronomía y Sociedad e Historia, aunque la propuesta para su recorrido es integradora y multidisciplinar.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Entrevista a Gladys Carina Antúnez, Coordinadora Técnica-Operativa de &lt;em&gt;Imaginario&lt;/em&gt; desde sus inicios.&lt;/strong&gt; - ¿Cómo es el público que visita el Museo?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Gladys Antúnez:&lt;/strong&gt; En su mayoría son estudiantes, de EGB, ESB, Polimodal, Inicial, Terciario y Universidad. En menor medida el público en general (familias, grupos de amigos, etc.)&lt;/p&gt;
&lt;p&gt;- &lt;em&gt;Imaginario&lt;/em&gt; se plantea que los visitantes se relacionen con el conocimiento de modos diferentes a los tradicionales. ¿Cuáles son?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Gladys Antúnez:&lt;/strong&gt; Básicamente mediante la interactividad, tanto con los módulos como con los guías-animadores científicos. Tratamos que el visitante se sienta protagonista de la visita, se divierta a través del juego y de paso aprenda sobre algunos temas vinculados con la ciencia y su trabajo. El visitante es el que decide sobre qué discutiremos a partir de sus preguntas e inquietudes.&lt;/p&gt;
&lt;p&gt;- &lt;em&gt;Imaginario&lt;/em&gt; se propone establecer fuertes lazos con la comunidad (vecinos, estudiantes, empresarios, ONGs y otras organizaciones) ¿Qué actividades desarrolla en este sentido?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Gladys Antúnez:&lt;/strong&gt; Además de las visitas guiadas que realizamos a cotidiano, organizamos eventos especiales como la Feria del Libro Infantil y Juvenil de la UNGS (coordinado por la Unidad de Biblioteca y Documentación de la UNGS), en cada Aniversario realizados exposiciones especiales, la muestra de talleres a final del año, la Semana de la Astronomía, etc. Una actividad muy importante son las Mateadas Científicas que realizamos una vez al mes invitando a un Investigador-Docente de la UNGS y otro de otra universidad, y tomando mate con el público en general, conversamos sobre algún tema de investigación que se vincule directamente con la comunidad.&lt;/p&gt;
&lt;p&gt;- ¿Cómo es la respuesta de la comunidad frente a estas propuestas?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Gladys Antúnez:&lt;/strong&gt; Generalmente es satisfactoria. De todas maneras estas son actividades que vamos sosteniendo en el tiempo porque la gente en general no está acostumbrada a participar de muchas actividades culturales (entre ellas las científicas), por lo que la participación no es masiva pero es satisfactoria para nosotros porque se va instalando en la comunidad.&lt;/p&gt;
&lt;p&gt;- Puede decirse que cada museo –como cada casa- es un mundo, y como tal no hay dos con las mismas características. ¿Cuál es la particularidad que hace diferente a &lt;em&gt;Imaginario&lt;/em&gt;?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Gladys Antúnez:&lt;/strong&gt; No sé si cada museo es un mundo, nosotros solemos visitar varios museos de nuestro país para realizar intercambio de experiencias y nos hemos encontrado con dificultades y soluciones similares, creo que es mucho lo que podemos aprender unos de otros. Sin embargo, comparto que a cada uno de los museos los caracteriza algo. En nuestro caso creo que además de lo multidisciplinar (son pocas las propuestas de museos interactivos con temáticas de Ciencias Sociales) lo que nos caracteriza es el contacto con la gente, el vínculo entre el visitante y el guía. Por lo menos esos es lo que dejan plasmado en los comentarios del libro de visitas. Pienso que es muy importante, al fin y al cabo mucho de lo que aprendemos lo hacemos con otros, uno de nuestros objetivos es comunicar y si podemos hacerlo con afecto, mucho mejor.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Información de contacto:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;E-mail:&lt;/strong&gt; &lt;a href=&#34;mailto:imaginario@ungs.edu.ar&#34;&gt;imaginario@ungs.edu.ar&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;web:&lt;/strong&gt; &lt;a href=&#34;https://www.ungs.edu.ar/cultura/imaginario-museo-interactivo/museo-imaginario&#34;&gt;www.ungs.edu.ar/museo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dirección:&lt;/strong&gt; Julio A. Roca 900, esquina Muñoz, San Miguel. &lt;strong&gt;Teléfono:&lt;/strong&gt; 011-4451-7924/25&lt;/li&gt;
&lt;/ul&gt;

        
      </description>
    </item>
    
    <item>
      <title>Médicos fumadores: Haz lo que yo digo...</title>
      <link>https://ciencianet.com.ar/post/medicos-fumadores-haz-lo-que-yo-digo/</link>
      <pubDate>Wed, 28 Nov 2007 01:12:20 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/medicos-fumadores-haz-lo-que-yo-digo/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Luis Pugnaloni.&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2007/11/pucho-698x462.jpg&#34; alt=&#34;Foto de Mathew MacQarrie en Unsplash&#34;&gt;
En Argentina los médicos fuman sólo un poco menos que el resto de nosotros. En países desarrollados los médicos dan el ejemplo y fuman mucho menos. En nuestro país mueren aproximadamente 45.000 personas por enfermedades relacionadas con el consumo de tabaco en todas sus formas. El trabajo de investigadores argentinos muestra que la capacitación de los médicos es clave para mejorar su posición respecto al tabaquismo y los consejos que brindan a sus pacientes.&lt;/p&gt;
&lt;p&gt;Horacio M. Zylbersztejn, Alberto Cardone, Nora Vainstein, Andrés Mulassi, José G. Calderón, Patricia Blanco, Enrique Pautasso, Aníbal Picarel, Román Cragnolino, Susana Fernández, Adriana Andina, Sebastián Saravia Toledo, Italo Torchio y César A. Belziti, junto a un importante número de colaboradores, realizaron una encuesta sobre 6497 médicos de todo el país. El estudio busca información sobre el hábito de consumo de tabaco entre los médicos y el impacto sobre sus pacientes.&lt;/p&gt;
&lt;p&gt;El trabajo de Zylbersztejn y colaboradores muestra que alrededor del 30% de los médicos fuma. En la población general este porcentaje es algo mayor (entre el 33 y el 39%). Sin embargo en países desarrollados los médicos fuman mucho menos que la población general. En Holanda, por ejemplo, el 40% de la población fuma, pero sólo el 6% de los médicos consume tabaco. El estudio hecho en Argentina destaca que los médicos que recibieron entrenamiento sobre el tratamiento del tabaquismo tienden a fumar menos ellos mismos y a dar consejos más a menudo a sus pacientes fumadores para ayudarlos a dejar el hábito.&lt;/p&gt;
&lt;p&gt;Algunos datos interesantes extraídos de la encuesta se refieren a la gran variación del porcentaje de fumadores por especialidad. Los médicos que trabajan en emergencia son mucho más propensos a fumar (21% más). Esto puede deberse al estrés asociado a este tipo de trabajo. Por otro lado, los cardiólogos y neumonólogos fuman proporcionalmente menos que los médicos del resto de las especialidades. Se especula que esto se debe a que los especialistas de corazón y pulmones son los que comprenden más claramente el efecto del humo del cigarrillo en el organismo. Curiosamente, los médicos que ejercen especialidades quirúrgicas fuman más que los médicos de otras especialidades, aún cuando algunos de estos pueden ver directamente los efectos devastadores del tabaco al operar a sus pacientes.&lt;/p&gt;
&lt;p&gt;Uno de los resultados más significativos del estudio, que confirma estudios hechos en la población general, es que los hijos de padres fumadores o exfumadores tienen hasta cuatro veces más posibilidades de volverse fumadores. Esto muestra cuan importante es el ejemplo de los padres hacia los hijos.&lt;/p&gt;
&lt;p&gt;Los investigadores insisten en que formar a los médicos argentinos en el área de tabaquismo es fundamental. Sólo un tercio de los médicos encuestados había tomado algún curso o asistido a charlas informales sobre tabaquismo. Los que así lo habían hecho recomendaban más frecuentemente a sus pacientes tratamientos para dejar de fumar. Con una inversión mínima en capacitación podrían obtenerse importantes mejoras en la salud pública.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; Tabaquismo en médicos de la República Argentina. Estudio TAMARA, &lt;a href=&#34;http://www.revista.sac.org.ar/&#34;&gt;Revista Argentina de Cardiología&lt;/a&gt; , vol 75, pp 109-116 (2007)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instituciones:&lt;/strong&gt; Consejo de Epidemiología y Prevención Cardiovascular y por el Área de Investigación de la &lt;a href=&#34;http://www.sac.org.ar/&#34;&gt;Sociedad Argentina de Cardiología&lt;/a&gt; .&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; Horacio Zylbersztejnmtsac (E-mail: &lt;a href=&#34;mailto:hzylber@intramed.net.ar&#34;&gt;hzylber@intramed.net.ar&lt;/a&gt; )&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Ciencia, tecnología y sociedad</title>
      <link>https://ciencianet.com.ar/post/ciencia-tecnologia-y-sociedad/</link>
      <pubDate>Thu, 15 Nov 2007 01:13:57 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/ciencia-tecnologia-y-sociedad/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Pablo Miguel Jacovkis&lt;/strong&gt;. Facultad de Ciencias Exactas y Naturales y Facultad de Ingeniería. Universidad de Buenos Aires.&lt;/p&gt;
&lt;p&gt;Como todos sabemos (y sufrimos) la Argentina está remontando una crisis política, económica, social y moral como probablemente no se recuerde desde la época de Rosas, y seguramente como ningún argentino recuerde haber experimentado alguna otra vez en su vida.&lt;/p&gt;
&lt;p&gt;En algún sentido me atrevería a decir que es mayor que en los tiempos de Rosas, porque en esa época los proscriptos tenían confianza en un futuro mejor, e ideaban una Argentina posible; tanto el formidable &lt;em&gt;Facundo&lt;/em&gt; como las &lt;em&gt;Bases y puntos de partida para la organización política de la Confederación Argentina&lt;/em&gt; eran análisis de lo que se debía cambiar y de cómo se debía cambiar. Y no parece haber ahora ni un Sarmiento ni un Alberdi.&lt;/p&gt;
&lt;p&gt;Naturalmente, la situación es mucho mejor que hace cinco años, y el actual gobierno ha logrado inyectar optimismo (o hacerlo recuperar) en forma impensable a principios de 2002. Además, a pesar de la falta de proyecto común visible, permanentemente se ve en la actividad cotidiana de la gente una potencialidad que reconforta, por un lado, y sorprende, por el otro: ¿De dónde sale una vitalidad como la de tantos argentinos? ¿Cómo es posible que la actividad cultural de Buenos Aires, por dar un ejemplo, sea tan impresionante, y provoque la admiración de los extranjeros que nos visitan? Y, para comenzar a entrar en el ámbito de la ciencia y de la tecnología, ¿cómo es posible que haya, en muchos lugares del país, y con las dificultades que todos conocemos, islotes de calidad científica y tecnológica inusitadas?&lt;/p&gt;
&lt;p&gt;Sin ir más lejos, los equipos olímpicos de estudiantes secundarios en ciencia obtienen éxito tras éxito en el exterior (en la Olimpíada Internacional de Matemática de 2006 en Eslovenia, por ejemplo, los chicos argentinos consiguieron dos medallas de plata, dos de bronce, y una mención de honor; en la Olimpíada Internacional de Química de 2005 en Taiwan, de los cuatro chicos participantes uno consiguió una medalla de oro y los otros tres consiguieron medallas de plata: se clasificaron en el séptimo-octavo puesto sobre 59 países, delante de Estados Unidos, Alemania, Gran Bretaña y todos los otros países latinoamericanos). Y no vaya a pensarse que los participantes son solamente porteños, o de las grandes ciudades. Pequeñas ciudades aisladas producen chicos deslumbrantes, que a pesar de que todo les juega en contra logran sobrevivir y triunfar.&lt;/p&gt;
&lt;p&gt;Lo que quiero indicar con esto no es un pesimismo abrumador, como la primera parte del párrafo anterior podría hacer pensar, ni un optimismo autista, como podría hacerlo pensar la segunda. La situación puede mejorar, y pronto. Usando lenguaje de matemática aplicada, hay solución factible.&lt;/p&gt;
&lt;p&gt;Ahora bien, introduje estas reflexiones con cuatro adjetivos referidos a la crisis: política, económica, social y moral. Me interesa sobretodo el último, porque es común que se lo deje de lado. Y sin embargo creo que ese adjetivo, o el adjetivo que cada uno considere más adecuado para describir lo que enumeraré a continuación, es la clave para la recuperación argentina, y permitirá también ubicar a la ciencia y a la tecnología en un lugar realista, como herramienta imprescindible pero no como panacea universal.&lt;/p&gt;
&lt;p&gt;La crisis moral se refleja en la falta de solidaridad y de proyecto de vida común, en el “todo vale”, en la dificultad de diálogo, en la imposibilidad de sacrificar un poquito los intereses personales, o de grupo, o de corporación, en aras del interés general, del “bien común”, en no ser concientes, como ciudadanos, que no sólo debemos exigir nuestro derechos, sino también cumplir nuestros deberes y, sobre todo, en admitir que ciertas ideas preconcebidas pueden estar equivocadas, que el futuro de nuestro país no es necesariamente la ruina definitiva, pero tampoco tenemos un destino manifiesto, ni estamos destinados al éxito, ni a descollar entre las naciones.&lt;/p&gt;
&lt;p&gt;Aceptemos por el momento un papel más modesto, y tratemos de salir adelante. Pasemos ahora a comentar un poco qué rol juega la ciencia en el marco de la crisis actual. Bajo ciertas condiciones, un rol fundamental. Más concretamente, no existe posibilidad alguna de desarrollo de ningún tipo sin tener ciencia y tecnología de primera. Es duro decirlo, pero es así. No podemos exportar solamente productos primarios porque puedan circunstancialmente venderse bien; cada vez es más importante exportar productos de alto valor agregado, y comercializarlos en el país para mejorar la calidad de vida de la Nación. Y sin ciencia y tecnología de primera calidad, no lo podremos hacer.&lt;/p&gt;
&lt;p&gt;Naturalmente, se puede pensar en un país completamente autárquico, aislado del mundo, con su población razonablemente alimentada, dada la capacidad enorme de producción agrícola-ganadera del país (no como ahora, que da vergüenza que todavía hay chicos que se mueren de hambre). Una Corea del Norte sin hambruna, digamos. Independientemente de lo deseable o no que sea ese país (y, aclaro, para mí no es deseable en absoluto), es inviable, salvo que se produzca una revolución como la de Corea del Norte, lo cual es bastante difícil, teniendo en cuenta además que ni siquiera en Corea del Norte el comunismo se hubiera impuesto sin la ocupación del entonces Ejército Rojo.&lt;/p&gt;
&lt;p&gt;Por consiguiente, tenemos que pensar un proyecto nacional razonablemente abierto, viable, que asegure el crecimiento, que tenga bastante consenso, y &lt;strong&gt;que provoque entusiasmo&lt;/strong&gt;. Es decir, que un número considerable de personas de todas las edades y posiciones sociales lo consideren parte de la identidad nacional.&lt;/p&gt;
&lt;p&gt;Esto no es tan utópico e iluso como se puede pensar. Hubo épocas en las que había temas consensuados por la mayor parte del país, y sobre todo por la mayor parte de los intelectuales, que son quienes tienen la tremenda responsabilidad (a la altura de la cual, lamentablemente, no han estado en nuestro país desde hace muchos años) de plantear los grandes problemas. En efecto, el paradigma de sustitución de importaciones era aceptado por la mayoría de los argentinos durante muchos años; que YPF fuera la empresa nacional de petróleo y la Comisión Nacional de Energía Atómica la empresa nacional de energía atómica, llegó a tener valor de mito.&lt;/p&gt;
&lt;p&gt;Con esto no quiero decir que eso estuviera bien o que estuviera mal: simplemente, que hay imágenes aceptadas por la sociedad como naturales de la identidad de un país, aunque esas imágenes van cambiando y, en el caso de los ejemplos dados, van desapareciendo, debido a su contradicción con la realidad general.&lt;/p&gt;
&lt;p&gt;Pero esas imágenes dan sensación de pertenencia, y si desaparecen y no son reemplazadas por otras se produce una pérdida de interés en nuestro país. Y no confundamos interés en nuestro país, y cariño por él, con nacionalismo barato. No es lo mismo. Tenemos entonces que plantear algunos objetivos deseables y factibles. Por un lado, se pueden (y se deben) plantear objetivos como disminución a niveles tolerables de la desigualdad social, desarrollo sostenido y sustentable, solidaridad social, reducción de la corrupción a su mínima expresión, garantía de educación, salud y seguridad para toda la sociedad, democracia, participación, transparencia, derecho a la “búsqueda de la felicidad”, como con una expresión extraordinariamente feliz lo plantearon los padres fundadores de los Estados Unidos. Pero además es necesario garantizar la factibilidad. Es necesario que numerosos profesionales, técnicos, idóneos, científicos, tecnólogos muestren la viabilidad técnica de las propuestas, y los políticos las incorporen.&lt;/p&gt;
&lt;p&gt;Si vamos a desarrollar energía nuclear, por ejemplo, ver sus pros y sus contras. Ese es un buen ejemplo: los científicos y tecnólogos del área nuclear deben tener claro que su porvenir no depende de una apuesta a la energía nuclear. Que si resulta que es mejor no involucrarse demasiado en ella, su capacidad como especialistas, por ejemplo, en ciencias de los materiales, servirá para cualquier proyecto (con esto no estoy diciendo que no hay que desarrollarse en energía nuclear, solamente estoy diciendo que debe asegurarse a los interesados que, si se da el caso, pueden arriesgarse a herir sus propios intereses en aras del bien común).&lt;/p&gt;
&lt;p&gt;En muchas partes del país, por ejemplo, el agua es un recurso escaso. Es necesario desarrollar la ciencia y tecnología hídricas. Los problemas relacionados con modelos hídricos ofrecen un ejemplo muy interesante de aplicación de diversas ramas de la matemática, la computación, la física, la ingeniería, la química, la meteorología y la geología: en efecto, un tramo fluvial o una cuenca se puede representar con un modelo hidrodinámico unidimensional no estacionario de aguas poco profundas con superficie libre.&lt;/p&gt;
&lt;p&gt;Éste es un problema de dinámica de fluidos muy elegante, que se formula matemáticamente mediante un sistema de ecuaciones diferenciales hiperbólicas en derivadas parciales casilineales puestas en forma de ley de conservación, que no tiene solución explícita y, por consiguiente, se resuelve mediante algún método numérico. Dicho método numérico debe luego ser implementado computacionalmente. A ese modelo se le deben ajustar determinados parámetros (muchos), los coeficientes de conducción, para que el modelo represente razonablemente la realidad, y para eso hay que resolver lo que en matemática se llama un problema inverso.&lt;/p&gt;
&lt;p&gt;Si no hay suficientes datos, por ejemplo en la alta cuenca, se simplifica la formulación con una ecuación hiperbólica casilineal escalar que representa un modelo hidrológico (la onda cinemática), y el número de parámetros a ajustar disminuye drásticamente. Las condiciones iniciales del modelo probablemente no se conozcan con precisión, y es necesario seguramente empezar con condiciones iniciales aproximadas, o por lo menos físicamente factibles, y llevar a cabo un proceso de “calentamiento” (warming up) del modelo. Como condiciones de contorno aguas arriba se pueden dar caudales en los puntos extremos del modelo, calculados gracias al conocimiento, mediante mediciones de pluviómetros, de las lluvias caídas en cada subcuenca de la alta cuenca. Para ello es necesario formular un modelo de balance hídrico que transforme lluvia en caudal.&lt;/p&gt;
&lt;p&gt;Teniendo en cuenta que la lluvia escurrirá superficial o subsuperficialmente, que hay escurrimiento subterráneo, y que se puede acumular en la reserva subterránea, el modelo requerirá, seguramente, el asesoramiento de ingenieros hidrólogos y geólogos y, por otra parte, dado que las escalas de velocidad del fluido superficial y subterráneo son distintas, es posible que haya problemas numéricos adicionales. Y aun así el modelo no está completo: los pluviómetros no están obligatoriamente ubicados en lugares representativos de las subcuencas, y es necesario asignar lluvias en pluviómetros a lluvias representativas en cada una de las subcuencas, mediante un proceso de “clustering” o agrupamiento, para lo cual se utilizan métodos de geometría computacional (diagramas de Voronoi o polígonos de Thiessen) y conviene la participación de meteorólogos.&lt;/p&gt;
&lt;p&gt;Y no hemos tenido en cuenta en esta descripción los problemas de contaminación, que en esencia son problemas de difusión que requieren ecuaciones diferenciales parabólicas y asesoramiento químico, ni los problemas de irrigación, que implican poder calcular, por ejemplo, el máximo alcance del frente de onda de caudales en surcos de riego. Y hay que mencionar además otros tipos de problemas: si aguas arriba o aguas abajo, o en el medio de un tramo, hay una represa, es usual tratar de elegir la operación óptima de la represa, en el sentido de maximizar una función objetivo, problema de optimización y control que requiere del apoyo de ingeniería económica. Y esto está enmarcado en un problema más ambicioso: optimización (o simulación) de aprovechamientos múltiples en recursos hídricos, en los cuales hay una vieja tradición en nuestro país.&lt;/p&gt;
&lt;p&gt;La optimización simplificada será programación lineal; un modelo más preciso es de optimización no lineal bajo restricciones no lineales, integrada con programación entera, temas todos de punta en matemática aplicada y computación.&lt;/p&gt;
&lt;p&gt;Pero esto es nada más que un ejemplo ambicioso, que utilicé ex profeso porque es mi especialidad. Volviendo al tema de un proyecto común, los argumentos de tipo “no vamos justo en esta crisis a dedicarnos a preparar proyectos nacionales, ahora hay que salir de esta coyuntura”, no sirven, y mucho menos ahora que, afortunadamente, se superó el problema del default de una manera que fue mucho más favorable a los intereses de nuestro país que lo que todos los agoreros de turno pronosticaban cuando empezaron las negociaciones.&lt;/p&gt;
&lt;p&gt;Desgraciadamente, una larga tradición de cultura inmovilista hace que en Argentina, si se puede patear un problema para adelante, se lo patea. Y en épocas de vacas gordas, que es cuando hay que hacer las reformas, nadie las hace. Y bueno, habrá que hacerlas en las épocas de vacas flacas. Y ese proyecto “entusiasmante” debe incluir obligatoriamente la participación significativa de la ciencia y la tecnología.&lt;/p&gt;
&lt;p&gt;Suelo usar últimamente un ejemplo contundente para mostrar la utilidad fabulosa de la ciencia y la tecnología: si la Unión Soviética sobrevivió a los nazis y compitió luego durante dos generaciones más con Occidente fue en medida no despreciable a que tenía muy buena ciencia, que se usó durante la guerra con fines militares, por supuesto. Y tenía muy buena ciencia porque la Rusia zarista tenía muy buena ciencia. Basta pensar en Mendeleyev, Tchebichev, Markov, Pavlov. La tradición científica de Rusia viene de la época de Pedro el Grande, que creó la Academia de Ciencias en San Petersburgo a principios del siglo XVIII, y llevó a cabo muchas otras reformas con el fin de crear las condiciones necesarias para el desarrollo de la ciencia y de la tecnología rusas y para el empleo más efectivo de los avances científicos tecnológicos de Europa Occidental, además de invitar a europeos occidentales ilustres a San Petersburgo, entre ellos a Euler, a Daniel Bernoulli, y a muchos otros científicos.&lt;/p&gt;
&lt;p&gt;En la Europa de principios del siglo XX había tres “imperios atrasados”: Rusia, Turquía y España. No entraré a analizar en detalle por qué sólo Rusia tenía buena ciencia y tecnología. Baste decir que España se había cerrado a la ciencia con el triunfo de la Contrarreforma, y que Turquía tenía la impronta musulmana de terrible atraso, que Kemal Ataturk trató de combatir, con éxito desigual. De hecho, según Zhores Medvedev la comunidad científica de Rusia en 1914 se calculaba en 11.000 personas, la mayor parte, por supuesto, en San Petersburgo y en Moscú, y practicaba ciencia (y hacía tecnología) absolutamente competitiva a nivel mundial. Era de muy buena calidad y tenía una excelente reputación en Europa. Ésa (11.000) es una cifra muy importante para la época; basta pensar que había en el mundo muchísimos menos científicos que ahora.&lt;/p&gt;
&lt;p&gt;Mencioné tecnología: efectivamente, basta pensar en algunos ingenieros emigrados anticomunistas que fueron tremendamente útiles en Occidente, como el ingeniero aeronáutico Sikorsky, el ingeniero en estructuras Timoshenko, etc. Es decir, en la Rusia zarista no solamente había ciencia “pura”, sino tecnología a partir de esa ciencia. Y esa tradición científica y tecnológica, más los recursos humanos que se quedaron en la URSS después de la Revolución, más el enorme apoyo que dieron las autoridades soviéticas en la década de 1920 a la ciencia y a la tecnología, permitieron a la Unión Soviética sobrevivir al ataque nazi. Con el inmenso heroísmo solo tal vez no hubiera alcanzado, o el saldo en muertos hubiera sido todavía mucho más espantoso de lo que fue.&lt;/p&gt;
&lt;p&gt;Lo que quiero decir con esto es que si queremos un país distinto y mejor, cualquiera sea la idea que tenga cada uno de lo que significa “mejor”, necesitamos ciencia – y tecnología – ahora. ¿Qué podemos hacer entonces en esta coyuntura? Por empezar, hay que aceptar que la ciencia está dolarizada y globalizada desde mucho antes de que ambas palabras empezaran a ser de uso común en el lenguaje cotidiano: los científicos necesitan equipamiento, insumos, visitas del exterior de profesores extranjeros, viajes a congresos, intercambio de estudiantes de doctorado, suscripciones a revistas, libros, todo lo cual hay que pagar ahora tres veces más caro que antes de la devaluación. Pero hay algo que se mantiene igual: el desinterés de muchos miembros de la clase dirigente argentina por la ciencia.&lt;/p&gt;
&lt;p&gt;Ese desinterés es, a mi juicio, un problema cultural, lo cual lo hace más grave aún, pues los cambios culturales suelen ser muy lentos, y nosotros estamos apurados. Es triste decirlo, pero Pedro el Grande de Rusia y sus sucesores, que no eran un dechado de progresismo, por decirlo de la manera más amable posible, tenían más idea de que la ciencia y la tecnología eran importantes de la que tienen muchos políticos argentinos actualmente. Y no solamente muchos políticos. También muchos empresarios y dirigentes gremiales. Y, lamento tener que decirlo, también muchos intelectuales presuntamente progresistas que adoptan posiciones… no sé cómo llamarlas: ¿posmodernas? de las cuales se burló astutamente Alan Sokal.&lt;/p&gt;
&lt;p&gt;Es llamativo (y para mí muy deprimente) el prestigio que tiene un conocido escritor argentino que suele despotricar públicamente contra la ciencia. No creo por otra parte que el desprecio de muchos miembros de la clase dirigente argentina por la ciencia sea producto de su estructura socioeconómica: dejando de lado el ejemplo ya mencionado de Pedro el Grande, o la transformación asombrosa del Japón a partir de la restauración Meiji en 1868 (que significó también un apoyo inmenso a la ciencia), Brasil, que no se puede decir que tenga una estructura socioeconómico más avanzada que la Argentina, posee desde hace muchos años una política de Estado de apoyo a la ciencia y a la tecnología cuyos resultados ya se están viendo, y Chile mismo está avanzando impresionantemente en algunas áreas, en particular en matemática aplicada.&lt;/p&gt;
&lt;p&gt;Es decir, tenemos un primer problema serio, previo a la crisis, que es que una parte considerable de la sociedad no entiende la utilidad de la ciencia ni está interesada en ella. Obsérvese que estoy hablando en términos totalmente pragmáticos de ciencia como herramienta de desarrollo, sin ponerme a discutir sobre el valor del conocimiento puro, etc., que amerita una buena discusión por separado: en última instancia, incluso si nos olvidamos de sus aplicaciones, la ciencia es parte de la cultura y habría que apoyarla como a los teatros nacionales y provinciales, y como a muchas otras actividades culturales. Aclaro, para terminar este párrafo, que se ven signos positivos en los últimos años, tanto en la mirada sobre la ciencia y la tecnología de algunos dirigentes políticos como en la de algunos dirigentes empresarios.&lt;/p&gt;
&lt;p&gt;No estoy tan seguro, y eso me deprime y me asombra, que esos signos positivos incluyan a los intelectuales progresistas, a muchos de los cuales, por ejemplo, la palabra “eficiencia”, básica para tener un país que funcione mejor, les inspira desconfianza o les da pánico. Y cuando digo que la ciencia es útil, antes de poner ejemplos “positivos” puedo poner ejemplos negativos, es decir, cuánto perdió el país por no usar ciencia (y tecnología) cuando debía y podía. Me limitaré a tres ejemplos, y puedo dar muchísimos más. El primero es el famoso papelón internacional cuando el Presidente Perón anunció, en 1951, que en la planta piloto de energía atómica en la isla Huemul, de San Carlos de Bariloche, se llevaron a cabo “reacciones termonucleares bajo condiciones de control en escala técnica”, es decir, energía atómica barata.&lt;/p&gt;
&lt;p&gt;Concretamente, convencido por un charlatán austríaco llamado Ronald Richter, que llegó a la Argentina después de la segunda guerra mundial como tantos de pasado sospechoso en los países del Eje, el gobierno de Perón, mal asesorado, gastó una enormidad de dinero en un proyecto sin ninguna seriedad científica, que terminó en el ridículo.&lt;/p&gt;
&lt;p&gt;El segundo es la grave situación que tuvieron los pescadores de merluza de Mar del Plata. Los científicos y tecnólogos del INIDEP habían recomendado mucho mayor control de pesca de merluza, pues preveían el agotamiento de las reservas; lamentablemente, la presión de las empresas, y también de los gremios, provocó que esas recomendaciones se ignoraran, con los resultados de grave crisis y desocupación pavorosa.&lt;/p&gt;
&lt;p&gt;El tercero es la falta de medidas tomadas ante los estudios meteorológicos y las previsiones sobre el fenómeno de “El Niño”, y los desastres que se podrían haber evitado debidos a inundaciones.&lt;/p&gt;
&lt;p&gt;En todos los casos mencionados (y hay muchos más, por supuesto) los recursos humanos y un mínimo equipamiento existían. Se perdieron muchos miles de millones de dólares (y en el caso de inundaciones, también vidas) por una dirigencia incapaz de prestar atención a sus científicos y tecnólogos; entre otros motivos, porque usualmente las propuestas de los científicos y tecnólogos son a largo plazo, y en Argentina existe el culto a la inmediatez. Bien. Las condiciones de contorno, por usar terminología de mi profesión, son una crisis y una falta de suficiente cultura científica de muchos sectores de la sociedad.&lt;/p&gt;
&lt;p&gt;¿Qué se puede hacer? Por un lado, por supuesto es necesario cambiar esa cultura, para lo cual también es necesaria la colaboración de los científicos, que muchas veces no saben cómo explicar sus actividades a la sociedad, y, como factor muy importante, la colaboración de las universidades. Por ejemplo, por hablar de mi universidad, la Universidad de Buenos Aires no realiza ningún tipo de planificación en la orientación de las vocaciones de sus estudiantes, y, desgraciadamente, muy pocos de ellos siguen carreras científicas o tecnológicas. Siempre digo que, aunque le moleste el adjetivo, la Universidad de Buenos Aires sigue respecto de sus alumnos una política neoliberal: existe un mercado, que son los alumnos, y hay que funcionar de acuerdo con el mercado. Si un año dado 20.000 alumnos deciden seguir la carrera X, pues corriendo a buscar profesores para la carrera X, sin tener en cuenta las necesidades de la Universidad ni las del país. Es decir, es necesario compenetrarse de que la ciencia y la tecnología son importantes, y definir políticas de Estado.&lt;/p&gt;
&lt;p&gt;Yo tengo algunos criterios, que por supuesto pueden no ser compartidos por otros, y merecen ser discutidos. Considero, por ejemplo, que hay que apoyar especialmente la oceanografía (los europeos y norteamericanos conocen mejor el mar Argentino que nosotros), las ciencias de la atmósfera (estudios serios del clima son fundamentales para cualquier estrategia productiva agrícola argentina), la biotecnología (tenemos masa crítica de investigadores), la computación (debemos exportar software como hacen las tres I: Israel, Irlanda, India, lo cual, aclaro, no es nada sencillo), la bioinformática, que es combinación de estas dos últimas y, por supuesto, los estudios hídricos, en un país en el cual la erosión avanza día a día. Además, naturalmente, de ciencias de materiales, en la cual la influencia de la Comisión Nacional de Energía Atómica fue crucial, y especialmente nanotecnología.&lt;/p&gt;
&lt;p&gt;Por otra parte, teniendo en cuenta la ya conocida coyuntura económica, deberán tener relevancia proyectos útiles para la Argentina y en los cuales transformemos desventajas comparativas en ventajas comparativas: la distancia, que encarece nuestro comercio, por ejemplo, no encarece significativamente las comunicaciones electrónicas, y eso permite trabajar en proyectos ambientales, geológicos, de biodiversidad, de ecología, los cuales o se estudian aquí o no se estudian; siempre y cuando privilegiemos los proyectos que nos interesan a nosotros respecto de los que interesan sólo a los otros.&lt;/p&gt;
&lt;p&gt;Y hay que evitar preconceptos: hace pocos años, para dar un ejemplo, un investigador de la Facultad de Ciencias Exactas de la Universidad de Buenos Aires obtuvo un importantísimo subsidio de los Institutos Nacionales de la Salud (NIH) de Estados Unidos para estudiar el mal de Chagas. ¿Por qué el mal de Chagas? ¿Qué les puede interesar a los norteamericanos del mal de Chagas? El mal de Chagas les interesa por dos razones: 1) Los procesos de contagio se modelizan matemáticamente de manera que puede ser análoga, y por consiguiente aplicable, a otras enfermedades que los afectan más; 2) ¡Sí los afecta el mal de Chagas!: los inmigrantes latinoamericanos, legales e ilegales, son muchas veces enfermos y su enfermedad impacta.&lt;/p&gt;
&lt;p&gt;En resumen, lo que creo que se necesita es, primero, conseguir que los intelectuales y dirigentes políticos lúcidos estén interesados en la ciencia y en la tecnología, y la consideren prioridad de Estado. Y para no centrarme exclusivamente en las usualmente denominadas “ciencias duras” les comento que la gente de ciencias sociales y humanas puede contribuir mucho a difundir la idea de la importancia de la ciencia.&lt;/p&gt;
&lt;p&gt;En este momento, no se nota demasiado apoyo. Por ejemplo, para dar un indicador interesante, ¿cuántos subsidios en universidades nacionales están dedicados a la historia de la ciencia en la Argentina, a los problemas que se plantearon siempre, a estudiar, por ejemplo, por qué las ciencias biomédicas se desarrollaron mucho y la ingeniería no, o por qué los científicos que emigraban de la dictadura militar de Onganía en 1966 eran recibidos con entusiasmo en Brasil por otra dictadura militar?&lt;/p&gt;
&lt;p&gt;Segundo, que no podemos esperar a convencer a todos los dirigentes. Tenemos que actuar ya; y en ese sentido, creo que lo más importante que podemos hacer, y en lo cual sí tenemos gran ingerencia, es contribuir a que las Universidades tomen políticas más activas en lo relativo a la ciencia y a la tecnología. Ello requiere algunas decisiones antipáticas, como por ejemplo apoyar carreras científicas y tecnológicas en detrimento de carreras más tradicionales, lo cual obviamente será mirado con extrañeza y probablemente con oposición por muchos sectores sociales (y por sectores influyentes de cada universidad).&lt;/p&gt;
&lt;p&gt;Es obvio que decisiones de este tipo no son fáciles: el ejemplo histórico de la Universidad Nacional de La Plata, creada por Joaquín V. González como universidad con marcado sesgo científico a principios del siglo XX (probablemente la universidad más moderna de América Latina en sus comienzos) no pudo resistir suficientemente la presión “profesionalizante” y fue perdiendo esas características que le daban personalidad única en el país (y en Latinoamérica). Debemos revertir esta situación, subiendo la apuesta: a nivel de muchas de las universidades nacionales, y en particular en muchas universidades de provincia.&lt;/p&gt;
&lt;p&gt;Debemos fomentar el intercambio de científicos evitando el &lt;em&gt;“inbreeding”&lt;/em&gt; o autoalimentación, y la colaboración entre distintos grupos. Al respecto, quiero hacer notar que, en mi opinión, el secuenciamiento en el año 2000 del genoma de la bacteria &lt;em&gt;Xylella fastidiosa&lt;/em&gt; por parte de una especie de consorcio de distintos grupos de investigación del Estado de San Pablo, en Brasil, es en buena medida un triunfo organizativo: un centenar de grupos distintos se unieron exitosamente en un proyecto común.&lt;/p&gt;
&lt;p&gt;Para finalizar, se puede observar que dediqué parte de este análisis, sobre todo al comienzo, a temas más generales que el papel de la ciencia y de la tecnología en la crisis actual. Es porque siento que una visión dedicada solamente a la ciencia y a la tecnología, por más planteos concretos que conlleve, es incompleta y, en la situación actual, probablemente poco exitosa. La base de la poca importancia que se le da a la ciencia y a la tecnología en nuestro país está en que la ciencia y la tecnología son poco importantes para la sociedad: la clase dirigente se limita a ser el reflejo de la sociedad. Es necesario cambiar esa imagen de la sociedad respecto de la ciencia y de la tecnología, surgida probablemente de esa fatua idea de que “Dios es argentino”, y “con una buena cosecha solucionamos los problemas”. Dejemos de lado que una verdadera clase dirigente no es solamente reflejo de la sociedad sino que trata de ver más lejos.&lt;/p&gt;
&lt;p&gt;Es necesario que la sociedad argentina cambie su imagen de la ciencia y de la tecnología, y para ello no solamente es necesario un proyecto abarcador, sino la integración en él de una masa crítica de científicos y tecnólogos. Soy optimista, creo que esa masa crítica existe y puede ser inmensamente útil. Y soy optimista, entre otros motivos, porque no siempre fue como es ahora. Daré dos ejemplos impactantes: en primer lugar, en 1864 y 1865 el Rector de la Universidad de Buenos Aires, Juan María Gutiérrez, gestionó la contratación en Europa de tres profesores con dedicación exclusiva en ciencias exactas y naturales, los tres primeros profesores universitarios con dedicación exclusiva del país. Estos profesores estaban destinados al flamante Departamento de Ciencias Exactas de la Universidad, base de las futuras Facultades de Ciencias Exactas y Naturales y de Ingeniería, que acababa de ser creado. Entretanto estalló la guerra del Paraguay, que se complicó con una virtual guerra civil, y con una crisis económica muy grave. Y a pesar de todo, y con el apoyo del Presidente Mitre y del gobernador Mariano Saavedra, Gutiérrez siguió adelante con la contratación, y los tres profesores vinieron. Dichos tres profesores contratados en Italia fueron Bernardo Speluzzi en matemática pura, Emilio Rossetti en matemática aplicada y Pellegrino Strobel en historia natural.&lt;/p&gt;
&lt;p&gt;El segundo ejemplo lo protagonizó Sarmiento, quien, a mi juicio, fue el político y estadista que mejor entendió y apoyó la ciencia en toda la historia del país: en 1871 creó el Observatorio Astronómico de Córdoba. ¡Creó un Observatorio Astronómico, en una ciudad de provincia de treinta mil habitantes, en un país de dos millones de habitantes, el ochenta por ciento de los cuales eran analfabetos, y trajo un distinguido astrónomo norteamericano, Benjamin Gould, para dirigirlo! No olvidemos que además creó la Academia Nacional de Ciencias en Córdoba, Academia que no tuvo muy buenas relaciones con la oscurantista y clerical Universidad de Córdoba. Su actividad en relación con la ciencia continuó mucho después de dejar de ser Presidente: fue él quien pronunció el 30 de mayo de 1882 un discurso memorable en homenaje al recién fallecido Charles Darwin, en cual demostró conocer – y sobre todo aceptar – la teoría de la evolución en una época en la que incluso en los países europeos era aún mirada por muchos con profunda desconfianza – no digamos en nuestro país.&lt;/p&gt;
&lt;p&gt;Estos ejemplos podrán descorazonarnos, al mostrarnos la decadencia de la clase dirigente argentina. Pero también sirven para decir: “Se puede. Hagámoslo”.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referencias.&lt;/strong&gt; Algunos de los tópicos mencionados pueden verse en detalle en:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Artículo de opinión, Genome sequencing for all, Nature, 406, 109, 13 de julio de 2000.&lt;/li&gt;
&lt;li&gt;Halperin Donghi, T., Historia de la Universidad de Buenos Aires, EUDEBA, Buenos Aires, 1962 (hay reimpresión de 2002).&lt;/li&gt;
&lt;li&gt;Mariscotti, M., El secreto atómico de Huemul, Sudamericana-Planeta, Buenos Aires, 1985.&lt;/li&gt;
&lt;li&gt;Medvedev, Z. A., Soviet Science, Oxford University Press, Oxford, 1979 (hay traducción castellana del Fondo de Cultura Económica).&lt;/li&gt;
&lt;li&gt;Overy, R., Russia’s war, Penguin, Nueva York, 1998.&lt;/li&gt;
&lt;li&gt;Pavlova, G. E. y A. S. Fiodorov, Mijail Vassilievich Lomonosov: vida y obra, Mir, Moscú, 1987 (traducción de la edición rusa de 1980).&lt;/li&gt;
&lt;li&gt;Sokal, A. D., Transgressing the boundaries: toward a transformative hermeneutics of quantum gravity, Social Texts, 46/47, 217-252, 1996.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Contacto&lt;/strong&gt;: &lt;a href=&#34;mailto:jacovkis@gmail.com&#34;&gt;Pablo Jacovkis&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nota del Editor:&lt;/strong&gt; Este artículo está basado en la charla que el autor diera oportunamente en el Foro del Club del Progreso, en febrero de 2007.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Transportando aminoácidos en las células del corazón</title>
      <link>https://ciencianet.com.ar/post/transportando-aminoacidos-en-las-celulas-del-corazon/</link>
      <pubDate>Wed, 31 Oct 2007 01:15:38 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/transportando-aminoacidos-en-las-celulas-del-corazon/</guid>
      <description>
        
          &lt;p&gt;Los aminoácidos representan los ladrillos a partir de los cuales se construyen las proteínas en los organismos biológicos, pero adicionalmente tienen asociadas funciones biológicas más complejas que las de solo ser la mínima unidad estructural de las proteínas. Daniel Peluffo, en un estudio reciente sobre células cardíacas de rata ha descripto corrientes eléctricas generadas por el aminoácido Arginina al atravesar la membrana celular utilizando un transportador específico de aminoácidos catiónicos (CAT).&lt;/p&gt;
&lt;p&gt;Por otro lado, este investigador estudió la directa vinculación de este transporte con la producción de Oxido Nítrico, un potente vasodilatador y agente regulador de la contractibilidad y la proliferación de las células cardíacas. A nivel fisiológico el aminoácido Arginina es esencial para organismos en fase de crecimiento, esta asociado con la liberación de hormonas como insulina, glucagón y prolactina en diferentes órganos y es precursor de diversos metabolitos, entre ellos el Oxido Nítrico de particular interés por su importante función regulatoria a nivel del sistema nervioso central y del sistema cardiovascular. Gases como oxígeno y dióxido de carbono así como moléculas no polares difunden pasivamente al (o desde el) interior de las células atravesando la membrana celular.&lt;/p&gt;
&lt;p&gt;Por otro lado, las sustancias cargadas (iones) deben atravesar la barrera interpuesta por la membrana celular por medio de proteínas inmersas en dichas membranas. Estas proteínas tienen una gran variedad de composiciones, selectividad iónica y mecanismos de transporte y se clasifican, según el caso, como canales, transportadores y bombas. En el caso del transporte de Arginina (un aminoácido con carga neta positiva) el transportador es una proteína de membrana conocida como Transportador de Aminoácidos Catiónicos (CAT). Estas proteínas transportan aminoácidos al interior celular transportando al exterior una o más moléculas cargadas (iones) por cada molécula de aminoácido que ingresa a la célula. La entrada de estas moléculas cargadas a través de la membrana produce verdaderas corrientes eléctricas que pueden ser medidas y evaluadas en respuesta a diferentes estímulos.&lt;/p&gt;
&lt;p&gt;En este trabajo se evidenció experimentalmente que las corrientes generadas se deben exclusivamente al transporte del aminoácido estudiado y no a otras especies cargadas que pudieran estar presentes en el medio de medición. Adicionalmente se demostró que la producción de Oxido Nítrico esta directamente vinculada con la cantidad de L-Arginina transportada al interior celular y que el transporte del aminoácido no requiere una fuente energética como el ATP sino que utiliza el transportador propulsado por la diferencia de concentración entre el interior y el exterior celular.&lt;/p&gt;
&lt;p&gt;Finalmente también se comprobó que el transportador es estéreoselectivo, es decir que solo transporta Arginina con una orientación particular (L) en su esqueleto de carbonos, así como otros aminoácidos que posean orientaciones del mismo tipo que la L-Arginina. Toda la información recabada por Peluffo y la descripción de las características del proceso de transporte es muy valiosa ya que las células cardíacas al igual que muchas otras células del organismo son incapaces de sintetizar Arginina y dependen exclusivamente de su transporte a través de la membrana. Está demostrado que el Oxido Nítrico juega un importante papel protector a nivel cardíaco, y se ha probado que altas dosis intravenosas u orales de L-Arginina mejoran la recuperación de pacientes con falla cardíaca congestiva. La continuación de estos estudios permitirá una mejor comprensión de los efectos de la Arginina a nivel cardíaco así como el desarrollo de terapéuticas más específicas.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; R. Daniel Peluffo, &lt;a href=&#34;https://physoc.onlinelibrary.wiley.com/doi/full/10.1113/jphysiol.2006.125054&#34;&gt;L-Arginine currents in rat cardiac ventricular myocytes, Journal of Physiology (London), 2007; 580: 925-936.&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Modificación de superficies de polímeros con haces de iones pesados</title>
      <link>https://ciencianet.com.ar/post/modificacion-de-superficies-de-polimeros-con-haces-de-iones-pesados/</link>
      <pubDate>Wed, 24 Oct 2007 01:19:05 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/modificacion-de-superficies-de-polimeros-con-haces-de-iones-pesados/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Verónica Chappa:&lt;/strong&gt; Unidad de Actividad Física, Centro Atómico Constituyentes, Comisión Nacional de Energía Atómica y Departamento de Física, Facultad de Ciencias Exactas, Universidad Nacional de La Plata.&lt;/p&gt;
&lt;p&gt;Un polímero es una sustancia compuesta por la unión de cientos de miles de moléculas pequeñas denominadas monómeros, que forman enormes cadenas de las formas más diversas. Las interacciones existentes entre estas grandes cadenas hacen que los materiales poliméricos presenten propiedades físicas y químicas muy diferentes a las que poseen aquellos constituídos por moléculas más sencillas.&lt;/p&gt;
&lt;p&gt;La irradiación de polímeros con el objeto de producir materiales avanzados, los cuales responden a distintos requerimientos, es un campo en permanente desarrollo. Por ejemplo, las irradiaciones utilizando radiación gamma o electrones de altas energías constituyen actualmente técnicas convencionales utilizadas a nivel comercial. No así es la aplicación de iones pesados con estos mismos fines ya que obtener y manipular haces de iones es más complejo y por lo tanto más costoso.&lt;/p&gt;
&lt;p&gt;Los iones pesados son átomos más o menos grandes a los que les faltan o sobran electrones y por lo tanto están cargados electricamente. Luego del descubrimiento de los materiales poliméricos se hizo evidente que eran muy sensibles a todo tipo de radiación. Se observó una degradación de los mismos al iluminarlos con luz y con radiación ultra violeta, también con rayos gamma y con partículas energéticas cargadas, como electrones e iones. Los cambios producidos en el material no necesariamente tienen un carácter negativo, muchas de las nuevas propiedades que presentan los polímeros irradiados hacen que sean aptos para diversas aplicaciones tecnológicas. Esto incrementó el interés de los investigadores que iniciaron estudios sistemáticos en este campo.&lt;/p&gt;
&lt;p&gt;Desde las décadas del 50 y 60, muchos científicos se dedicaron a estudiar los procesos radioquímicos generados en los polímeros luego de irradiarlos con rayos gamma y electrones. A diferencia de las radiaciones gamma, que afectan el material como un todo, o la irradiación con un haz de electrones, los cuales penetran unos pocos milímetros de profundidad con una dirección errática, los iones pesados depositan una altísima densidad de energía en un rango de sólo unos pocos micrones. Debido a este hecho se inducen cambios físico-químicos muy complejos en la superficie del material irradiado.&lt;/p&gt;
&lt;p&gt;El polietileno de ultra alto peso molecular -UHMWPE- constituye un excelente biomaterial, ya que es biocompatible y por lo tanto no es rechazado por el cuerpo después de su implantación. Su estructura única le da excelentes propiedades, tales como bajo coeficiente de fricción al rozar contra una superficie metálica, excelente resistencia al desgaste y dureza. Debido a estas características el UHMWPE ha sido elegido como el material para las prótesis de cadera y rodilla utilizándolo durante más de 30 años.&lt;/p&gt;
&lt;p&gt;Uno de los efectos producidos en el UHMWPE por la irradiación con iones pesados es el aumento local del peso molecular mediante la ligadura de cadenas poliméricas adyacentes -&lt;em&gt;crosslinking&lt;/em&gt;-, originando un aumento de la dureza superficial del material. A diferencia de otro tipo de radiación, la acción del ion pesado es muy eficiente y localizada en la formación de crosslinking y el efecto contrario, el corte de cadenas -&lt;em&gt;scission&lt;/em&gt;- es muy reducido en el caso particular del UHMWPE. Lograr una mejora en la resistencia al desgaste de los materiales utilizados en el reemplazo total de articulaciones mediante técnicas de ingeniería de superficies permite aumentar la vida útil de estas prótesis y por lo tanto evitar intervenciones quirúrgicas necesarias para su reemplazo.&lt;/p&gt;
&lt;p&gt;En este estudio se determinó que existe un número particular e iones por unidad de área, denominado fluencia óptima, para a cual se maximiza la formación de dobles enlaces entre carbonos de una misma cadena polimérica en el UHMWPE. Esta fluencia óptima depende fuertemente no sólo del material irradiado sino también de las características de la radiación utilizada, como el tipo de ion y su energía. Este comportamiento se había encontrado previamente al estudiar la resistencia al desgaste de este mismo material al irradiarlo. Se observó una mejora en la resistencia al desgaste que aumenta con a radiación hasta un cierto valor, y luego disminuye si se continua irradiando aun más.&lt;/p&gt;
&lt;p&gt;A partir del modelado y del estudio cuidadoso de la forma de las curvas experimentales de la formación de los enlaces dobles carbono-carbono -la unión básica para formar las cadenas poliméricas- en función de la fluencia, se obtuvo información relevante sobre los efectos producidos por el ión en el polímero a nivel microscópico.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;CienciaNet&lt;/strong&gt; consultó a Guillermo Marti del Centro Atómico Constituyentes (CNEA), quien comenta &amp;quot;El estudio encarado por Verónica Chappa como tema de su tesis doctoral resulta particularmente interesante, porque en este trabajo, centrado en el estudio de las interacciones de iones pesados con la materia, además de ser muy original, utiliza una herramienta (el acelerador de iones pesados TANDAR), generalmente pensada y usada para hacer investigación básica, para desarrollar una moderna aplicación tecnológica. Por otra parte, en la misma, se balancean perfectamente aspectos teóricos y experimentales de las reacciones con iones pesados y el desarrollo del código específico de análisis de datos obtenidos en base a la técnica denominada ERDA, es un aporte significativo que puede encontrar en el futuro otras aplicaciones importantes.&amp;quot; Guillermo Marti fue uno de los jurados del trabajo de Verónica Chappa.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; Este estudio forma parte de la tesis defendida por la autora de esta nota el 27 de marzo de 2007 en la Facultad de Ciencias Exactas (UNLP), para optar al grado de Doctor. Título: &amp;quot;Aplicación de haces de iones pesados en el análisis y modificación de la superficie de materiales&amp;quot;, Director: Gerardo García Bermúdez, Copias de la tesis pueden solicitarse directamente por email a su autora, Contacto: V. Chappa (Email: &lt;a href=&#34;mailto:chappa@tandar.cnea.gov.ar&#34;&gt;chappa@tandar.cnea.gov.ar&lt;/a&gt;)&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Divulgación científica y tecnológica. Un ejemplo</title>
      <link>https://ciencianet.com.ar/post/divulgacion-cientifica-y-tecnologica-un-ejemplo/</link>
      <pubDate>Fri, 19 Oct 2007 01:24:02 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/divulgacion-cientifica-y-tecnologica-un-ejemplo/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Luis Pugnaloni&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP).&lt;/p&gt;
&lt;p&gt;La revista argentina de divulgación científica y tecnológica Ciencia Hoy cumple 100 números; una publicación de la mejor calidad producida por una asociación civil sin fines de lucro. Que una producción de este tipo continúe tan viva después de 20 años es ejemplificador para todos. &lt;strong&gt;CienciaNet&lt;/strong&gt; conversó con Aníbal Gattone, uno de los editores responsables de Ciencia Hoy.&lt;/p&gt;
&lt;p&gt;-¿En qué consiste divulgar ciencia y tecnología? &lt;strong&gt;Aníbal Gattone:&lt;/strong&gt; Esto tiene muchos niveles que van desde la nota periodística, pasa por los profesionales que trabajan en el área y llega hasta el investigador que da charlas más o menos entendibles. La idea es que el bien social &amp;quot;conocimiento científico&amp;quot;, o sea, aquél adquirido por un grupo entrenado dentro del método científico, llegue al resto de la sociedad para beneficio de ésta. -&lt;/p&gt;
&lt;p&gt;-¿Qué publica Ciencia Hoy en sus páginas? &lt;strong&gt;Aníbal Gattone:&lt;/strong&gt; Dentro de esta gama CH se propone llegar al público ilustrado con notas escritas por los propios autores y trabajadas de forma tal de hacerlas entendibles por el equivalente a un profesor de enseñanaza media. Nuestra visión es producir cosas como la norteamericana Scientific American o la francesa La Recherche.&lt;/p&gt;
&lt;p&gt;-¿A qué lectores apunta la revista? &lt;strong&gt;Aníbal Gattone:&lt;/strong&gt; El lector debe estar algo informado para aprovechar mejor el contenido. Lo dicho acerca del profesor de enseñanaza media es una buena definición. Las encuestas nos muestran que tiene también bastantes curiosos que buscan saber de qué se trata.&lt;/p&gt;
&lt;p&gt;-¿Quienes son sus principales competidoras y qué hace a Ciencia Hoy diferente de ellas? &lt;strong&gt;Aníbal Gattone:&lt;/strong&gt; en Argentina no tiene. Es decir, no hay revistas de divulgación de la producción nacional escritas por los propios investigadores.&lt;/p&gt;
&lt;p&gt;-¿Qué objetivos considera que Ciencia Hoy ha podido alcanzar en sus primeros 20 años de existencia y en cuáles ha fracasado? &lt;strong&gt;Aníbal Gattone:&lt;/strong&gt; El haber llegado a 20 años ha sido el éxito más notorio. Una de las cosas propuestas por la asociación fue llevar el tema ciencia más allá del ámbito cultural y hacerlo popular; no se logró hasta ahora.&lt;/p&gt;
&lt;p&gt;-¿Cuáles son los desafíos de hoy? &lt;strong&gt;Aníbal Gattone:&lt;/strong&gt; las formas de comunicación se han modificado a lo largo de estos años. Textos, gráficos, leyendas, infografías, cuadros y todas las técnicas gráficas de representación cambian permanentemente y más rápido de lo que quisiésemos. Mantenernos actuales es el mayor desafío.&lt;/p&gt;
&lt;p&gt;-¿Cuales son los principales problemas que enfrentas cotidianamente en tu tarea de editor? &lt;strong&gt;Aníbal Gattone:&lt;/strong&gt; Hacer que los físicos escriban fácil (broma). La realidad es que la tarea de editor exige mucho trabajo. Uno tiene la mente del investigador pero debe tratar el texto mirándolo como un lego. Ese esfuerzo permanente cansa bastante.&lt;/p&gt;
&lt;p&gt;-¿Cuántas personas trabajan en la redacción? &lt;strong&gt;Aníbal Gattone:&lt;/strong&gt; Una secretaria editorial y nueve editores especializados.&lt;/p&gt;
&lt;p&gt;-¿Cómo se financia la publicación? &lt;strong&gt;Aníbal Gattone:&lt;/strong&gt; Ventas en kioscos e institucionales, venta de publicidad y suscripciones.&lt;/p&gt;
&lt;p&gt;-¿Los verdaderos lectores de Ciencia Hoy son en su mayoría científicos, docentes, estudiantes, profesionales...? &lt;strong&gt;Aníbal Gattone:&lt;/strong&gt; En orden decreciente, docentes, científicos, curiosos, profesionales y estudiantes...&lt;/p&gt;
&lt;p&gt;-¿Qué tirada tiene la revista, dónde se puede conseguir? &lt;strong&gt;Aníbal Gattone:&lt;/strong&gt; 5000 ejemplares por número, seis veces por año. Se la consigue en kioscos (pídala si no la tienen), suscripción (por Internet en &lt;a href=&#34;https://ciencianet.com.ar/www.cienciahoy.org.ar&#34;&gt;www.cienciahoy.org.ar&lt;/a&gt; ) o llamando al (011) 4961-1824.&lt;/p&gt;
&lt;p&gt;-¿Qué deseo pidió Ciencia Hoy cuando apagó las velas de su torta de 100 números? &lt;strong&gt;Aníbal Gattone:&lt;/strong&gt; Hay uno desde el inicio. Vivir lo suficiente como para publicar un día la sección &amp;quot;CH hace 50 años&amp;quot;  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Más información:&lt;/strong&gt; Sitio de la revista Ciencia Hoy: &lt;a href=&#34;https://cienciahoy.org.ar/&#34;&gt;https://cienciahoy.org.ar/&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Sempiterna aqua</title>
      <link>https://ciencianet.com.ar/post/sempiterna-aqua/</link>
      <pubDate>Wed, 03 Oct 2007 01:26:09 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/sempiterna-aqua/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Roberto Fernández Prini.&lt;/strong&gt; INQUIMAE-DQIAQF, Facultad de Ciencias Exactas y Naturales, Universidad de Buenos Aires y Unidad de Actividad Quimica, Comisión Nacional de Energía Atómica.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2007/10/agua.jpg&#34; alt=&#34;Foto de Aaron Burden en Unsplash.&#34;&gt;&lt;/p&gt;
&lt;p&gt;Las propiedades del agua líquida son consideradas anómalas. Lo son porque se comportan de manera distinta a los demás líquidos. Por ello resulta paradójico que el agua sea el único líquido puro con el que el hombre se encuentra cotidianamente y que se lo piense anómalo. El ejemplo más destacado de esa anomalía puede resumirse en que el sólido (hielo) es menos denso –flota– que el líquido.&lt;/p&gt;
&lt;p&gt;El agua debe sus propiedades &lt;em&gt;curiosas&lt;/em&gt; a su pequeño tamaño molecular y a la fuerza relativa de las interacciones que existen entre sus moléculas. Mucho se ha especulado sobre el comportamiento de esta importante sustancia dado que H2O es sinónimo de probabilidad de vida. Entonces no es tan llamativo que el coportamiento del agua genere muchas especulaciones y hallazgos que no se han demostrardo fehacientemente.&lt;/p&gt;
&lt;p&gt;La cantidad de investigaciones que se han realizado sobre los sistemas acuosos, es decir agua que contiene otras moléculas –sales, sustancias orgánicas como la urea o los alcoholes, sustancias de interés biológico como proteínas, acido desoxirribonucleico, etc.–, son numerosísimas y de muy buena calidad. Por lo tanto, queda claro que algo se puede decir con certidumbre sobre esta sustancia. En el agua hay importantes interacciones entre las moléculas, relaciones de atracción y repulsión que forman agregados de existencia efímera que perduran algunos pico segundos (billonésimos de segundo), pero no hay estructuras que permanezcan estáticas, invariables en el tiempo, más allá de ese lapso fugaz.&lt;/p&gt;
&lt;p&gt;Algunos informes científicos han sugerido que el agua tiene memoria de las moléculas han sido disueltas en ella. Este tema siempre atrae porque se lo vincula inexorablemente con la homeopatía. Que el agua no conserva estructuras en su seno depués que las moléculas que se dice que las puedan generar ya no están más en ella, está claro. Una evidencia cotidiana lo da el efecto que tienen las microondas que se utilizan para calentar agua o soluciones acuosas. El campo de electromagnético de microondas oscila unas 100.000 millones de veces por segundo y si las moléculas no tuvieran movimientos de ese mismo orden de magnitud, el agua no se calentaría.&lt;/p&gt;
&lt;p&gt;En la actualidad hay mucho interés en conocer el comportamiento del agua en sistemas confinados. Esto es importante porque su comportamiento cambia, fenómeno que también ocurre con otras sustancias, pero el agua, dadas sus características, tiene comportamientos inesperados y que es necesario conocer porque sistemas confinados que contienen H2O son comunes en la biología, en sistemas naturales y en la tecnología. Sistemas confinados son los que tienen por lo menos una dimensión que es muy pequeña, por ejemplo en el orden de algunos mil millonésimos de metro. Ejemplos son alambres o tubos muy delgados, planos o superficies muy delgadas, en estos ejemplos &lt;em&gt;muy delgado&lt;/em&gt; remite a que una de sus dimensiones, el diámetro en los cables o el espesor en las superficies, son muy pequeñas involucrando pocas moléculas.&lt;/p&gt;
&lt;p&gt;También son sistemas confinados los que están constituidos por agregados de pocas moléculas, cristalitos o gotículas. A esto debe agregarse que la fuerte interacción entre las moléculas de agua, que cuando tienen espacio (sistemas macroscópicos) se disponen &lt;em&gt;en promedio&lt;/em&gt; con un hábito tetraédrico como el hielo, constituye una razón para que el agua confinada no admita fácilmente en su seno sustancias que tienen una interacción distinta, especialmente cuando perturban su estructura. La consecuencia es que son repelidas o segregadas lo más posible para que el resto no se vea perturbado.&lt;/p&gt;
&lt;p&gt;La gran afinindad de H2O por H2O y el sostenimiento de la estructura tetraédrica cuando es posible también se observa en la superficie de sistemas macroscópicos, esa zona se llama interfaz y también confina al H2O en una dirección. Así las propiedades y las posibles reacciones químicas de H2O podrán ser distintas en las superficies y en los sistemas confinados. La gran capacidad del agua para tratar de adaptarse al medio confinante en que se encuentra tiene consecuencias interesantes. Así cuando se introduce H2O en nanotubos de carbono que tienen un diámetro de 14 nm, es decir catorce mil-millonésimas de un metro que es como 30.000 veces más pequeño que el diámetro de un cabello, las moléculas se acomodan en la superficie formando como un &lt;em&gt;forro&lt;/em&gt; interno pero, debido al confinamiento no pueden estructurarse como en el hielo, sino que se unen (por uniones hidrógeno) fomando cuadrados.&lt;/p&gt;
&lt;p&gt;Ahora bien, si se agrega más agua cuando ya está completo el forro, las nuevas moléculas H2O presentan propiedades muy distintas a las que forman el forro dado que &lt;em&gt;no ven&lt;/em&gt; a la superficie de átomos de carbono; esas moléculas centrales, que son como el relleno de un churro, tienen una velocidad de desplazamiento a lo largo del eje del nanotubo mucho mayor que las del forro. Es como si hubiera dos tipos de moléculas H2O distintas, esto porque el ambiente en que se encuentran las dos poblaciones son muy distintos. El agua pura tiene tendencia a formar espontáneamente, aunque en muy pequeña cantidad, iones OH&lt;sup&gt;-&lt;/sup&gt; y H3O&lt;sup&gt;+&lt;/sup&gt; (iones hidroxilo e hidronio o protones hidratados). Ocurre que en conglomerados de pocas moléculas de H2O que forman como gotículas el ion H3O&lt;sup&gt;+&lt;/sup&gt; sólo vive en la superficie interior del conglomerado. Esto se atribuye a que la geometría del ion hidronio es muy distinta a la de H2O y por lo tanto el hidronio es rechazado. Lo mismo se observó ya hace tiempo con la gotículas que contienen iones esféricos, los iones negativos no están en el centro de las gotículas sino que tienden a posicionarse fuera del centro, esto a pesar que tanto los iones como el conglomerado de H2O son esféricos.&lt;/p&gt;
&lt;p&gt;En el caso de que en lugar de iones, los que tienen fuerte interacción electrostática con H2O, hubiera en la gotícula un átomo neutro, como uno de gas interte, después de un breve tiempo la molécula de gas es totalmente expulsada de la gota y sólo se adhiere a su superficie exterior.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Sempiterna aqua.&lt;/em&gt; El tema es apasionante y estoy seguro que siempre estaremos preocupados por el agua; por eso esta nota, como &lt;em&gt;Finnegans Wake&lt;/em&gt;, termina como comenzó.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>El cerebro predice nuestra percepción del mundo exterior</title>
      <link>https://ciencianet.com.ar/post/el-cerebro-predice-nuestra-percepcion-del-mundo-exterior/</link>
      <pubDate>Mon, 01 Oct 2007 01:27:25 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/el-cerebro-predice-nuestra-percepcion-del-mundo-exterior/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro&lt;/strong&gt;. Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLPP) y Universidad Tecnológica Nacional.&lt;/p&gt;
&lt;p&gt;El cerebro humano anticipa nuestra percepción del mundo exterior. Por ejemplo, es capaz de predecir si vamos a percibir una estimulación táctil de intensidad débil, o por el contrario, si una estimulación más intensa será percibida más o menos dolorosamente.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2007/09/cerebro-297x300.png&#34; alt=&#34;Imágenes del cerebro: Correlatos neuronales de la consciencia de estímulos somatosensoriales.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Si las personas pueden realizar estas observaciones empíricamente, un equipo de investigadores del Grupo de la Ciencia del Coma de la Universidad de Liege (Cyclotron Research Centre), y del Departamento de Neurología del Hospital Universitario de Liege (Bélgica), está demostrándolo científicamente a través de la medición de la actividad espontánea del cerebro, y de las relaciones entre las diferentes regiones cerebrales involucrada.&lt;/p&gt;
&lt;p&gt;En un estudio publicado recientemente en los &lt;em&gt;Proceedings of the National Academy of Sciences&lt;/em&gt; (PNAS), el equipo de científicos liderados por Mélanie Boly y Steven Laureys muestra que la actividad espontánea medida en ciertas partes del cerebro tiene una influencia directa sobre nuestra percepción consciente y sobre nuestra percepción de la intensidad del dolor. &amp;quot;Nuestro cerebro nunca está realmente descansando, y la ciencia no tiene una buena comprensión de cómo la actividad espontánea y continua de nuestras neuronas influye sobre nuestra percepción del mundo. Nuestro estudio contribuye a develar una pequeña parte de estos mecanismos&amp;quot;, dicen los investigadores.&lt;/p&gt;
&lt;p&gt;Utilizando imágenes de resonancia magnética funcional (fMRI) y estimulación con láser de las estructuras nerviosas de la piel (que dura una milésima de segundo), los investigadores determinaron las regiones específicamente involucradas en esta actividad espontánea del cerebro. De este modo, la conciencia de uno mismo y de nuestro mundo interno está conectada a la actividad en una red que incluye el precúneo, el cortex cingular posterior y la juntura temporoparietal. Sin embargo, la conciencia de nuestro mundo externo está vinculada con la actividad medida en el tálamo y en las zonas corticales frontoparietales. En cuanto al cortex cingular anterior y el cortex insular, éstos predicen cuándo la estipulación con láser será percibida como más o menos dolorosa.&lt;/p&gt;
&lt;p&gt;Estos resultados mejoran nuestro conocimiento sobre los mecanismos involucrados en la conciencia humana y permiten una mejor comprensión de por qué a veces somos más sensibles al dolor que en otras ocasiones. &amp;quot;La conciencia tiene dos componentes: la vigilia por un lado, y la conciencia del entorno (el mundo externo) de uno mismo (el mundo interno) por el otro. Ya conocíamos las regiones involucradas en la vigilia. Con este estudio, demostraremos ahora que la conciencia de nuestro mundo interno y externo tiene correlatos neuronales diferentes, y cómo ellos interactúan&amp;quot;, concluyen los investigadores.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fuente:&lt;/strong&gt; &lt;a href=&#34;http://www.physorg.com/news103265598.html&#34;&gt;Physorg&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Artículo completo:&lt;/strong&gt; &lt;a href=&#34;http://www.pnas.org/cgi/content/abstract/0611404104v1&#34;&gt;http://www.pnas.org/cgi/content/abstract/0611404104v1&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>El problema de Fermi, Pasta y Ulam: “Un pequeño descubrimiento”</title>
      <link>https://ciencianet.com.ar/post/el-problema-de-fermi-pasta-y-ulam-un-pequeno-descubrimiento/</link>
      <pubDate>Fri, 24 Aug 2007 01:30:37 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/el-problema-de-fermi-pasta-y-ulam-un-pequeno-descubrimiento/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Fernando Vericat:&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2007/08/maniac-1-300x224.jpg&#34; alt=&#34;MANIAC.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;A principios de 1952 la computadora MANIAC-I (Mathematical Analyzer Numerical Integrator And Calculator) fue puesta en servicio en Los Alamos. A mediados de ese mismo año, el físico Enrico Fermi decidió poner a prueba la potencialidad de la misma y propuso considerar la posibilidad de utilizarla como herramienta para investigar problemas dinámicos no lineales mediante “experimentos” numéricos. Con ese fin se asoció con el especialista en computación John Pasta y el matemático Stanislaw Ulam.&lt;/p&gt;
&lt;p&gt;El problema que eligieron para comenzar puede calificarse de modesto para los estándares actuales de computación: un conjunto de N = 32 masas puntuales iguales que pueden moverse a lo largo de una recta. Cada masa está unida a la anterior y a la siguiente mediante un resorte y los extremos de esta cadena están fijos. Los resortes fueron considerados como cuasi ideales, superponiendo a la fuerza lineal, característica de resortes que se comportan idealmente como osciladores armónicos, una pequeña fuerza perturbativa, cuadrática con la distancia entre las correspondientes masas vecinas.&lt;/p&gt;
&lt;p&gt;Sabemos de los textos de Física básica que para el sistema no-perturbado, las N acciones, además de la energía total, son también constantes de movimiento. En consecuencia, para un conjunto de acciones dadas, el sistema no puede recorrer toda la superficie de energía total constante sino que la trayectoria estará restringida a una curva específica (la curva que es la intersección de todas las superficies admitidas, una para cada acción).&lt;/p&gt;
&lt;p&gt;El sistema es completamente integrable. No existe ninguna incertidumbre acerca del movimiento. El movimiento claramente es no-ergódico. Esto significa que si distribuimos inicialmente la energía total, de manera que esté toda concentrada en un solo modo normal, ésta permanecerá siempre en él, que será por lo tanto el único modo excitado. Por supuesto que todas estas cuestiones eran bien conocidas por Fermi, Pasta y Ulam. Pero ellos esperaban que, al introducir la menor perturbación al sistema descrito por el Hamiltoniano no-perturbado, con el tiempo la energía se distribuiría equitativamente entre todos los modos normales transformándose el movimiento en ergódico.&lt;/p&gt;
&lt;p&gt;Cabe señalar a esta altura, que a fines del siglo XIX y principios del siglo XX, estaba claro que los métodos analíticos desarrollados durante los siglos anteriores, por Lagrange, Laplace, Hamilton, Jacobi, Liouville y otros ilustres físico-matemáticos tenían limitaciones de tipo operativo y que problemas aparentemente tan sencillos como el movimiento de tres cuerpos interactuando entre sí, no eran integrables y no admitían en consecuencia soluciones analíticas cerradas.&lt;/p&gt;
&lt;p&gt;Fue en esas circunstancias que el matemático francés Henri Poincaré reconoció la necesidad de utilizar un enfoque diferente para tratar sistemas dinámicos que eran, debido a su complejidad, no integrables. En lugar de tratar de obtener en forma explícita y cuantitativa las trayectorias de los sistemas dinámicos consideró la posibilidad de estudiar las propiedades de las mismas más cualitativamente desde un punto de vista geométrico y topológico.&lt;/p&gt;
&lt;p&gt;Sin embargo este enfoque, en particular, y los estudios de sistemas dinámicos clásicos, en general, se vieron relegados de la atención de los físico-matemáticos durante prácticamente toda la primera mitad del siglo XX en razón del arrasador éxito de la Mecánica Cuántica, al cual, dicho sea de paso, el propio Fermi contribuyó significativamente.&lt;/p&gt;
&lt;p&gt;Esa era, en líneas generales, la situación cuando Fermi, Pasta y Ulam realizaron la simulación numérica de la cadena de resortes. Según ya comentamos, ellos esperaban que la adición del término perturbativo, aún para una intensidad pequeña, se tradujera en la ergodicidad del sistema y la equipartición de la energía entre sus N modos normales. Sin embargo, y para su sorpresa, lo que observaron fue que, partiendo de un estado en el que toda la energía estaba concentrada en el armónico fundamental, la energía comenzaba, efectivamente a distribuirse entre los demás modos, pero esto ocurría hasta solamente el cuarto o quinto. Luego, con el tiempo, empezaba a concentrase nuevamente en el primer modo para luego recomenzar a distribuirse nuevamente entre esos pocos primeros armónicos, siguiendo un comportamiento cuasi-periódico que “modulaba” al comportamiento periódico de los modos normales no perturbados.&lt;/p&gt;
&lt;p&gt;Donde esperaban ver el desorden de la ergodicidad, ellos encontraron en realidad orden. Esto resultó inesperado para FPU a tal punto que Fermi llegó a hablar de “un pequeño descubrimiento”. Esto constituye un ejemplo, quizás el primero, de una regla bastante aceptada en los estudios modernos de sistemas no lineales y complejos: que lo interesante generalmente está en encontrar no lo que uno esperaría sino lo inesperado. Fermi murió en noviembre de 1954, y los resultados de esa primera simulación en dinámica nunca fueron formalmente publicados. Sin embargo un borrador con los mismos circuló entre unos pocos físicos y matemáticos especialistas, contribuyendo a incentivar significativamente los estudios en dinámica clásica que, incipientemente, eran retomados por algunos de ellos.&lt;/p&gt;
&lt;p&gt;Muchos de los nuevos esfuerzos fueron orientados a explicar el comportamiento del modelo de FPU lo cual a su vez generó, por retroalimentación, nuevos desarrollos. En general los intentos para resolver el problema de FPU se pueden dividir en dos grandes grupos: Uno de ellos considera al problema FPU como un claro caso de la llamada estabilidad KAM (por Kolmogorov, Arnold y Moser); el otro como un ejemplo de solitones KdV (por Korteweg-deVries). Tanto la demostración de la conjetura de Kolmogorov por Arnold y Moser (teoría KAM), dentro de la línea geométrico-topológica de Poincaré, así como la aparición de solitones como solución de la ecuación KdV, fueron publicadas en la década del 60 e inmediatamente se pensó en aplicarlos para explicar el problema de FPU.&lt;/p&gt;
&lt;p&gt;No es el espíritu de esta nota mostrar los aciertos y limitaciones de cada uno de estos (y otros) enfoques para explicar los resultados de FPU. Esto requeriría de una serie de detalles técnicos más allá de las pretensiones de la misma. Simplemente hemos querido señalar, en una perspectiva más bien histórica, la trascendencia del problema FPU dentro de una rama de la Ciencia que, desde la segunda mitad del siglo XX, viene creciendo en forma notable: la dinámica no-lineal o, en general, la física de sistemas complejos.&lt;/p&gt;
&lt;p&gt;Términos como ergódico, caos determinístico, atractores, puntos periódicos, ciclos, estabilidad, sensibilidad a condiciones iniciales, fractales, etc, se han vuelto comunes en la jerga científica. También las simulaciones numéricas mediante nuevas computadoras, con capacidades cada vez mayores, son hoy en día rutina, así como las colaboraciones entre físicos, matemáticos, programadores y científicos de los más diversos campos. El “pequeño descubrimiento” de Fermi, Pasta y Ulam puede considerarse, en muchos aspectos, como pionero en relación a esos conceptos y metodologías.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Lo técnico y lo humano en el desarrollo en electrónica</title>
      <link>https://ciencianet.com.ar/post/lo-tecnico-y-lo-humano-en-el-desarrollo-en-electronica/</link>
      <pubDate>Mon, 13 Aug 2007 01:32:00 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/lo-tecnico-y-lo-humano-en-el-desarrollo-en-electronica/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Diego Vallejo&lt;/strong&gt;: Departamento de Ciencias Básicas, Facultad de Ingeniería, Universidad Nacional de La Plata.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Sin ciencia sería muy difícil mantener un estilo de vida como el que pretendemos tener.&lt;/em&gt;&lt;/strong&gt; ¿Qué cambia en un dispositivo electrónico cuando éste es llevado al espacio exterior? ¿Cómo se modifica un elemento de un circuito eléctrico frente al cambio de un material por otro? Y con una mirada más amplia: ¿qué contexto humano motiva y promueve el anteriormente citado trabajo de investigación? Estos son algunos de los temas conversados con los integrantes del Grupo de Estudio de Materiales y Dispositivos Electrónicos (GEMyDE), una unidad de investigación y desarrollo dependiente de la Facultad de Ingeniería de la Universidad Nacional de La Plata.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2007/08/foto0.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Cordialmente, con un mate compartido, entre libros, computadoras y la luz que penetra a través de los cristales del edificio de Electrotecnia de la Universidad Nacional de La Plata, comenzó la entrevista con los integrantes del GEMyDE: el Dr. Eitel Peltzer y Blancá (coordinador del grupo), el Ing. Ariel Cédola (docente investigador) y el Ing. Marcelo Cappelletti (becario y estudiante del doctorado).&lt;/p&gt;
&lt;h3 id=&#34;i-el-contexto-técnico&#34;&gt;I. El Contexto Técnico&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2007/08/imagen-link.jpg&#34; alt=&#34;:left&#34;&gt;
–¿Cuáles son los objetivos del GEMyDE?&lt;/p&gt;
&lt;p&gt;–&lt;strong&gt;Eitel Peltzer&lt;/strong&gt;: La idea central es tratar de desarrollar dispositivos electrónicos a través de modelos matemáticos, de manera de poder estudiarlos bajo cualquier medio ambiente y con diferentes materiales. Las actividades del grupo se inician en el año 2002, con una colaboración con la Comisión Nacional de Actividades Espaciales (CONAE).&lt;/p&gt;
&lt;p&gt;-&lt;strong&gt;Eitel Peltzer&lt;/strong&gt;: Ellos querían saber qué ocurría cuando sobre un dispositivo incide la radiación solar: protones, neutrones, además de luz, que producen disfunciones en el dispositivo. Nosotros estudiamos cuáles son las respuestas del elemento irradiado.&lt;/p&gt;
&lt;p&gt;-&lt;strong&gt;Ariel Cédola&lt;/strong&gt;: Particularmente en los satélites aparecen los efectos de la radiación espacial. Enfocamos nuestro análisis sobre los fotodiodos (que transforman luz en energía eléctrica), estudiando su degradación por el impacto de la radiación.&lt;/p&gt;
&lt;p&gt;-&lt;strong&gt;Marcelo Cappelletti&lt;/strong&gt;: Dado que estos diodos conducen sólo en presencia de radiación, resultan de especial interés para utilizarse como detectores de la misma.&lt;/p&gt;
&lt;p&gt;-&lt;strong&gt;Eitel Peltzer&lt;/strong&gt;: Por otro lado estudiamos la “generación de dispositivos de diseño”, tema que comprende el estudio de la utilización de nuevos materiales dentro de la electrónica.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2007/08/foto2.jpeg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;-¿Cuál es el beneficio para la comunidad de su trabajo?&lt;/p&gt;
&lt;p&gt;-&lt;strong&gt;Eitel Peltzer&lt;/strong&gt;: Reducir costos, entre otros. Utilizar modelos matemáticos-computacionales con poder predictivo permite eliminar los ciclos de prueba y error que se requieren para la obtención de dichos dispositivos, con la consecuente disminución de los montos involucrados. Desde hace mucho tiempo se conoce la teoría de la interacción de la radiación con la materia. Sin embargo la falta de herramientas computacionales apropiadas limitaba el alcance de las predicciones sobre el comportamiento de los dispositivos electrónicos en esas condiciones. En el pasado la electrónica de un satélite o una nave espacial se triplicaba por seguridad, con el incremento en peso (que debe uno quitar de la tierra, y por tanto, el incremento en costo). Para estudiar estos temas, los integrantes del GEMYDE han decidido desarrollar un programa con código propio (en FORTRAN y Visual C++). Esto les significó una importante inversión en horas de trabajo de análisis y codificación, con la valiosa ventaja de poseer total libertad a la hora de modificarlo e implementar nuevas funcionalidades, y nuevos algoritmos, cosa imposible en los programas de &amp;quot;código cerrado&amp;quot;.&lt;/p&gt;
&lt;h3 id=&#34;ii-el-contexto-humano&#34;&gt;II. El Contexto Humano&lt;/h3&gt;
&lt;p&gt;-Estas actividades ocurren en un contexto personal y humano. En ese orden, Dr. Peltzer, recientemente ha sido usted designado Secretario de Ciencia y Técnica de la Universidad. ¿Qué rol le cabe al Secretario dentro de la Universidad?&lt;/p&gt;
&lt;p&gt;-&lt;strong&gt;Eitel Peltzer&lt;/strong&gt;: La Universidad de La Plata fue pensada de manera diferente de otras universidades donde sólo se pretendía enseñar. En la Universidad de la Plata se pretende enseñar e investigar. En ese sentido, la Secretaría de Ciencia y Técnica debería estar muy desarrollada, debería establecer una fuerte conexión entre las Facultades, no solamente de la Secretaría con las Facultades, sino fomentar especialmente la conexión de las Facultades entre sí, para tratar de desarrollar tareas multidisciplinarias. Las ciencias deberían estar conectadas entre ellas buscando las respuestas que la sociedad requiere. Obviamente sin ciencia sería muy difícil mantener un estilo de vida como el que pretendemos tener. -En los organismos científicos algunas veces parece notarse que la evaluación de las actividades de los investigadores no promueve estos objetivos.&lt;/p&gt;
&lt;p&gt;-&lt;strong&gt;Eitel Peltzer&lt;/strong&gt;: Nosotros no tenemos una cultura de interdisciplinariedad. Es un desafío, como para que organismos como el CONICET, Secyt, FONCYT interpreten que se puedan establecer tanto nuevos mecanismos de funcionamiento entre grupos como evaluaciones que los puedan contemplar.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2007/08/foto1.jpeg&#34; alt=&#34;:left&#34;&gt;
-Para ustedes, en este trabajo, ¿en qué consiste tener éxito?&lt;/p&gt;
&lt;p&gt;-&lt;strong&gt;Ariel Cédola&lt;/strong&gt;: Bueno, estamos contentos, no importa que no seamos tan numerosos, que no tengamos tantos recursos, pero trabajando de esta manera, con la comunión, con el ritmo, con los objetivos a corto y mediano plazo, pienso que el éxito es cuestión de tiempo, está asegurado. La buena relación que tengamos entre nosotros hace que el clima de trabajo sea bueno, que sea con toda seriedad con todo el compromiso, y eso genera una unión muy fuerte.&lt;/p&gt;
&lt;p&gt;-&lt;strong&gt;Marcelo Cappelletti&lt;/strong&gt;: Eitel nos brinda la libertad para trabajar e incursionar sobre los temas que nos interesan, esto lo hace un ambiente muy favorable.&lt;/p&gt;
&lt;p&gt;-Me despido, ha sido un placer estar con ustedes. -&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ariel Cédola&lt;/strong&gt;: Bueno, también para nosotros ha sido un placer. Por último les comentamos que estamos abiertos a las visitas y preguntas que puedan surgir. Saludos cordiales.&lt;/p&gt;
&lt;p&gt;El GEMYDE mantiene colaboraciones con grupos de investigación europeos en Bélgica, Dinamarca y Alemania. En el año 2005 el grupo organizó un seminario internacional sobre métodos predictivos del comportamiento de nuevos materiales a partir de los &amp;quot;primeros principios&amp;quot; o leyes básicas de la Física Cuántica.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Para saber más o comunicarse&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GEMyDE&lt;/strong&gt;: &lt;a href=&#34;http://gemyde.uids.testing.sedici.unlp.edu.ar/&#34;&gt;http://gemyde.uids.testing.sedici.unlp.edu.ar/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ingeniería, UNLP&lt;/strong&gt;: &lt;a href=&#34;http://www.ing.unlp.edu.ar/&#34;&gt;http://www.ing.unlp.edu.ar/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CONAE&lt;/strong&gt;: &lt;a href=&#34;http://www.conae.gov.ar/&#34;&gt;http://www.conae.gov.ar/&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>¿Cómo se formó el Cerro Azul en la provincia de La Pampa?</title>
      <link>https://ciencianet.com.ar/post/como-se-formo-el-cerro-azul-en-la-provincia-de-la-pampa/</link>
      <pubDate>Wed, 08 Aug 2007 01:39:30 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/como-se-formo-el-cerro-azul-en-la-provincia-de-la-pampa/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Graciela Visconti:&lt;/strong&gt; Facultad de Ciencias Exactas y Naturales (UNLPam).&lt;/p&gt;
&lt;p&gt;El estudio sedimentológico de un conjunto de rocas que afloran en el centro-este de la provincia de La Pampa permitió conocer el ambiente en el cual se formaron, durante un lapso de tiempo comprendido entre los 9 y 5 millones de años de antigüedad (Mioceno superior).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2007/08/visconti.jpg&#34; alt=&#34;Fotografía de una de las afloraciones de la formación &#39;Cerro Azul&#39; en la proviancia de La Pampa. En el centro de la imagen puede verse a la autora del estudio.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Este conjunto, de más de 100 metros de espesor, está constituido por limolitas arcillosas y arenosas, y ha sido depositado principalmente por acción del viento. Cuando cesaba la deposición eólica y el clima era más estable, se desarrollaron suelos, constituyendo una sucesión donde alternan capas eólicas con paleosuelos intercalados. En el sector inferior se encontraron algunas capas producidas por lagunas, y en el superior otras dejadas por ríos. Toda la sucesión está coronada por dos metros de toscas.&lt;/p&gt;
&lt;p&gt;Mediante observaciones al microscopio, se pudo establecer que los fragmentos que constituyen estas rocas provienen del sur de Mendoza y norte de Río Negro y Neuquén, que han sido erosionadas y transportadas por el viento. El estudio de los paleosuelos demostró que la vegetación dominante era de gramíneas, con algunos árboles intercalados, semejante a un paisaje de sabana. Los isótopos de carbono permitieron establecer el tipo de fotosíntesis realizada por las plantas y los isótopos de oxígeno las variaciones de temperatura a lo largo del tiempo.&lt;/p&gt;
&lt;p&gt;Con todos estos datos, se dedujeron las condiciones climáticas imperantes y las oscilaciones en la velocidad de sedimentación. Durante la deposición de la sección inferior, el clima fue más cálido y más húmedo y la velocidad de sedimentación menor, con un desarrollo mayor de los paleosuelos. Hacia arriba, el clima se hizo más frío y seco y aumentó la sedimentación de las capas eólicas, mientras que la formación de suelos fue menor.&lt;/p&gt;
&lt;p&gt;Este deterioro climático coincide con lo observado por otros autores que realizaron análisis de isótopos marinos, del contenido de vertebrados fósiles y de variables orbitales.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Comentario de Marcelo Zárate:&lt;/strong&gt; En mi opinión, la investigación que ha desarrollado Graciela Visconti constituye sin duda, una contribución muy importante sobre el origen y composición de los sedimentos que se extienden en gran parte de la provincia de la Pampa y áreas vecinas. El estudio brinda un marco de referencia valioso para interpretar la historia geológica de la región central de Argentina durante el lapso comprendido aproximadamente entre 9 y 5 millones de años.  &lt;/p&gt;
&lt;p&gt;Marcelo Zárate es investigador de CONICET en la Facultad de Ciencias Exactas y Naturales de la Universidad Nacional de La Pampa y formó parte del jurado que evaluó esta tesis.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; Este estudio forma parte de la tesis defendida por Graciela Visconti el 5 de julio de 2007 en la Facultad de Ciencias Exactas y Naturales (UBA), para optar al título de Doctor de la Universidad de Buenos Aires en el área de Geología.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Título:&lt;/strong&gt; &amp;quot;Sedimentología de la Formación Cerro Azul (Mioceno Superior) de la provincia de La Pampa&amp;quot;,&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Director:&lt;/strong&gt; Ricardo Melchor. &lt;strong&gt;Codirector:&lt;/strong&gt; Carlos O. Limarino.&lt;/p&gt;
&lt;p&gt;Copias de la tesis pueden solicitarse directamente a la &lt;a href=&#34;mailto:gvisconti@exactas.unlpam.edu.ar&#34;&gt;autora&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Otros efectos interesantes del Viagra</title>
      <link>https://ciencianet.com.ar/post/otros-efectos-interesantes-del-viagra/</link>
      <pubDate>Mon, 16 Jul 2007 01:42:05 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/otros-efectos-interesantes-del-viagra/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Osvaldo Chara.??&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP).&lt;/p&gt;
&lt;p&gt;El Sildenafil, el principio activo del Viagra®, es conocido por empleárselo en el tratamiento de la disfunción eréctil. Tal y como ha sucedido con otros medicamentos, como la aspirina, se han encontrado efectos nuevos de esta droga. Científicos de la Universidad Nacional de Quilmes acaban de descubrir que el Sildenafil podría ser un candidato a resolver los trastornos ocasionados por los viajes en avión a través del mundo.&lt;/p&gt;
&lt;p&gt;En los mamíferos cada proceso fisiológico transcurre siguiendo su propio ritmo. Sin embargo, estos ritmos no son independientes: están acoplados entre sí. Este acople muestra una suerte de reloj biológico. Un ejemplo de esto lo constituyen las hormonas, mensajeros químicos que son permanentemente intercambiados entre las células en un incesante diálogo. Estos mensajeros son vertidos hacia la sangre con mayor probabilidad en un momento del día que en otro, siguiendo lo que se conoce como ritmo circadiano.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2007/07/Fig1C_sildenafil-768x565.jpg&#34; alt=&#34;Figura: Efecto del sildenafil en la respuesta al cambio lumínico.&#34;&gt;&lt;/p&gt;
&lt;p&gt;En los mamíferos la generación y el entrenamiento del ritmo circadiano reside en una estructura denominada núcleo supraquiasmático hipotalámico. La principal señal que estimula este núcleo, ajustando el reloj biológico, con el transcurrir el tiempo del ambiente es un ciclo luz-oscuridad. En hamsters, la respuesta a un ciclo luz-oscuridad es mediada por una vía de señalización común que produce un mediador denominado GMPc. La activación de este mediador produce, en última instancia la activación de ciertos genes que producen la alteración del reloj biológico. Interesantemente, la concentración de GMPc en el sistema nervioso del hamster sufre variaciones diarias y circadianas. Existe una enzima, la PDE, que hidroliza el GMPc, disminuyendo su concentración. El Sildenafil es un fármaco que inhibe específicamente esta enzima (estrictamente, la isoforma PDE5 de esta enzima).&lt;/p&gt;
&lt;p&gt;Patricia V. Agostino y colaboradores, de la Universidad Nacional de Quilmes, investigaron si el Sildenafil, inhibiendo la PDE, podría incrementar la concentración de GMPc en el sistema nervioso central, provocando algún efecto sobre el entrenamiento del reloj biológico de los hamsters. EL equipo encontró que el empleo de Sildenafil produce un desfasaje en la respuesta frente al cambio de luz a oscuridad (ver Figura). De este modo, este fármaco podría ser un candidato interesante en el tratamiento del jet-lag, este trastorno que aparece cuando volamos en dirección perpendicular a los meridianos terrestres. Como suele ocurrir en Farmacología, el descubrimiento de nuevos efectos en una droga ya conocida tiene la ventaja de que la mayoría de los efectos secundarios de esta ya se conocen.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referencia:&lt;/strong&gt; &lt;a href=&#34;https://doi.org/10.1073/pnas.0703388104&#34;&gt;Agostino, P.V., Plano, S.A., Golombek, D.A. Sildenafil accelerates reentrainment of circadian rhythms alter advancing light schedules. Proceedings of the National Academy of Sciences. 104 (23), 9834-9839. 2007&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nota de CienciaNet&lt;/strong&gt;. Este trabajo fue reconocido con el &lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_Ig_Nobel_Prize_winners#2007&#34;&gt;Ig Nobel&lt;/a&gt;, el 4 de octubre de 2007.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Fí­sicos juegan a los flippers para mejorar procesos industriales</title>
      <link>https://ciencianet.com.ar/post/fisicos-juegan-a-los-flippers-para-mejorar-procesos-industriales/</link>
      <pubDate>Fri, 13 Jul 2007 01:48:17 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/fisicos-juegan-a-los-flippers-para-mejorar-procesos-industriales/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Luis Pugnaloni&lt;/strong&gt;. Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP).&lt;/p&gt;
&lt;p&gt;Un grupo de investigadores publicó un estudio sobre el camino aleatorio que sigue una pelotita que rueda por una mesa inclinada con obstáculos; un fenómeno que importa a los fanáticos de los flippers. Se conoce desde hace más de un siglo que la posición donde cae la pelotita al final de la mesa tiene una distribución de probabilidad con la forma de una campana de Gauss si la pelotita nunca toca los lados de la mesa.&lt;/p&gt;
&lt;p&gt;Este nuevo estudio muestra que para mesas angostas, donde la pelotita inevitablemente choca con los bordes laterales, esta distribución es uniforme. El fenómeno puede ser usado para la mezcla de materiales granulares en la industria.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2007/07/galton-439x600.jpg&#34; alt=&#34;Tablero de Galton: Esta fotografía corresponde al tablero usado por los investigadores de la Universidad de Buenos Aires. Puede observase la disposición regular de los obstáculos.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;El trabajo de J. G. Benito, G. Meglio, I. Ippolito, M. Re y A. M. Vidales, una colaboración entre la Universidad Nacional de San Luis, la Universidad de Buenos Aires y la Universidad Nacional de Córdoba, presenta estudios experimentales, de simulación por computadora y teóricos de un dispositivo conocido desde hace más de un siglo como &amp;quot;el tablero de Galton&amp;quot; (ver foto).&lt;/p&gt;
&lt;p&gt;Un cierto número de clavos clavados a intervalos regulares sobre la mesa entorpecen el camino de una bolita que rueda cuesta abajo sobre la misma. Ya en el siglo XIX el científico británico Sir Francis Galton habí­a mostrado que si uno arroja un gran número de bolitas (una por vez) y las junta en casilleros dispuestos al final de la mesa los casilleros centrales estarían más llenos que los laterales. Esto es, hay mayor probabilidad de que una bolita caiga en un casillero en el centro que el los costados. La forma exacta de la distribución de las bolitas corresponde a una campana de Gauss.&lt;/p&gt;
&lt;p&gt;J. G. Benito y colaboradores estudiaron el efecto que produce el uso de mesas angostas donde las bolitas rebotan contra los laterales. Todos los estudios anteriores fueron hechos con mesas muy anchas. Tanto los experimentos como las simulaciones muestran que para mesas suficientemente angostas (en comparación con la longitud de la misma) las bolitas se distribuyen en forma uniforme al llegar a la base. De este modo la probabilidad de que una bolita caiga en el centro es la misma de que caiga en el borde de la mesa.&lt;/p&gt;
&lt;p&gt;Los autores del trabajo proponen que este efecto puede ser usado para mezclar granos de diferentes tipos en un proceso industrial. Es sabido que conseguir una mezcla uniforme de granos (cereales, piedras, peletes, etc.) es muy difí­cil si los granos difieren un poco en tamaño, forma o caracterí­sticas superficiales. Una de estas mesas podría usarse para hacer fluir simultáneamente granos de diferentes tipos de modo que las colisiones con los obstáculos los mezclen unos con otros.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; &lt;a href=&#34;https://doi.org/10.1007/s10035-007-0036-4&#34;&gt;Granular Matter, vol. 9, pp 159 (2007)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instituciones:&lt;/strong&gt; Laboratorio de Ciencia de Superficies y Medios Porosos, Departamento de Fí­sica (UNSL), Grupo de Medios Porosos, Facultad de Ingenierí­a (UBA), Facultad de Matemá¡tica, Astronomí­a y Fí­sica, (UNC).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; A. Vidales (E-mail: &lt;a href=&#34;mailto:avidales@unsl.edu.ar&#34;&gt;avidales@unsl.edu.ar&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Más información:&lt;/strong&gt; &lt;a href=&#34;http://es.wikipedia.org/wiki/Distribuci%C3%B3n_de_probabilidad&#34;&gt;Distribución de probabilidad&lt;/a&gt;.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Cristales de yeso gigantes en Naica (México)</title>
      <link>https://ciencianet.com.ar/post/cristales-de-yeso-gigantes-en-naica-mexico/</link>
      <pubDate>Fri, 13 Jul 2007 01:44:55 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/cristales-de-yeso-gigantes-en-naica-mexico/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro&lt;/strong&gt;, Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - CIC).&lt;/p&gt;
&lt;p&gt;En el interior de una mina de plata localizada en el estado de Chihuahua (norte de México) y a 290 metros de profundidad, se hallaron 3 cavidades naturales tapizadas de cirstales de yeso (selenita) con longitudes de 10 metros y diámetros de 1 metro. Sin duda estos cristales están entre los más grandes encontrados a nivel mundial.&lt;/p&gt;
&lt;p&gt;La temperatura en el interior de las cavernas alcanza un promedio de 47ºC y el aire está saturado de humedad. La historia de Naica está estrechamente ligada al desarrollo de la minería (principalmente de plata), cuya actividad comenzó hacia fines del 1.800. El yacimiento era conocido a nivel mundial desde 1.910 debido al descubrimiento de una caverna (Cueva de las Espadas) con cristales de yeso de hasta 2 metros de longitud.&lt;/p&gt;
&lt;p&gt;Casi 100 años después, los trabajadores mineros decubrieron otras tres cavernas naturales portadoras de cristales de yeso translúcidos, puros y anormalmente grandes. Las cavernas quedaron al descubierto debido al avance de los trabajos de exploración/explotación propios de mina. Para mantener los trabajos es necesario bombear agua constantemente con el fin de bajar el nivel de agua natural (nivel freático) e impedir de esta manera la inundación de las galerías. Esto hace que la posibilidad de visitar y estudiar los cristales sea efímera.&lt;/p&gt;
&lt;p&gt;Otro problema a salvar son la alta temperatura y humedad del ambiente, para esto se diseñaron trajes especiales refrigerados, los que permiten a los investigadores permanecer en el interior de las cavernas por algunas horas. Recientemente se ideó un proyecto multidisciplinario en el cual participan varias universidades europeas y americanas, con el fin de responder algunas de las numerosas preguntas surgidas luego del descubrimiento, por ejemplo: ¿Cómo y por qué se formaron estos cristales gigantes?, ¿Cuánto tiempo atrás?, ¿Los cristales son estables?, ¿Existe alguna forma de vida en este ambiente?.&lt;/p&gt;
&lt;p&gt;Si bien todavía no comenzaron los estudios experimentales, existen varias hipótesis para explicar la cristalización de yeso en Naica. Una de ellas indica que los cristales se formaron bajo agua, en un punto donde las aguas termales profundas, calientes y saturadas de sulfuros se pusieron en contacto con aguas externas frías y ricas de oxígeno, que se infiltraban naturalmente en la montaña. Estas dos aguas no podían directamente mezclarse entre ellas, debido a la mayor densidad del agua profunda y mineralizada. A lo largo de la superficie de contacto entre estas dos aguas, se verificaba la difusión del oxígeno en el estrato inferior, provocando la oxidación de los iones de sulfuro a sulfato; estos últimos daban lugar a una ligera sobresaturación respecto al yeso, lo cual daba como resultado una lenta cristalización.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Modelo matemático para el tratamiento de quimioterapia sobre tumores cancerosos</title>
      <link>https://ciencianet.com.ar/post/modelo-matematico-para-el-tratamiento-de-quimioterapia-sobre-tumores-cancerosos/</link>
      <pubDate>Wed, 27 Jun 2007 01:54:52 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/modelo-matematico-para-el-tratamiento-de-quimioterapia-sobre-tumores-cancerosos/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Luis Pugnaloni.&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP).&lt;/p&gt;
&lt;p&gt;El cáncer es originado por la mutación en una única célula que exhibe un crecimiento descontrolado, rompiendo la cooperación que mantiene la integridad del organismo. Un modelo matemático permite simular el crecimiento de un tumor de células cancerosas y los efectos que la quimioterapia tiene sobre él.&lt;/p&gt;
&lt;p&gt;El trabajo de S. Menchón y C. Condat, de la Facultad de Matemática, Astronomía y Física de la Universidad Nacional de Córdoba, presenta un modelo mesoscópico de comportamiento de un tumor. Los casilleros de una gran grilla se llenan con números que representan la cantidad de células cancerosas vivas, células cancerosas muertas y nutrientes disponibles en una región del tejido afectado. Una simulación por computadora se ocupa de ir cambiando estos valores según las células se alimentan reproducen, migran de una región a otra y mueren. Las células se reproducen cuando la cantidad de alimentos es abundante y migran o mueren cuando los alimentos escasean. El tejido así simulado es alimentado por un vaso sanguíneo próximo que suministra nutrientes en forma constante.&lt;/p&gt;
&lt;p&gt;La simulación muestra que, en determinadas condiciones, durante su crecimiento, un tumor desarrolla una forma redondeada con células proliferativas (que se reproducen mucho) sobre su superficie dado que allí hay más nutrientes. Hacia en el centro del tumor las células disponen de menos alimento dado que las de la periferia lo consumen rápidamente y por lo tanto no consiguen reproducirse e incluso mueren. El tamaño del tumor crece inicialmente en forma exponencial.  &lt;/p&gt;
&lt;p&gt;Más interesante aún, los autores simulan el efecto de la introducción de una droga mezclada entre los nutrientes que tiene el efecto de inducir la muerte de las células cancerosas. Aquí resulta fundamental el hecho de que algunas células cancerosas suelen ser intrínsecamente resistentes a la droga. Cuando esto sucede, las simulaciones muestran que estas células regeneran el tumor atacado por la droga que ya no será sensible al tratamiento dado que las células que lo forman nacieron de madres resistentes. También es posible que algunas células adquieran resistencia después de que el tumor sea expuesto a la droga a causa de mutaciones genéticas. Las simulaciones muestran que la resistencia adquirida suele ser un mejor mecanismo de defensa del tumor que la resistencia intrínseca ya que los tumores crecen más rápidamente en el primer caso.&lt;/p&gt;
&lt;p&gt;Las dosis de drogas antitumorales se aplican generalmente espaciadas por varios días para minimizar los efectos colaterales que éstas conllevan. En el período comprendido entre dos aplicaciones, si un tumor presenta resistencia (intrínseca o adquirida), el cáncer puede volver a crecer. Aún cuando la siguiente dosis vuelve a matar muchas células, el tumor se ha vuelto más resistente y la terapia comienza a ser menos efectiva. La alternancia de tipos de drogas es una estrategia muy común para combatir la resistencia intrínseca o adquirida.&lt;/p&gt;
&lt;p&gt;La autora de la tesis sugiere que, luego de un desarrollo más completo (como la inclusión de mezclas de drogas), estas simulaciones podrían ser usadas para estudiar diferentes planes de tratamientos quimioterapéuticos según el tipo de tumor de cada paciente para analizar cuál resultaría más efectivo antes de realizar la terapia.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; S. A. Menchón, &lt;a href=&#34;https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=2ahUKEwiP_c3UlfTtAhX5IbkGHerlAJYQFjAAegQIAhAC&amp;amp;url=https%3A%2F%2Fwww.famaf.unc.edu.ar%2Fdocuments%2F1037%2FDFis126.pdf&amp;amp;usg=AOvVaw3_86RGOwaMHYDffuHO2jnt&#34;&gt;&lt;em&gt;Modelado de las diversas etapas del crecimiento del cancer y de algunas terapias antitumorales&lt;/em&gt;&lt;/a&gt;, Tesis doctoral, Universidad Nacional de Córdoba.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instituciones:&lt;/strong&gt; &lt;a href=&#34;http://www.famaf.unc.edu.ar/&#34;&gt;Facultad de Matemática, Astronomía y Física&lt;/a&gt; (UNC)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; S. Menchón (E-mail: &lt;a href=&#34;mailto:silmenchon@gmail.com&#34;&gt;silmenchon@gmail.com&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Más información:&lt;/strong&gt; &lt;a href=&#34;http://es.wikipedia.org/wiki/C%C3%A1ncer_%28enfermedad%29&#34;&gt;Cáncer&lt;/a&gt;, &lt;a href=&#34;http://es.wikipedia.org/wiki/Quimioterapia&#34;&gt;Quimioterapia&lt;/a&gt; .&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>¿Wi Tricity? Transferencia de potencia sin cables</title>
      <link>https://ciencianet.com.ar/post/wi-tricity-transferencia-de-potencia-sin-cables/</link>
      <pubDate>Sat, 16 Jun 2007 01:57:04 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/wi-tricity-transferencia-de-potencia-sin-cables/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Ramiro Irastorza.&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP).&lt;/p&gt;
&lt;p&gt;Imagine un futuro en el que sea posible la transmisión de potencia a través del aire: teléfonos celulares (o móviles), robots hogareños, reproductores de mp3, computadoras personales y otros dispositivos electrónicos portátiles que sean capaces de recargar sus baterías por sí solos sin que tengamos que conectar cable ni cargador alguno. Un equipo del MIT (integrado por el departamento de Física, el de Ingeniería Eléctrica y el de Ciencias de la Computación) en conjunto con el &lt;em&gt;Institute for Soldier Nanotechnologies&lt;/em&gt; (ISN) desarrolló experimentalmente un importante paso en esa dirección. Usando bobinados resonantes fuertemente acoplados, demostraron una transferencia no radiante y eficiente (40%) a distancias de hasta ocho veces el radio de los resonadores (ellos mismos la denominaron WiTricity). Desarrollaron un modelo cuantitativo capáz de describir la transferencia de potencia con errores de solo el 5%.&lt;/p&gt;
&lt;p&gt;La transmisión de potencia sin cable es conocida hace ya varias décadas, quizas el mejor ejemplo sea la radiación electromagnética (ondas de radio). Si bien este tipo de transferencia es buena para la información no es eficiente para la transmisión de potencia. Gran parte de de la misma se perdería en el espacio porque la radiación se emite en todas direcciones. Por el contrario, WiTricity se basa en el uso de objetos resonantes acoplados. Dos objetos resonantes acoplados con la misma frecuencia intercambian energía de manera eficiente (por ejemplo: un transformador), si en cambio no tienen la misma frecuencia de resonancia la interacción es débil. Un ejemplo acústico: imagine una habitación con vasos de vino cada llenados con distintos niveles. Si un cantante de opera emite una nota lo suficientemente alta, será capaz de romper el vaso &amp;quot;afinado&amp;quot; a esa nota mientras que los otros vasos permanecen inadvertidos.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2007/06/witricity.png&#34; alt=&#34;Esquema del dispositivo experimental.&#34;&gt;&lt;/p&gt;
&lt;p&gt;Este grupo de investigadores se concentró en un sistema de dos bobinados de cobre (uno conectado a la fuente y el receptor a una bombilla de 60W, ver Imagen) acoplados fuertemente a través de sus campos magnéticos (a una frecuencia del orden de los MHz).&lt;/p&gt;
&lt;p&gt;Lo interesante de esto es que los medios biológicos interactúan muy debilmente con los campos magnéticos , así como también los objetos no acoplados, por lo que sería una forma segura de transmisión. Además, al ser no radiativa, la energía que no se transfiere al bobinado receptor queda limitada en el bobinado fuente.&lt;/p&gt;
&lt;p&gt;Si bien el principio es conocido hace más de un siglo la demanda de este tipo de transmisión es reciente. No existían dispositivos electrónicos portátiles, laptops, teléfonos celulares, robots, etc. Este tipo de invenciones serviría para recargar las batería de forma automática o, eventualmente, ni si quiera utilizarlas.&lt;/p&gt;
&lt;p&gt;Este artículo fue reportado el 7 de junio en Science Express, la publicación online del journal Science.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fuente:&lt;/strong&gt; &lt;a href=&#34;http://www.sciencedaily.com/releases/2007/06/070607171130.htm&#34;&gt;ScienceDaily&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; André Kurs, Aristeidis Karalis, Robert Moffatt, J. D. Joannopoulos, Peter Fisher, Marin Soljačić. &lt;a href=&#34;https://science.sciencemag.org/content/317/5834/83&#34;&gt;Wireless Power Transfer via Strongly Coupled Magnetic Resonances&lt;/a&gt;. Science, 06 Jul 2007 : 83-86.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Vacuna contra el Chagas... pero para animales domésticos</title>
      <link>https://ciencianet.com.ar/post/vacuna-contra-el-chagas-pero-para-animales-domesticos/</link>
      <pubDate>Fri, 08 Jun 2007 02:02:27 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/vacuna-contra-el-chagas-pero-para-animales-domesticos/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Luis Pugnaloni.&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP).&lt;/p&gt;
&lt;p&gt;El enfermedad de Chagas afecta a todo Latinoamérica y puede producir daños cardíacos, neurológicos y en esófago e intestino grueso en los humanos infectados. Este mal se produce como consecuencia de la picadura de un insecto, la vinchuca, que inocula un parásito, el &lt;em&gt;Trypanosoma cruzi&lt;/em&gt;. Infortunadamente, aún no se ha encontrado una vacuna para seres humanos. Un estudio sobre perros muestra que una vacuna basada en &lt;em&gt;Trypanosoma rangeli,&lt;/em&gt; un parásito que no produce síntomas de Chagas en humanos, aumenta la resistencia contra el &lt;em&gt;Trypanosoma cruzi&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;El trabajo fue realizado por B. Basso, I. Castro, V. Introini, P. Gil y E. Moretti de la Facultad de Ciencias Médicas de la Universidad Nacional de Córdoba y el Servicio Nacional de Chagas con la colaboración de C. Truyens del Laboratorio de Parasitología de la Universidad Libre de Bruselas. Tres perros fueron vacunados con tres dosis de &lt;em&gt;Trypanosoma rangeli&lt;/em&gt; separadas por aproximadamente 30 días una de otras. Luego los perros fueron infectados con &lt;em&gt;Trypanosoma cruzi&lt;/em&gt;. Otros tres perros no vacunados fueron también infectados del mismo modo para comparar la respuesta de los perros vacunados. Los perros fueron tratados siguiendo los estándares de ética para la experimentación con animales y se les suministró antiparasitarios después del experimento.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2007/06/chagas.jpg&#34; alt=&#34;Trypanosoma cruzi: Imagen tomada con un microscopio electrónico del Trypanosoma cruzi rodeado de globulos rojos.:left&#34;&gt; Los investigadores observaron que tanto los perros sin vacunar como los perros vacunados contrajeron la enfermedad de Chagas. Sin embargo los perros vacunados habían desarrollado anticuerpos contra el &lt;em&gt;Trypanosoma cruzi&lt;/em&gt; y tanto el período agudo de la infección como su intensidad se vio reducida gracias a la vacuna. Por otro lado, el equipo de trabajo estudió la infección de vinchucas que se alimentaron de la sangre de estos perros. Se encontró que las vinchucas que se alimentaban de los perros con Chagas que habían sido vacunados tenían menor probabilidad de quedar infectadas que las que se alimentaban de los perros con Chagas que no recibieron la inmunización correspondiente.&lt;/p&gt;
&lt;p&gt;Se conoce que parte del contagio a humanos se debe a vinchucas que se contagiaron primero de perros domésticos portadores de Chagas. Los autores sugieren que esta vacuna aplicada a perros tendría el potencial de reducir el nivel de contagio sobre humanos.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; &lt;a href=&#34;https://doi.org/10.1016/j.vaccine.2007.01.114&#34;&gt;Vaccine, vol. 25, pp 3855 (2007)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instituciones:&lt;/strong&gt; &lt;a href=&#34;http://www.fcm.unc.edu.ar/&#34;&gt;Facultad de Ciencias Médicas&lt;/a&gt; (Universidad Nacional de Córdoba), Servicio Nacional de Chagas, &lt;a href=&#34;http://www.ulb.ac.be/&#34;&gt;Universidad Libre de Bruselas&lt;/a&gt; (Bélgica).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; B. Basso (E-mail: &lt;a href=&#34;mailto:ebi@fcm.unc.edu.ar&#34;&gt;ebi@fcm.unc.edu.ar&lt;/a&gt; )&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Más información:&lt;/strong&gt; &lt;em&gt;&lt;a href=&#34;http://es.wikipedia.org/wiki/Trypanosoma_cruzi&#34;&gt;Trypanosoma cruzi&lt;/a&gt; ,&lt;/em&gt; &lt;em&gt;&lt;a href=&#34;http://encolombia.com/medicina/academedicina/ag-02hgroot.htm&#34;&gt;Trypanosoma rangeli&lt;/a&gt; ,&lt;/em&gt; &lt;a href=&#34;http://es.wikipedia.org/wiki/Enfermedad_de_Chagas&#34;&gt;Enfermedad de Chagas-Mazza&lt;/a&gt; .&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>“Delivery” farmacológico celular: Liposomas cubiertos con proteínas de Lactobacilus</title>
      <link>https://ciencianet.com.ar/post/delivery-farmacologico-celular-liposomas-cubiertos-con-proteinas-de-lactobacilus/</link>
      <pubDate>Sat, 02 Jun 2007 02:05:03 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/delivery-farmacologico-celular-liposomas-cubiertos-con-proteinas-de-lactobacilus/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro&lt;/strong&gt;. Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP) y Universidad Tecnológica Nacional.&lt;/p&gt;
&lt;p&gt;Los liposomas son vesículas lipídicas artificiales ampliamente empleadas como sistemas de liberación de sustancias en el interior de las células. Una suerte de “delivery”. De este modo, estas vesículas son capaces de transportar, por ejemplo, fármacos quimioterapéuticos usados en el tratamiento del cáncer o genes incluidos en plásmidos empleados en terapia génica. Estos liposomas pueden, entonces, ser incorporados a través de la boca por el tracto gastrointestinal y ser absorbidos por las células de interés.&lt;/p&gt;
&lt;p&gt;Cuando la membrana lipídica que forma el liposoma se une con la membrana plasmática de la célula, mediante toda una variedad de procesos, el fármaco ingresa a la célula. En cualquiera de los procesos mediante los cuales esta liberación del fármaco se produce, las propiedades interfaciales del liposoma juegan un papel fundamental.&lt;/p&gt;
&lt;p&gt;Las proteínas de capa-S (PCS) son arreglos de origen proteico que se encuentran en la membrana de bacterias. Se trata de proteínas simples o glucosiladas (unidas a un azúcar) que representan la más simple membrana biológica desarrollada durante la evolución. Estas proteínas tienen la peculiaridad de que muestran comportamientos de ensamblado &lt;em&gt;in vitro&lt;/em&gt;, en superficies interfaciales que incluyen, entre otras, liposomas. Debido a estas propiedades, y al hecho de que pueden ostentar en superficies proteínas y epitopes de anticuerpos, las PCS pueden emplearse para transportar antígenos.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2007/06/liposomas.jpg&#34; alt=&#34;Liposomas con y sin cobertura PCS : Fotografías de microscopía electrónica de liposomas sin cobertura PCS (A y B) y con cobertura PCS (C y D).:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Uno de los problemas del uso de liposomas como transportadores de drogas por vía oral (es decir, cuando el medicamento es administrado por la boca) es que estos deben sobrevivir a los efectos de los medios biológicos del tracto gastrointestinal: terribles cambios de pH, exposición a sales biliares con gran capacidad emulsificante, acción de las enzimas pancreáticas, cambios de temperatura, por nombrar algunas. En un trabajo reciente de A. Hollmann (2007), del Laboratorio de Microbiología Molecular, Departamento de Ciencia y Tecnología de la Universidad Nacional de Quilmes, en colaboración con el Laboratorio de Fisicoquímica de Membranas Lipidicas y Liposomas de la Facultad de Farmacia y Bioquímca de la Universidad de Buenos Aires y el Laboratorio de Microbiología del CIDCA (Universidad Nacional de La Plata) se llevó a cabo la caracterización de la unión de PCS del &lt;em&gt;Lactobacilus brevis&lt;/em&gt; y del &lt;em&gt;Lactobacilus kefir&lt;/em&gt; sobre superficies lipídicas de liposomas, así como la estabilidad resultante de estos.&lt;/p&gt;
&lt;p&gt;En este trabajo se estudió experimentalmente, por primera vez, la estabilidad de los liposomas cubiertos con estas proteínas provenientes de los lactobacilos utilizados en alimentos lacteos. Los factores probados fueron los mencionados arriba: la temperatura (T), acidez (pH), la acción de un extracto pancreático (EP) y el efecto a la presencia de las sales biliares (SB). En primer lugar se midió el potencial Z mediante electroforesis de los liposomas cubiertos con PCS en comparación con liposomas sin cobertura. En la electroforesis se establece un campo eléctrico uniforme gracias al cual las partículas cargadas pueden migrar. El potencial Z da idea de la carga que tienen las partículas que migran. En este caso, las partículas son los liposomas cargados.&lt;/p&gt;
&lt;p&gt;Los resultados mostraron que la cobertura de los liposomas con PCS redujo el potencial Z, mostrando que la carga disminuyó como consecuencia de la interacción entre los lípidos del liposoma y las PCS. Experimentos de microscopía electrónica mostraron que las PCS cubren los liposomas comparados con los liposomas sin cobertura (ver Figura).&lt;/p&gt;
&lt;p&gt;Con el objeto de medir estabilidad se llevaron a cabo experimentos mediante “cargado” de los liposomas (con o sin PCS) con una sustancia fluorescente y se observó cuanto porcentaje podían retener de esta sustancia al cabo de cada tratamiento (T, pH, EP y SB). En los experimentos a distinta T se observó que la presencia de una cobertura con PCS mejoraba la estabilidad de los liposomas. Se observó que a pH gastrointestinal los liposomas cubiertos con PCS muestran más estabilidad que sin cobertura. Finalmente, la estabilidad bajo la acción del extracto pancreático (EP) y de las sales biliares (SB) es mayor sobre los liposomas cubiertos con PCS que sin cobertura. De este modo, este trabajo muestra que liposomas cubiertos con estas PCS podrían sortear las barreras que todo medicamento administrado por vía oral debe superar. Un rasgo adicional, pero no poco importante, es que estas proteínas son extraídas de bacterias de una familia que se emplea cotidianamente en la industria alimenticia.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referencia:&lt;/strong&gt; Hollmann, A., Delfederico, L., Glikmann, G., De Antoni, G., Semorile, L., Disalvo, E. A. Characterization of liposomes coated with S-layer proteins from lactobacilli. &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0005273606003531&#34;&gt;Biochimica et Biophysica Acta. 1768. 393-400. 2007.&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>La corrupción induce ineficiencia en las empresas</title>
      <link>https://ciencianet.com.ar/post/la-corrupcion-induce-ineficiencia-en-las-empresas/</link>
      <pubDate>Fri, 25 May 2007 02:11:24 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/la-corrupcion-induce-ineficiencia-en-las-empresas/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP) y Universidad Tecnológica Nacional.&lt;/p&gt;
&lt;p&gt;Un análisis sobre datos de 80 empresas de electricidad de latinoamérica muestra que aquellas que operan en países más corruptos tienden a ser menos eficientes. Los encargados de organizar las actividades de la empresa emplearían mucho tiempo en &amp;quot;hacer lobby&amp;quot; y descuidarían las áreas técnicas. Para resolver los problemas de producción tienden a contratar más empleados en lugar de organizar mejor el trabajo reduciendo así su eficiencia.&lt;/p&gt;
&lt;p&gt;El estudio fue realizado por E. Dal Bó, de la Universidad de California-Berkeley (EEUU), y M. A. Rossi, de la Universidad de San Andrés. Los autores presentan un modelo matemático donde asumen, por un lado, que cuanto mayor es la corrupción en un país mayores son los beneficios que se obtienen de actividades no directamente ligadas a la producción de bienes y servicios (lobby). Por otro lado, y esto es algo sorprendente, los autores suponen que la corrupción no mejora ni empeora las ganancias de la empresa ni de sus empleados corruptos. Mediante consideraciones matemáticas muy generales se puede demostrar que a mayor corrupción se obtendrá menor eficiencia en la empresa.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2007/05/corrupcion.jpg&#34; alt=&#34;Índice de corrupción en latinoamérica: Cuanto menor es el índice mayor es la corrupción. Valores medios sobre el período 1994-2001 según International Country Risk Guide.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;La idea es simple; si la corrupción es alta, los directivos de una dada empresa preferirán poner esfuerzo en conseguir un aumento de precios haciendo lobby ante funcionarios de gobierno antes que atender el proceso de producción. Como el modelo se basa en empresas de electricidad que están obligadas a proveer toda la energía que demandan los cientes, la falta de organización será suplantada por los directivos contratando más empleados para satisfacer la demanda. De este modo usan cada vez más empleados para proveer la misma cantidad de energía.&lt;/p&gt;
&lt;p&gt;Los autores del trabajo apoyan esta teoría con un conjunto de datos de 80 empresas de electricidad de 13 países latinoamericanos. Mediante un cuidado estudio estadístico pueden mostrar que en los países más corruptos (según diferentes índices internacionales de corrupción) las empresas de electricidad son más ineficientes. El análisis descarta que otros efectos que distraen la atención de los directivos de su función principal de coordinar la producción tengan un impacto que justifique la ineficiencia sin recurrir al argumento de la corrupción.&lt;/p&gt;
&lt;p&gt;Los autores aseguran que el impacto en la economía de un país es muy significativo. Por ejemplo, si Brasil (que tiene una corrupción media) bajara su corrupción a los niveles de Costa Rica (que tiene la más baja corrupción dentro de latinoamérica), podría aumentar un 7% su eficiencia.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; &lt;a href=&#34;https://doi.org/10.1016/j.jpubeco.2006.11.005&#34;&gt;Journal of Public Economics, vol. 91, pp. 939 (2007)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instituciones:&lt;/strong&gt; &lt;a href=&#34;http://www.udesa.edu.ar/&#34;&gt;Universidad de San Andrés&lt;/a&gt; , &lt;a href=&#34;http://www.berkeley.edu/&#34;&gt;Universidad de California-Berkeley&lt;/a&gt; (EEUU).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; M. A. Rossi (E-mail: &lt;a href=&#34;mailto:mrossi@udesa.edu.ar&#34;&gt;mrossi@udesa.edu.ar&lt;/a&gt;), E. Dal Bó (E-mail: &lt;a href=&#34;mailto:dalbo@haas.berkeley.edu&#34;&gt;dalbo@haas.berkeley.edu&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Más información:&lt;/strong&gt; &lt;a href=&#34;http://es.wikipedia.org/wiki/%C3%8Dndice_de_percepci%C3%B3n_de_corrupci%C3%B3n&#34;&gt;Índices de corrupción&lt;/a&gt;, &lt;a href=&#34;http://es.wikipedia.org/wiki/Eficiente&#34;&gt;Ineficiencia&lt;/a&gt;.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Atracción y desorden entre proteínas y fosfolípidos: Un estudio experimental de termodinámica de superficies</title>
      <link>https://ciencianet.com.ar/post/atraccion-y-desorden-entre-proteinas-y-fosfolipidos-un-estudio-experimental-de-termodinamica-de-superficies/</link>
      <pubDate>Wed, 16 May 2007 02:14:07 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/atraccion-y-desorden-entre-proteinas-y-fosfolipidos-un-estudio-experimental-de-termodinamica-de-superficies/</guid>
      <description>
        
          &lt;p&gt;Los fosfolípidos que conforman las membranas celulares son capaces de interaccionar con proteínas pertenecientes a estas mismas membranas y con otras provenientes del exterior o del interior celular. Un ejemplo de estas últimas es una proteína producida por el oncogen &lt;em&gt;c-fos&lt;/em&gt;. La palabra “oncogen” hace referencia a un gen que regula la transcripción de genes involucrados en el crecimiento normal o patológico de las células. Se sabe que este oncogen tiene un papel como modulador de la síntesis (generación) de fosfolípidos.&lt;/p&gt;
&lt;p&gt;Por otro lado, se sabe que esta proteína c-Fos es capaz de “discernir” entre los fosfolípidos de acuerdo a la estructura de la cabeza polar y su carga. Estudios detallados muestran que este “discernimiento” molecular se debería a que la fuerza impulsora termodinámica sería diferente entre cada par (fosfolípido/c-Fos). De este modo, el c-Fos podría participar en la transducción de información molecular a nivel de la membrana de una célula. Se sabe que c-Fos exhibe interacciones no ideales, dependientes de la composición, con dos fosfolípidos: Fosfatidil Colina (PC) y Fosfatidil Inositol -4,5 di-Fosfato (PIP2).&lt;/p&gt;
&lt;p&gt;En el reciente trabajo de Graciela A. Borioli y Bruno Maggio (2006), del Departamento de Química Biológica-CIQUIBIC, Facultad de Ciencias Químicas de la Universidad de Córdoba-CONICET, se propone estudiar el tipo de interacciones que establecen la proteína c-Fos con los lípidos PC y PIP2. Como se verá a continuación, en la interacción entre lípidos y proteínas, algunas veces gana la entropía y otras tantas la entalpía.&lt;/p&gt;
&lt;p&gt;En el trabajo de Borioli y Maggio (2006) se analizó en detalle la termodinámica de superficie de films formados con mezclas binarias de c-Fos con PIP2 y con PC en diferentes proporciones evaluando la energía libre superficial, el empaquetado molecular, la composición lípido-proteína y la electrostática superficial. La disección de la energía libre en dos componentes (entropía y entalpía) permite profundizar en la energética de las interacciones proteína-lípido en una superficie. La entropía da una idea del desorden del sistema proteína-lípido mientras que la entalpía da una idea de la fuerza de la interacción entre la proteína y los lípidos.&lt;/p&gt;
&lt;p&gt;Los autores realizaron experimentos en los que se medía tanto el potencial eléctrico superficial como la variación de presión superficial necesaria para expandir o comprimir un área de fosfolípido esparcido sobre la superficie de una solución, a temperatura constante. Encontraron que el comportamiento de las mezclas lípido/c-Fos se desvían de la idealidad, dependiendo del lípido, de la historia del sistema y de la proporción de la proteína en el film. Borioli y Maggio calcularon la energía libre, entropía y entalpía superficiales ideales y de mezcla. A partir de estas obtuvieron la energía libre, entropía y entalpía superficiales de exceso. Los pares proteína/lípido mostraron histéresis; es decir que la energía libre de exceso superficial durante la compresión era distinta que durante la expansión. Interesantemente, tanto la entalpía como la entropía de exceso superficial mostraron un comportamiento siempre creciente con la concentración del fosfolípido cuando este era PC y un comportamiento más complicado cuando este era PIP2 (ver Figura 1).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2007/05/Fig4Langmuir.jpg&#34; alt=&#34;Energía libre de exceso de la mezcla de c-Fos y fosflolídos: En la parte superior de la figura se muestra el componente entálpico de la energía libre de la mezcla c-Fos y PC (A) o PIP2 (B). En la parte inferior de la figura se muestra el componente entrópico de la energía libre de exceso de la mezcla c-Fos y PC (C) o PIP2 (D).:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;El balance entre entropía y entalpía de exceso fue diferente en el par c-Fos/PC comparado con el par c-Fos/PIP2. En efecto, la interacción electrostática entre c-Fos y PIP2, “pagada” por la interacción electrostática dipolar entre las dos moléculas, cubrió holgadamente el ordenamiento entrópico desfavorable. Por el contrario, el desorden configuracional generado por el par c-Fos y PC supera la interacción entre estas dos moléculas.&lt;/p&gt;
&lt;p&gt;Los hallazgos experimentales de los autores indicarían que las interacciones entre c-Fos y los fosfolípidos involucrarían diferentes compensaciones entálpicas-entrópicas que podrían dar cuenta del comportamiento experimental observado. De este modo, el par c-Fos/fosfolípidos emergería como una poderosa, sensible y selectiva maquinaria molecular auto-organizada bidimensional capaz de traducir cambios estructurales interfaciales.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; Borili, G.,A. &amp;amp; Maggio, B. &lt;em&gt;Surface thermodynamics reveals selective structural information storage capacity of c-Fos-phospholipid interactions.&lt;/em&gt; &lt;a href=&#34;http://pubs3.acs.org/acs/journals/doilookup?in_doi=10.1021/la0525168&#34;&gt;Langmuir, 22. 1775-1781. 2006.&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Las señales de Calcio intracelular: Un lenguaje en la intimidad de las células. Un estudio matemático</title>
      <link>https://ciencianet.com.ar/post/las-senales-de-calcio-intracelular-un-lenguaje-en-la-intimidad-de-las-celulas-un-estudio-matematico/</link>
      <pubDate>Fri, 27 Apr 2007 02:21:49 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/las-senales-de-calcio-intracelular-un-lenguaje-en-la-intimidad-de-las-celulas-un-estudio-matematico/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Osvaldo Chara.&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP).&lt;/p&gt;
&lt;p&gt;La dinámica del Calcio intracelular provee un muy interesante ejemplo en el cual la controversia “simple” versus “complejo” puede ser investigada. En la ciencia pueden verse dos casos límites en la exploración del conocimiento. Estos dos paradigmas pueden encontrarse en la física y en la biología. Clásicamente, la física suele llevar a cabo aproximaciones que tienen una tendencia hacia la simplicidad. Por el contrario, las aproximaciones de la biología suelen tener una tendencia a la complejidad.&lt;/p&gt;
&lt;p&gt;Una manera de establecer una suerte de puente entre estos dos tipos de aproximaciones (y entre estas dos disciplinas) es llevar a cabo modelización matemática inspirándose en problemas físicos pero en el contexto de un problema biológico. La idea, simplemente, es llevar a cabo un modelo matemático, descrito por alguna o algunas expresiones matemáticas, que de cuenta del fenómeno biológico en estudio. Recientemente, Alejandra Ventura y colaboradores (2006), en el grupo de Silvina Ponce Dawson, en el Departamento de Física de la Facultad de Ciencias Exactas y Naturales, UBA estudiaron el calcio intracelular siguiendo esta línea de pensamiento.&lt;/p&gt;
&lt;p&gt;El calcio es un ión que aparece como un mensajero intracelular. Ciertos eventos extracelulares generan cambios en la concentración de este ión que son interpretados por la maquinaria celular promoviendo comportamientos diversos. De este modo, el Calcio participa de un “lenguaje intracelular”. Una forma de comprender este lenguaje que es hablado permanentemente por las estructuras de nuestras células es hacer un modelo matemático que de cuenta del comportamiento celular observado.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2007/04/ventura.jpg&#34; alt=&#34;Calcio intracelular: Se muestran las fuentes y sumideros del catión.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Calcio intracelular:&lt;/strong&gt; La concentración de este ión libre resulta de la combinación de los aportes provenientes de bombas, transportadores, reservorios o &lt;em&gt;buffers&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Ahora bien, el calcio puede ingresar a las células a través de canales de calcio insertados en la membrana plasmática o en membranas intracelulares. Una vez dentro del citoplasma el calcio puede difundir o unirse a estructuras móviles o inmóviles que pueden atraparlo por un tiempo breve y luego soltarlo nuevamente. Estas estructuras son generalmente proteínas y se denominan en inglés &lt;em&gt;buffers&lt;/em&gt; (podríamos traducirlos como amortiguadores, pues en virtud de su capacidad de almacenar calcio podrían amortiguar los cambios de la concentración intracelular de calcio). Por otro lado pueden ser exportadas hacia fuera de las células mediante otras proteínas llamadas transportadores o bombas. De este modo, hacer un modelo matemático de la concentración intracelular de calcio en todo el espacio interior de la célula en función del tiempo implica incorporar todos estos procesos. Para ello se puede emplear un sistema de ecuaciones llamadas de Reacción-Difusión, como ya fuera llevado a cabo previamente, incorporando con sumo detalle todos y cada uno de los procesos comentados.&lt;/p&gt;
&lt;p&gt;El problema, como puede imaginarse, es que el modelo puede tornarse enormemente complicado. La pregunta que intentan responder los autores es ¿Cuánto detalle deberían incorporar en un modelo para extraer información cuantitativa de los experimentos? Los autores proponen que modelos muy simples – que no incorporen demasiado detalle de los procesos arriba mencionados – podrían reproducir no sólo información cualitativa sino también cuantitativa. Ahora bien, ¿cómo construir modelos “simples”? Una forma que ya fuera explorada previamente, es reducir un modelo complicado a uno más sencillo. Otra forma es llevar a cabo modelos conducidos por experimentos (&lt;em&gt;data-driven models&lt;/em&gt; en inglés). Ventura y colaboradores llevaron a cabo este tipo de modelización para estudiar las “señales de Calcio”, es decir, el cambio de la concentración intracelular del calcio como respuesta a la entrada del propio Calcio a la célula.&lt;/p&gt;
&lt;p&gt;Las señales de calcio pueden tener distintas formas y los modelos conducidos por experimentos deben incorporar estas formas con funciones o, al menos, valores de parámetros diferentes a partir de soluciones individuales. La ventaja práctica de esta aproximación a las señales de calcio es obtener la amplitud y las cinéticas de la concentración de calcio intracelular que corresponde a cada observación experimental. De este modo la función que describe la entrada de Calcio a la célula no se propone &lt;em&gt;a priori&lt;/em&gt;, sino que se obtiene por ajuste de los datos experimentales.&lt;/p&gt;
&lt;p&gt;Los autores muestran que un sencillo modelo, cuyos valores de los parámetros se ajustan a partir de los datos experimentales, puede reproducir ciertas señales de calcio observadas experimentalmente. Interesantemente, este modelo no solo describe cualitativamente sino también cuantitativamente resultados experimentales, intentando conciliar la aparente dicotomía de “simple” versus “complejo”.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; Ventura, A.C., Bruno, L. &amp;amp; Ponce Dawson, S. 2006. Simple data-driven models of intracellular calcium dynamics with predictive power. &lt;a href=&#34;https://doi.org/10.1103/PhysRevE.74.011917&#34;&gt;Phys. Rev. E 74, 011917-1:11&lt;/a&gt;.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>El Oxígeno y las Hemo-proteínas: una danza estudiada por simulación</title>
      <link>https://ciencianet.com.ar/post/el-oxigeno-y-las-hemo-proteinas-una-danza-estudiada-por-simulacion/</link>
      <pubDate>Wed, 25 Apr 2007 02:25:31 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/el-oxigeno-y-las-hemo-proteinas-una-danza-estudiada-por-simulacion/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Osvaldo Chara&lt;/strong&gt;. Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP).&lt;/p&gt;
&lt;p&gt;Las hemo-proteínas son, como su nombre lo indica, proteínas, que poseen un grupo químico llamado hemo que se encuentran muy difundidas en todos los organismos vivos. Se encargan de una amplia variedad de tareas: desde el transporte de electrones hasta oxidación de moléculas orgánicas y el sensado y transporte de pequeñas moléculas que suelen estar en estado gaseoso. Entre estas, principalmente, Oxígeno (O2), Monóxido de Carbono (CO) y Monóxido de Nitrógeno (NO).&lt;/p&gt;
&lt;p&gt;El Oxígeno es el más abundante de estos tres ligandos (esta palabra hace referencia a que la Hemo-proteína establece una suerte de “ligadura” con cada uno de estos compuestos) y, al mismo tiempo, es el que tiene la menor afinidad por el grupo hemo libre (es decir, el grupo hemo sin formar parte de una Hemo-proteína). De este modo, la sutil regulación de la afinidad del Oxígeno es entonces uno de los problemas clave para determinar la función de una hemo-proteína.&lt;/p&gt;
&lt;p&gt;Un trabajo reciente de Luciana Capece y colaboradores (2006), del grupo de Darío Estrin, en el INQUIMAE, Facultad de Ciencias Exactas y Naturales, UBA abordó este problema empleando simulación por Dinámica Molecular. En la mayoría de las hemo-proteínas el sitio activo, dónde se une el Oxígeno, consiste en una cavidad ubicada sobre el grupo hemo, conocida como bolsillo distal (&lt;em&gt;distal pocket&lt;/em&gt; en inglés). El grupo hemo posee un átomo de hierro coordinado ecuatorialmente por los cuatro nitrógenos del macrociclo porfirínico que conforman el anillo hemo y, axialmente a un quinto nitrógeno o ligando proximal bajo el anillo hemo. Este ligando proximal puede ser uno de los siguientes tres aminoácidos: histidina (Hys), cisteina (Cys) o tirosina (Tyr).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2007/04/figura1.png&#34; alt=&#34;Figura 1.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Trabajos previos investigaron la influencia de residuos en la región distal que establecían puentes de hidrógeno con el uno de los átomos de Oxígeno de la molécula de Oxígeno (ver parte superior de la Figura 1). Se ha encontrado que, en general, cuanto mayor es la posibilidad de que el Oxígeno pueda interaccionar con un residuo distal, menor es la tasa de disociación de la molécula de Oxígeno con la Hemo-proteína, y, por lo tanto, mayor sería la afinidad. No obstante, hay una excepción interesante: la leg-hemoglobina (Lb).&lt;/p&gt;
&lt;p&gt;Al igual que las hemo-proteínas que encontramos en mamíferos, como la mioglobina (Mb), la Lb - que se encuentra en las leguminosas - posee una histidina en la cavidad distal. Ahora bien, un reemplazo de esta histidina por un residuo que no pueda formar un puente de hidrógeno disminuye 100 veces la afinidad del Oxígeno por la Mb, mientras que no cambia prácticamente la afinidad del Oxígeno por la Lb. Posiblemente, la causa de esta diferencia entre el cambio de afinidad entre el oxígeno y la hemo-proteína se deba a la interacción con residuos proximales (ver parte inferior de la Figura 1). El estudio de este problema es, en principio, accesible por simulación computacional.&lt;/p&gt;
&lt;p&gt;Para ciertos fenómenos que no involucran formación o ruptura de enlaces covalentes se pueden emplear simulaciones de dinámica molecular empleando una función de energía potencial clásica Este tipo de simulación consiste, esencialmente, en resolver las famosas ecuaciones en las que están escritas las leyes de Newton para cada uno de los átomos de las moléculas que conforman el sistema a estudiar: la hemo-proteína y el Oxígeno. Por otra parte, es preciso emplear herramientas basadas en la Mecánica Cuántica para describir la energía potencial en situaciones en las cuales si existe formación o ruptura de enlaces químicos.&lt;/p&gt;
&lt;p&gt;El empleo de estas herramientas está limitado por razones computacionales en el tamaño de los sistemas a ser estudiados a unas pocas decenas de átomos, por lo que resultaría imposible modelar una proteína. Por esta razón, el grupo de Darío Estrin investigó este problema utilizando un esquema que emplea un modelo de energía potencial híbrido que combina la mecánica cuántica para el sitio activo de la proteína y un potencial clásico para el resto de la misma (QM-MM). Empleando estas herramientas, el grupo de Darío Estrin pudo poner de manifiesto que las interacciones debidas a los residuos proximales pueden “sintonizar” la afinidad del Oxígeno por la hemo-proteína.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referencia:&lt;/strong&gt; Capece, L., Marti, M.A., Crespo, A., Doctorovich, F &amp;amp; Estrin, D.A. 2006. Heme Protein Oxygen affinity regulation exerted by proximal effects. &lt;a href=&#34;https://doi.org/10.1021/ja0620033&#34;&gt;J. Am. Chem. Soc. 128: 12455-12461.&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Adsorción de proteínas del suero sanguíneo a nanotubos de carbono</title>
      <link>https://ciencianet.com.ar/post/adsorcion-de-proteinas-del-suero-sanguineo-a-nanotubos-de-carbono/</link>
      <pubDate>Tue, 24 Apr 2007 02:34:12 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/adsorcion-de-proteinas-del-suero-sanguineo-a-nanotubos-de-carbono/</guid>
      <description>
        
          &lt;p&gt;Investigadores de la Universidad de Córdoba estudian como se adhieren y despegan proteínas del suero sanguíneo a nanotubos de carbono que son usados para aplicaciones médicas. El trabajo de L. E. Valenti y C. E. Giacomelli del Departamento de Fisicoquímica de la Universidad de Córdoba, en colaboración con P. A. Fiorito de la Universidad de San Pablo y C. D. García de la Universidad de Texas, muestra cómo la técnica de reflectometría puede usarse para analizar el proceso por el cual la albumina de suero bovino (BSA por sus siglas en inglés) se absorbe a (y desorbe de) las paredes de un nanotubo de carbono.&lt;/p&gt;
&lt;p&gt;Los nanotubos de carbono son tubos de tamaño molecular (alrededor de 1 nm de diámetro) que son muy livianos, resistentes y presentan una gran superficie disponible al contacto con moléculas en solución. Estos nanotubos están empezando a ser usados en aplicaciones biomédicas para tratamiento y diagnóstico de enfermedades. La mayoría de estas aplicaciones hacen uso de la capacidad de estos materiales de adherir proteínas a su superficie y luego liberarlas cuando sea necesario.&lt;/p&gt;
&lt;p&gt;L. E. Valenti y colaboradores han observado que las paredes de nanotubos de carbono son capaces de absorber hasta el doble de BSA comparado con superficies de silicio. La principal razón para esto es que la superficie de los nanotubos de carbono son hidrófobas (desfavorecen el contacto con el agua) por lo que es más favorable que las proteínas se acomoden entre el agua de la solución y las paredes de carbono. Aún cuando el pH de la solución de BSA es cambiado para promover una repulsión electrostática entre los nanotubos y las proteínas la absorción es muy eficiente gracias el efecto hidrófobo.&lt;/p&gt;
&lt;p&gt;Sin embargo, las proteínas cargadas eléctricamente se repelen entre si y no pueden amontonarse mucho en la superficie del nanotubo con lo que no se alcanzan tan altas cantidades de absorción de BSA si el pH es distinto de 4.8. Los autores comentan que los resultados de este trabajo tienen impacto en el desarrollo de biosensores, productos farmacéuticos e implantes ortopédicos.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; &lt;a href=&#34;https://doi.org/10.1016/j.jcis.2006.11.046&#34;&gt;Journal of Colloid and Interface Science, vol. 307, pp. 349 (2007)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Instituciones:** &lt;a href=&#34;http://www.fcq.unc.edu.ar/site/todo.htm&#34;&gt;Departamento de Fisicoquímica de la Universidad de Córdoba&lt;/a&gt; , &lt;a href=&#34;http://www2.usp.br/portugues/index.usp&#34;&gt;Universidad de San Pablo&lt;/a&gt; (Brasil), &lt;a href=&#34;http://www.utexas.edu/&#34;&gt;Universidad de Texas&lt;/a&gt; (EEUU).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; C. E. Giacomelli (E-mail: &lt;a href=&#34;mailto:giacomel@fcq.unc.edu.ar&#34;&gt;giacomel@fcq.unc.edu.ar&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Más información:&lt;/strong&gt; &lt;a href=&#34;http://es.wikipedia.org/wiki/Nanotubo&#34;&gt;Nanotubos de carbono&lt;/a&gt; , &lt;a href=&#34;http://es.wikipedia.org/wiki/Albumina&#34;&gt;albúmina&lt;/a&gt;.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>La forma de las nanopartículas afecta el modo en que fluyen</title>
      <link>https://ciencianet.com.ar/post/la-forma-de-las-nanoparticulas-afecta-el-modo-en-que-fluyen/</link>
      <pubDate>Mon, 16 Apr 2007 02:35:30 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/la-forma-de-las-nanoparticulas-afecta-el-modo-en-que-fluyen/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP) y Universidad Tecnológica Nacional.&lt;/p&gt;
&lt;p&gt;Nanopartículas con forma de filamento pueden circular en la sangre aproximadamente 10 veces más tiempo que sus contrapartes esféricas, según el trabajo de investigadores en US. Este resultado podría ser importante para el mejor diseño de sistemas para la entrega de drogas en tratamientos contra el cáncer. De hecho, resultados preliminares muestran que las &amp;quot;filomiscelas&amp;quot; nanométricas puede efectivamente entregar la droga contra el cáncer paclitaxel y reducir tumores de origen humano en ratones.&lt;/p&gt;
&lt;p&gt;Aunque los científicos han estudiado extensamente la forma en que partículas esféricas interactúan con células biológicas y en animales, no han prestado mucha atención a cómo se comportan partículas no esféricas (con la excepción de los nanotubos de carbono).&lt;/p&gt;
&lt;p&gt;Para analizar esto, Dennis Discher y colegas de la Universidad de Pennsylvania inyectaron arreglos de miscelas poliméricas de escala nanométrica, o filomiscelas, en las venas de las colas de ratas y siguieron el movimiento de los filamentos utilizando imágenes fluorescentes. Los investigadores encontraron que una gran fracción de las filomiscelas circularon en el flujo sanguíneo hasta por una semana luego de haber sido inyectadas. Esto es aproximadamente 10 veces más que nanopartículas esféricas u otras estructuras rígidas, como nanotubos de carbono, que son eliminadas del cuerpo típicamente en unas pocas horas, y son también más persistentes que cualquier otra nanopartícula sintética conocida.&lt;/p&gt;
&lt;p&gt;Los científicos encontraron también que las filomiscelas cargadas con paclitaxel pueden reducir tumores en los ratones, siendo más efectivos los cilindros más largos a una dosis dada. De acuerdo con el equipo, estos resultados destacan los efectos de la forma de las nanopartículas en los sistemas biológicos. El trabajo es también el primero en demostrar el uso efectivo de nanopartículas no esféricas para la entrega de drogas, dice Discher.&lt;/p&gt;
&lt;p&gt;&amp;quot;Como algunos virus naturales, nanotransportes biocompatibles pueden ser formados como filamentos&amp;quot;, explicó Discher. &amp;quot;Los filamentos se estiran en flujos, tal como en el sanguíneo, y parecen ser los transportes sintéticos de mayor circulación que se han hecho. Esto significa que pueden exponer el cuerpo a compuestos terapéuticos por mucho más tiempo, incrementando de este modo su efectividad&amp;quot;.&lt;/p&gt;
&lt;p&gt;Discher dijo a nanotechweb.org que si equipo está ahora desarrollando filamentos para transportar otros compuestos terapéuticos y grupos funcionales &amp;quot;targeting&amp;quot; que dirigirán la terapia de genes específicamente a células madre, así como a tumores y otras enfermedades.&lt;/p&gt;
&lt;p&gt;Los investigadores informaron su trabajo en &lt;a href=&#34;http://www.nature.com/nnano/journal/v2/n4/abs/nnano.2007.70.html&#34;&gt;Nature Nanotechnology&lt;/a&gt; 2, 249 - 255 (2007).&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Almidón de amaranto precocido con aire caliente</title>
      <link>https://ciencianet.com.ar/post/almidon-de-amaranto-precocido-con-aire-caliente/</link>
      <pubDate>Sat, 31 Mar 2007 02:37:01 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/almidon-de-amaranto-precocido-con-aire-caliente/</guid>
      <description>
        
          &lt;p&gt;Investigadores de cuatro universidades argentinas estudian las propiedades fisicoquímicas que se pueden dar a una harina de amaranto cruda usando un proceso de precocido en seco. El amaranto es rico en almidón y proteínas y era muy usado por las culturas precolombinas.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2007/03/Amaranthus_cruentus1.jpg&#34; alt=&#34;Amaranthus cruentus: especie cultivada en Río Cuarto para la producción de la harina. Imagen de Kurt Stüber CC BY-SA 3.0:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;R. Gonzalez, E. Tosi, E. Ré, M. C. Añón, A. M. R. Pilosof y K. Martinez usaron harina de amaranto convenientemente molida para obtener un alto contenido de almidón y precocieron la harina con aire a alta temperatura (alrededor de 200 grados centígrados) durante 18 segundos usando un bajo contenido de humedad. Luego realizaron estudios detallados de propiedades fisicoquímicas de la harina como la solubilidad en agua, la capacidad de absorción de agua (fundamental a la hora del amasado), la capacidad de formar geles y las propiedades elásticas y viscosas de estos geles.&lt;/p&gt;
&lt;p&gt;La harina de amaranto que se utiliza como materia prima es obtenida por la implementación de un sistema de molienda y separación denominado &lt;em&gt;molienda diferencial&lt;/em&gt; desarrollado en el CIDTA. Con la aplicación de esta técnica se obtienen tres harinas: una rica en almidón, otra rica en fibra dietética y una tercera rica en proteínas. La primera de estas es usada para la cocción con aire caliente.&lt;/p&gt;
&lt;p&gt;Los investigadores consiguieron obtener harinas de amaranto de alto contenido de almidón con baja solubilidad en agua y de carácter sólido y alta consistencia después de cocidas. Estas propiedades son especialmete deseables en aplicaciones industriales. Por otro lado, el cocido en seco es muy rápido y evita el proceso de precocción húmeda que requiere un posterior secado para su posterior procesamiento y comercialización.&lt;/p&gt;
&lt;p&gt;Los resultados del estudio aportan información fundamental sobre los procesos industriales que pueden resultar más eficientes para obtener un producto alimenticio comercial con las características buscadas por consumidores. Este trabajo da un paso más hacia la producción industrial de harina de amaranto. **&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; Food Chemistry 103 (2007) 927&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instituciones:&lt;/strong&gt; Instituto de Tecnología de Alimentos (Fac. Ing. Química, UNL), CIDTA (Fac. Reg. Rosario, UTN), CIDCA (UNLP–CONICET), Dpto. Industrias (Fac. Cs. Ex. Nat., UBA).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; E. Tosi (E-mail: : &lt;a href=&#34;mailto:etosi@ciudad.com.ar&#34;&gt;etosi@ciudad.com.ar&lt;/a&gt;)&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Reptiles moleculares</title>
      <link>https://ciencianet.com.ar/post/reptiles-moleculares/</link>
      <pubDate>Wed, 28 Mar 2007 02:39:00 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/reptiles-moleculares/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Luis Pugnaloni.&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP).&lt;/p&gt;
&lt;p&gt;Un estudio teórico muestra cómo moléculas largas en forma de cadena reptan como gusanos al ser puestas bajo un campo eléctrico. Un modelo muy simple de cuentas de collar que se mueven a lo largo de una recta ayuda a analizar la velocidad con que repta una molécula polimérica bajo la acción de un campo eléctrico. El trabajo de G. Terranova, H. O. Mártin y C. M. Aldao, investigadores de la Universidad Nacional de Mar del Plata y del CONICET, presenta resultados analíticos para un modelo muy simple de una cadena de partículas de longitud arbitraria que se mueve en una red unidimensional bajo la acción conjunta de una fuerza externa y de la agitación browniana.&lt;/p&gt;
&lt;p&gt;Los investigadores consideran tres casos: cuando la fuerza es aplicada a todas las partículas (&amp;quot;uniforme&amp;quot;), cuando la fuerza tíra desde la cabeza (&amp;quot;de tiro&amp;quot;) y cuando la fuerza empuja desde la cola del polímero (&amp;quot;de empuje&amp;quot;). El primer caso representaría una molecula bajo un campo eléctrico que posee cargas distribuídas a lo largo de la cadena. Los dos restantes corresponden a moléculas donde la carga está concentrada en un extremo.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2007/03/reptiles_moleculares.jpg&#34; alt=&#34;Reptación: Una molécula de tres unidades funcionales se estira y encoge mientras avanza de izquierda a derecha.:left&#34;&gt; Los resultados muestran que para el caso &amp;quot;uniforme&amp;quot; la velocidad del polímero no depende de la longitud de la cadena, mientras que para los casos &amp;quot;de tiro&amp;quot; &amp;quot;de empuje&amp;quot; las cadenas más cortas se mueven más rápido bajo la acción de la fuerza externa. Este efecto se debe a que cuando la cabeza se mueve un paso necesita que todo el resto de la molécula se acomode para poder dar el siguiente; cuanto más larga la molécula más tiempo lleva el proceso.&lt;/p&gt;
&lt;p&gt;Otro resultado intersante es que para el caso &amp;quot;uniforme&amp;quot; la velocidad de reptación es directamente proporcional a la fuerza aplicada. Para los casos &amp;quot;de tiro&amp;quot; o &amp;quot;de empuje&amp;quot; la velocidad crece con la fuerza aplicada pero llega a un valor máximo límite para fuerzas muy grandes. El estudio proporciona ecuaciones simples para un modelo básico del problema de reptación bajo una fuerza externa que es fundamental en aplicaciones tecnológicas como la electroforesis usada para separar moléculas por tamaños en laboratorios de todo el mundo.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; &lt;a href=&#34;https://doi.org/10.1103/PhysRevE.74.021116&#34;&gt;Physical Review E 74, 021116 (2006)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instituciones:&lt;/strong&gt; &lt;a href=&#34;http://www.intema.gov.ar/&#34;&gt;INTEMA&lt;/a&gt; , &lt;a href=&#34;http://www.mdp.edu.ar/&#34;&gt;Dpto. de Física UNMDP&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Más información:&lt;/strong&gt; &lt;a href=&#34;http://es.wikipedia.org/wiki/Pol%C3%ADmero&#34;&gt;Polímeros&lt;/a&gt; , &lt;a href=&#34;http://prola.aps.org/abstract/PRL/v62/i24/p2877_1&#34;&gt;Modelo del reptón&lt;/a&gt; , &lt;a href=&#34;http://es.wikipedia.org/wiki/Electroforesis&#34;&gt;Electroforesis&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Nanosondas calentadas destruyen células de cáncer de mama en ratones</title>
      <link>https://ciencianet.com.ar/post/nanosondas-calentadas-destruyen-celulas-de-cancer-de-mama-en-ratones/</link>
      <pubDate>Sun, 18 Mar 2007 02:41:23 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/nanosondas-calentadas-destruyen-celulas-de-cancer-de-mama-en-ratones/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro.&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP) y Universidad Tecnológica Nacional.&lt;/p&gt;
&lt;p&gt;En experimentos con ratones de laboratorio que llevan un agresivo cáncer de mama humano, investigadores de la Universidad de California (UC Davis) utilizaron nanosondas calientes para disminuir el crecimiento de tumores sin dañar el tejido sano que los rodean. Los investigadores describen su trabajo en la edición de marzo del &lt;a href=&#34;http://jnm.snmjournals.org/cgi/content/abstract/48/3/437?maxtoshow=&amp;amp;HITS=10&amp;amp;hits=10&amp;amp;RESULTFORMAT=&amp;amp;author1=DeNardo&amp;amp;searchid=1&amp;amp;FIRSTINDEX=0&amp;amp;sortspec=relevance&amp;amp;resourcetype=HWCIT&#34;&gt;Journal of Nuclear Medicine&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2017/12/nanosondas-CN.jpg&#34; alt=&#34; Nanosonda: (ScienceDaily).:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;&amp;quot;Hemos demostrado que el sistema es factible en ratones de laboratorio. El próximo paso será la prueba clínica en pacientes&amp;quot;, dice Sally DeNardo, proesora de medicina interna y radiología en UC Davis y autora principal del estudio.&lt;/p&gt;
&lt;p&gt;Muchos investigadores han estudiado el calor como tratamiento potencial del cáncer, pero la dificultad de confinar calor dentro del tumor y la predicción de una dosis de calor efectiva han limitado su uso. La investigación en UC Davis, realizada en colaboración con científicos de Triton BioSystems en Boston, pareciera haber resuelto estos problemas.&lt;/p&gt;
&lt;p&gt;El sistema experimental utiliza biosondas creadas uniendo nanoesferas magentizadas de óxido de hierro con anticuerpos monoclonales radiomarcados. Las biosondas son camufladas en polímeros y azúcares que las hacen casi invisibles al sistema inmunológico corporal.&lt;/p&gt;
&lt;p&gt;DeNardo y sus colegas inyectaron miles de millones de sondas (más de 10.000 pueden ubicarse en la cabeza de un alfiler) en los flujos sanguíneos de los ratones de laboratorio que portan tumores de cáncer de mama humanos. Una vez en el flujo sanguíneo, las sondas localizan y se adhieren a receptores en la superficie de las células malignas.&lt;/p&gt;
&lt;p&gt;Tres días después, el equipo aplicó un campo magnético alterno (CMA) a la región del tumor, causando que las nanoesferas magnéticas adheridas a las células del tumor cambien su polaridad miles de veces por segundo, generando calor instantáneamente. Tan pronto como el CMA se detiene, las biosondas se enfrían.&lt;/p&gt;
&lt;p&gt;Los ratones del estudio recibieron series de ráfagas de CMA en tratamientos únicos de 20 minutos. La dosis fue calculada utilizando una ecuación que incluye la concentración de biosondas en el tumor, la velocidad de calentamiento de las partículas a diferentes amplitudes, y el intervalo entre las ráfagas de CMA.&lt;/p&gt;
&lt;p&gt;La velocidad de crecimiento del tumor disminuyó en los animales tratados, una respuesta estrechamente correlacionada con la dosis de calor. No se observó ninguna toxicidad relacionada con las biosondas.&lt;/p&gt;
&lt;p&gt;&amp;quot;El uso de calor para matar células cancerígenas no es un concepto nuevo&amp;quot;, dice DeNardo.&amp;quot;Los mayores problemas han sido cómo aplicarlo solo al tumor, cómo predecir la cantidad necesaria y cómo determinar su efectividad. Combinando nanotecnología, la terapia focalizada de CMA y técnicas de imagen molecular cuantitativas, hemos desarrollado una técnica segura que podría combinarse con otras modalidades para el tratamiento del cáncer de mama y otros tipos de cáncer&amp;quot;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fuente:&lt;/strong&gt; &lt;a href=&#34;http://www.sciencedaily.com/releases/2007/03/070307075602.htm&#34;&gt;ScienceDaily&lt;/a&gt;.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Péndulo oscila lejos de la energía oscura</title>
      <link>https://ciencianet.com.ar/post/pendulo-oscila-lejos-de-la-energia-oscura/</link>
      <pubDate>Sat, 17 Mar 2007 02:45:48 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/pendulo-oscila-lejos-de-la-energia-oscura/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro.&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP) y Universidad Tecnológica Nacional.&lt;/p&gt;
&lt;p&gt;Físicos en USA encontraron que la ley de gravedad de Newton es válida con un alto nivel de precisión a distancias tan pequeñas como 55 micrómetros. Las mediciones fueron realizadas con una balanza de torsión y muestran que no hay evidencia de que la &amp;quot;energía oscura&amp;quot; debilite la atracción gravitacional a esta escala. El resultado es un revés en la búsqueda de los efectos gravitacionales de la energía oscura, que los cosmólogos creen deberían aparecer a estas distancias.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2007/03/torsion_0.jpg&#34; alt=&#34;Péndulo de torsión.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Un misterio importante que enfrentan los cosmólogos es que la velocidad de expansión del universo parece incrementarse en el tiempo. Los físicos habrían esperado que esta velocidad disminuya debido a que la energía finita de expansión es superada por la atracción gravitacional que mantiene al universo unido. Los cosmólogos han tratado de explicar el aumento en la velocidad en términos de la &amp;quot;energía oscura&amp;quot;, que sostiene la expansión del universo contrarrestando el efecto de la gravedad. Para ser efectiva, la energía oscura debería dar cuenta de aproximadamente el 70% de toda la energía del universo, pero aún no ha sido observado directamente.&lt;/p&gt;
&lt;p&gt;Los cosmólogos han calculado que una nueva fuerza asociada con la energía oscura debería manifestarse a distancias relativamente cortas -del orden de 85 micrómetros. Surge entonces que uno de los mejores lugares para buscar evidencias de la energía oscura a estas distancias no es en el espacio profundo exterior, sino en un simple exprerimento de laboratorio que mida la atracción gravitacional entre dos placas, con el propósito de encontrar alguna desviación de la clásica ley del inverso del cuadrado de la distancia.&lt;/p&gt;
&lt;p&gt;Dan Kapner y colegas de la Universidad de Washington utilizaron una balanza de torsión para medir la fuerza de gravedad a 55 micrómetros de distancia y encontraron que obedece la ley del cuadrado de la distancia más allá de los 85 micrómetros con una certeza del 95%. Aunque esto no invalida la teoría de la energía oscura, permitió a Kapner y sus colegas concluir que la intensidad de la nueva fuerza gravitacional no aparece a dicha escala.&lt;/p&gt;
&lt;p&gt;Aunque otros grupos han medido la fuerza de gravedad a distancias aún menores, Kapner afirma que el experimento de Washinton ofrece la más alta sensibilidad a la escala de longitudes asociada con la energía oscura. Esto es debido a que hace interactuar más masa a las distancias requeridas que otras mediciones. Los investigadores ahora están construyendo una balanza de torsión mejorada que podría aumentar la sensibilidad de las mediciones por un factor de 100.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fuente:&lt;/strong&gt; &lt;a href=&#34;https://physicsworld.com/a/pendulum-swings-away-from-dark-energy/&#34;&gt;physicsworld&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; &lt;a href=&#34;https://doi.org/10.1103/PhysRevLett.98.021101&#34;&gt;D. J. Kapner, T. S. Cook, E. G. Adelberger, J. H. Gundlach, B. R. Heckel, C. D. Hoyle, y H. E. Swanson
Phys. Rev. Lett. 98, 021101&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Electrones forman un cristal líquido</title>
      <link>https://ciencianet.com.ar/post/electrones-forman-un-cristal-liquido/</link>
      <pubDate>Thu, 15 Mar 2007 02:48:39 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/electrones-forman-un-cristal-liquido/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro&lt;/strong&gt;. Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP).&lt;/p&gt;
&lt;p&gt;Un experimento realizado sobre el óxido de un metal de transición muestra que en condiciones muy especiales un líquido de electrones presenta resistencias diferentes en direcciones diferentes. Es la primera evidencia directa de la existencia de una fase con anisotropía (nemática) en un líquido de electrones. El trabajo de R. A. Borzi y S. A. Grigera, de la Universidad Nacional de La Plata y el CONICET, en colaboración con J. Farrell, R. S. Perry, S. J. S. Lister, S. I. Lee, D. A. Tennant, Y. Maeno y A. P. Mackenzie, presenta medidas de resistencia eléctrica en cristales de Sr3Ru2O7 de alta pureza.&lt;/p&gt;
&lt;p&gt;Para un rango muy estrecho campos magnéticos aplicados y a temperaturas inferiores a 1 K, en la vecindad de un punto crítico metamagnético, la resistencia eléctrica en una dirección es dos veces mayor que en la dirección perpendicular a ésta. Este tipo de anisotropía ha sido observada con anterioridad pero el efecto podría explicarse si fuera producido por deformaciones espontáneas del cristal formado por los iones del material y no debida a los electrones de conducción.&lt;/p&gt;
&lt;p&gt;Los investigadores muestran que para este material la deformación del cristal de iones es imperceptible y que el efecto obtenido se debe esencialmente a la formación de una nueva fase del fluido de electrones de conducción. Los líquidos anisótropos o nemáticos son comunes en fluidos moleculares. Las moléculas con formas alargadas tienden a alinearse unas con otras de modo que las propiedades del fluido son diferentes en la dirección paralela al eje longitudinal de las moléculas y en la dirección perpendicular a esta. Los electrones en cambio son partículas puntuales y no pueden orientarse.&lt;/p&gt;
&lt;p&gt;Sin embargo, según comenta E. Fradkin de la Universidad de Illinois, los electrones podrían ordenarse en forma de una red regular anisótropa: con forma tiras en una dirección. Si esta red de electrones se desordena parcialmente, en un sentido cuántico, se podría transformar en un fluido de tiras de electrones. Estas tiras de electrones se comportarían como las moléculas alargadas de un líquido nemático clásico.&lt;/p&gt;
&lt;p&gt;Este modelo explicaría los hallazgos de Borzi y colaboradores. Los autores del trabajo aseguran que algunas características del fenómeno sugieren que la formación de un líquido de electrones nemático puede presentarse en otros materiales que se usan para la fabricación de componentes electrónicos como el arseniuro de galio (GaAs). R. Borzi comenta sobre este punto: &amp;quot;Yo diría que intentamos unir intuitivamente, con éxito modesto, lo que ocurre en ambos sistemas: el nuestro y otro más exótico, basado en GaAs&amp;quot;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; &lt;a href=&#34;http://www.sciencemag.org/cgi/content/abstract/315/5809/214&#34;&gt;Science, vol. 315, pp. 214 (2007)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instituciones:&lt;/strong&gt; &lt;a href=&#34;http://www.inifta.unlp.edu.ar/&#34;&gt;Instituto de Investigaciones Fisicoquímicas Teóricas y Aplicadas&lt;/a&gt; (&lt;a href=&#34;https://ciencianet.com.ar/www.conicet.gov.ar&#34;&gt;CONICET&lt;/a&gt; ), &lt;a href=&#34;https://ciencianet.com.ar/www.fisica.unlp.edu.ar&#34;&gt;Departamento de Física de La Plata&lt;/a&gt; (&lt;a href=&#34;https://ciencianet.com.ar/www.unlp.edu.ar/&#34;&gt;UNLP&lt;/a&gt; ), &lt;a href=&#34;https://ciencianet.com.ar/www.iflysib.unlp.edu.ar&#34;&gt;Instituto de Física de Líquidos y Sistemas Biológicos&lt;/a&gt; (CONICET), &lt;a href=&#34;http://www.st-andrews.ac.uk/physics/&#34;&gt;Escuela de Física y Astronomía de San Andrés&lt;/a&gt; (Escocia), &lt;a href=&#34;http://www.hmi.de/&#34;&gt;Instituto Hahn-Meitner&lt;/a&gt; (Alemania), &lt;a href=&#34;http://www.scphys.kyoto-u.ac.jp/index-e.html&#34;&gt;Departamento de Física de Kyoto&lt;/a&gt; (Japón).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; R. A. Borzi (E-mail: &lt;a href=&#34;https://ciencianet.com.ar/r.chufo@gmail.com&#34;&gt;r.chufo@gmail.com&lt;/a&gt; )&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Más información:&lt;/strong&gt; &lt;a href=&#34;http://www.sciencemag.org/cgi/content/summary/sci;315/5809/196&#34;&gt;Comentarios de E. Fradkin y colaboradores&lt;/a&gt; , &lt;a href=&#34;http://es.wikipedia.org/wiki/Cristal_l%C3%ADquido&#34;&gt;cristales líquidos&lt;/a&gt; .&lt;/p&gt;

        
      </description>
    </item>
    
  </channel>
</rss>
