<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Física on CienciaNet</title>
    <link>https://ciencianet.com.ar/categories/f%C3%ADsica/</link>
    <description>Recent content in Física on CienciaNet</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Copyright © 2008–2018, Steve Francia and the Hugo Authors; all rights reserved.</copyright>
    <lastBuildDate>Tue, 15 Jun 2021 18:36:17 -0300</lastBuildDate><atom:link href="https://ciencianet.com.ar/categories/f%C3%ADsica/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Una mirada física al juego del fútbol: ¡que el borracho no te saque la pelota!</title>
      <link>https://ciencianet.com.ar/post/una_mirada_fisica_al_juego_del_futbol/</link>
      <pubDate>Tue, 15 Jun 2021 18:36:17 -0300</pubDate>
      
      <guid>https://ciencianet.com.ar/post/una_mirada_fisica_al_juego_del_futbol/</guid>
      <description>
        
          &lt;h4 id=&#34;juan-pablo-álvarez-y-emilio-a-winograd&#34;&gt;Juan Pablo Álvarez y Emilio A. Winograd.&lt;/h4&gt;
&lt;p&gt;⊣ Foto de &lt;a href=&#34;https://unsplash.com/@jaenix?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&#34;&gt;Jannik Skorna&lt;/a&gt; en &lt;a href=&#34;https://unsplash.com/?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&#34;&gt;Unsplash&lt;/a&gt; ⊢&lt;/p&gt;
&lt;p&gt;La toma de decisiones basada en el análisis de grandes volúmenes de datos es una actividad cada vez más frecuente en todas las disciplinas y el deporte, en particular, sigue esa misma tendencia. El béisbol, básquet y fútbol americano son ejemplos pioneros y paradigmáticos de ello. La forma en que se analizan los jugadores y las acciones del juego en estos deportes han modificado notablemente el espectáculo. En nuestro fútbol, también las decisiones basadas en datos son cada vez más habituales y su uso está bastante extendido en lo que concierne al fichaje de jugadores. Sin embargo, su aplicación a los partidos en sí está más retrasada que en otros deportes, principalmente por tratarse de un juego con dinámica compleja, con situaciones con baja repetitividad, con baja cantidad de recompensas (goles) y sujeto a acciones imprevisibles.&lt;/p&gt;
&lt;p&gt;Sin embargo, por más complejo que sea, el fútbol no responde a la dinámica de lo impensado en lo que al juego se refiere. En la ciencia, sabemos que la dinámica de procesos con variables aleatorias puede ser predicha. Ejemplos abundan en todas las disciplinas, desde el movimiento browniano explicado en uno de los célebres trabajos de Einstein en 1905, pasando por las predicciones climáticas, aplicaciones a la economía, entre tantas otras. Esto también se expresa en el trabajo publicado por Chacoma, Almeira, Perotti, y Billoni, del Instituto de Física Enrique Gaviola, en Physical Review E, &amp;quot;Modeling ball possession dynamics in the game of Football&amp;quot; [&lt;a href=&#34;#1&#34;&gt;1&lt;/a&gt;], donde logran mostrar que, a pesar de su complejidad, en el fútbol es posible identificar y modelar patrones del juego de forma relativamente sencilla.&lt;/p&gt;
&lt;p&gt;Utilizando la reciente recopilación de eventos que ocurrieron durante 1941 partidos de las cinco mayores ligas de Europa, de la Copa Mundial 2018 y de la Eurocopa 2016 [&lt;a href=&#34;#2&#34;&gt;2&lt;/a&gt;], Chacoma y colaboradores analizan, principalmente, cómo es la dinámica del juego durante la tenencia del balón. Allí logran identificar que, en la mayoría de los casos, hay solo dos o tres jugadores involucrados, y que el evento más habitual es el pase, seguido por las acciones uno contra uno (duelos). Estos últimos son los mayores responsables de las pérdidas de balón. Las observaciones las utilizan para alimentar un modelo de tres agentes (jugadores), donde dos compañeros de equipo se enfrentan a un rival, quien intenta hacerse con el balón. Este modelo logran reducirlo a un sistema unidimensional de marcha aleatoria, donde la marcha (la jugada) finaliza cuando el agente (el defensor) se aproxima a la barrera absorbente (el balón). A pesar de su simplicidad, este modelo captura adecuadamente los comportamientos estadísticos de las longitudes de los pases, los tiempos de posesión y del número de pases realizados y sirve de base para el diseño de entrenamientos específicos en la búsqueda de mejorar aspectos relacionados con la posesión del balón.&lt;/p&gt;
&lt;p&gt;Sería interesante analizar cuáles de estos hallazgos son aplicables al fútbol argentino y sudamericano. En el juego de posesión, el pase es un elemento central y es parte esencial del modelo descrito. Las características de nuestro fútbol, presumiblemente más friccionado, con mayor cantidad de interrupciones, de pases largos a disputar con el rival y más propenso a las acciones individuales, ¿responde al mismo comportamiento? Andrés Chacoma, primer autor del estudio, intuye que no deberían existir diferencias significativas, aunque destaca la necesidad de obtener y analizar datos de esas ligas para responder esta inquietud.&lt;/p&gt;
&lt;h2 id=&#34;referencias&#34;&gt;Referencias:&lt;/h2&gt;
&lt;p&gt;&lt;a id=&#34;1&#34;&gt; 1. &lt;/a&gt; Chacoma, A., N. Almeira, J. I. Perotti, and O. V. Billoni. &amp;quot;Modeling ball possession dynamics in the game of football.&amp;quot; &lt;a href=&#34;https://doi.org/10.1103/PhysRevE.102.042120&#34;&gt;Physical Review E 102, no. 4 (2020): 042120&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;2&#34;&gt; 2. &lt;/a&gt; Pappalardo, Luca, Paolo Cintia, Alessio Rossi, Emanuele Massucco, Paolo Ferragina, Dino Pedreschi, and Fosca Giannotti. &amp;quot;A public data set of spatio-temporal match events in soccer competitions.&amp;quot; &lt;a href=&#34;https://doi.org/10.1038/s41597-019-0247-7&#34;&gt;Scientific data 6, no. 1 (2019): 1-15&lt;/a&gt;.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Desarrollo matemático permite mejorar la interpretación de imágenes médicas</title>
      <link>https://ciencianet.com.ar/post/desarrollo-matematico-mejora-imagenes-medicas/</link>
      <pubDate>Thu, 20 May 2021 19:56:13 -0300</pubDate>
      
      <guid>https://ciencianet.com.ar/post/desarrollo-matematico-mejora-imagenes-medicas/</guid>
      <description>
        
          &lt;h4 id=&#34;paula-bergero-instituto-de-investigaciones-fisicoquímicas-teóricas-y-aplicadas-inifta-y-universidad-nacional-de-la-plata&#34;&gt;&lt;strong&gt;Paula Bergero&lt;/strong&gt;. Instituto de Investigaciones Fisicoquímicas Teóricas y Aplicadas (INIFTA) y Universidad Nacional de La Plata.&lt;/h4&gt;
&lt;p&gt;⊣ Foto de &lt;a href=&#34;https://unsplash.com/@bacila_vlad?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&#34;&gt;Bacila Vlad&lt;/a&gt; en &lt;a href=&#34;https://unsplash.com/?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&#34;&gt;Unsplash&lt;/a&gt; ⊢&lt;/p&gt;
&lt;p&gt;El aporte hace posible la corrección de errores de proyección y de baja resolución en imágenes de ecografías, radiografías y otros estudios y así perfeccionar los diagnósticos que se basan en el análisis fractal, como el caso de enfermedades vasculares del ojo.&lt;/p&gt;
&lt;p&gt;Los diagnósticos a partir de imágenes de estudios médicos podrán mejorar gracias a un desarrollo matemático realizado por investigadores de la Universidad Nacional de La Plata y el CONICET. Los investigadores encontraron que la proyección de una estructura curva e irregular se distorsiona al ser tomada demasiado “de cerca”, arrojando valores erróneos. Según el trabajo publicado en la revista Physical Review E, las distorsiones causan diferencias en la dimensión fractal -el parámetro que caracteriza la irregularidad y ramificación de la imagen- que pueden llegar hasta el 6%.&lt;/p&gt;
&lt;p&gt;“Lo que nosotros encontramos es cómo recuperar la dimensión fractal verdadera a partir de la imagen distorsionada”, precisó la física Isabel Irurzun, directora del grupo de Modelado y Experimentación en Sistemas Complejos y coautora del trabajo junto a Juan Tenti y Sabrina Hernández Guiance. En particular, este desarrollo permitirá mejorar el diagnóstico de enfermedades vasculares del ojo, en las que las imágenes son tomadas a muy corta distancia. La dimensión fractal de la imagen refleja el estado vascular del ojo, y un cambio del 6% debido a una deformación podría interpretarse erróneamente como una patología. La mejora en el diagnóstico podrá beneficiar a las personas afectadas por la retinopatía diabética, quienes representan aproximadamente el 25% de los diabéticos, según la encuesta realizada en 2019 por el Consejo Oftalmológico Argentino.&lt;/p&gt;
&lt;p&gt;El aporte realizado por los investigadores permite corregir dos tipos de deformaciones que pueden afectar el análisis de las imágenes con fines diagnósticos. Por un lado, las distorsiones que se deben a la proyección que realiza el equipo médico para generar la representación plana del órgano, y por el otro, las que son causadas por una insuficiente resolución de la imagen. “El análisis fractal es una técnica matemática incorporada en diversos equipos de imágenes biomédicas. Sin embargo presenta algunas limitaciones, ya que incluso cuando el diseño del equipo es adecuado, el análisis puede estar afectado por las condiciones del estudio”, explicó Irurzun. “Las superficies corporales son curvas, así que tanto el problema de la proyección como el de la resolución afectarán en mayor o menor medida a todas las imágenes médicas: tomografías, ecografías, radiografías, entre otras”, agregó.&lt;/p&gt;
&lt;p&gt;El avance permitirá perfeccionar los diagnósticos basados en el análisis de la irregularidad y ramificación de las estructuras, como la presencia de placas ateromatosas en la aorta y la clasificación de tumores por su malignidad. En referencia a las enfermedades vasculares del ojo, Elizabeth Santiago-Cortés, investigadora de la Corporación Universitaria del Cauca en Colombia y autora de varias publicaciones sobre análisis fractal en estudios de la retina, viene reclamando junto a sus colaboradores que a menos que se estandarice, la utilidad del análisis como herramienta de diagnóstico es dudosa. Esta deuda podrá ser saldada gracias al desarrollo de Irurzun y su equipo.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2021/05/ojo-fractal.png&#34; alt=&#34;Izquierda: Imagen de la retina humana y del árbol circulatorio. Derecha: estructura fractal creada por computadora por Irurzun y colaboradores.&#34;&gt;&lt;/p&gt;
&lt;p&gt;Para estudiar las distorsiones, los investigadores crearon estructuras 3D por computadora que imitan la red de vasos sanguíneos del ojo y generaron imágenes como las obtenidas por estudios médicos. Así, encontraron la resolución de las imágenes que minimiza la distorsión en la dimensión fractal debido a la relación de tamaño entre el grosor de las ramas y la curvatura. Según Irurzun, la solución del problema es técnicamente sencilla, aunque incrementaría los costos. Actualmente están trabajando en desarrollar algoritmos para equipos médicos que capturen la dimensión fractal correcta a pesar de las deformaciones. “Esperamos tener resultados pronto”, se entusiasmó.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; &lt;a href=&#34;https://doi.org/10.1103/PhysRevE.103.012138&#34;&gt;Fractal dimension of diffusion-limited aggregation clusters grown on spherical surfaces&lt;/a&gt;. J. M. Tenti , S. N. Hernández Guiance, and I. M. Irurzun PHYSICAL REVIEW E 103, 012138 (2021).&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Sobre la medición reciente del muón</title>
      <link>https://ciencianet.com.ar/post/sobre-la-medicion-reciente-del-muon/</link>
      <pubDate>Wed, 14 Apr 2021 15:13:35 -0300</pubDate>
      
      <guid>https://ciencianet.com.ar/post/sobre-la-medicion-reciente-del-muon/</guid>
      <description>
        
          &lt;p&gt;Foto de Reidar Hahn. Imagen con permiso de Fermilab (en alta resolución &lt;a href=&#34;https://mod.fnal.gov/mod/stillphotos/2013/0200/13-0243-05D.hr.jpg&#34;&gt;aquí&lt;/a&gt;).&lt;/p&gt;
&lt;h4 id=&#34;gastón-giribet-facultad-de-ciencias-exactas--naturales-universidad-de-buenos-aires-y-conicet&#34;&gt;&lt;strong&gt;Gastón Giribet.&lt;/strong&gt; Facultad de Ciencias Exactas  Naturales, Universidad de Buenos Aires y CONICET.&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Los muones son partículas fundamentales. Son, en efecto, partículas muy comunes en nuestro entorno. Acaso alcance con decir que un muón atraviesa tu corazón cada segundo. Y al igual que éste, parecen encerrar secretos.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;el-espín-del-muón&#34;&gt;El espín del muón&lt;/h3&gt;
&lt;p&gt;En la rapsódica clasificación de partículas elementales que damos en llamar Modelo Estándar de la física de partículas, los muones pertenecen a la subfamilia cuyos miembros se denominan “leptones”, entre los que también encontramos al electrón. De hecho, el muón puede ser considerado un hermano mayor del electrón, ya que comparte con éste todas sus propiedades, excepto su masa: Los muones tienen una masa de aproximadamente 105 MeV, lo que los hace unas doscientas veces más masivos que el electrón; pero, más allá de esa diferencia, son exactamente iguales: tienen la misma carga eléctrica que el electrón (&lt;em&gt;e&lt;/em&gt; = -1) y tienen el mismo espín que el electrón (&lt;em&gt;s&lt;/em&gt; = ½).&lt;/p&gt;
&lt;p&gt;El espín, como sabemos, es un grado de libertad que las partículas fundamentales exhiben. El fotón tiene espín &lt;em&gt;s&lt;/em&gt; = 1, los leptones tienen espín &lt;em&gt;s&lt;/em&gt; = ½, al igual que los quarks, y las otras partículas tienen otros valores del espín, éstos siempre dados por números semi-enteros: los fotones y los gluones, por ejemplo, tienen espín &lt;em&gt;s&lt;/em&gt; = 1, al igual que las partículas W y las Z, mientras que la el bosón de Higgs es la única partícula de las 37 que conforman el Modelo Estándar que no presenta espín, i.e. cuyo espín es &lt;em&gt;s&lt;/em&gt; = 0.&lt;/p&gt;
&lt;p&gt;El espín es, decíamos, un grado de libertad que las partículas presentan. Así como tienen masa, carga eléctrica u otros tipos de carga, las partículas también tienen espín. El espín puede considerarse como una unidad elemental de giro. Al ser las partículas fundamentales puntos en el espacio, el espín de éstas no puede, &lt;em&gt;stricto sensu&lt;/em&gt;, ser pensado como el giro de la misma, ya que no hay en ellas un volumen que gire. Cabe recalcar que las partículas elementales no son pequeñas, ni muy pequeñas, ni muy muy pequeñas, sino que son infinitamente pequeñas; son, hasta donde sabemos, puntos en el espacio. Debido a esto, el espín debe ser pensado como una “unidad de giro”, un giro elemental, un giro inmanente de esos puntos en el espacio a los que llamamos partículas, un giro que está antes que aquello que gira. Esto, lo sé, puede resultar conceptualmente escurridizo, pero eso sólo se debe a que nuestro entendimiento, perezoso, insiste con pensar a todo girar como el girar de algo. No ocurre así con las partículas puntuales: El espín es giro, un giro inmanente, no es el girar de algo.&lt;/p&gt;
&lt;p&gt;Pero, ¿por qué hablamos tanto del espín? Pues bien, … es que el espín es importante para entender la cuestión que nos convoca. Expliquemos el porqué.&lt;/p&gt;
&lt;p&gt;Al ser partículas cargadas eléctricamente y al tener espín, los muones, así como los electrones, presentan un momento magnético. Es decir, podemos pensarlos como pequeños dipolos magnéticos que, en tanto tales, se acoplan al campo magnético como si fueran pequeños imanes. Esto ofrece una manera sencilla de medir el denominado “momento magnético” del electrón; es decir, el factor que da cuenta de cuánto se acoplan esos pequeños imanes puntuales que llamamos electrones a un campo magnético externo. La manera de medirlo es la siguiente: si uno dispone un electrón en presencia de un campo magnético externo, el influjo de este último hará que el electrón, en tanto pequeño dipolo magnético, adquiera un movimiento de precesión en torno a la dirección en la que el campo apunta. La velocidad de ese giro resulta ser proporcional al acoplamiento con el campo magnético externo. Así, la medición de esa frecuencia de precesión permite obtener de manera directa el momento magnético, que suele denotarse factor-&lt;em&gt;g&lt;/em&gt;, o simplemente &lt;em&gt;g&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;El factor-&lt;em&gt;g&lt;/em&gt; del electrón, decíamos, no es sino la propensión que esta partícula tiene a interactuar con el campo magnético en el que se encontrara embebida. Este campo externo puede estar dado por el núcleo atómico en torno al cual el electrón se encuentra orbitando, o dado por otro electrón compañero, o por un arreglo experimental que se hubiere montado. En todos esos casos, el acoplamiento entre el electrón y el campo magnético en el que éste se encuentra inmerso está dado por el mismo factor-&lt;em&gt;g&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Fue muy al comienzo de la física de partículas cuando se advirtió que el valor del factor-&lt;em&gt;g&lt;/em&gt; para un electrón parecía ser &lt;em&gt;g&lt;/em&gt; = 2; un valor peculiar ya que si se pensaba al electrón clásicamente era esperable un factor diferente. No obstante, ya a comienzos de la década del 30 se entendía la razón del valor 2 para el factor-&lt;em&gt;g&lt;/em&gt; electrónico; la ecuación de Dirac, que es la ecuación que describe el campo cuyas excitaciones llamamos “electrón”, predice exactamente ese valor, &lt;em&gt;g&lt;/em&gt; = 2. En efecto, la explicación de ese valor es uno de los grandes primeros logros de la teoría cuántica relativista.&lt;/p&gt;
&lt;h3 id=&#34;el-vacío-cuántico&#34;&gt;El vacío cuántico&lt;/h3&gt;
&lt;p&gt;Ahora bien, los datos experimentales de la medición del factor-&lt;em&gt;g&lt;/em&gt; llegarían unos años más tarde: En 1948, Kusch y Foley midieron por primera vez el factor-&lt;em&gt;g&lt;/em&gt;, obteniendo como resultado el valor &lt;em&gt;g&lt;/em&gt; = 2,00232, ligeramente superior al límpido 2 predicho por la teoría. Esta discrepancia con el valor &lt;em&gt;g&lt;/em&gt; = 2 es lo que se conoce como “momento magnético anómalo”, y es la razón por la cual uno refiere a la cantidad &lt;em&gt;g&lt;/em&gt;-2 como la verdaderamente relevante.&lt;/p&gt;
&lt;p&gt;Esta discrepancia entre el valor &lt;em&gt;g&lt;/em&gt; = 2,00232 y el valor &lt;em&gt;g&lt;/em&gt; = 2, sin embargo, se entendió al poco tiempo. Fue Schwinger quien propuso la explicación correcta de por qué el valor experimental obtenido para &lt;em&gt;g&lt;/em&gt; no era 2 sino un valor ligeramente mayor. La razón es la mecánica cuántica: según la teoría cuántica, lo que conocemos como vacío es algo muy diferente al concepto clásico que tenemos de él. El vacío no es la ausencia total de materia y energía en una región del espacio. La teoría cuántica de campos nos enseña que, de hecho, el vacío no es la nada sino algo muy diferente: Es un constante fluctuar de partículas y antipartículas que se crean y aniquilan incesantemente, dando origen a una estructura efervescente que, por consiguiente, denominamos “vacío cuántico”. Así, la mera presencia de una partícula en el espacio, por caso un electrón o un muón, genera en torno a sí una polarización de ese vacío cuántico, y la partícula termina vistiéndose de un halo fluctuante de partículas y antipartículas virtuales arrancadas a la nada.&lt;/p&gt;
&lt;p&gt;Podemos entender esta poiesis cuántica de partículas virtuales y cómo ésta afecta al comportamiento de un electrón en presencia de un campo externo de la siguiente manera: en la teoría cuántica de campos, la interacción de un electrón y el campo magnético externo se describe en términos del intercambio de partículas. La interacción electromagnética está mediada por un fotón, y ese fotón colisiona con el electrón afectando su comportamiento. Mientras que la visión macroscópica que tenemos del fenómeno es la de un electrón exhibiendo un movimiento de precesión debido al influjo de un campo magnético externo, la visión microscópica del mismo fenómeno nos devela que es la colisión entre el electrón y uno de los fotones que conforman el campo magnético lo que acaece (ver Figura 1-A). Al interactuar con el fotón, el electrón reacciona de la manera observada. Ahora bien, debido a la fluctuante naturaleza del vacío cuántico, el electrón tiene permitido efectuar muchas más peripecias que el mero interactuar con el fotón. Por ejemplo, antes de hacer esto último, el electrón pudo haber elegido emitir su propio fotón virtual, luego impactar con aquel que le trae la información del campo magnético externo, y finalmente reabsorber el fotón virtual originalmente emitido (Figura 1-B). Pero hay más: como si se tratara de un malabarista que encuentra desafiante poner más bolas en el aire, el electrón puede generar, no sólo uno, sino dos, tres o más fotones virtuales y, luego de impactar con aquel que le trae la información del campo magnético externo, reabsorberlos (Figura 1-C). Puede hacer todo eso y muchas cosas más, muchas otras piruetas cuánticas, cada una de ellas con cierta probabilidad de realización. El resultado, entonces, se obtiene luego de sumar sobre todas esas peripecias, sobre todas esas historias; mientras que el resultado clásico &lt;em&gt;g&lt;/em&gt; = 2 sólo corresponde a tener en cuenta el impacto directo entre electrón y el fotón (Figura 1-A). En otras palabras y tal como explicaba Schwinger, el resultado experimental para &lt;em&gt;g&lt;/em&gt; será un poco mayor que 2 debido a que el electrón tiene a su disposición la creación de pares de partículas y antipartículas virtuales que terminan vistiendo el valor clásico &lt;em&gt;g&lt;/em&gt; = 2 de otras posibles historias en ese interactuar, modificando así el valor del momento magnético ligeramente.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2021/04/muon-figura-1.png&#34; alt=&#34;Las líneas sólidas en color negro representan electrones (o muones), mientras que las líneas punteadas en color rojo representan fotones.&#34;&gt;&lt;/p&gt;
&lt;p&gt;Los cálculos teóricos sobre todas las posibles contribuciones de pares de partículas y antipartículas en la interacción entre el electrón y el campo magnético externo arrojan una predicción al valor de g extremadamente precisa, con más de una decena de cifras significativas de exactitud. El hecho de que los resultados experimentales para el factor-&lt;em&gt;g&lt;/em&gt; del electrón concuerden con los cálculos teóricos a semejante orden de precisión se yergue, hoy, como el resultado más preciso de toda disciplina científica.&lt;/p&gt;
&lt;h3 id=&#34;el-momento-magnético-del-muón&#34;&gt;El momento magnético del muón&lt;/h3&gt;
&lt;p&gt;Ahora bien, ¿ocurre lo mismo con el muón? Siendo que el muón y el electrón son dos partículas que, junto al tauón, coinciden en todas sus propiedades con excepción de la masa, ¿no debería, acaso, el resultado del momento magnético del muón arrojar resultados con precisión semejante a la obtenida para el caso del electrón? Pues, la historia no es tan sencilla: al ser el muón unas doscientos veces más masivo que el electrón, ése es mucho más propenso que este último a interactuar con partículas de gran masa. Típicamente, el acoplamiento de una partícula es proporcional al cuadrado de su masa, de manera tal que, al ser 200 veces más masivo que el electrón, el muón resulta ser unas 40.000 veces más susceptible a la interacción con las partículas virtuales. Esto requiere, en particular, mucho más detalle en el cálculo de las distintas contribuciones al acoplamiento entre el muón y el fotón. Al disponer un muón en un campo magnético externo, el muón, al igual que un electrón lo haría, comenzará a experimentar una precesión en torno a su eje de giro. Esa interacción entre el campo magnético externo y el muón se describirá, también como en el caso del electrón, en términos de intercambios de partículas, incluyendo las peripecias cuánticas que involucran partículas virtuales: al interactuar con el fotón portador del campo magnético externo, el muón puede colisionar directamente, pero puede también generar partículas virtuales antes de hacerlo y reabsorberlas luego de ello; puede generar fotones virtuales, partículas Z virtuales, e incluso generar partículas de Higgs virtuales, entre muchas otras cosas. El resultado teórico correspondería, pues, a sumar sobre todas esas posibilidades.&lt;/p&gt;
&lt;p&gt;Ese cálculo, aunque la teoría para llevarlo a cabo es bien conocida y, por ello, no presupone mayor desarrollo teórico, sí requiere de paciencia y de una notable pericia a la hora de emplear las técnicas de cálculo. Es un cómputo extremadamente sensible a errores y a omisiones. Una tarea minuciosa es necesaria a la hora de tener “todas” las configuraciones intermedias en cuenta y con su adecuado “peso”. La comunidad especializada ha llegado a un “resultado de consenso” acerca del cálculo teórico para el factor-&lt;em&gt;g&lt;/em&gt; del muón; a saber: &lt;em&gt;g&lt;/em&gt; = 2,0023318319.&lt;/p&gt;
&lt;p&gt;Ahora sólo quedaría cotejar con el experimento.&lt;/p&gt;
&lt;h3 id=&#34;los-experimentos&#34;&gt;Los experimentos&lt;/h3&gt;
&lt;p&gt;El momento magnético del muón se midió por primera vez a comienzos de la década del 50 en la Universidad de Columbia, y una década más tarde el CERN arrojaba resultados en concordancia. El advenimiento de mejoras tecnológicas y nuevas técnicas experimentales permitieron un aumento significativo en la precisión, y hacia la década de 1990 los nuevos experimentos con los que hoy contamos estaban ya siendo diseñados. En 2001, el Laboratorio Nacional de Brookhaven, en los Estados Unidos, daba a conocer el valor medido experimentalmente para el factor-&lt;em&gt;g&lt;/em&gt; del muón; éste era: &lt;em&gt;g&lt;/em&gt; = 2,0023318404.&lt;/p&gt;
&lt;p&gt;Como vemos, el valor experimental para &lt;em&gt;g&lt;/em&gt;-2 difiere del teórico en la novena cifra significativa, lo que podría hacernos creer que se trata de un éxito de la teoría. Pero lo cierto es que no es así: se trata de una medición muy precisa, por lo que los errores en la medición son extremadamente pequeños, lo suficiente como para considerar que se trata de un desacuerdo.&lt;/p&gt;
&lt;p&gt;A partir de 2013, partes del experimento de Brookhaven se trasladaron al Fermilab, en Chicago, donde se montó un arreglo similar pero con notables mejoras. Funciona allí la colaboración Muon &lt;em&gt;g&lt;/em&gt;-2, que realiza el experimento más preciso hasta el momento para medir el factor-&lt;em&gt;g&lt;/em&gt; del muón. El experimento consiste en un anillo de unos 15 metros de perímetro en el que se aceleran muones a muy altas velocidades. Luego de ser inyectados en el anillo, los muones giran mientras se los somete a un campo magnético externo de intensidad muy controlada. Debido al campo magnético, los muones experimentan el mentado movimiento de precesión cuya frecuencia es medida con la precisión requerida.&lt;/p&gt;
&lt;p&gt;El pasado 7 de abril, en conferencia de prensa, la colaboración trabajando en el experimento Muon &lt;em&gt;g&lt;/em&gt;-2 dio a conocer sus resultados, que vienen a confirmar la discrepancia entre el valor para el factor-&lt;em&gt;g&lt;/em&gt; medido experimentalmente y la predicción teórica basada en el Modelo Estándar. En otras palabras, los resultados de Muon &lt;em&gt;g&lt;/em&gt;-2 resultan ser consistentes con los experimentos de Brookhaven, exhibiendo, cuando se los considera en conjunto, una tensión de unas 4,2 sigmas de desviación estándar con respecto al valor teórico.&lt;/p&gt;
&lt;p&gt;Esto nos enfrenta a una discrepancia entre teoría y experimento, ahora corroborada y medida con mayor precisión. Surge así la pregunta: ¿Estamos ante el descubrimiento de una grieta en el edificio teórico que llamamos Modelo Estándar de partículas fundamentales? ¿Estamos ante lo que los físicos de partículas llaman “nueva física”? La verdad es que no lo sabemos con certeza.&lt;/p&gt;
&lt;p&gt;Hay, al menos, tres posibles explicaciones para la discrepancia observada: una posibilidad es que ésta se deba a una fluctuación estadística. Que los datos de las corridas del experimento analizadas hasta la fecha arrojen como resultado una discrepancia con el valor teórico de consenso a 4,2 sigmas hace que este sea un caso serio contra la teoría que tenemos; pero en física de partículas, dada la enorme exactitud que se ha alcanzado en los experimentos y dada la gran dependencia de los resultados con la estadística, la convención es que un descubrimiento se considera tal sólo cuando se han alcanzado los 5 sigmas de desviación, y para ello hace falta seguir analizando los resultados que la colaboración Muon &lt;em&gt;g&lt;/em&gt;-2 ha tomado en los últimos años.&lt;/p&gt;
&lt;p&gt;La segunda posible explicación para la discrepancia es que a la hora de efectuar los cálculos teóricos se hayan subestimado las contribuciones de partículas virtuales a la hora de pensar todas las formas en las que el muón puede interactuar con el campo magnético externo. Al tener una masa considerable, el muón es también propenso a emitir partículas virtuales que no son solamente partículas elementales, sino también hadrones. Los hadrones son estados ligados que están, a su vez, formados de muchas otras partículas que interactúan mediante la fuerza nuclear fuerte. Esos estados están descriptos por la teoría llamada cromodinámica cuántica en un régimen en la que la misma es difícil de domeñar. Ejemplos de hadrones son el pión y el kaón, formados éstos por un par quark-antiquark embebido en un orgiástico menjunje de gluones y acompañado con una fluctuante maraña de pares de quarks-antiquarks virtuales que se crean y aniquilan incesantemente. ¡Algo muy complejo! La consideración de la posibilidad de que el muón, antes de interactuar con el fotón del campo magnético externo, emita y luego reabsorba uno de estos hadrones complica mucho el cálculo teórico, ya que los hadrones tienen, per se, una complicada estructura interna. Y aunque la física de hadrones es en alguna medida conocida y contamos con datos experimentales para extrapolar resultados, ponderar su contribución es, sin lugar a dudas, la parte más abstrusa del cálculo. En el caso del electrón esto no representaba un inconveniente, ya que el electrón, debido a su baja masa (0,51 MeV), no tiene alta propensión a este tipo de interacciones; pero sí es menester tenerlas en cuenta en el caso del muón.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2021/04/muon-figura-2.png&#34; alt=&#34;Representación del acoplamiento de los muones (µ) y los fotones (̉γ) que constituyen el campo magnético externo, lo que incluye las correcciones cuánticas debido al intercambio de distintas partículas virtuales.&#34;&gt;&lt;/p&gt;
&lt;p&gt;La tercera posible explicación, y la que se anuncia con más acento debido a las posibles implicancias que ésta tendría para la física, es que el Modelo Estándar no sea un modelo completo; es decir, que deba el Modelo ser completado con la adición de partículas que aún no conocemos. Podría estar ocurriendo que el muón, en su interactuar con el fotón del campo magnético externo, estuviera generando otras partículas virtuales cuya existencia desconocemos pero de las que el muón sí se entera. La posibilidad de que el muón pueda emitir y luego reabsorber una partícula tal, además de hacerlo con las que ya conocemos, aumentaría el valor de &lt;em&gt;g&lt;/em&gt; acercándolo al valor experimental observado. Así, sugieren algunos, podríamos estar observando indirectamente la presencia de esas partículas nuevas, desconocidas, nuevas partículas fundamentales que deberíamos agregar a nuestro Modelo; en breve, podríamos estar frente a lo que los físicos de partículas insisten con llamar “nueva física”.&lt;/p&gt;
&lt;p&gt;Tenemos por delante una década de refinamiento de los cálculos teóricos, de mejoras en las técnicas de observación, y de recopilación de más datos estadísticos. El tiempo nos sabrá decir cuál es la explicación a la sutil discrepancia entre el valor teorizado y el valor observado para el factor-&lt;em&gt;g&lt;/em&gt; del muón. Pero, insisto, seamos cautelosos; no sería la primera vez que el Modelo Estándar saliese victorioso luego de haber sido desafiado, y aunque la precisión alcanzada en el experimento Muon &lt;em&gt;g&lt;/em&gt;-2 es asombrosa, es necesario esperar.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>La teoría de la relatividad y el centro galáctico</title>
      <link>https://ciencianet.com.ar/post/la-teoria-de-la-relatividad-y-el-centro-galactico/</link>
      <pubDate>Sun, 19 Aug 2018 23:49:23 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/la-teoria-de-la-relatividad-y-el-centro-galactico/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Gaston Giribet.&lt;/strong&gt;  Center for Cosmology and Particle Physics de New York University, Universidad de Buenos Aires y CONICET.&lt;/p&gt;
&lt;p&gt;La vía láctea, galáctica espiral de la que nuestro sistema solar forma modesta parte, alberga en su centro un enorme agujero negro cuya masa es cuatro millones de veces superior a la de nuestro sol. Sus coordenadas ubican al astro central en la región del cielo que los antiguos convinieron en llamar la constelación de Sagitario, hecho que explica en parte su nombre: Sagittarius A.&lt;/p&gt;
&lt;p&gt;Cientos de miles de millones de estrellas forman las aspas de la galaxia. Cientos de miles de millones de soles arremolinados en torno a un eje común. Algunas de esas estrellas, las más cercanas al centro, orbitan en torno a Sagittarius A zamarreadas por el intenso campo gravitatorio que éste genera. Una de esas estrellas, llamada S2, es la protagonista de esta historia.&lt;/p&gt;
&lt;p&gt;S2 tiene la masa equivalente a la de diez soles como el nuestro. Sisífica, circunda al agujero negro del centro galáctico trazando una órbita cuasi-elíptica que completa y repite al cabo de dieciséis años. Los astrónomos vienen siguiendo el movimiento de S2 con perseverancia desde 1992; una tarea que al día de hoy, luego de veintiséis años de observación, los ha dotado de un registro detallado de su trayectoria. Esto, junto con la proximidad que S2 alcanza con Sagittarius A, hace de esta estrella la candidata perfecta para poner a prueba nuestras teorías acerca de qué ocurre en el centro galáctico, esa región distante en la que nacen estrellas y anida un gigante.&lt;/p&gt;
&lt;p&gt;Esto nos permite, en particular, testar varias de las predicciones de la teoría de la relatividad. Según la teoría de la relatividad general, formulada por Einstein en 1915, el tiempo transcurre más lento en las regiones en las cuales el campo gravitatorio es más intenso. Y en la proximidad de un agujero negro como Sagittarius A el campo gravitatorio alcanza una intensidad tal que el tiempo, allí, se enlentece apreciablemente. La forma en la que dicha dilatación temporal puede ser medida es mediante la observación del color de la luz proveniente de esas regiones de intensa gravedad: La luz es una onda, y el color es la forma en la que las ondas de luz expresan el paso del tiempo. La luz de color violeta, por ejemplo, corresponde a una vibración de setecientos billones de veces por segundo, mientras que la luz de color rojo corresponde a una vibración de cuatrocientos billones de veces por segundo, poco más que la mitad.&lt;/p&gt;
&lt;p&gt;Debido a esto, la luz que nos llega de los procesos físicos que acaecen en regiones donde la gravedad es más intensa y, por lo tanto, donde el tiempo fluye más lento, ha de ser de un tinte más rojizo que el esperado. En otras palabras, el tiempo transcurrido entre dos oscilaciones de una onda de luz generada por un dado proceso físico es mayor donde la gravedad es más intensa, lo que se traduce en que recibimos de allí menos oscilaciones por cada segundo de los que nuestros relojes terrestres marcan. Este efecto se denomina “corrimiento al rojo gravitatorio”.&lt;/p&gt;
&lt;p&gt;Al corrimiento al rojo se le suma un segundo efecto, también predicho por Einstein. Éste recibe el nombre de “efecto Doppler transversal”, aunque poco tiene que ver con el efecto Doppler usual que conocemos para las ondas acústicas emitidas por fuentes que se acercan o alejan. Según la teoría de la relatividad especial, de 1905, el tiempo también transcurre más lento para los cuerpos que llevan grandes velocidades, velocidades tales como los vertiginosos veinticinco millones de kilómetros por hora que S2 alcanza en su orbitar cerca de Sagittarius A. Así, según la teoría, en el caso de S2 el efecto de corrimiento al rojo gravitatorio y el efecto Doppler transversal deberían potenciarse y dotar a la estrella de un sospechoso tinte rojizo al encontrarse ésta en el periastro de su órbita.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2018/08/S2-768x489.jpg&#34; alt=&#34;Representación artística de la órbita de S2 en torno a Sagittarius A. Crédito de la imagen: ESO/M. Kornmesser.&#34;&gt;&lt;/p&gt;
&lt;p&gt;Fue precisamente ese ligero corrimiento al rojo en la luz emitida por S2 lo que los astrónomos del Observatorio Europeo en el Sur (ESO) se dispusieron a observar en mayo de 2018, momento en que la estrella se encontró en su punto más cercano al gigante Sagittarius A. Una observación astronómica de esta naturaleza requiere una precisión inédita. Si bien el radio de la órbita de la estrella en su punto de acercamiento máximo al agujero negro es de unos veinte mil millones de kilómetros, esa distancia empalidece ante los veintiséis mil años luz que separan el centro galáctico de la Tierra.&lt;/p&gt;
&lt;p&gt;Esto hace que pretender observar la órbita de S2 con suficiente detalle demande el abuso de cuatro grandes telescopios operando en tándem para formar, así, un gran telescopio resultante. Las señales de los cuatro telescopios se combinan en un sofisticado sistema óptico, denominado GRAVITY, especialmente diseñado para la empresa -y se emplean otros sistemas, como el denominado SINFONI-. Esta tecnología le permitió al grupo del Instituto Max Planck de Alemania liderado por Reinhard Genzel obtener una sensibilidad única y, gracias a esto, un resultado contundente: Al pasar cerca del agujero negro del centro galáctico la estrella S2 se mostró sonrojada, en perfecta concordancia con las predicciones de la teoría de Einstein.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referencias:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://doi.org/10.1051/0004-6361/201833718&#34;&gt;&lt;em&gt;&amp;quot;Detection of the gravitational redshift in the orbit of the star S2 near the Galactic centre massive black hole&amp;quot;&lt;/em&gt;&lt;/a&gt;, R. Abuter et al., GRAVITY Collaboration, Astronomy and Astrophysics 615 (2018) L15.  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.sciencemag.org/news/2018/07/star-s-black-hole-encounter-puts-einstein-s-theory-gravity-test&#34;&gt;&lt;em&gt;&amp;quot;Milky Way’s black hole provides long-sought test of Einstein’s general relativity&amp;quot;&lt;/em&gt;&lt;/a&gt;, en Nature, July 26th 2018.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

        
      </description>
    </item>
    
    <item>
      <title>Einstein, Tucumán y el infinito</title>
      <link>https://ciencianet.com.ar/post/einstein-tucuman-y-el-infinito/</link>
      <pubDate>Wed, 04 Jul 2018 03:53:12 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/einstein-tucuman-y-el-infinito/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Gaston Giribet.&lt;/strong&gt; Investigador del Center for Cosmology and Particle Physics de New York University. Profesor de la UBA e Investigador Principal del CONICET.&lt;/p&gt;
&lt;p&gt;Hace 75 años, Albert Einstein y Wolfgang Pauli escribían en coautoría un artículo en el que estudiaban las divergencias de la teoría de Kaluza y Klein, la teoría que sostiene que nuestro universo podría tener una quinta dimensión. Al poco de comenzar a leer ese artículo, publicado en &lt;em&gt;Annals of Mathematics&lt;/em&gt; en 1943, uno inevitablemente atiende a una curiosa cita bibliográfica que aparece al pie de la primera página. Se trata de la cita a una ignota precuela de 1941 publicada por Albert Einstein en la Revista de la Universidad Nacional de Tucumán. En su artículo tucumano, Einstein daba una demostración ingeniosa de la inexistencia de soluciones masivas y finitas en su teoría de la Relatividad General.&lt;/p&gt;
&lt;p&gt;El artículo de Tucumán fue encargado a Einstein por el matemático italiano Alessandro Terracini interpósita persona. Fue Guido Fubini quien, a mediados de 1941, intercedió para pedirle a Einstein que considerara la posibilidad de enviar una contribución para la Revista de la Universidad Nacional de Tucumán, revista que por aquel entonces estaba siendo fundada por Terracini y colaboradores (Terracini, 1941; 1944). Einstein aceptó amablemente la invitación y cumplió con enviar un artículo breve, al que se refirió como “&lt;em&gt;ein hübscher beweis&lt;/em&gt;”, una donosa demostración (Einstein, 1941c).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2018/07/BetoII-238x300.jpg&#34; alt=&#34;Albert Einstein, en Princeton NJ.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;En su artículo, Einstein presentaba una demostración sucinta de la inexistencia de soluciones esféricamente simétricas y regulares en la teoría de la Relatividad General. Más precisamente, Einstein demostraba en su trabajo que sus ecuaciones para el campo gravitatorio no admitían soluciones que cumplieran con los siguientes cuatro requisitos simultáneamente:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;a)&lt;/em&gt; exhibir simetría esférica,&lt;/li&gt;
&lt;li&gt;&lt;em&gt;b)&lt;/em&gt; no depender explícitamente del tiempo,&lt;/li&gt;
&lt;li&gt;&lt;em&gt;c)&lt;/em&gt; aproximar a largas distancias las soluciones de las ecuaciones newtonianas,&lt;/li&gt;
&lt;li&gt;&lt;em&gt;d)&lt;/em&gt; no presentar divergencias del campo.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Obedecer los cuatro requisitos implicaría un absurdo, concluía el trabajo. Estas cuatro propiedades son, desde el punto de vista de la física, requerimientos muy naturales. En efecto, se trata de las propiedades que uno esperaría del campo gravitatorio generado, por ejemplo, por una partícula: Si uno piensa en una partícula u otro objeto masivo, compacto, esférico y en reposo, entonces las condiciones &lt;em&gt;a)&lt;/em&gt; y &lt;em&gt;b)&lt;/em&gt; no son sino la exigencia de que el campo generado por el objeto respete las simetrías del objeto mismo –esférico y en reposo–. Por su parte, pedir la validez de la condición &lt;em&gt;c)&lt;/em&gt; es simplemente pedir que lejos del objeto, donde el campo gravitatorio generado por éste es desdeñable, dicho campo sea bien aproximado por la teoría gravitatoria de Newton, que se sabe cierta precisamente en el régimen de campo débil. Es la requisito &lt;em&gt;d)&lt;/em&gt;, acaso, el que demanda más justificación. La razón fundamental para pedir, o al menos desear, que el campo generado por un objeto no sea divergente sino finito es que las singularidades –los infinitos– que con frecuencia aparecen en las soluciones a las ecuaciones de campo introducen un alto grado de arbitrariedad en la teoría, algo que se considera inaceptable para una teoría fundamental. Cuando las ecuaciones arrojan soluciones que en algún punto o región del espacio o del tiempo resultan infinitas, entonces las ecuaciones pierden predictibilidad, ya que el infinito no es “mucho” sino una indeterminación del valor calculado.&lt;/p&gt;
&lt;p&gt;Este problema se acentúa cuando hablamos del campo gravitatorio, ya que, según la teoría de Einstein, el campo gravitatorio es inextirpable de la geometría del espacio-tiempo mismo. Debido a esto, un resultado infinito de las ecuaciones del campo gravitatorio implica una fisura en el entramado espacio-temporal. Asimismo, Einstein encontraba un segundo inconveniente en el infinito: Era su íntima convicción que la materia debía ser parte inescindible de la teoría del campo, y que los objetos no eran los que generaban campos –gravitatorios o eléctricos– sino que éstos eran, per se, campo. Es decir, para Einstein la materia, las partículas, debía poder ser explicada como acumulación finita de campo; como si las partículas que gravitan no fueran otra cosa que la mera concentración del mismo campo gravitatorio que parecen generar. Su conclusión de que el campo en el centro del objeto que lo generaba debía ineluctablemente ser infinito parecía atentar contra esta bella idea.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2018/07/PaperCopy.png&#34; alt=&#34;El artículo de Einstein en la Revista UNT 1941&#34;&gt;&lt;/p&gt;
&lt;p&gt;El trabajo de Einstein (Einstein, 1941a) venía demostrar de una manera ingeniosa que las soluciones de las ecuaciones del campo gravitatorio eran inevitablemente divergentes, inevitablemente infinitas en algún punto del espacio. El título original del borrador de Einstein en alemán era “&lt;em&gt;Beweis der Nichtexistenz von Singularitätsfreien Gravitationsfeldern mit nicht Verschwindender Gesamtmasse&lt;/em&gt;”, traducción literal de los títulos con los que después se publicaría (Einstein, 1941a; 1941d): “Demostración de la no existencia de campos gravitacionales sin singularidades de masa total no nula”.&lt;/p&gt;
&lt;p&gt;El manuscrito se encuentra en posesión de la Biblioteca Matemática de la Universidad de Turín. Se trata de un trabajo no muy conocido, aunque aparece citado en algunas fuentes, e.g. (Föolsing, 1993; Earman y Eisenstaedt, 1999; van Dongen, 2002). Fue Abraham Taub, un reconocido relativista, quien reseñó el artículo de Einstein para Mathematical Review (Taub, 1942). La traducción del original en alemán fue supervisada por el mismo Terracini, quien en una carta dirigida a Einstein le informó de que se iban a preparar dos traducciones, una al castellano y otra al inglés (Terracini, 1941). Ambas versiones (Einstein, 1941d; 1941a) se publicaron en el mismo volumen de la serie A de la Revista de la Universidad Nacional de Tucumán en diciembre de 1941.&lt;/p&gt;
&lt;p&gt;En las ediciones siguientes la Revista continuaría publicando artículos en el tema; en especial, trabajos relacionados con teorías de campo unificado, e.g. (Santaló, 1954; 1959). La demostración de Einstein de la inexistencia de soluciones no-divergentes a las ecuaciones de campo adquiere importancia, no por tratarse de un resultado novedoso en el contexto de la Relatividad General, sino debido a su cualidad de ser fácilmente generalizable a otras teorías de campos. Una de esas generalizaciones es la que presentaron Einstein y Pauli en su artículo (Einstein y Pauli, 1943), en el que investigaban la existencia de soluciones regulares en la teoría de Kaluza-Klein.&lt;/p&gt;
&lt;p&gt;Otra generalización de la demostración de Einstein fue la adaptación que, en 1948, Papapetrou hizo al caso de la teoría no-simétrica del campo unificado en la que en ese momento Einstein y su soldadesca se encontraban trabajando (Einstein, 1945; Einstein and Straus, 1946; Einstein, 1949; Einstein and Kaufman, 1952; 1953; 1954; 1955). Papapetrou argüía en su trabajo (Papapetrou, 1948) que tampoco la generalización no-simétrica de la Relatividad General estaba a salvo de los temibles infinitos.&lt;/p&gt;
&lt;p&gt;El criterio de existencia de soluciones no-divergentes a las ecuaciones de campo llegó a convertirse en un regente severo en la búsqueda de Einstein de una teoría del campo unificado. Su firme creencia en que en una teoría final los constituyentes de la materia deberían poder ser entendidos como acumulaciones finitas de campo puro lo llevó, en varias ocasiones, a abandonar líneas de investigación que al comienzo se mostraban promisorias. Algunos incluso afirman que, para Einstein, el hecho de que la materia y los campos fueran entidades de naturaleza distinta era tan poco satisfactorio como el hecho de que las distintas fuerzas de la naturaleza no nacieran de una única e irreducible ecuación.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Agradecimientos:&lt;/strong&gt; El autor le agradece a Mariano Galvagno por la colaboración en la redacción de este texto, con las traducciones, la bibliografía, y por las discusiones sobre temas relacionados.&lt;/p&gt;
&lt;h3 id=&#34;referencias&#34;&gt;Referencias&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Earman J. and Eisenstaedt J., Einstein and singularities, Stud. Hist. Mod. Phys. 30, 2 (1999), 185.&lt;/li&gt;
&lt;li&gt;Einstein A. and Rosen N., The particle problem in the general theory of relativity, Phys. Rev. 48 (1935), 73. - Einstein A., On a stationary system with spherical symmetry consisting of many gravitating masses, Ann. of Math. 40, 2 (1939), 922.&lt;/li&gt;
&lt;li&gt;Einstein A., Demonstration of the non-existence of gravitational fields with a non-vanishing total mass free of singularities, Revista de la Universidad Nacional de Tucum´an, A2, (1941), 11.&lt;/li&gt;
&lt;li&gt;Einstein A., letter to G. Fubini, dated on August 5 of 1941. Italian translation of the letter addressed to G. Fubini, dated on August 5 of 1941.&lt;/li&gt;
&lt;li&gt;Einstein A., letter to G. Fubini, dated on August 5 of 1941; in German. Copy at The Albert Einstein Archives, The Hebrew University of Jerusalem, Israel.&lt;/li&gt;
&lt;li&gt;Einstein A., Demostración de la no existencia de campos gravitacionales sin singularidades de masa total no nula, Revista de la Universidad Nacional de Tucumán, A2, (1941), 5.&lt;/li&gt;
&lt;li&gt;Einstein A. and Pauli W., On the non-existence of regular stationary solutions of relativistic field equations, Ann. of Math. 44, 2 (1943), 131.&lt;/li&gt;
&lt;li&gt;Einstein A., A generalization of the relativistic theory of gravitation, Ann. of Math. 46, 4 (1945), 578.&lt;/li&gt;
&lt;li&gt;Einstein A. and Straus E., A generalization of the relativistic theory of gravitation. II, Ann. of Math. 47, 4 (1946), 731.&lt;/li&gt;
&lt;li&gt;Einstein A., Autobiographical Notes, Open Court Publishing Company, La Salle, Illinois (1949).&lt;/li&gt;
&lt;li&gt;Einstein A. and Kaufman B., Volume in honor to Louis de Broglie, Paris, (1952), 321.&lt;/li&gt;
&lt;li&gt;Einstein A., The meaning of relativity, Princeton University Press, 1953.&lt;/li&gt;
&lt;li&gt;Einstein A. and Kaufman B., Algebraic properties of the field in the relativistic theory of the asymmetric field, Ann. of Math. 59, 2 (1954).&lt;/li&gt;
&lt;li&gt;Einstein A. and Kaufman B., A new form of the general relativistic field equations, Ann. of Math. 52, 1 (1955).&lt;/li&gt;
&lt;li&gt;Einstein A., Über die spezielle und die allgemeine Relativistätsheorie, Vieweg, 1969.&lt;/li&gt;
&lt;li&gt;Einstein A. and Besso M., Correspondance (1903-1955), Spanish translation, Tusquets Editores, Barcelona, 1994.&lt;/li&gt;
&lt;li&gt;Fölsing A., Einstein A., Biographie, Suhrkamp Taschenbuch 3087, Suhrkamp Verlag Frankfurt am Main, Ffm 1993A (1993).&lt;/li&gt;
&lt;li&gt;Galvagno M., Giribet G., The particle problem in classical gravity: A historical note on 1941, Eur. J. Phys. 26 (2005), 97.&lt;/li&gt;
&lt;li&gt;Hlavaty V., Geometry of Einstein’s unified field theory, P. Noordhoff-Gromingen-Holland, 1957.&lt;/li&gt;
&lt;li&gt;Lichnerowicz A., Théories relativistes de la gravitation et de l’electromagnétisme, Masson, 1955.&lt;/li&gt;
&lt;li&gt;Pauli W., Zur Theorie der Gravitation und der Elektrizität von Hermann Weyl, Physikalische Zeitschrift, 20 (1919), 10.&lt;/li&gt;
&lt;li&gt;Pauli W., Writtings on Physics and Phylosophy, Springer-Verlag Berlin Heidelberg, 1994.&lt;/li&gt;
&lt;li&gt;Papapetrou A., The question of non-singular solutions in the generalized theory of gravitation, Phys. Rev. 73, 9 (1948), 1105.&lt;/li&gt;
&lt;li&gt;Santaló L.A., Revista de la Universidad Nacional de Tucumán A1 10 (1954), 19.&lt;/li&gt;
&lt;li&gt;Santaló L.A., Revista de la Universidad Nacional de Tucumán A1 XII (1959), 31.&lt;/li&gt;
&lt;li&gt;Schrödinger E., Proc. R. Ir. Acad. 49A, 43 (1943), 225.&lt;/li&gt;
&lt;li&gt;Schrödinger E., Proc. R. Ir. Acad. 52A (1948), 1.&lt;/li&gt;
&lt;li&gt;Schrödinger E., Space-Time Structure, Cambridge University Press, 1950.&lt;/li&gt;
&lt;li&gt;Sen D.K., Particles and/or fields, Academic Press Inc. (London), 1968.&lt;/li&gt;
&lt;li&gt;Taub A.H., Review of Demonstration of the non-existence of gravitational fields with a non-vanishing total mass free of singularities by A. Einstein; Mathematical Review, AMS 2004, MR0006877.&lt;/li&gt;
&lt;li&gt;Terracini A., letter to A. Einstein, dated on October 21 of 1941; in German. The Albert Einstein Archives, The Hebrew University of Jerusalem, Israel.&lt;/li&gt;
&lt;li&gt;Terracini A., letter to A. Einstein, dated on August 6 of 1944; in Spanish. The Albert Einstein Archives, The Hebrew University of Jerusalem, Israel.&lt;/li&gt;
&lt;li&gt;Tonnelat M.A., La Théorie du champ unifié d’ Einstein et quelques-uns de ses développements, Gauthier-Villars, 1955. Vizgin V.P., Unified field theory in the first third of the 20th century, Historical Studies, Birkhäuser Verlag (1994).&lt;/li&gt;
&lt;/ul&gt;

        
      </description>
    </item>
    
    <item>
      <title>La torre donde la luz gravita</title>
      <link>https://ciencianet.com.ar/post/la-torre-donde-la-luz-gravita/</link>
      <pubDate>Thu, 07 Jun 2018 18:08:52 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/la-torre-donde-la-luz-gravita/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Gaston Giribet.&lt;/strong&gt; Center for Cosmology and Particle Physics de New York University, Universidad de Buenos Aires y CONICET.&lt;/p&gt;
&lt;p&gt;Unas semanas atrás visité a dos colegas en la universidad de Harvard. Mientras trabajábamos los tres en el altillo del ala oeste del Laboratorio Jefferson, en un espacio luminoso rodeado de pizarrones, recordé el dato –anecdótico, reconozco– de que fue precisamente allí, en la torre que ese altillo corona, donde en 1959 se realizó uno de los experimentos que vendrían a convalidar las predicciones de la teoría de la relatividad general de Einstein. En aquella torre, hace casi sesenta años, se medía por primera vez que la luz también cae.&lt;/p&gt;
&lt;h3 id=&#34;en-la-torre&#34;&gt;En la torre&lt;/h3&gt;
&lt;p&gt;En efecto, la luz gravita, y fue en esa torre que se midió por primera vez su peso. Según la teoría de Einstein, y mal que le pese a Machado, por más sutil que un cuerpo sea jamás llegará a ser ingrávido. Todas las formas de energía y materia, incluso la gravedad misma, están a merced del influjo gravitatorio. La luz no escapa a tal suerte; si bien carece de masa, igual gravita.&lt;/p&gt;
&lt;p&gt;Esto se debe a que los efectos de la gravedad no son sino la deformación misma del espacio y el tiempo, la curvatura del escenario en el que ocurren todos los procesos físicos. La materia, la energía, y la luz en particular se ciñen a la forma que el continuo espaciotiemporal adopta alrededor. Así, desde Einstein, las órbitas planetarias, las parábolas balísticas y todos los fenómenos que asociamos a la fuerza gravitatoria se entienden, no como el influjo de la fuerza gravitatoria sobre los astros y otros objetos, sino como la curvatura del mismísimo espaciotiempo en el que esos entes existen, y a esas formas curvas del espacio han estos entes de amoldar su surcarlo.  &lt;/p&gt;
&lt;h3 id=&#34;la-curvatura-del-espacio-la-dilatación-del-tiempo&#34;&gt;La curvatura del espacio, la dilatación del tiempo&lt;/h3&gt;
&lt;p&gt;Ya en 1919, cuatro años después de la formulación definitiva de la teoría de la relatividad general, Arthur Eddington observaba la deflexión de la luz producida por el campo gravitatorio del sol. Porque nos convoca otro asunto, no ahondaremos aquí en esa historia, nada desprovista de anécdotas y detalles curiosos. Nos alcanzará con decir que la campaña de Eddington para observar el eclipse solar del 29 de mayo de 1919, en la que se observaría por primera vez la deflexión de la luz producida por la gravedad del gran astro, terminaría por dotar a Einstein de una fama mundial que, hasta el momento, se limitaba a dextros académicos.&lt;/p&gt;
&lt;p&gt;Eddington comprobaba con su telescopio ya en 1919 que la gravedad curvaba la trayectoria de la luz y que lo hacía en la cantidad precisa que Einstein había predicho en 1915. En otras palabras, se verificaba que la luz ceñía su trayectoria a la curvatura del espacio. Ahora bien, ¿qué del tiempo? Está en las bases de la teoría de la relatividad la naturaleza inescindible del espacio y el tiempo, que desde Einstein pasan a formar parte de un entramado único e inseparable, el espaciotiempo.&lt;/p&gt;
&lt;p&gt;Así, si la teoría predice que la gravedad genera una deformación del espacio entonces ha el tiempo de sufrir lo suyo. El tiempo también debe deformarse en presencia del campo gravitatorio. Si en el caso del espacio es ya complicado aprehender la noción de curvatura, qué esperar del tiempo, concepto tanto más escurridizo. ¿Qué significa que el tiempo se deforme? ¿Cómo se expresaría tal deformación en el fluir de los procesos?&lt;/p&gt;
&lt;p&gt;La relatividad nos lo explica: Desde la formulación de la teoría especial de la relatividad, en 1905, sabemos que el transcurrir del tiempo no es un fenómeno absoluto. Para diferentes observadores el ínterin entre dos sucesos físicos puede ser distinto. Por ejemplo, puede darse que, según uno de ellos, hayan transcurrido unos pocos minutos entre un proceso A y un proceso B, mientras que para el otro haya ese lapso sido de varios años. Para que esto se dé, alcanza con que la velocidad relativa entre un observador y el otro sea lo suficientemente alta. Cuanto más alta la velocidad con la que un observador se aleja del (o acerque al) otro, más grande será la discrepancia entre el lapso que cada uno medirá entre los mismos eventos.&lt;/p&gt;
&lt;p&gt;Y cabe aclarar que no se debe esta discrepancia a efectos de la percepción, sino a que el tiempo verdaderamente transcurre de manera diferente para ambos. Este efecto, conocido como dilatación temporal, permite explicar muchos fenómenos físicos que incluso podemos comprobar a escalas terrestres. En particular, explica por qué, cada segundo que pasa, nuestro cuerpo es atravesado por una decena de muones (partículas subatómicas similares a los electrones pero doscientas veces más pesadas).&lt;/p&gt;
&lt;p&gt;Esos muones, generados en la alta atmósfera, tienen una vida media muy corta. Debido a ello, según nuestro terrícola parecer no debería ese efímero existir alcanzarles para llegar desde la alta atmósfera hasta nosotros. Aun así, lo logran. ¿Cómo? Debido al efecto de la dilatación temporal que acabamos de explicar: Al moverse los muones a gran velocidad respecto de nosotros, la discrepancia entre el tiempo de su existencia que nosotros esperaríamos y el tiempo que ellos mismos experimentan es muy grande. Debido a esto, mientras nosotros esperaríamos que se extinguieran antes de llegar al suelo, el reloj interno de los muones atrasa con respecto a nuestra expectativa, y esto les permite sobrevivir al largo –no tan largo para ellos– viaje y alcanzarnos.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2018/06/RebkaFoto-228x300.jpg&#34; alt=&#34;Robert Pound, Harvard University.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Pero esto no es todo; ni siquiera lo más sorprendente: Hay, además del que acabamos de explicar, un segundo efecto relativista que también produce la dilatación temporal. Este fue propuesto por Einstein un poco más tarde, en 1911, a medio camino entre su teoría especial de 1905 y su teoría general de 1915. A diferencia del efecto de dilatación que describimos arriba, que tiene que ver con la discrepancia entre los lapsos observados por dos experimentalistas que se mueven a gran velocidad uno respecto del otro, existe un efecto de dilatación más desconcertante, por cuanto no tiene que ver con el movimiento relativo entre observadores sino con el lugar en el que ellos se encuentran.&lt;/p&gt;
&lt;p&gt;Según predijo Einstein en 1911, en las regiones del espacio en las cuales el campo gravitatorio es más intenso, el tiempo transcurre más lento &lt;a href=&#34;#1&#34;&gt;[1]&lt;/a&gt;. Tan cierto cuanto extraño pueda sonar. En particular, en el piso de nuestra habitación, al encontrarse éste más cerca del centro gravitatorio de la tierra, el tiempo transcurre más lento que en el techo. Estrictamente hablando, uno envejece más rápido si duerme en la litera de arriba que en la de abajo. Esto no se debe, aclaremos, a que los relojes o aparatos de medición que empleáremos para medir el paso del tiempo se estropeen. Se trata de una verdadera deformación en el tiempo producida por el campo gravitatorio. En el caso de nuestra habitación, al ser el campo gravitatorio del planeta tierra no muy fuerte y al ser el planeta tanto más grande que el tamaño la habitación misma, el efecto es despreciable y no debemos tenerlo en cuenta a la hora de elegir si dormir arriba o abajo.&lt;/p&gt;
&lt;p&gt;Para hacernos una idea de cuán sutil es el fenómeno a escalas terrestres, digamos que si decidiéramos hacer un experimento y dispusiéramos para ello un reloj en el techo de la habitación y otro en el suelo con la intención de medir cómo el segundo atrasa respecto al primero, deberíamos esperar cientos de millones de años para que tal retraso alcance a ser notable. Ahora bien, este efecto sí es apreciable en las cercanías de astros muy densos con campos gravitatorios fuertes. Por ejemplo, la radiación que nos llega de las cercanías de los agujeros negros presenta un corrimiento del espectro de la luz emitida que es muestra de esa dilatación temporal. En la superficie misma de los agujeros negros el efecto de letargo temporal debido a la intensidad del campo gravitatorio es tan fuerte que, literalmente, el tiempo allí deja de transcurrir.&lt;/p&gt;
&lt;p&gt;Resumiendo, entonces: La relatividad predice dos fenómenos por los cuales el transcurrir del tiempo no es un absoluto sino que pasa a ser relativo, en el sentido de que observadores diferentes discreparían en el lapso acaecido entre dos fenómenos. El primero de estos dos efectos es el de la dilatación temporal debida a la velocidad relativa entre dos observadores (Einstein 1905); el segundo, la dilatación temporal debida a que los dos observadores se encuentran en regiones donde el campo gravitatorio tiene diferentes intensidades (Einstein 1911; cf. Einstein 1915).&lt;/p&gt;
&lt;p&gt;Es el segundo de estos efectos el que se midió en aquella torre del Laboratorio Jefferson. Allí, en 1959, Robert Pound y su alumno Glen Rebka realizaron un ingenioso experimento que ellos mismos habían diseñado más temprano ese mismo año &lt;a href=&#34;#2&#34;&gt;[2]&lt;/a&gt;. Según sus propias palabras, ellos lograron medir el peso aparente de los fotones, dizque el peso de la luz. En aquella torre se encerraron durante diez días y llevaron a cabo sus mediciones con meticuloso empeño. Como ya mencionamos, el efecto de la dilatación temporal debido a la altura es extremadamente pequeño como para ser apreciables a escalas cotidianas. —Si no fuera así, uno debería haber visto a sus vecinos del piso de arriba envejecer más rápido que los del piso de abajo—. Debido a esto, medir tal efecto requería una precisión experimental extrema, una exactitud que es difícil de imaginar con la tecnología rudimentaria de aquellos años.&lt;/p&gt;
&lt;p&gt;Hoy, en épocas en la que los observatorios LIGO y VIRGO miden ondas gravitacionales con una precisión inaudita, en la que los parámetros de la física de partículas se miden en el acelerador LHC en perfecto ajuste con la predicción teórica, en vísperas de la observación de astros distantes con una resolución angular de los pocos microsegundos de arco, es pertinente recordar aquel experimento pionero de Pound y Rebka, en el que con un arreglo experimental casero estos dos físicos lograban medir el peso de la luz.  &lt;/p&gt;
&lt;h3 id=&#34;el-aparente-peso-de-los-fotones&#34;&gt;El aparente peso de los fotones&lt;/h3&gt;
&lt;p&gt;El experimento fue propuesto por Pound y su estudiante en 1959, y ese mismo año lo llevaron a cabo &lt;a href=&#34;#3&#34;&gt;[3]&lt;/a&gt;. Los resultados fueron publicados a comienzo de 1960 en la revista &lt;em&gt;Physical Review Letters&lt;/em&gt; bajo el título de “&lt;em&gt;Apparent weight of photons&lt;/em&gt;”; esto es, “El aparente peso de los fotones” (los fotones son las partículas de las que la luz se compone).&lt;/p&gt;
&lt;p&gt;La primera vez que supe del experimento de Pound y Rebka quedé atónito. Fue en una clase de relatividad general en la Universidad de Buenos Aires, en 1997. El profesor, Rafael Ferraro, nos contaba de los detalles, detalles que están resumidos con su incomparable claridad y concisión en su libro “El espacio-tiempo de Einstein”. ¿Cómo no sorprenderse? Ferraro nos contaba que ya a fines de los años 50s los físicos habían verificado experimentalmente la predicción de Einstein de que la luz pesaba, que la luz caía, gravitaba, que el tiempo se detenía junto a su frecuencia.&lt;/p&gt;
&lt;p&gt;La forma de verificar esto experimentalmente fue la siguiente: Pound y Rebka dispusieron dos fuentes de radiación, una a cada extremo de la torre. Una en aquel altillo (aunque el sitio ha sido refaccionado hace quince años y es difícil vislumbrar hoy su forma original) y otra en el subsuelo del edificio. Esas fuentes de radiación estaban compuestas de un isótopo de hierro, el 57Fe, un átomo que emite o absorbe un rayo gama (i.e. luz de una frecuencia alta muy específica) cuando sus electrones cambian su estado de configuración. La fuente en lo alto de la torre funcionaba como emisor, emitiendo un rayo gama; la fuente en el subsuelo, por el contrario, oficiaba de receptor y venía a absorber el rayo gama emitido por su compañera a más de 22 metros de altura.&lt;/p&gt;
&lt;p&gt;El empleo de 57Fe les permitió a los experimentalistas lograr una precisión fina de las energías emitidas y absorbidas. Habían tratado ya con otros tipos de fuente, de otros materiales, como el isótopo 67Zn, pero los resultados no alcanzaban la precisión requerida.&lt;/p&gt;
&lt;p&gt;El principio físico básico que está detrás del experimento es el siguiente: Por un lado, la energía del rayo gama emitido o absorbido por la fuente es directamente proporcional a la frecuencia de la radiación electromagnética (i.e. al color de la luz). Por otro lado, la frecuencia de la luz (es decir, su color) es la forma en la que la luz expresa el paso del tiempo. La luz se vuelve más azulada cuando el tiempo transcurre más rápido, y se vuelve más rojiza cuando el tiempo transcurre más lento.&lt;/p&gt;
&lt;p&gt;Así, si era cierto que en lo alto de la torre, donde el campo gravitatorio de la tierra es más tenue, el tiempo habría de transcurrir más rápido que en las profundidades del subsuelo, donde el campo gravitatorio de la tierra es más fuerte, entonces la fuente emisora en el altillo emitiría un fotón con una frecuencia que cambiaría ligeramente en su trayecto hasta el sótano. Por lo tanto, abajo, el receptor debería medir un fotón más azulado que aquél que habría dejado la fuente.&lt;/p&gt;
&lt;p&gt;El color de la luz (i.e. la frecuencia) debería cambiar, lo que se entendería, por un lado, como la diferencia en el transcurrir del tiempo en ambos extremos de la torre, y, por el otro, como la ganancia de energía (i.e. de frecuencia) que la luz acumularía durante su caída. El resultado fue contundente: La luz cambió de color. El tiempo transcurría más lento en aquel sótano. Los fotones, aunque siempre viajando a la misma velocidad, igual ganaban energía en su caída, viraban al tono azul, caían. En breve, la luz gravita.&lt;/p&gt;
&lt;h3 id=&#34;el-experimento-de-pound-y-rebka&#34;&gt;El experimento de Pound y Rebka&lt;/h3&gt;
&lt;p&gt;Hubo muchos detalles a tener en cuenta. El experimento pretendía medir un efecto casi inapreciable, de una parte en mil billones; una precisión como aquella de la que aún hoy se enorgullecen los físicos de partículas que la logran en sus aceleradores modernos. El primero de los detalles a tener en cuenta en el arreglo experimental fue el del control de los efectos externos que pudieran disturbar las mediciones. Era imprescindible controlar muy bien el ruido que pudiera provenir de fuentes exógenas. Al tratarse de un experimento que involucraba la emisión y absorción de radiación, era menester blindar el dispositivo.&lt;/p&gt;
&lt;p&gt;También era necesario minimizar las colisiones que los fotones de la radiación gama podrían llegar a sufrir con las moléculas de aire a lo largo de los 22 metros de caída en la torre. Para minimizar los efectos espurios, el experimento fue enfundado por bolsas de un material de aspecto metalizado, similar al que se emplea para empacar algunos componentes electrónicos comerciales.&lt;/p&gt;
&lt;p&gt;Tratándose de un experimento tan sensible, era necesario repetir la emisión y absorción de los fotones muchas veces y durante un tiempo prolongado, a efectos de tener una estadística confiable y minimizar errores de medición. El experimento se llevó a cabo durante diez días en condiciones controladas y con un protocolo preciso. La forma ingeniosa en la que se controló que lo que se estaba observando era, en efecto, la dilatación temporal debido al campo gravitatorio fue la siguiente: Los experimentalistas combinaron ese efecto con el otro efecto de dilatación temporal relativista que mencionamos arriba. Esto es, movieron la fuente a medida que ésta emitía los fotones. Lo hicieron empleando un cilindro hidráulico que les permitía controlar la velocidad con precisión.&lt;/p&gt;
&lt;p&gt;Así, movieron la fuente hacia arriba, alejándola del subsuelo, para que el efecto de dilatación temporal y el efecto Doppler produjeran un corrimiento al rojo que compensara el corrimiento al azul que el fotón acumulaba en su caída. Luego, revirtieron la velocidad, moviendo la fuente hacia abajo, acercándola hacia el emisor, para que el efecto de dilatación temporal y el efecto Doppler ahora se sumaran al del campo gravitatorio y lo reforzaran. La alternancia de ese movimiento permitió evidenciar el efecto más claramente.&lt;/p&gt;
&lt;p&gt;Otro de los problemas más importantes que realizar un experimento de esta naturaleza implicaba era el control de la temperatura de la fuente. La temperatura no es sino la agitación térmica de los átomos que componen un material; en este caso, el movimiento de los átomos que constituían las fuentes. Tal movimiento producía per se un ruido que enmascaraba los efectos que se deseaba medir. Este problema, además, se acentuaba si la temperatura de la fuente emisora y la de la fuente receptora eran diferentes, lo que no era difícil de imaginar tratándose de un subsuelo versus un altillo. Así, era importante no sólo mantener las fuentes lo suficientemente frías sino también asegurarse de que la diferencia de temperatura entre ellas no alcanzara la décima de grado centígrado. Dispusieron, por esto, de una termocupla para controlar la diferencia de temperaturas. No atender a este detalle habría sido fatal para la exactitud de las mediciones. De hecho, intentos por medir este mismo efecto, llevados a cabo por Cranshaw, Schiffer y Whitehead de unos meses antes, habían fracasado por este motivo.&lt;/p&gt;
&lt;p&gt;Por último, algo que lejos de ser un detalle fue crucial para el experimento, llevó a Pound y Rebka a considerar lo que en ese momento era un reciente descubrimiento de la física teórica, el efecto Mössbauer &lt;a href=&#34;#4&#34;&gt;[4]&lt;/a&gt;. Pound era un reconocido físico nuclear y un experto en la interacción entre la materia y la radiación gama. A él se le atribuyen muchos aportes en el área y, al respecto, cabe agregar que su condición de no-Nobel no está libre de controversias. Por lo tanto, él estaba al tanto del efecto descubierto un año atrás, en 1958, por Rudolf Mössbauer; efecto que luego daría origen a la técnica de espectroscopía que lleva el mismo nombre.&lt;/p&gt;
&lt;p&gt;Lo que Mössbauer había mostrado era que el brutal retroceso que un átomo aislado sufre al ser alcanzado por un fotón gama, sería despreciable si tal átomo, en lugar de estar aislado, formara parte de una red cristalina de muchos átomos. El impulso propinado en ese choque se repartiría entre los otros átomos que lo acompañan en la red, de manera tal que el culatazo se tornaría desdeñable. Esto fue muy importante para el experimento de la torre ya que, así, no era necesario preocuparse por la perturbación que tal choque produciría sobre los efectos sutiles que se pretendía medir.&lt;/p&gt;
&lt;p&gt;Todo esto da cuenta del desafío tecnológico que un experimento así implicaba en los años 50s – 60s, un desafío que demandó el ingenio infinito y la atención incorruptible de esos dos grandes físicos. Al cabo de diez días de operación, el experimento cesó. El cociente relativo medido entre la frecuencia de la luz emitida por la fuente y la frecuencia absorbida por el receptor fue de 5,13 10&lt;sup&gt;-15&lt;/sup&gt;, con un error del 10%, en plena concordancia con la predicción de Einstein: 4,9 10&lt;sup&gt;-15&lt;/sup&gt;. Esto no sólo nos da una idea de la increíble precisión del experimento de Pound y Rebka, sino también de la sutil pequeñez del efecto relativista a escalas terrestres.&lt;/p&gt;
&lt;p&gt;Aun así, aunque ligera, la luz llegó a pesar.    &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referencias&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;1&#34;&gt; 1. &lt;/a&gt; A. Einstein, &lt;em&gt;Ann. Physik 35 (1911) 898.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;2&#34;&gt; 2. &lt;/a&gt; R.V. Pound and G.A. Rebka, &lt;em&gt;Phys. Rev. Lett. 3 (1959) 439.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;3&#34;&gt; 3. &lt;/a&gt; R.V. Pound and G.A. Rebka, &lt;em&gt;Phys. Rev. Lett. 4 (1960) 337.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;4&#34;&gt; 4. &lt;/a&gt; R.L. Mossbauer, &lt;em&gt;Z. Physik 151 (1958) 124.&lt;/em&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>¿Cómo hace una bacteria para sobrevivir en condiciones extremas? Nuevos avances acerca de cómo un motor molecular regula la dirección de transporte del cromosoma.</title>
      <link>https://ciencianet.com.ar/post/como-hace-una-bacteria-para-sobrevivir-en-condiciones-extremas-nuevos-avances-acerca-de-como-un-motor-molecular-regula-la-direccion-de-transporte-del-cromosoma/</link>
      <pubDate>Tue, 01 May 2018 23:15:32 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/como-hace-una-bacteria-para-sobrevivir-en-condiciones-extremas-nuevos-avances-acerca-de-como-un-motor-molecular-regula-la-direccion-de-transporte-del-cromosoma/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Augusto Borges&lt;/strong&gt; y &lt;strong&gt;Osvaldo Chara&lt;/strong&gt;. &lt;a href=&#34;https://sysbioiflysib.wordpress.com/&#34;&gt;SysBio&lt;/a&gt;, Instituto de Física de Líquidos y Sistemas Biológicos (CONICET, UNLP), La Plata, Argentina.&lt;/p&gt;
&lt;p&gt;Las bacterias poseen la capacidad de intercambiar &lt;a href=&#34;https://es.wikipedia.org/wiki/%C3%81cido_desoxirribonucleico&#34;&gt;ADN&lt;/a&gt; como un mecanismo de adaptación y supervivencia. Para ello, se pueden valer de motores moleculares capaces de transportar ADN a través de membranas celulares a gran velocidad y con una dirección definida (&lt;em&gt;i.e&lt;/em&gt;. la bacteria que posee el ADN debe asegurarse que la bacteria receptora reciba una copia completa de ADN una vez que el proceso se ha iniciado). &lt;em&gt;Bacillus subtilis&lt;/em&gt; es una bacteria que, si las condiciones ambientales no fueran favorables, posee la asombrosa capacidad de crear una nueva versión de sí misma, capaz de sobrevivir durante decenas de años en un estado ‘durmiente’. Para lograr esto, &lt;em&gt;B. subtilis&lt;/em&gt; lleva adelante un proceso conocido como &lt;a href=&#34;https://es.wikipedia.org/wiki/Esporulaci%C3%B3n&#34;&gt;esporulación&lt;/a&gt; donde una copia completa del &lt;a href=&#34;https://es.wikipedia.org/wiki/Cromosoma&#34;&gt;cromosoma&lt;/a&gt; bacteriano se transporta rápidamente hacia una célula hija. Esta célula hija (conocida como espora) posee un tamaño mucho menor que la célula madre y su contenido y membrana estarán adaptados para conferir resistencia a numerosas perturbaciones externas (tales como cambios de temperatura, deshidratación o radiaciones). Para garantizar que este proceso se lleve a cabo de manera exitosa, la bacteria emplea un motor molecular, la proteína SpoIIIE, capaz de transportar ADN usando la energía provista por la hidrólisis de &lt;a href=&#34;https://es.wikipedia.org/wiki/Adenos%C3%ADn_trifosfato&#34;&gt;ATP&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/cUlbXnchQ_M&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Descripción del video:&lt;/strong&gt; Interacción del motor SpoIIIE con una molécula única de ADN  que contiene secuencias específicas (SRS) en presencia de ATP. Se muestra la dinámica de interacción y translocación de ADN predicha por un modelo matemático que asume que el motor SpoIIIE se une/desune del ADN y es capaz de  deslizarse mediante difusión y translocación a lo largo del ADN. La región coloreada en azul indica la posición de las secuencias específicas. En rojo y verde se observan eventos de translocación y unión-difusión. En blanco se muestran sitios del ADN no ocupados por el motor.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;El mecanismo por el cual este motor logra unirse al cromosoma y desplazarlo en la dirección adecuada aún no había sido completamente elucidado. Pocos días atrás, publicamos un trabajo en &lt;a href=&#34;https://www.nature.com/srep/&#34;&gt;&lt;strong&gt;Scientific Reports&lt;/strong&gt;&lt;/a&gt;, en el que descubrimos lo que creemos es un paso clave en el mecanismo subyacente al accionar del motor molecular SpoIIIE. Este trabajo es el fruto de una colaboración entre nuestro grupo de trabajo, &lt;a href=&#34;https://sysbioiflysib.wordpress.com/&#34;&gt;SysBio&lt;/a&gt;, en el &lt;a href=&#34;http://iflysib.unlp.edu.ar/&#34;&gt;Instituto de Física de Líquidos y Sistemas Biológicos&lt;/a&gt; (CONICET, UNLP) en La Plata y el grupo de Marcelo Nöllmann, en el &lt;a href=&#34;http://marcnol.weebly.com/&#34;&gt;Centre de Biochimie Structurale&lt;/a&gt; (CNRS/INSERM/UM) Montpellier, Francia.&lt;/p&gt;
&lt;p&gt;En un &lt;a href=&#34;http://embor.embopress.org/content/14/5/473&#34;&gt;artículo previo&lt;/a&gt; publicado en &lt;strong&gt;EMBO Reports&lt;/strong&gt;, esta colaboración nos permitió determinar que, en ausencia de ATP, el motor molecular SpoIIIE es capaz de explorar rápidamente largas regiones de ADN por un mecanismo de difusión lineal para encontrar secuencias específicas (llamadas SRS) que informan la dirección de transporte de ADN (&lt;em&gt;i.e.&lt;/em&gt; secuencias que actúan como señales de tráfico e indican en qué dirección avanzar). Una vez que el motor se une a estas secuencias, el mecanismo que estabiliza esta interacción es regulado por la diferencia en la velocidad de disociación con respecto al ADN no específico.&lt;/p&gt;
&lt;p&gt;En el nuevo trabajo publicado en &lt;strong&gt;Scientific Reports&lt;/strong&gt;, investigamos si las secuencias específicas SRS regulan la dirección  de transporte del ADN al reclutar y orientar SpoIIIE o si simplemente catalizan su actividad de translocación. Para ello, empleamos técnicas de microscopía de fuerza atómica y ensayos de translocación de cinética rápida para determinar la localización y la dinámica de la difusión y la translocación de complejos SpoIIIE en el ADN, con o sin secuencias específicas SRS. Luego, combinamos nuestros resultados con modelos matemáticos para encontrar que SpoIIIE determina la dirección de transporte del ADN, a través de la regulación catalítica de su actividad motora. De este modo, SpoIIIE reconoce las secuencias específicas SRS que determinan la dirección de translocación y en ese caso su probabilidad de activación se ve incrementada varios órdenes de magnitud.&lt;/p&gt;
&lt;p&gt;Este simple mecanismo, combinado un mecanismo complementario basado en la exploración activa de ADN por SpoIIIE, sería suficiente para garantizar la direccionalidad de transporte de ADN &lt;em&gt;in vivo&lt;/em&gt; durante la esporulación.&lt;/p&gt;
&lt;p&gt;Entender cómo funcionan este tipo de motores moleculares puede tener repercusiones en distintas áreas de la biología, ya que estos motores pertenecen a una gran familia de proteínas que incluye translocasas y helicasas. Estas proteínas tienen papeles importantes, no solo en el transporte de ADN, sino también en la replicación, recombinación y reparación de ADN, regulación de expresión génica, maduración y transporte de ARN mensajero y muchos otros.&lt;/p&gt;
&lt;hr&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/c6BNcfPHF4Y&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Descripción del video:&lt;/strong&gt; Interacción del motor SpoIIIE con un ADN teniendo regiones específicas (SRS) en ausencia de ATP predichas por el modelo matemático mencionado en el video anterior. En el panel superior y medio, se muestran los motores uniéndose o deslizándose en el ADN en tiempo real y su acumulación, respectivamente. Se utilizan cinco ADNs representativos. El panel inferior muestra la frecuencia de motores unidos a la secuencia específica del ADN a medida que avanza el tiempo.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; &lt;a href=&#34;https://www.nature.com/articles/s41598-018-23400-8&#34;&gt;Sequence-dependent catalytic regulation of the SpoIIIE motor activity ensures directionality of DNA translocation&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Patricia Massolo - La mujer que nunca conocí</title>
      <link>https://ciencianet.com.ar/post/patricia-massolo/</link>
      <pubDate>Sat, 28 Apr 2018 14:59:04 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/patricia-massolo/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero.&lt;/strong&gt; Investigadora de CONICET en el INIFTA (Instituto de Investigaciones Fisicoquímicas Teóricas y Aplicadas). Docente en la UNLP.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;La mujer que nunca conociste&lt;/strong&gt;&lt;/em&gt; es el nombre de una campaña de Wikipedia destinada a visibilizar el trabajo de las mujeres en la cual se invita a crear entradas sobre mujeres destacadas en alguna de las áreas del desarrollo humano. La ciencia es una de las áreas en la que los aportes de las mujeres no han sido suficientemente difundidos o valorados. De modo que, aunque no como entrada de Wikipedia sino como un escrito de tipo más personal, acepto el desafío de escribir sobre una mujer destacada de la Física, a quien -lamentablemente- nunca conocí. Claudia Patricia Massolo -Patricia, como la llamaba su entorno- fue una física argentina que se especializó en el área de Materia Condensada y Física Nuclear. Nacida en Monte Grande, se recibió de Licenciada en Física en la Facultad de Ciencias Exactas de la UNLP, donde se doctoró en 1977. Fue investigadora de CONICET en La Plata y se desempeñó como docente del Departamento de Física, alcanzando el cargo de profesora en 1986. A lo largo de su carrera realizó contribuciones disciplinares pero también se destacó por su compromiso con el medio ambiente y la salud, con los derechos humanos y con la docencia. Patricia Massolo falleció tempranamente, en Junio de 1993, a la edad de 43 años, de un cáncer de estómago.  Fue compañera del físico platense Fidel Schaposnik y los hijos de ambos también se dedican a la ciencia.&lt;/p&gt;
&lt;h3 id=&#34;compromiso-con-el-medio-ambiente&#34;&gt;&lt;strong&gt;Compromiso con el medio ambiente&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2018/04/a10-634x1024.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Patricia  tuvo varias participaciones en la determinación de contaminación radiactiva en el ambiente y alimentos. Respecto de este compromiso que asumía al poner sus saberes en función de aportes concretos a la sociedad, decía &amp;quot;es indispensable que la comunidad sepa que este es uno de los roles fundamentales de la Universidad&amp;quot;. Junto a su colega Judith Desimoni, también física de La Plata, Patricia participó de la identificación de componentes radioactivas en residuos de origen industrial que fueron encontrados en el año 1989 por vecinos  en una cantera cercana al Aeropuerto de la ciudad de La Plata &lt;a href=&#34;#1&#34;&gt;[1]&lt;/a&gt;. Tal vez por este antecedente, en marzo de 1992 la Fundación Greenpeace  de América Latina contactó a Patricia Massolo y le hizo llegar muestras de leche en polvo marca Jorgiano que el Estado había comprado a proveedores afines al gobierno, destinada a ser entregada a niños y embarazadas en planes sociales.&lt;/p&gt;
&lt;p&gt;Esta leche presentaba características macroscópicas que evidenciaban que no estaba en buen estado. Se sospechaba que podría ser mezcla de leches adquiridas por los empresarios a muy bajo precio a países afectados por desechos de la &lt;a href=&#34;https://es.wikipedia.org/wiki/Accidente_de_Chern%C3%B3bil&#34;&gt;explosión de Chernobyl&lt;/a&gt;.  Según relata Guillermo Bibiloni en su artículo &lt;em&gt;De la Espectroscopía Óptica a la Nuclear. Dos Mujeres de la Física Preocupadas por La Salud&lt;/em&gt; &lt;a href=&#34;#2&#34;&gt;[2]&lt;/a&gt;, Massolo preparó muestras de leche marca Jorgiano y marca Cotar (de Santa Fe, que podía tomarse como estándar) y midió tanto la presencia de &lt;a href=&#34;https://es.wikipedia.org/wiki/Is%C3%B3topo&#34;&gt;isótopos&lt;/a&gt; radiactivos de &lt;a href=&#34;https://es.wikipedia.org/wiki/Potasio&#34;&gt;potasio&lt;/a&gt; (compatibles con la radiactividad natural de las pasturas de la zona) como también de isótopos radiactivos de &lt;a href=&#34;https://es.wikipedia.org/wiki/Cesio&#34;&gt;cesio&lt;/a&gt;, que son subproductos de la actividad nuclear.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2018/04/espectros-768x664.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;El detector semiconductor para bajo conteo de germanio hiperpuro que tenía disponible en su laboratorio no era la mejor herramienta para la tarea, pero eso no la detuvo. Con ayuda a una colega de la Comisión Nacional de Energía Atómica, Patricia Massolo obtuvo espectros de radiación gamma de las diferentes muestras de leche y pudo determinar que en la leche Cotar no se detectó la presencia de cesio radioactivo pero en la leche Jorgiano detectó los isótopos 137Cs y 134Cs en proporciones tales que coincidían con la que debía esperarse si se hubieran producido en la Central de Chernobyl y en una fecha coincidente con la explosión. Para poder afirmar esto Massolo utilizó el coeficiente de &lt;a href=&#34;https://es.wikipedia.org/wiki/Abundancia_natural&#34;&gt;abundancia&lt;/a&gt; relativa de los isótopos de Cesio en muestras de leche de marca Frisolac, recogidas en Holanda poco después del accidente de Chernobyl, y le aplicó un factor de corrección que contempla la diferente vida media de los isótopos. Por otro lado, la menor concentración del isótopo 40K en la leche Jorgiano respecto a los valores esperables para Argentina sugería que podría tratarse de una mezcla de leche argentina e importada de países afectados.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2018/04/a16-502x1024.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Las serias implicaciones de las conclusiones que arrojaban estos estudios, realizados entre junio y octubre de 1992, motivaron a que Patricia Massolo pidiera ayuda a sus colegas europeos para una verificación de los resultados. De este modo, Patricia Massolo hizo llegar muestras de leche a Holanda, al Kernfysisch Versneller Instituut de la Universidad de Groninger, donde los hallazgos fueron confirmados poco después.  A raíz de este caso, se presentó en la Cámara de Senadores de la Provincia de Buenos Aires un proyecto para controles de contaminantes y substancias radioactivas que incluía explícitamente al Laboratorio de Física Nuclear del Departamento de Física de la UNLP como ente de control. Además de tener sustancias radiactivas la leche estaba contaminada también con escherichia coli. La situación tomó amplio estado público como uno de los mayores casos de corrupción de Argentina, ya que  los dueños de la empresa proveedora de la leche, &lt;a href=&#34;https://es.wikipedia.org/wiki/Miguel_%C3%81ngel_Vicco&#34;&gt;Miguel Angel Vicco&lt;/a&gt; y &lt;a href=&#34;https://es.wikipedia.org/wiki/Carlos_Spadone&#34;&gt;Carlos Spadone&lt;/a&gt;, fueron secretarios privados de Carlos Menem, el entonces presidente.  Luego de idas y vueltas legales, los empresarios responsables,  Vicco y Carlos Spadone, fueron sobreseídos y la causa fue cerrada en 2005, al considerarla proscripta por cuestiones técnicas. Poco antes de su fallecimiento fue nuevamente consultada por Greenpeace cuando en el gobierno de Menem se reflotó un proyecto para la instalación de un repositorio nuclear o &lt;strong&gt;&lt;em&gt;&amp;quot;basurero nuclear&amp;quot;&lt;/em&gt;&lt;/strong&gt; en Gastre (Chubut) que recibiría desechos de combustibles agotados provenientes de las plantas argentinas y de otros países.&lt;/p&gt;
&lt;h3 id=&#34;docencia&#34;&gt;&lt;strong&gt;Docencia&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Patricia Massolo desempeñó sus tareas como docente en el Departamento de Física de la Facultad de Ciencias Exactas de la UNLP. Durante los años en que ejerció la docencia ocurrió la transición entre el viejo plan de estudios  y el nuevo diseño del año 1989, de modo que se crearon nuevas asignaturas, como Experimentos Cuánticos I y II y se modificaron algunas metodologías de trabajo. En ese contexto Patricia fue partícipe de varias experiencias innovadoras, en algunas junto a su compañero. La publicación de los pequeños trabajos de investigación que hacían los jóvenes estudiantes de la Licenciatura en Física para graduarse (Tesinas o Trabajos de Diploma) fue un aspecto en el que también fue pionera en el Departamento. También tuvo a su cargo, junto a Guillermo Bibiloni, la formación de tesistas de doctorado: Mario Rentería, Sergio Moreno, Félix Requejo y Jorge Shitu. Patricia animaba fuertemente a sus tesistas a viajar a otros países como parte de su formación como científicos, aún en temas que no fueran estrictamente los de su plan de trabajo.&lt;/p&gt;
&lt;h3 id=&#34;derechos-humanos&#34;&gt;&lt;strong&gt;Derechos humanos&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Otro de los aspectos en que Patricia Massolo tuvo un rol activo es el de los Derechos Humanos. Patricia presentó junto a su compañero en 1986 una solicitud a la Universidad Nacional de La Plata, avalada por más de 100 docentes, estudiantes e investigadores del Departamento de Física de la Facultad de Ciencias Exactas, para realizar un homenaje a las víctimas que había generado en la comunidad académica la dictadura cívico-militar de 1976 &lt;a href=&#34;#3&#34;&gt;[3]&lt;/a&gt;. Esta iniciativa, que se considera pionera en la UNLP, se concretó con la colocación de una placa en el acceso al edificio del Departamento de Física donde se lee:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Son varios los miembros de este Departamento que han sufrido persecución, tortura y muerte. Jorge Bonafini, Federico Ludden y María de los Ángeles Valeriani aún permanecen desaparecidos. Esta placa representa nuestro reclamo permanente a su aparición con vida y el compromiso indestructible de exigir justicia e impedir que se repita lo sucedido&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Me animo a afirmar que Patricia se sentiría orgullosa al saber que su discípulo, el físico platense Mario Rentería, es el director &lt;em&gt;ad honorem&lt;/em&gt; de  DD. HH. de la Facultad de Ciencias Exactas, y gracias a su iniciativa uno de los edificios de la institución recibió el nombre de &lt;em&gt;Edificio Abuelas de Plaza de Mayo&lt;/em&gt;. Su temprano fallecimiento dejó una profunda ausencia que se percibe entre quienes trabajaron con ella. Sus colegas escribieron una sentida dedicatoria para homenajearla, en la cual la describen como una persona cálida, inteligente, entusiasta y plena de alegría &lt;a href=&#34;#4&#34;&gt;[4]&lt;/a&gt;. En el año 2003 se realizó una muestra para recordar a Patricia en el &lt;a href=&#34;http://museo.fisica.unlp.edu.ar/&#34;&gt;Museo del Departamento de Física&lt;/a&gt; de la Facultad de Ciencias Exactas de la UNLP, con participación de la comunidad académica. Para la ocasión, el periodista Horacio Verbitsky escribió una síntesis sobre el caso de la leche contaminada y el rol que tuvo Patricia en él:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;quot;Me impresiona advertir cuántos años han pasado. Recordé los hechos en septiembre de 2002, cuando un juez que jugaba al tenis con Menem cerró la causa y absolvió a los últimos procesados en la causa que se inició con las notas de Página/12. Por si alguien aún no lo supiera, el juez Ballestero recordó en su sentencia que ninguno de los procesados había pasado un solo día privado de su libertad, ni el secretario presidencial Miguel Angel Vicco, ni los hermanos Carlos y Lorenzo Spadone, ni los menos conocidos responsables de la venta al Estado para los planes de asistencia a los pobres más pobres de leche contaminada con bacterias y sustancias radiactivas, tal vez provenientes de Chernobyl. El inicuo fallo consigna que la causa no puso avanzar porque el ministerio de Salud, que los mismos delincuentes controlaban, celebró un acuerdo con las empresas proveedoras y desistió de la acción. Ahora que nuestro maltratado país intenta revalorizar lo público, recordamos la actitud de Claudia Patricia Massolo. Ella expresa a quienes en aquellos años sórdidos no se apartaron de los principios éticos más elementales y pusieron su saber al servicio de la sociedad. Ya que no cárcel, vergüenza eterna para quienes son su contracara y usaron sus cargos en beneficio personal con desprecio absoluto por la suerte de los demás, incluyendo la salud y la vida de los más indefensos. Horacio Verbitsky&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Volviendo a la época en que supe de la existencia de Patricia, recuerdo que su compañero Fidel Schaposnik solía buscar la motivación de los alumnos en las clases de cuántica diciéndonos, por ejemplo, que &lt;em&gt;&amp;quot;Dirac, a la edad que tienen ustedes, ya había hecho todo esto…&amp;quot;&lt;/em&gt; Su intención creo que era buena, pero a algunos Dirac no nos entusiasmaba particularmente. Algunos queríamos -algún día- parecernos un poquito a Patricia.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Agradezco a Mario Rentería y a Jorge Shitu por las charlas y el material que generosamente compartieron conmigo. Un especial agradecimiento para Fidel Schaposnik por su tiempo, sus relatos, y sobre todo, por la recomendación que me dio -previa  a la lectura del texto- de hacer públicas estas palabras sobre Patricia aún si él no estuviera de acuerdo... alentándome a hacer justamente lo que hacía ella.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;referencias&#34;&gt;Referencias&lt;/h4&gt;
&lt;p&gt;&lt;a name=&#34;1&#34;&gt;&lt;/a&gt;1. C.P. Massolo y J. Desirnoni, &amp;quot;Estudio sobre componentes radioactivas presentes en residuos encontrados en la cantera situada en 6bis y 622, Zona Aeropuerto, La Plata&amp;quot;, Informe Final, 1989.&lt;/p&gt;
&lt;p&gt;&lt;a name=&#34;2&#34;&gt;&lt;/a&gt;2. A.G. BIBILONI. &lt;em&gt;De la Espectroscopía Óptica a la Nuclear. Dos Mujeres de la Física Preocupadas por La Salud.&lt;/em&gt; Acta Farm. Bonaerense 18 (3): 231-5 (1999).&lt;/p&gt;
&lt;p&gt;&lt;a name=&#34;3&#34;&gt;&lt;/a&gt;3. Materia Pendiente Nº 25. Revista de la Facultad de Ciencias Exactas de la UNLP. &lt;a href=&#34;http://www.exactas.unlp.edu.ar/uploads/docs/materia&#34;&gt;http://www.exactas.unlp.edu.ar/uploads/docs/materia&lt;/a&gt;_pendiente__25.pdf.&lt;/p&gt;
&lt;p&gt;&lt;a name=&#34;4&#34;&gt;&lt;/a&gt;4. Gary Crawley and Sydney Gales. Nuclear Physics A Volume 569, Issues 1–2, 7 March 1994, Page vii  &lt;a href=&#34;https://doi.org/10.1016/0375-9474(94)90089-2&#34;&gt;https://doi.org/10.1016/0375-9474(94)90089-2&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;comentarios&#34;&gt;Comentarios&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;renteria@fisica.unlp.edu.ar&#34;&gt;Mario Rentería&lt;/a&gt; - &lt;time datetime=&#34;2018-04-30 05:23:26&#34;&gt;Apr 1, 2018&lt;/time&gt;&lt;/p&gt;
&lt;p&gt;Bueno, siendo las primeras horas del 30 de Abril de 2018, a 41 años de la primera ronda de las Madres de Plaza de Mayo, me alegra constatar que las semillas que se siembran broten en el momento menos esperado, y más oportuno, en estos días nuevamente aciagos en nuestro país. Leo esto con gran orgullo, con la vista más que nublada por lágrimas, que ya no son de tristeza, pero tuvieron que pasar muchos años... Aprovecho este espacio para hacer algunos comentarios. Me sorprendió ver el texto de la primera placa que pusimos en 1986 en homenaje a los compañeros detenidos-desaparecidos del Departamento de Física durante la última dictadura cívico-militar-clerical (sólo se decía &amp;quot;militar&amp;quot; en esos años) . Homenaje que en realidad comprendía, y sigue siendo, el reclamo de aparición con vida al Estado y la reivindicación de los ideales por los que lucharon los 30.000, que nos permitieron recobrar la democracia, como se puede leer en parte del expediente que le dio origen. Tardé en darme cuenta que ese texto provenía de la nota de la referencia 3, en la cual es errónea la mención a María &amp;quot;de los Angeles&amp;quot; Valeriani. Solo el apellido de María de los Milagros Baleriani estaba mal escrito en esa primera placa de vidrio, debido a dificultades en el proceso de reconstrucción de Memoria. Horacio Verbitsky fue el único periodista que respondió rápida y positivamente a una serie de mails que envié en 2003 a las plumas más conocidas de los periódicos en esos años. Sus palabras, que se leían en una gigantografia en papel que colgaba desde los balcones del Museo de Física, aún hoy resuenan muy actuales... Los dos colegas que describen a Patricia en la referencia 4 son dos enormes físicos nucleres experimentales, el primero de la Michigan State University (USA) y el segundo del Institut de Physique Nucleaire d´ Orsay (Francia), además de relevante gestor de la ciencia francesa. Finalmente, me quedo con las palabras de Fidel, que nos alienta a hacer lo que justamente hacía Patricia, dar testimonio de lo que creemos verdadero aún cuando no todos puedan estar de acuerdo. Gracias Paula por este trabajo de memoria y por un nuevo, y para mí siempre insuficiente, homenaje a Patricia.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;admedus@gmail.com&#34;&gt;Andrés&lt;/a&gt; - &lt;time datetime=&#34;2018-05-02 10:59:40&#34;&gt;May 3, 2018&lt;/time&gt;&lt;/p&gt;
&lt;p&gt;Brillante! Muy lindo texto. Gracias Paula!&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Materia oscura, ondas gravitacionales y agujeros negros primordiales</title>
      <link>https://ciencianet.com.ar/post/materia-oscura-ondas-gravitacionales-y-agujeros-negros-primordiales/</link>
      <pubDate>Thu, 19 Apr 2018 15:31:31 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/materia-oscura-ondas-gravitacionales-y-agujeros-negros-primordiales/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Gastón Giribet.&lt;/strong&gt; Investigador del Center for Cosmology and Particle Physics de New York University. Profesor de la UBA e Investigador Principal del CONICET.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Trabajos recientes sugieren que una parte importante (se estima que hasta un 1%) de la materia oscura del universo podría estar formada por agujeros negros primordiales; es decir, por agujeros negros que no se formaron a partir de la muerte de estrellas sino que existen desde los orígenes del universo. Esta es una idea que tomó fuerza en los últimos dos años y que intentamos resumir aquí.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Dos años atrás, luego de que la colaboración LIGO &lt;a href=&#34;#1&#34; title=&#34;1&#34;&gt;[1]&lt;/a&gt; detectara ondas gravitacionales producidas por lo que, según entendemos, es la coalescencia de dos agujeros negros de aproximadamente 30 masas solares, una vieja pero renovada idea comenzó a ganar terreno: Los agujeros negros con masas dentro del rango de 20 a 100 masas solares podrían explicar una porción -acaso substancial- de la materia oscura del universo.&lt;/p&gt;
&lt;p&gt;La materia oscura, sabemos, es la materia responsable de mantener las galaxias unidas, impidiendo que éstas se disgreguen debido a la fuerza centrífuga. En efecto, la velocidad de rotación observada en las galaxias es mucho mayor que la que uno podría esperar si ellas sólo contuviesen la materia que en ellas vemos brillar. A tales velocidades de rotación la materia que “vemos” no sería suficiente como para contrarrestar la fuerza centrífuga y las galaxias terminarían por desmembrarse y esparcirse en el medio intergaláctico.&lt;/p&gt;
&lt;p&gt;Se infiere de esto –y de otras observaciones de diferente naturaleza– que hay en el universo mucha más materia que la que nosotros vemos. La gravedad que genera esa “materia oscura” termina por delatarla. La materia que “vemos” es la poca que tiene la suerte de interactuar con la luz que nos permite verla. La gran mayoría de la materia es oscura o, mejor dicho, perfectamente transparente; es sólo a través de su influencia gravitatoria que sabemos de ella.&lt;/p&gt;
&lt;p&gt;Se impone, así, la pregunta: ¿Qué es la materia oscura? ¿Qué la compone? ¿Está formada acaso de partículas que, como los neutrinos, no interactúan con la luz y sólo lo hacen débilmente con la materia nuclear? ¿Está formada la materia oscura de partículas que ni siquiera con la fuerza nuclear interactúan dándole la exclusividad a la fuerza gravitatoria? ¿Es la materia oscura simplemente un gas de objetos estelares oscuros, compactos, opacos?, ¿o se trata de un montón de partículas frías débilmente interactuantes que inundan el espacio y nos atraviesan imperceptibles por aquello de que percibir es también ver? En definitiva, ¿qué tipo de materia es la materia oscura?&lt;/p&gt;
&lt;p&gt;Una vieja idea que, aunque se creyó descartada hace tiempo, hoy reaparece con renovado ímpetu propone que parte de la materia oscura se compone de objetos astronómicos masivos y compactos; más precisamente, de agujeros negros con masas que rondan la decena de veces la masa de nuestro sol. No toda la materia oscura puede deberse a ellos, pero sí es posible que alguna porción de ella sea simplemente eso, agujeros negros.&lt;/p&gt;
&lt;p&gt;Dos artículos sobre esta idea que llamaron mi atención en su momento, allá por marzo de 2016. Éstos son &lt;a href=&#34;#2&#34; title=&#34;2&#34;&gt;[2]&lt;/a&gt; y &lt;a href=&#34;#3&#34; title=&#34;3&#34;&gt;[3]&lt;/a&gt;, ambos publicados en &lt;em&gt;Physical Review Letters&lt;/em&gt;. En esos trabajos se especula que la abundancia de sistemas binarios de agujeros negros podría dar cuenta de parte de la materia oscura que sabemos que existe en el universo. Un sistema binario –en este caso de agujeros negros– es un par de astros que rota uno en torno al otro para entrar, al cabo de tiempos prolongados, en un movimiento espiral que culmina en la coalescencia de ambos y la consecuente producción de ondas gravitacionales.&lt;/p&gt;
&lt;p&gt;Es eso lo que LIGO observa, esas ondas. Esto permite estimar la tasa de ocurrencia de esos eventos de coalescencia: del orden de las decenas de eventos por cada Giga-Pársec cubo por cada año. Teniendo en cuenta esta tasa, los trabajos de marzo de 2016 afirman que los sistemas binarios de agujeros negros con masas de alrededor de algunas docenas de masas solares podrían ser, en efecto, la huella de la materia oscura &lt;a href=&#34;#2&#34; title=&#34;2&#34;&gt;[2]&lt;/a&gt;. Estos agujeros negros tienen que ser, por supuesto, agujeros negros primordiales; es decir, agujeros negros que no se formaron por el colapso gravitacional tras la muerte de estrellas sino que nacieron mucho antes de que ninguna estrella haya existido, durante las primeras fracciones de segundo del universo, una etapa evolutiva del cosmos en la que la física de partículas fundamentales era la que dominaba la escena.&lt;/p&gt;
&lt;p&gt;Este origen “primordial” de los agujeros negros que formarían parte de la materia oscura es necesario debido a que sabemos que la materia oscura ha estado en el universo desde sus comienzos; por lo tanto, si no queremos entrar en conflicto con lo que sabemos sobre la cosmología del universo temprano, es mejor que estos objetos hayan estado allí desde el inicio, i.e. que sean primordiales.  &lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2018/04/agujeros.png&#34; alt=&#34;Los sistemas binarios de agujeros negros de origen primordial, con masas de alrededor de algunas docenas de masas solares, podrían ser responsables de una parte de la materia oscura.&#34;&gt;&lt;/p&gt;
&lt;p&gt;La idea de que la materia oscura podría estar compuesta de astros masivos no es nueva sino que data de varias décadas. Incluso durante mucho tiempo se la creyó una idea anticuada, ya descartada. Se pensaba hasta no hace más de uno o dos años que la cantidad de agujeros negros primordiales en el universo no podía ser apreciable debido a que, si lo fuera, no podríamos explicar los datos cosmológicos que observamos.&lt;/p&gt;
&lt;p&gt;Las restricciones observacionales sobre la abundancia de agujeros negros primordiales vienen principalmente de dos fenómenos: los efectos distorsivos de lentes gravitacionales y el efecto que este tipo de astros habría tenido sobre la radiación cósmica de fondo. No haber observado esos efectos hizo pensar a los especialistas por más dos décadas que, de existir, los agujeros negros primordiales debían ser muy pocos como para que su presencia en el cosmos fuera relevante.&lt;/p&gt;
&lt;p&gt;La historia cambió en 2016-2017, cuando nuevos estudios advirtieron que se habían sobreestimado las cotas para la abundancia de agujeros negros primordiales. Quedaba una posibilidad: Un análisis detallado muestra que existe una ventana de valores de masa, que va de entre 20 a 100 masas solares, para los cuales las cotas no son tan estrictas. Esto es, las observaciones cosmológicas no descartan una abundancia notable de agujeros negros primordiales siempre y cuando la masa de éstos sea de unas pocas decenas de masas solares.&lt;/p&gt;
&lt;p&gt;Lo interesante es que, justamente, ¡es ese el rango de masas detectado por LIGO! ¿Podemos decir, entonces, que lo que LIGO detectó no fueron sólo ondas gravitacionales sino también materia oscura? Fue esa la posibilidad que se sugería en &lt;a href=&#34;#2&#34; title=&#34;2&#34;&gt;[2]&lt;/a&gt;.  Esto reabrió la puerta a la idea de que la materia oscura o, mejor dicho, un porcentaje de ella, podría estar formada por agujeros negros, agujeros negros primordiales que forman sistemas binarios de algunas decenas de masas solares, sistemas binarios que hoy están en coalescencia y cuyo estallido final estamos comenzando a oír. Los autores de &lt;a href=&#34;#3&#34; title=&#34;3&#34;&gt;[3]&lt;/a&gt; hacen declaraciones concisas al respecto: Primero, el primer evento de ondas gravitatorias detectado por LIGO, denominado GW150914, se puede explicar por la coalescencia de los agujeros negros primordiales. Segundo, la tasa de fusión prevista para los agujeros negros primordiales estaría en concordancia con la tasa estimada por las colaboraciones de LIGO y Virgo si se asume que tales objetos constituyen cierta fracción de materia oscura. Una revisión más reciente del análisis, incorporando cálculos más detallados y teniendo en cuenta más efectos &lt;a href=&#34;#4&#34; title=&#34;4&#34;&gt;[4]&lt;/a&gt;, concluye que, según lo que entendemos hoy, hasta el 1% de la materia oscura podría estar constituida de agujeros negros. (Uno de los autores del trabajo &lt;a href=&#34;#4&#34; title=&#34;4&#34;&gt;[4]&lt;/a&gt; confirma que esta estimación es aceptada actualmente). Cabe decir también que esta idea, aunque compartida por varios expertos, ha sido recibida con escepticismo por otra parte de la comunidad especializada.&lt;/p&gt;
&lt;p&gt;Fuera como fuere, es una idea ciertamente interesante: ¿Está la materia oscura constituida de un conjunto de astros con unos pocos cientos de kilómetros de diámetro cada uno?  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referencias:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;1&#34;&gt; 1. &lt;/a&gt; LIGO and The Virgo Scientific collaborations, Phys. Rev. Lett. 116, 061102 (2016), &lt;a href=&#34;https://arxiv.org/abs/1602.03837&#34;&gt;arxiv:1602.03837&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;2&#34;&gt; 2. &lt;/a&gt;  S. Bird et al. Phys. Rev. Lett. 116, 201301 (2016), &lt;a href=&#34;https://arxiv.org/pdf/1603.00464.pdf&#34;&gt;arxiv:1603.00464&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;3&#34;&gt; 3. &lt;/a&gt;  M. Sasaki et al., Phys. Rev. Lett. 117, 061101 (2016), &lt;a href=&#34;https://arxiv.org/pdf/1603.08338.pdf&#34;&gt;arxiv:1603.08338&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;4&#34;&gt; 4. &lt;/a&gt; Y. Ali-Haïmoud et al., Phys. Rev. D 96, 123523 (2017), &lt;a href=&#34;https://arxiv.org/pdf/1709.06576.pdf&#34;&gt;arxiv:1709.06576&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Memorias del horizonte</title>
      <link>https://ciencianet.com.ar/post/memorias-del-horizonte/</link>
      <pubDate>Thu, 22 Mar 2018 15:48:26 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/memorias-del-horizonte/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Gastón Giribet.&lt;/strong&gt; Investigador del Center for Cosmology and Particle Physics de New York University. Profesor de la UBA e Investigador Principal del CONICET.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;En 1976, Stephen Hawking publicó en The Physical Review un artículo inquietante en el que exponía su ya célebre “paradoja de la pérdida de información en los agujeros negros”, una observación que vendría a conturbar el ambiente de la física teórica por más de cuatro décadas. Durante este tiempo, la comunidad científica especializada ha visto retornar ese monstruo de la razón con formas renovadas, cada vez más temibles, una y otra vez; desde aquel artículo seminal &lt;a href=&#34;#1&#34; title=&#34;2&#34;&gt;[1]&lt;/a&gt; en los 1970s hasta las más recientes discusiones &lt;a href=&#34;#2&#34; title=&#34;2&#34;&gt;[2]&lt;/a&gt;, reavivadas con especial ahínco desde 2012. Aquí, un sesgo dialéctico nos lleva a recortar la enmarañada historia del problema y centrarnos en lo que fue el último intento de Hawking por resolver su célebre acertijo; lo que podríamos considerar su última idea.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;la-pérdida-de-información-en-los-agujeros-negros&#34;&gt;La pérdida de información en los agujeros negros&lt;/h3&gt;
&lt;p&gt;Los agujeros negros son los objetos más intrigantes del universo. Siendo la causa de los eventos más enérgicos y violentos en el cosmos, estos objetos astrofísicos desafían nuestra intuición como ningún otro fenómeno en la naturaleza, obligándonos a reconsiderar las leyes fundamentales de la física. Una de las propiedades fascinantes de los agujeros negros es su universalidad: Los agujeros negros son, en cierto sentido, únicos. Independientemente de las características de la estrella moribunda, que, después de su muerte, da lugar a la formación de un agujero negro, la forma de este último resulta ser la misma, indiferente a los detalles de la estrella original; el agujero negro sólo heredará la masa total y la velocidad de rotación del astro progenitor, y ningún otro rasgo.&lt;/p&gt;
&lt;p&gt;Esto significa que, independientemente de la composición química de la estrella, o su densidad, o cualquier otra característica, las propiedades del agujero negro resultante sólo dependerán de dos datos, la masa y la velocidad de rotación, sin tener en cuenta ninguna otra información. Cuando se observó por primera vez, este fenómeno de universalidad de los agujeros negros no representó más que una curiosidad; sin embargo, adquirió una dimensión drástica cuando, en 1974, Stephen Hawking descubrió que la mecánica cuántica predice que los agujeros negros emiten una forma débil pero persistente de radiación. Esta radiación, producida por la creación de partículas y antipartículas en las proximidades del agujero negro, es un proceso imparable que finalmente conduce a la evaporación total del cuerpo y, en consecuencia, a la desaparición de toda la información contenida en su interior.&lt;/p&gt;
&lt;p&gt;Por otra parte, de acuerdo con la teoría de la relatividad general, la naturaleza de la radiación emitida por el agujero negro no puede contener ninguna información sobre la materia que quedó atrapada en su interior. Esto da lugar a la siguiente pregunta: ¿Qué ocurrió con la información contenida en la estrella original antes de que se formara el agujero negro?&lt;/p&gt;
&lt;p&gt;Por un lado, los principios de la mecánica cuántica exigen que se preserve la información –esto se conoce como principio de unitariedad, que nos habla de la reversibilidad en la evolución hamiltoniana en la teoría cuántica–; por otro lado, la teoría de la relatividad general parece decirnos que los agujeros negros no tienen memoria y que toda la información contenida en el agujero negro se pierde para siempre. Esta es, precisamente, la paradoja de Hawking, y expresa mejor que cualquier otro resultado en la física la tensión existente entre la teoría de la relatividad y la teoría de los cuantos, los dos pilares sobre los que construimos todo el andamiaje teórico de la física.&lt;/p&gt;
&lt;h3 id=&#34;la-naturaleza-paradojal-y-la-física-fundamental&#34;&gt;La naturaleza paradojal y la física fundamental&lt;/h3&gt;
&lt;p&gt;La tensión existente entre la teoría de la relatividad y la teoría cuántica lleva en germen un verdadero cisma en nuestro corpus teórico. En el contexto que nos convoca, este cisma se expresa de la siguiente forma: Los agujeros negros están destinados a evaporarse mediante el proceso de radiación descubierto por Hawking, proceso que finalmente los llevará a desaparecer junto con toda la información que en ellos se encuentra encerrada. La mecánica cuántica, por su parte, prohíbe un acto de magia cósmico de estas características ya que dicta que la información habrá de ser preservada. ¿Cómo conciliar, pues, estas dos descripciones? ¿Cómo resolver la paradoja? ¿Cómo vivir tranquilos en un edificio teórico que presenta fisuras en su estructura basal?&lt;/p&gt;
&lt;p&gt;Una actitud frecuente para sobrellevar esta situación paradojal y apaciguar así la angustia del físico teórico es abusar del pragmatismo: Uno puede sentirse tentado a argüir que la radiación de los agujeros negros es, aunque persistente, extremadamente tenue; algo así como 10&lt;sup&gt;-28&lt;/sup&gt; Watts para un agujero negro de masa estelar; una potencia de radiación desdeñable, propia de un astro con una temperatura de tan sólo 10&lt;sup&gt;-7&lt;/sup&gt; Kelvin. Esto implica que el proceso de evaporación, aunque ineluctable, es inimaginablemente lento; tan lento que no sólo ninguno de los agujeros negros que existieron en la historia del universo ha tenido tiempo de evaporarse aún, sino que todos ellos tardarán en hacerlo un tiempo descomunal: 10&lt;sup&gt;67&lt;/sup&gt; años; es decir, cincuenta y siete órdenes de magnitud más que la edad actual del universo. Entonces, ¿por qué iría uno a preocuparse por un aspecto paradojal de la física que no se expresará en la naturaleza hasta dentro de un tiempo más largo que todo aquel que haya alguna vez transcurrido?&lt;/p&gt;
&lt;p&gt;La respuesta es simple: Cuando se trata de leyes fundamentales, una inconsistencia lógica, aunque se desarrolle ésta en el futuro remoto, es acuciante hoy. Estamos hablando aquí de teorías fundamentales de la física que presentan incompatibilidades inherentes, teorías sobre las que construimos toda la física que entendemos, toda nuestra concepción del cosmos. Aunque lenta y sutil, con persistencia neurálgica, la pérdida de información de los agujeros negros atenta contra la salud de nuestro saber más básico y es imprescindible el esfuerzo por resolverla.&lt;/p&gt;
&lt;h3 id=&#34;la-paradoja-y-sus-secuelas&#34;&gt;La paradoja y sus secuelas&lt;/h3&gt;
&lt;p&gt;Stephen Hawking formuló la primera versión de su paradoja en su artículo &lt;a href=&#34;#1&#34; title=&#34;1&#34;&gt;[1]&lt;/a&gt; de 1976. Y fue él quien advirtió antes que nadie las profundas implicancias conceptuales de su descubrimiento. Como consecuencia de esto, dedicó incansables esfuerzos a resolver el rompecabezas que él mismo supo plantear, esfuerzos que no cesaron hasta sus últimos días de actividad. Al comienzo, y durante los 1980s y 1990s, quizá debido a su sesgo inicial de físico relativista, Hawking interpretó su paradojal resultado como un indicio de que ciertos aspectos de la mecánica cuántica necesitaban ser reformulados para que se volviera esta teoría compatible con la de Einstein. Más tarde, llegado el siglo XXI, probablemente rendido ante la evidencia que la conjetura de Maldacena presentaba a favor de la mecánica cuántica de los agujeros negros &lt;a href=&#34;#3&#34; title=&#34;3&#34;&gt;[3]&lt;/a&gt;, Hawking anunció haber cambiado de parecer y adherir desde entonces a la idea de que es la teoría de Einstein la que debe sufrir modificaciones.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2018/03/stephen.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Desde el trabajo iniciático de 1976, muchos elaboraron versiones refinadas de la paradoja de la información, ensayaron diferentes respuestas a esas tantas versiones, y propusieron todo tipo de ideas especulativas sobre la estructura del espacio-tiempo para poder resolver el acertijo. Ideas de frontera en la física teórica tales como la teoría de las bolas difusas, la complementariedad de los agujeros negros, la pared de fuego, la correspondencia entre entrelazamiento cuántico y agujeros de gusano, nacen todas a partir del intento por resolver la paradoja de Hawking.&lt;/p&gt;
&lt;h3 id=&#34;la-última-idea-de-stephen-hawking&#34;&gt;La última idea de Stephen Hawking&lt;/h3&gt;
&lt;p&gt;En los últimos años, desde mediados de 2015, Hawking y sus colaboradores de la Universidad de Cambridge y de la Universidad de Harvard se vieron atraídos hacia una nueva idea; una idea promisoria a la que Hawking destinó sus exiguos últimos esfuerzos: Los agujeros negros podrían preservar la información de la estrella progenitora almacenándola cerca de su superficie –el denominado horizonte de eventos– para después devolverla al medio interestelar de una manera codificada en la radiación que ellos emiten.&lt;/p&gt;
&lt;p&gt;De alguna forma, esto contradice aquello que dijimos al comienzo acerca de que los agujeros negros sólo son capaces de recordar la masa y la velocidad angular de la estrella original. ¿Deja esto de ser cierto? En efecto, Hawking y sus colaboradores creyeron haber encontrado una manera de circunvalar esta limitación y entrevieron una forma en la que estos astros podrían guardar mucha más información de lo que se creía.&lt;/p&gt;
&lt;p&gt;En una conferencia &lt;a href=&#34;#4&#34; title=&#34;4&#34;&gt;[4]&lt;/a&gt; dictada en agosto de 2015 en el Instituto Real de Tecnología de Estocolmo, Hawking presentó su idea de manera proverbial: “La paradoja de la pérdida de información puede ser resuelta si uno considera [las perturbaciones del espacio-tiempo que] las partículas que entran al agujero negro causan en el horizonte. Así, la información puede ser recobrada”. En otras palabras, Hawking, basado en el trabajo previo de otros físicos entre los que descuella Andrew Strominger, propuso que, a diferencia de lo que se creía hasta el momento, la estructura del espacio-tiempo en las cercanías de los agujeros negros puede sufrir un tipo de deformación generada por la materia que cae dentro de ellos y usar esto como mecanismo de registro: Esta perturbación del espacio-tiempo podría servir para guardar de manera codificada la información de la materia en acreción.&lt;/p&gt;
&lt;p&gt;Esto se conoce como “efecto de memoria gravitacional” y significa que la superficie del astro se convertiría en algo así como un gran ábaco en el que se lleva la cuenta de las partículas que caen dentro de él, dejando cada una de ellas una huella en la estructura misma del espacio-tiempo que lo circunda. El tipo de perturbaciones que, según Hawking, las partículas producen en el horizonte de los agujeros negros al caer dentro de ellos puede ser pensada como un pequeño desplazamiento en la superficie del astro, algo así como un desarreglo minucioso de esa superficie que bien podemos pensar elástica; como si, al caer, las partículas produjeran una &lt;em&gt;traslación&lt;/em&gt; diferente en diferentes puntos del horizonte de eventos, arrugándolo.&lt;/p&gt;
&lt;p&gt;Es debido a esto que este mecanismo de almacenamiento de información mediante la deformación de la geometría del espacio-tiempo recibe el nombre de &lt;em&gt;super-traslación en el horizonte&lt;/em&gt;. La idea que Hawking anunció en su conferencia de Estocolmo era hasta ese momento muy vaga debido a que no dio allí ningún detalle técnico acerca de cómo calcular matemáticamente esas super-traslaciones del horizonte. Fue en un artículo posterior, escrito en colaboración con Malcom Perry y Andrew Strominger, que los detalles matemáticos fueron publicados &lt;a href=&#34;#5&#34; title=&#34;5&#34;&gt;[5]&lt;/a&gt;. La idea de Hawking, Perry y Strominger acerca de que las super-traslaciones en los horizontes de los agujeros negros podrían proveer un mecanismo mediante el cual estos astros almacenarían la información codificada del proceso que les dio origen es una idea tentadora que atrajo no poca atención.&lt;/p&gt;
&lt;p&gt;Desde hace poco más de dos años, esta idea no ha dejado de generar secuelas y generalizaciones, y ha suscitado también una aguerrida controversia; todo esto, muestra cabal de la manera en la que el genio de Stephen Hawking ha influenciado el quehacer en la física teórica hasta sus últimos días.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Agradecimientos&lt;/strong&gt; El autor le agradece a Laura Donnay por fragmentos del texto y discusiones sobre el contenido.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Artículos originales&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;1&#34;&gt; 1. &lt;/a&gt; S. Hawking, “&lt;em&gt;Breakdown of Predictability in Gravitational Collapse&lt;/em&gt;”, Phys. Rev. D 14 (1976) 2460. &lt;a href=&#34;https://journals.aps.org/prd/abstract/10.1103/PhysRevD.14.2460&#34;&gt;https://journals.aps.org/prd/abstract/10.1103/PhysRevD.14.2460&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;2&#34;&gt; 2. &lt;/a&gt; A. Almheiri, D. Marolf, J. Polchinski, J. Sully, &lt;em&gt;Black Holes: Complementarity or Firewalls?&lt;/em&gt;, JHEP 1302 (2013) 062. &lt;a href=&#34;https://arxiv.org/pdf/1207.3123.pdf&#34;&gt;https://arxiv.org/pdf/1207.3123.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;3&#34;&gt; 3. &lt;/a&gt; J. Maldacena, &lt;em&gt;Eternal Black Holes in AdS&lt;/em&gt;, JHEP 0304 (2003) 021. &lt;a href=&#34;https://arxiv.org/pdf/hep-th/0106112.pdf&#34;&gt;https://arxiv.org/pdf/hep-th/0106112.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;4&#34;&gt; 4. &lt;/a&gt; S. Hawking, &lt;em&gt;The Information Paradox for Black Holes&lt;/em&gt;, Talk given on 28 August 2015 at KTH Royal Institute of Technology, Stockholm. &lt;a href=&#34;https://arxiv.org/pdf/1509.01147.pdf&#34;&gt;https://arxiv.org/pdf/1509.01147.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;5&#34;&gt; 5. &lt;/a&gt; S. Hawking, M. Perry, A. Strominger, &lt;em&gt;Soft Hair on Black Holes&lt;/em&gt;, Phys. Rev. Lett. 116 (2016) 231301. &lt;a href=&#34;https://arxiv.org/pdf/1601.00921.pdf&#34;&gt;https://arxiv.org/pdf/1601.00921.pdf&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Matemáticas para estudiar el corazón: Pérdida de correlaciones de largo alcance en el ritmo cardíaco de enfermos de Chagas</title>
      <link>https://ciencianet.com.ar/post/matematicas-para-estudiar-el-corazon-perdida-de-correlaciones-de-largo-alcance-en-el-ritmo-cardiaco-de-enfermos-de-chagas/</link>
      <pubDate>Sun, 17 Jul 2016 22:02:42 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/matematicas-para-estudiar-el-corazon-perdida-de-correlaciones-de-largo-alcance-en-el-ritmo-cardiaco-de-enfermos-de-chagas/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero.&lt;/strong&gt; Investigadora de CONICET en el INIFTA (Instituto de Investigaciones Fisicoquímicas Teóricas y Aplicadas). Docente en la UNLP.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;En este artículo reseñamos el trabajo “Losses of Long-Range Correlation in the Heart Rate Variability of Patients With Chagas’ Disease”, publicado en la edición de Junio de 2016 de la revista Global Heart. Los investigadores Isabel Irurzun, Magdalena Defeo, Regina de Battista y Eduardo Mola presentaron recientemente los resultados de un desarrollo proveniente de técnicas de análisis de series temporales a registros electrocardiográficos de personas con la enfermedad de Chagas.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2017/12/Fig1-768x230.gif&#34; alt=&#34;Figura 1&#34;&gt;&lt;/p&gt;
&lt;p&gt;En trabajos previos, los investigadores del INIFTA (Instituto de Investigaciones Fisicoquímicas Teóricas y Aplicadas, UNLP-CONICET) desarrollaron un índice denominado FNNF10 basándose en las características no lineales de la variabilidad del ritmo cardíaco humano. El nuevo índice, cuya sigla significa &lt;em&gt;Fracción residual de falsos vecinos a dimensión 10&lt;/em&gt;, fue testeado en aproximadamente 500 personas con diferentes condiciones cardíacas, observándose su sensibilidad para detectar alteraciones en el balance de la acción de los sistemas nerviosos simpático y parasimpático sobre el corazón. Esta capacidad señalaba al índice FNNF10 como una potencial herramienta de diagnóstico temprano de enfermedades cardíacas.&lt;/p&gt;
&lt;p&gt;El principal objetivo del trabajo interdisciplinar que aquí se reseña es evaluar la utilidad del FNNF10 para la detección temprana de casos de disautonomía parasimpática chagásica. Las afecciones cardíacas son la principal causa de muerte en enfermos de Chagas. La disautonomía parasimpática es uno de los mecanismos posibles de daño, en este caso por la pérdida de conexiones entre los nervios y el músculo cardíaco, que puede estar presente antes de que exista una disfunción detectable. El índice propuesto por los investigadores se basa en el método de falsos vecinos.&lt;/p&gt;
&lt;p&gt;Según este método, dos puntos vecinos en el espacio de fase de un sistema determinístico (de dimensión de &lt;em&gt;embedding&lt;/em&gt; &lt;em&gt;d&lt;/em&gt;) se mantendrán como vecinos en dimensiones superiores. Si los puntos en cambio se separan, se los identifica como falsos vecinos. Se supone que para dimensiones mayores a &lt;em&gt;d&lt;/em&gt; la fracción de falsos vecinos será muy pequeña. El FNNF10 es en este caso la fracción de falsos vecinos cuando la serie temporal de intervalos entre latidos se analizan a dimensión 10, que es la máxima dimensión de embedding obtenida en individuos sanos.&lt;/p&gt;
&lt;p&gt;El índice fue calculado sobre las series temporales de intervalos entre latidos de 130 individuos caracterizados como sanos, 128 sujetos con Chagas pero asintomáticos (con electrocardiograma y holter de 24 horas normales) y 27 pacientes con Chagas con registros de actividad cardíaca eléctrica anormales. Los resultados obtenidos reflejan, por un lado, que el índice depende de la edad en individuos sanos: el valor de FNNF10 decrece con la edad siguiendo una ley de potencia.&lt;/p&gt;
&lt;p&gt;Por otro lado, los pacientes chagásicos presentan valores de FNNF10 mayores que los individuos sanos, aún en los casos en que no presentan alteraciones cardíacas detectables por los métodos usuales de diagnóstico. Esta elevación de la fracción residual de falsos vecinos a dimensión 10 es interpretada como una pérdida en las correlaciones de largo alcance, es decir, como si la “memoria” del corazón del ritmo cardíaco que llevaba un tiempo atrás fuera menor para los pacientes chagásicos que para las personas sanas.&lt;/p&gt;
&lt;p&gt;Esta diferencia significativa hallada en el comportamiento de los intervalos entre latidos sugiere una potencial utilidad del índice FNNF10 para el diagnóstico temprano de la disautonomía chagásica en su estadio asintomático. Respecto de las perspectivas futuras en esta temática, Isabel Irurzun nos cuenta que el grupo que dirige está avanzando en dos líneas de trabajo: establecer correlaciones de FNNF10 con el nivel de anticuerpos antimuscarínicos (que son indicadores bioquímicos de disautonomía) y determinar si la la disautonomía es anterior al daño miocárdico producido por fibrosis. El artículo original, titulado “Losses of Long-Range Correlation in the Heart Rate Variability of Patients With Chagas’ Disease” está disponible &lt;a href=&#34;https://doi.org/10.1016/j.gheart.2016.03.166&#34;&gt;aquí&lt;/a&gt;.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Supercomputadoras para el diseño de fármacos</title>
      <link>https://ciencianet.com.ar/post/supercomputadoras-para-el-diseno-de-farmacos/</link>
      <pubDate>Sat, 14 Feb 2015 22:22:53 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/supercomputadoras-para-el-diseno-de-farmacos/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Diego Masone&lt;/strong&gt;. CONICET - Facultad de Ciencias Exactas y Naturales, Universidad Nacional de Cuyo.&lt;/p&gt;
&lt;p&gt;La mayor parte de los procesos biológicos que tienen lugar en los seres vivos involucran interacciones entre proteínas, estas agrupaciones organizadas de aminoácidos que cumplen funciones vitales de la célula. Es así que la correcta identificación y caracterización de las interacciones entre proteínas y el conocimiento de la estructura y las propiedades de los complejos que forman, resulta crucial a la hora de comprender cómo funcionan las células. Este tipo de problema es abordado desde hace décadas utilizando modelos computacionales para el desarrollo de métodos de diseño de nuevos fármacos. Aunque no podía anticiparse en 1938, el problema se parece al descripto por &lt;a href=&#34;http://es.wikipedia.org/wiki/Albert_Einstein&#34;&gt;Albert Einstein&lt;/a&gt; y Leopold Infeld en el inolvidable libro “La evolución de la física”. Dice así:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“...los conceptos físicos son creaciones libres del espíritu humano y no están, por más que lo parezca, determinados unívocamente por el mundo exterior. En nuestro empeño de concebir la realidad, nos parecemos a alguien que tratara de descubrir el mecanismo invisible de un reloj, del cual ve el movimiento de las agujas, oye el tic-tac, pero no le es posible abrir la caja que lo contiene. Si se trata de una persona ingeniosa e inteligente, podrá imaginar un mecanismo que sea capaz de producir todos los efectos observados; pero nunca estará segura de si su imagen es la única que los pueda explicar. Jamás podrá compararla con el mecanismo real, y no puede concebir, siquiera, el significado de una tal comparación. Como él, el hombre de ciencia creerá ciertamente que, al aumentar su conocimiento, su imagen de la realidad se hará más simple y explicará mayor número de impresiones sensoriales. Puede creer en la existencia de un límite ideal del saber, al que tiende el entendimiento humano, y llamar a este límite la verdad objetiva.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;En la segunda mitad del siglo XIX el límite de resolución de un microscopio óptico había sido definido matemáticamente por &lt;a href=&#34;http://es.wikipedia.org/wiki/Ernst_Abbe&#34;&gt;Ernst Karl Abbe&lt;/a&gt; y se había alcanzado el punto en que ya no era posible ir más allá de las observaciones de medio micrómetro (0,0000005 m). Sin embargo el desarrollo del tubo de rayos catódicos impulsó el gran salto en microscopía: usando electrones en lugar de luz visible &lt;a href=&#34;http://es.wikipedia.org/wiki/Gustav_Hertz&#34;&gt;Gustav Ludwig Hertz&lt;/a&gt; demostró teóricamente cómo un pequeño solenoide podía hacer converger haces de electrones de forma análoga a como una lente convergía rayos de luz visible.&lt;/p&gt;
&lt;p&gt;Aunque originalmente concebido por &lt;a href=&#34;http://es.wikipedia.org/wiki/Le%C3%B3_Szil%C3%A1rd&#34;&gt;Leó Szilárd&lt;/a&gt; en 1933, los ingenieros alemanes &lt;a href=&#34;http://es.wikipedia.org/wiki/Ernst_Ruska&#34;&gt;Ernst Ruska&lt;/a&gt; y &lt;a href=&#34;http://es.wikipedia.org/wiki/Max_Knoll&#34;&gt;Maximillion Knoll&lt;/a&gt; construyeron el primer microscopio electrónico que tenía una resolución de 50 nanómetros (0,000000050 m). Lamentablemente el estallido de la Segunda Guerra Mundial retrasó los avances y sólo después de 20 años de terminada la guerra los microscopios electrónicos fueron capaces de alcanzar 1 nanómetro de resolución (0,000000001 m). Estos fueron los primeros pasos hacia las técnicas de cristalografía por rayos X con las que hoy se estudian las proteínas en gran detalle.&lt;/p&gt;
&lt;p&gt;En un esfuerzo por almacenar, organizar y compartir información cristalográfica la base de datos de proteínas de libre acceso en internet (véase &lt;a href=&#34;www.pdb.org&#34;&gt;www.pdb.org&lt;/a&gt;) fue creada en los Laboratorios Nacionales de Brookhaven (BLN) en 1971. En sus comienzos contenía sólo 13 estructuras, hoy cuenta con más de 100.000. Esto significa que la humanidad dispone de la información estructural de esta cantidad de proteínas a sólo un click de distancia, muchas de ellas responsables directa o indirectamente de las más horribles patologías y enfermedades. Es sin duda una situación sin precedentes para la investigación biomédica.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2015/02/fig1-290x300.jpg&#34; alt=&#34;Figura 1: Estructura tridimensional de la proteína de la albúmina humana.:left&#34;&gt; Es entonces muy fácil para nosotros descargar de internet una proteína de nuestro agrado y “ver” cómo luce su disposición tridimensional en una computadora de escritorio, como muestra la figura 1. Pero podemos hacer mucho más, y de eso se encarga la biomedicina computacional. Podemos estudiar cómo interacciona nuestra proteína con otras proteínas o cómo lo hace con pequeñas moléculas. Podemos diseñar nuestras propias moléculas en la computadora y simular su interacción con nuestra proteína para por ejemplo, inhibir cierta función relacionada con alguna enfermedad. Es decir, podemos diseñar fármacos virtuales que luego sean probados experimentalmente hasta llegar a convertirse en fármacos reales.&lt;/p&gt;
&lt;p&gt;Desde luego, detrás de este proceso computacional están las leyes físicas y químicas que gobiernan los átomos que componen los aminoácidos y que su vez forman las proteínas. La dinámica de moléculas es en términos computacionales un experimento virtual donde hemos incluido todo lo que sabemos sobre fuerzas interatómicas. Es el descomunal poder de cálculo de una máquina quien simula el paso del tiempo para que observemos la evolución de nuestro sistema virtual y podamos dilucidar sus mecanismos de funcionamiento, detectar problemas y proponer modelos. Con este fin se han construido centenares de supercomputadoras de uso académico en diversas universidades del mundo (véase &lt;a href=&#34;www.top500.org&#34;&gt;www.top500.org&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Siguiendo esta línea de investigación hemos desarrollado junto a nuestros colaboradores del Centro Nacional de Supercomputación en Barcelona (véase &lt;a href=&#34;www.bsc.es&#34;&gt;www.bsc.es&lt;/a&gt;), un método de análisis de complejos entre proteínas. Dado un par de proteínas que sabemos se asocian para formar una nueva estructura y de la cual poseemos información cristalográfica detallada, generamos de forma virtual centenares de posibles complejos según un criterio geométrico. Luego, para decidir cuales de ellos son factibles de existir en la naturaleza, utilizamos un tipo muy especial de interacción intermolecular: los enlaces de hidrógeno. Este tipo de interacción resulta sumamente importante durante el reconocimiento molecular como se ha demostrado experimentalmente mediante cálculos de estructura electrónica.&lt;/p&gt;
&lt;p&gt;De esta forma extendiendo el proceso a una buena cantidad de proteínas distintas, verificamos que los cálculos de energías de interacción que incluían una completa optimización de los enlaces de hidrógenos de las proteínas en su totalidad, introducían una significativa mejora en el discernimiento entre los complejos generados geométricamente. Esto significa que luego de generar en una computadora centenares de complejos entre proteínas de una manera más bien caprichosa, somos capaces de distinguir cuales tienen una buena posibilidad de existir realmente y con un poco de suerte, tendremos un número reducido de candidatos.&lt;/p&gt;
&lt;p&gt;Este último conjunto de estructuras es hasta donde la biología computacional puede llegar (por ahora) y que a continuación sigue la demoledora etapa experimental. Pero no es difícil imaginar un futuro con diseño de fármacos “a la carta” específicos para cada necesidad. Como “Los cazadores de microbios” de Paul de Kruif, queda en manos de los científicos la búsqueda de la verdad objetiva que nos permita avanzar en el entendimiento de las interacciones entre proteínas.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; &lt;a href=&#34;mailto:dmasone@fing.uncu.edu.ar&#34;&gt;dmasone@fing.uncu.edu.ar&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Artículo original&lt;/strong&gt;: H-bond network optimization in protein–protein complexes: Are all-atom force field scores enough? Diego Masone, Israel Cabeza de Vaca, Carles Pons, Juan Fernandez Recio and Victor Guallar. &lt;em&gt;Proteins: Structure, Function, and Bioinformatics&lt;/em&gt; Volume &lt;strong&gt;80&lt;/strong&gt;, Issue 3, pages 818–824, March 2012. &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1002/prot.23239/abstract&#34;&gt;http://onlinelibrary.wiley.com/doi/10.1002/prot.23239/abstract&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Flujo intermitente de sistemas particulados y transición de atasco</title>
      <link>https://ciencianet.com.ar/post/flujo-intermitente-de-sistemas-particulados-y-transicion-de-atasco/</link>
      <pubDate>Thu, 15 Jan 2015 23:01:34 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/flujo-intermitente-de-sistemas-particulados-y-transicion-de-atasco/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Gago.&lt;/strong&gt; Facultad Regional La Plata. Universidad Tecnológica Nacional y CONICET.&lt;/p&gt;
&lt;p&gt;¿Existen similitudes en el comportamiento de la sal en un salero, ovejas saliendo del establo a pastar y un grupo de personas abandonando un edificio? Intentaré convencerlos de que sí, contando algunas de las similitudes que se han encontrado recientemente en un trabajo conjunto que realizamos con colegas de la Universidad Tecnológica Nacional, el Instituto Tecnológico Buenos Aires, la Universidad de Navarra, la Universidad de Edimburgo, la Universidad de Zaragoza, la Escuela Superior de Física y Química Industrial de París y la Universidad de Barcelona.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2015/01/ilustracion-300x190.jpg&#34; alt=&#34;Imagen tomada de http://despicableme.wikia.com/wiki/Minions?file=Minions3.png. CC-BY-SA.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;La primera similitud que podemos sugerir es que todos estos sistemas (junto con una gran variedad de otros) pertenecen a la categoría de sistemas particulados; esto es, son &lt;em&gt;un conjunto numeroso de elementos de tamaño macroscópico (que se aprecian a simple vista) que interactúan entre ellos&lt;/em&gt;. Pensemos por ejemplo en los diferentes recorridos que atraviesa una persona caminando por una calle vacía o por una calle repleta de otras personas. En el primer caso el recorrido típico será en línea recta, del lugar donde se encuentra al lugar a donde desea dirigirse, en el segundo caso será necesario realizar un camino zigzagueante que permita evitar el contacto con los demás peatones.&lt;/p&gt;
&lt;p&gt;En la definición dada de sistema particulado no nos interesará entonces el comportamiento individual de cada elemento, sino el comportamiento del conjunto, causado por las interacciones presentes. Si aceptamos esta primera similitud podemos avanzar a la que nos convoca, el flujo de estos sistemas particulados a través de orificios o aberturas angostas.&lt;/p&gt;
&lt;h3 id=&#34;un-salero-que-se-tapa&#34;&gt;Un salero que se tapa&lt;/h3&gt;
&lt;p&gt;Podemos decir que los sistemas particulados poseen la capacidad de &amp;quot;fluir&amp;quot; a través de un orificio o abertura, tal como lo hace la arena del reloj de arena, o la gente a la salida del cine. Para esto necesita algún tipo de fuerza impulsora (en el caso de la arena la gravedad, en el caso de las personas alguna motivación). En estos casos el sistema se comporta como una especie de fluido que puede adaptarse a la forma de la abertura y a la situación (es decir, intentar moverse más rápido o más lento dependiendo de la fuerza o motivación).&lt;/p&gt;
&lt;p&gt;Sin embargo, un salero que se tapa es un fenómeno de todos los días y esta es otra propiedad que tienen en común los sistemas particulados. Si prestamos atención veremos que el flujo de personas saliendo del cine no es continuo ni homogéneo. Presenta interrupciones de diferente duración sólo que aquí, a diferencia de lo que ocurre con la sal, las mismas &amp;quot;partículas/personas&amp;quot; son las que se encargan, mediante tirones y empujones, de destrabar el embotellamiento y seguir saliendo. En el caso del salero, como bien sabemos, resulta necesario sacudirlo o golpearlo un poco para obtener el mismo efecto de reanudación del flujo.&lt;/p&gt;
&lt;p&gt;Al aplicar algún mecanismo para reactivar el flujo de estos sistemas se obtiene lo que se llama un &lt;em&gt;flujo intermitente de sistemas particulados&lt;/em&gt;. En estos casos el tiempo que le tomará al sistema reanudar el flujo una vez producido el atasco dependerá de muchos factores: la humedad de la sal, el tamaño del orificio del salero, la fuerza que impulsa al sistema a fluir, etc; así como del mecanismo de desbloqueo aplicado.&lt;/p&gt;
&lt;p&gt;Para muchos fines prácticos, conocer el tiempo que lleva restablecer estos flujos tiene gran importancia. Para esto, lo que haríamos generalmente sería repetir varias veces el &amp;quot;experimento&amp;quot; midiendo el tiempo de desatasco en cada caso y después promediando estos tiempos obtendríamos una estimación del tiempo característico de atascamiento. Sin embargo, este método de promediado sólo es válido para algunos fenómenos, en el problema que nos convoca no lo es.&lt;/p&gt;
&lt;p&gt;Este problema de flujo intermitente en sistemas particulados requiere de un análisis diferente. Tratemos de entender porqué.&lt;/p&gt;
&lt;h3 id=&#34;las-leyes-de-probabilidad&#34;&gt;Las leyes de probabilidad&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2015/01/gauss_3-300x120.png&#34; alt=&#34;Ejemplo de distribución normal. Vemos que este tipo de distribución es simétrica y podemos notar además que de las 1000 repeticiones del experimento, aproximadamente 700, es decir la gran mayoría, duraron entre 55 y 65 minutos, es decir que esta es una distribución localizada.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Cuando queremos dar información sobre un hecho repetitivo (el tiempo que tarda el tren en ir de A a B por ejemplo) hablamos de forma intuitiva del promedio de estos valores sobre el número total de medidas realizadas (el número de veces que viajamos). No nos importa cuanto tardó ayer ni cuanto tardó hoy; no queremos el detalle del tiempo de cada viaje, queremos información general, una cantidad que nos sirva para predecir de un modo aceptable la duración de los viajes siguientes. En la figura de la izquierda podemos ver un ejemplo de lo que se llama una &lt;em&gt;&lt;a href=&#34;http://es.wikipedia.org/wiki/Distribuci%C3%B3n_normal&#34;&gt;ley de probabilidad &amp;quot;normal&amp;quot;&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Supongamos que viajamos 1000 veces de A a B. Cada linea en la figura representa la cantidad de veces que el tren tardó 40, 45, 50, 55, etc minutos en el trayecto. Claramente, la mayoría de las veces, aproximadamente 160, el tren tarda una hora, aunque puede tardar unos minutos más o unos minutos menos (seguramente dependiendo del tránsito). Según este experimento, con gran probabilidad llegaremos a destino a tiempo si tomamos el tren una hora antes. Este tipo de distribución también se distingue por ser simétrica, es decir que podemos con igual probabilidad llegar 5 minutos antes o cinco minutos después y el promedio o media coincide con el tiempo que se repitió la mayor cantidad de veces, llamado &lt;em&gt;moda&lt;/em&gt;. Las distribuciones normales ocupan un lugar importante en el estudio probabilístico de algunos fenómenos de la naturaleza, es por esto que sus propiedades (es decir, las propiedades de las cantidades que poseen este tipo de distribución) han sido intensamente estudiadas.&lt;/p&gt;
&lt;h3 id=&#34;flujo-intermitente-de-sistemas-granulados&#34;&gt;Flujo intermitente de sistemas granulados&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2015/01/long_tail-300x165.png&#34; alt=&#34;Ejemplo del tipo de distribución de probabilidades obtenida para los tiempos de atasco en el flujo intermitente de sistemas particulados. La línea de trazo roja indica el valor promedio de esta distribución.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Si miramos ahora la distribución que resulta de medir los tiempos de duración de los atascos en el flujo intermitente de los sistemas particulados veremos que esta distribución no resulta del tipo normal sino que tiene la forma mostrada en la figura de la derecha. En esta puede apreciarse que existe una alta probabilidad asociada a tiempos cortos pero que también existe probabilidad, aunque cada vez menor, de tener atascos de muy larga duración.&lt;/p&gt;
&lt;p&gt;En esta figura la línea de trazo en rojo marca el valor correspondiente al promedio de los tiempos obtenidos. Podemos ver que en este caso esta cantidad no nos provee mayor información de lo que pasa en el sistema. No nos permite predecir, ya que la mayoría de las veces los atascos se romperán rápidamente, pero existe una probabilidad de que duren mucho tiempo más de lo que indica el promedio.&lt;/p&gt;
&lt;p&gt;En el caso de estar interesados, por ejemplo, en predecir de forma aceptable el tiempo que lleva evacuar determinado edificio será necesario entonces intentar un nuevo enfoque. Construyamos entonces las llamadas &lt;em&gt;&lt;a href=&#34;http://es.wikipedia.org/wiki/Funci%C3%B3n_de_distribuci%C3%B3n&#34;&gt;distribuciones acumuladas&lt;/a&gt;&lt;/em&gt;, que en este caso significa simplemente &amp;quot;la probabilidad de que la duración de un atasco sea mayor que&amp;quot;. Es decir, no el número de veces que un atasco duró 3, 5 o 7 minutos, sino el número de atascos que duraron más de 3, más de 5 o más de 7 minutos. En la figura de abajo podemos ver el resultado de tomar la distribución acumulada para los tiempos de atascos correspondiente a la descarga de un silo con granos que posee una abertura pequeña y se vibra constantemente para romper los atascos que se forman durante el flujo.&lt;/p&gt;
&lt;p&gt;En esta figura las dos curvas representadas corresponden a dos fuerzas de gravedad efectiva distintas con las que las partículas son empujadas a moverse a través del orificio de salida del silo. Esto se consigue cambiando la inclinación del silo con respecto a la vertical. Podemos ver que para la curva roja, la probabilidad de tener atascos largos es mayor que para la curva azul, correspondiente a una aceleración mayor. Este es un fenómeno interesante, que sin embargo no voy a poder contar en esta nota.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2015/01/distribucion_plano.png&#34; alt=&#34;(Der.) Esquema del dispositivo empleado para los experimentos de atascos de granos. Consiste en un silo bidimensional sobre un plano inclinado que se vibra usando un parlante. El silo cuanta con una abertura pequeña que se atasca en forma permanente si no se usan vibraciones. (Izq.) Distribución de probabilidades de los tiempos de atascos correspondientes a la descarga del silo. Las distintas curvas corresponden a distintas inclinaciones del silo. Pueden verse los valores de alpha obtenidos en cada caso.&#34;&gt;&lt;/p&gt;
&lt;p&gt;Los datos de este gráfico se encuentran representados en una escala llamada escala logarítmica y así representados puede verse que en la región de tiempos largos estas gráficas se asemejan a una linea recta. Este tipo de comportamiento puede describirse matemáticamente mediante lo que se conoce como una ley de potencias: $$P(T &amp;gt; τ) = τ^{(-α)}$$
Sin entrar en detalles, lo que quiero es remarcar la importancia del parámetro α en esta ecuación. El valor de α será diferente para las distintas curvas en el gráfico y nos indica que tan rápido o lento decrece la probabilidad de tener tiempos de atascos largos. Por ejemplo, para la evacuación de un edificio, querríamos conseguir un exponente α lo mayor posible, lo que aseguraría muy baja probabilidad de tiempos de atasco, y por consiguiente tiempos de evacuación, largos.&lt;/p&gt;
&lt;p&gt;Sin embargo, ¿cuál sería el valor mínimo aceptable o el valor óptimo en cada caso? Esto es algo que no puede definirse de forma simple y necesita de un estudio más profundo a partir de las situaciones específicas. Lo que si podemos decir es que estas distribuciones presentan características completamente diferentes para valores de α por encima y por debajo de 2. En la mayoría de los casos queremos asegurar un valor α &amp;gt; 2.&lt;/p&gt;
&lt;h3 id=&#34;transición-de-atasco&#34;&gt;Transición de &amp;quot;atasco&amp;quot;&lt;/h3&gt;
&lt;p&gt;El tipo de función matemática propuesta para los tiempos de atasco tiene la particularidad de que para un valor de α mayor que 2, luego de realizar un número suficientemente grande de repeticiones (suficientemente grande dependiendo del caso puede se 100, 1000, 10000 o más), el valor promedio de la distribución no variará significativamente con las repeticiones subsiguientes. Sin embargo, valores α menores que 2 implican que el valor promedio que obtendremos si realizamos repetidamente este experimento (por ejemplo la descarga de un silo o salero) no será el mismo si repetimos el experimento 100, 1000 o 10000 de veces, dependerá de las repeticiones realizadas y su valor puede cambiar de forma radical entre la medida número 10000 y la 10001!&lt;/p&gt;
&lt;p&gt;En la figura vemos que para los tiempos de atasco de un silo bidimensional es posible que el valor del parámetro α tome valores por debajo y por encima del valor crítico 2, tan solo cambiando la fuerza que impulsa a las partículas hacia la salida. Este fenómeno, que ha sido observado también en el flujo intermitente de otros sistemas partículados, como un rebaño de ovejas o en la evacuación de personas (este último estudiado mediante modelos de simulación por computadora) ha sido llamado transición de atasco (&lt;em&gt;clogging transition&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Las transiciones son saltos bruscos de las propiedades de los sistemas al variar levemente algún parámetro de control. En el caso del agua hirviendo, por ejemplo, la densidad cambia de forma brusca de líquido a vapor, mientras que la temperatura del sistema sólo necesita cambiar de digamos 99C a 101C. En el caso que nos ocupa es posible, variando características propias del sistema estudiado (ancho del orificio de descarga o puerta, aceleración del sistema, urgencia por salir, etc), que el exponente α tome valores por encima y por debajo del 2. Por lo tanto, los tiempos medios de evacuación pasarían de estar bien definidos luego de un número suficientemente grande de medidas, a seguir teniendo la capacidad de variar siempre que agreguemos más medidas al experimento.&lt;/p&gt;
&lt;h3 id=&#34;entonces&#34;&gt;Entonces...&lt;/h3&gt;
&lt;p&gt;Existen similitudes entre la descarga de un silo o el comportamiento de la gente a la salida de un estadio. Entre estas similitudes pudimos ver que el flujo intermitente presenta, para los tiempos de atasco, una transición de fase conocida como transición de atasco. Este tipo de resultado tiene uno de sus mayores puntos de interés en la nueva perspectiva que ofrece al estudio de las normas de seguridad para evacuación peatonal, donde resulta fundamental tener la mayor precisión posible en los tiempos que el proceso lleva.&lt;/p&gt;
&lt;p&gt;También resulta útil a la hora de analizar procesos productivos, donde los materiales particulados son ampliamente manipulados y suelen encontrarse con la necesidad de lidiar con este tipo de atascos. Pero además de sus aplicaciones técnicas, desde un punto de vista científico, la existencia de una transición de fase en este tipo de sistemas ofrece un amplio e interesante campo de investigación. Las transiciones de fase y los saltos bruscos y discontinuidades que implican, siempre han resultado fascinantes para quienes estudian la naturaleza y quieren intentar comprender el cómo y del porqué de estas discontinuidades.&lt;/p&gt;
&lt;p&gt;La existencia de esta transición en flujos intermitentes de sistemas particulados con características tan variadas hacen que su estudio y generalización resulten atrayentes para intentar dilucidar los fenómenos que subyacen a la naturaleza de los mismos.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; I. Zuriguel, D. R. Parisi, R. Cruz Hidalgo, C. Lozano, A. Janda, P. A. Gago, J. P. Peralta, L. M. Ferrer, L. A. Pugnaloni, E. Clement, D. Maza, I. Pagonabarraga, A. Garcimartin, &lt;a href=&#34;http://dx.doi.org/10.1038/srep07324&#34;&gt;Clogging transition of many-particle systems flowing through bottlenecks&lt;/a&gt;, Scientific Reports 4, 7324 (2014)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; Paula Gago (&lt;a href=&#34;mailto:paulaalejandrayo@gmail.com&#34;&gt;paulaalejandrayo@gmail.com&lt;/a&gt;)&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Regeneración de tejidos. Entrevista a Osvaldo Chara</title>
      <link>https://ciencianet.com.ar/post/regeneracion-de-tejidos-entrevista-a-osvaldo-chara/</link>
      <pubDate>Fri, 07 Mar 2014 23:55:58 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/regeneracion-de-tejidos-entrevista-a-osvaldo-chara/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Luis Pugnaloni.&lt;/strong&gt; Dpto. Ing. Mecánica, UTN-Fac. Reg. La Plata.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2014/03/chara1-225x300.jpg&#34; alt=&#34;Osvaldo Chara junto al cluster de cálculo del IFLYSIB.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Osvaldo Chara trabaja en el Instituto de Física de Líquidos y Sistemas Biológicos (CONICET-UNLP) en La Plata. Desde hace unos años coopera con colegas de Alemania, Suiza, EEUU y España en un proyecto que tiene como objetivo comprender el proceso de regeneración de tejidos en especies con capacidad de crecer un miembro amputado. Además del interés académico de una capacidad tan peculiar de algunos seres (como las salamandras), resulta esencial comprender estos mecanismos para nuevos tratamientos médicos que se basan en la regeneración de tejidos para tratar diversas enfermedades o sus secuelas. Recientemente Osvaldo publicó un artículo en la revista Science con un valioso avance en la comprensión del proceso de regeneración (ver &lt;a href=&#34;http://dx.doi.org/10.1126/science.1241796&#34;&gt;aquí&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;- La ciencia ficción siempre inspira. Sé que te apasionan los comics ¿estudiás este problema de regeneración motivado por historias fantásticas de seres extraterrestres que pueden recuperar partes de su cuerpo luego de una amputación?&lt;/strong&gt; - Es cierto que me gustan los comics, o historietas como decimos en Argentina. Un buen ejemplo en esa dirección es Spiderman. Uno de los enemigos del hombre araña es, precisamente, &lt;em&gt;the lizard&lt;/em&gt; (conocido en estas latitudes como el lagarto). Originalmente, este villano era un biólogo que precisamente investigaba la habilidad que ciertos reptiles tienen para regenerar sus patas, motivado por la ausencia de su brazo derecho. Él va a inyectarse un suero extraído de un lagarto para poder regenerar su brazo. Infortunadamente eso lo transforma en un monstruo.&lt;/p&gt;
&lt;p&gt;De todas formas, la motivación por la cual comencé a interesarme en estos temas fue más bien por azar. En la época en que estaba haciendo mi tesis doctoral, en la UBA, me contactó un médico cirujano (el Dr. Daniel Wainstein del Hospital Tornu) que estaba lidiando con un problema relacionado con el daño de tejidos. Él estaba trabajando en un método que implicaba aplicación de vacío, lo cual implicaba algún conocimiento de física, así que lo llamo por teléfono al Prof. Mario Parisi, que es un referente en la biofísica argentina. Yo estaba, de casualidad, en la misma oficina de Mario, discutiendo con él sobre la física de un problema de transporte de agua. Cuando Mario entendió que se precisaba alguien que supiera de “presiones”, me mira a mí y le dice al interlocutor que tiene que hablar conmigo.&lt;/p&gt;
&lt;p&gt;Así comencé a interiorizarme en este tipo de trabajos. Por lo demás, me dedico a hacer modelos matemáticos de sistemas biológicos complejos. La regeneración de tejidos es un excelente ejemplo de estos últimos.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;- ¿Qué seres de nuestro mundo real pueden regenerar partes de su cuerpo?&lt;/strong&gt; - Si bien no todas las especies pueden regenerar, hay algunas que no están necesariamente cerca filogenéticamente que si pueden hacerlo. Entre ellas podes encontrar Hydra, Axolotl, Planaria. A su vez hay animales que, en ciertos periodos embrionarios pueden regenerar una parte de su cuerpo. Un ejemplo de estos es la mosca de la fruta (Drosophila melanogaster).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2014/03/chara2.jpg&#34; alt=&#34;Axolotl. Gentileza del Center for Regenerative Therapies Dresden (CRTD). Esta fotografía sólo puede ser reproducida con autorización del tenedor del Copyright Center for Regenerative Therapies Dresden&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;- En tu artículo se dice algo sobre que las células de las yemas de los dedos son diferentes a las del resto del brazo. ¿Entendí bien? ¿Eso pasa en los humanos o en todos los seres con dedos? ¿Hay otras partes del cuerpo con células así?&lt;/strong&gt; - En las células de las patas de los animales se manifiesta una propiedad que podríamos llamar “distalización”. Si la célula se encuentra más lejos del hombro (más distal), esta propiedad se intensifica. Esencialmente, esta propiedad se ve reflejada en la expresión de una serie de genes (o sea por la generación o no de ciertas proteínas codificadas en esos genes). Esto sucede en los miembros de diversos animales, incluido el hombre.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;- En tu artículo aparece mucho la palabra &amp;quot;blastema&amp;quot; ¿Qué es eso?&lt;/strong&gt; - Primero valdría la pena entender que las células pueden ser más o menos diferenciadas. Cuando una célula es poco diferenciada, puede reproducirse mucho y no se especializa en una función determinada. Un ejemplo de esto son las células madre (o &lt;em&gt;stem cells&lt;/em&gt;, en la literatura angloparlante). Las células más diferenciadas pierden un poco la capacidad de reproducirse y se especializan en determinadas funciones. Ejemplos de estas son las neuronas, los glóbulos rojos, etc.&lt;/p&gt;
&lt;p&gt;Cuando se amputa la pata de un Axolotl, las células de los tejidos que formaban pate de la pata a la altura de la amputación (musculo, piel, etc.) están bastante diferenciadas. Al amputar, estas células se de-diferencian y forman células poco diferenciadas como las células madre y se cubren por otras que forman una capa de células epiteliales. Todo esto (las células de-diferenciadas y la capa que las rodea) es lo que se conoce como blastema.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;- Si me lastimo superficialmente generalmente se me forma una cicatriz, ¿es diferente este proceso de la regeneración de una cola completa de una salamandra? ¿Por qué?&lt;/strong&gt; - Bueno, la respuesta corta es que no lo sabemos. Es cierto que cuando un humano se lastima en la piel, por ejemplo, en la herida se desencadena el proceso de cicatrización. Una hipótesis que concebimos es que el proceso de regeneración de alguna manera apareció tempranamente en la evolución y que luego desapareció. Lo cierto es que los procesos regenerativos son muy costosos, así que, tal vez, la cicatrización es una suerte de solución de compromiso evolutivo que especies como la nuestra tienen.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;- Entiendo que tu trabajo demuestra finalmente que el mecanismo de regeneración de una pata para una salamandra mexicana (el axolotl) es uno de los dos que se han propuesto en forma teórica (especificación progresiva versus intercalación de segmentos). ¿Podrías explicar estos dos mecanismos teóricos?&lt;/strong&gt; - En el desarrollo normal de un brazo nuestro, o en el caso de un Axolotl, de una pata, las células van adquiriendo identidades desde el tronco hacia afuera (del hombro a los dedos). Se van haciendo progresivamente más “distales”, como te decía antes. En los procesos regenerativos de la pata de un Axolotl, esto es, luego de amputarlo, se creía que el mecanismo que operaba era de intercalación. De acuerdo a este mecanismo, las células cercanas a la zona de amputación, adquirían una característica extremadamente distal. En otras palabras, se transformaban en células típicas de los dedos.&lt;/p&gt;
&lt;p&gt;Entonces, la vecindad entre las células inmediatamente cercanas a estas, y estas células típicas de los dedos recién formadas inducia que las primeras se reproduzcan rápidamente y vayan adquiriendo características intermedias entre ambos tipos de células, es decir “intercalen”. El mecanismo alternativo, que encontramos nosotros, es que las células cercanas a la zona de amputación van, gradualmente, adquiriendo características más distales.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;- ¿Cuál fue tu trabajo dentro del equipo de investigación? ¿Qué formación tuviste para poder hacer este tipo de cosas?&lt;/strong&gt; - Nosotros participamos en distintas áreas relacionadas con lo que hoy se llama biología de sistemas. Esencialmente hicimos análisis de datos experimentales, que incluye análisis estadístico así como análisis de imágenes, y modelado matemático. Mi formación es la que me brindo la universidad pública argentina, tanto la Universidad de Buenos Aires como la Universidad Nacional de la Plata. Estudié farmacia, bioquímica y física cuando era joven y luego hice un doctorado en biofísica seguido de un postdoctorado en física.&lt;/p&gt;
&lt;p&gt;Si bien comencé el doctorado trabajando en biofísica y fisiología experimental, rápidamente me di cuenta que lo que realmente me encanta es hacer modelado matemático. Pero mi formación experimental previa me es fundamental para trabajar en combinación con grupos experimentales. Por eso, el trabajo de modelado en biología de sistemas lo pensamos de forma de involucrarnos fuertemente con la problemática biológica que estamos estudiando, tanto desde el punto de vista de la pregunta y el marco epistemológico de esta como desde el punto de vista de las técnicas experimentales empleadas.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;- ¿Qué te gustaría comprender sobre la regeneración de tejidos que imaginás que puede tomar 10 o 20 años en llegar a dilucidarse?&lt;/strong&gt; - Las preguntas que nos gustaría responder en ese plazo están relacionadas con la posición evolutiva de los procesos regenerativos así como el vínculo que estos procesos tengan con otros procesos de la fisiología de estos animales. Está claro que no todos los animales pueden regenerar, ¿por qué hay animales que pueden regenerar y otros, como los humanos, que no pueden hacerlo? ¿Será que el proceso de regeneración apareció más temprano en la evolución luego se perdió, como te mencionaba antes? ¿La regeneración se produce mediante un mecanismo único compartido por las especies que te mencionaba antes?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;- ¿Cuáles serán tus siguientes pasos en esta línea de investigación?&lt;/strong&gt; - Nuestra propuesta es modelar los procesos regenerativos que tienen lugar en los distintos animales capaces de hacerlo. Para ello tenemos que desarrollar modelos que sean capaces de reproducir datos experimentales obtenidos hasta el presente y hacer predicciones que sean contrastadas por nuestros colaboradores experimentales. Todo esto implica un trabajo importante, imagínate que el descubrimiento oficial de que un animal, la Hydra puede regenerar se lo debemos a Abraham Trembley en 1744. Luego, trataremos de ver si estos modelos pueden clasificarse de forma de identificar características que apunten a un mecanismo común o no y al mismo tiempo, trataremos de identificar si hay una línea temporal evolutiva subyacente.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;- ¿Cuáles serán tus siguientes pasos en cuanto a tu carrera profesional?&lt;/strong&gt; - Me apasiona la ciencia y la docencia en la frontera entre la biología y la física. Planeo seguir encaminando mis pasos a seguir trabajando en problemas localizados en esta frontera y dar clases en alguna universidad para entusiasmar más gente en el camino. Como hace poco que volví al país, está claro que seguiré haciendo este tipo de trabajo científico, porque el CONICET siempre me abrió sus puertas. Sera cuestión de encontrar una Universidad a la que también le interese este tipo de investigación y docencia.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Bajas temperaturas en La Plata</title>
      <link>https://ciencianet.com.ar/post/bajas-temperaturas-en-la-plata/</link>
      <pubDate>Tue, 31 Dec 2013 00:19:40 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/bajas-temperaturas-en-la-plata/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro&lt;/strong&gt;. Instituto de Física de Líquidos y Sistemas Biológicos (CONICET).&lt;/p&gt;
&lt;p&gt;En la oficina vecina a la que ocupo en el &lt;a href=&#34;http://www.iflysib.unlp.edu.ar/&#34;&gt;IFLYSIB&lt;/a&gt; trabaja Santiago Grigera, responsable de la flamante instalación y puesta en marcha del Laboratorio de Bajas Temperaturas (BT La Plata). Aprovechamos una pausa durante una tarde de junio para charlar sobre esta singularidad térmica en el Universo.
&lt;img src=&#34;https://ciencianet.com.ar/images/2013/12/BTLP-01-300x200.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CN&lt;/strong&gt;: -¿Qué actividad se realiza en el BT La Plata?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SG&lt;/strong&gt;: -La idea básica es que en el Laboratorio se pueden alcanzar temperaturas muy bajas y campos magnéticos muy altos. Las temperaturas que alcanzamos son realmente bajas, la menor de 260 mK (-273 °C), que es más baja que la del &lt;a href=&#34;http://es.wikipedia.org/wiki/Radiaci%C3%B3n_de_fondo_de_microondas&#34;&gt;espacio exterior&lt;/a&gt; (aproximadamente 2.7 K). Estas temperaturas muy bajas se alcanzan en un volumen relativamente pequeño, de forma cilíndrica de 30 mm de diámetro por 100 mm de alto. Los campos que podemos alcanzar son de hasta 9 T (esto es, unas 200 mil veces el campo magnético de la Tierra en su superficie).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CN&lt;/strong&gt;: -¿Cuál es el propósito de medir algo a tan baja temperatura?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SG&lt;/strong&gt;: -La temperatura de un sistema se puede pensar en términos de la agitación térmica de los átomos y moléculas que componen un material. Esta agitación destruye el orden que existe en estos sistemas, así que al enfriarlo se pueden observar las propiedades del mismo con la menor perturbación posible, recuperando el orden que no se puede observar a mayores temperaturas. En especial, ganan relevancia a bajas temperaturas los efectos cuánticos.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2013/12/BTLP-04-200x300.jpg&#34; alt=&#34;:right&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CN&lt;/strong&gt;: -¿Qué tipos de muestras analizan?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SG&lt;/strong&gt;: -Se puede analizar lo que uno quiera, siempre y cuando haya alguna propiedad de interés que se pueda manifestar a baja temperatura. Por ejemplo, se pueden analizar gases, que se licúan a esas temperaturas tan frías, y es entonces cuando aparecen propiedades cuánticas que no se observan a temperaturas más altas. Una de esas propiedades conocidas es la superfluidez del helio. Nosotros apuntamos a mirar sistemas electrónicos y magnéticos. Nos gustaría entender cómo funciona un sistema de muchas partículas (aproximadamente 1024, esto es, un uno seguido de veinticuatro ceros) con interacciones fuertes entre sí a un nivel cuántico, que están muy correlacionadas. Decimos que están muy correlacionadas en el sentido de que una perturbación de una partícula, afecta no solo a ella y sus vecinas inmediatas, sino también al resto del sistema.&lt;/p&gt;
&lt;p&gt;Uno de los casos que uno puede pensar como sistema con comportamiento cuántico colectivo es el de los electrones dentro de un metal. Lo mismo se puede pensar con otra de las propiedades fundamentales de interés, en este caso magnética y de origen relativista, como es el espín. Dentro de los sistemas magnéticos, un caso de interés para nosotros son los sistemas frustrados. Estos sistemas tienen, además de una interacción fuerte, una o varias interacciones adicionales no compatibles con las primeras. Cada &amp;quot;individuo&amp;quot; del sistema está sujeto a muchas condiciones y no puede satisfacer todas al mismo tiempo. Se pueden analizar y comparar las propiedades clásicas y cuánticas de estos sistemas, y ver cómo se resuelven estas frustraciones y qué tipo de fenómeno colectivo emerge en cada caso. A bajas temperaturas, la frustración evita la aparición de los estados simples ya conocidos, y quedan en evidencia comportamientos nuevos y complejos. La descripción de estos sistemas se complementa con modelos teóricos cuyos resultados y predicciones se ponen a prueba con las mediciones que realizamos.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2013/12/BTLP-03-300x200.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CN&lt;/strong&gt;: -Además de realizar investigación básica, ¿es posible que el Laboratorio tenga alguna vinculación con el desarrollo tecnológico?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SG&lt;/strong&gt;: - Nosotros en este momento estamos trabajando en problemas de  ciencia básica. A veces mirando este tipo de problemas nos encontramos con materiales que tienen propiedades de interés tecnológico (como ha sido el caso histórico de los semiconductores). Por otra parte, en principio, el Laboratorio también es una herramienta, que puede ser utilizada para estudiar otros tipos de problemas con interés más aplicado. Nuestro objetivo es justamente ampliar gradualmente nuestro ámbito de investigación para incluir problemas aplicados y desarrollo tecnológico en criogenia (la producción de bajas temperaturas).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CN&lt;/strong&gt;: -¿Quiénes trabajan en el Laboratorio?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SG&lt;/strong&gt;: -Actualmente somos tres investigadores, tres becarios doctorales y un estudiante de grado que está haciendo su tesis de licenciatura.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CN&lt;/strong&gt;: -El Laboratorio es nuevo, recién poniéndose en marcha. ¿Cómo fue posible su construcción?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SG&lt;/strong&gt;: -Como es usual, lo primero es conseguir el financiamiento. Más del 90% del mismo provino de la Agencia Nacional de Promoción Científica y Tecnológica, con un subsidio para grandes equipamientos. La Agencia misma puso dinero también para acondicionar el lugar. Algunas donaciones de equipamiento más pequeño llegaron desde la Universidad de Saint Andrews de Escocia.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2013/12/BTLP-02-300x200.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CN&lt;/strong&gt;: -Una vez obtenido el financiamiento, ¿qué dificultades hubo para llevar a cabo la construcción?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SG&lt;/strong&gt;: -Parece que obtener el dinero es la parte difícil, pero en realidad es más complicado llevar el proyecto a la realización. Una de las dificultades es que los mecanismos para ello, pese a las buenas intenciones, no están pensados para proyectos de gran alcance o larga duración. También hay algunas incoherencias como que podés recibir dinero para comprar el equipamiento pero no para su funcionamiento. Otra dificultad es la forma que tienen las unidades administrativas (en este caso la Universidad Nacional de La Plata) para contratar los servicios de construcción o instalación eléctrica. Son mecanismos complejos y tortuosos, y uno siente que pierde tiempo y dinero.&lt;/p&gt;
&lt;p&gt;Hay otra cuestión que afecta seriamente, y que surge del hecho que Argentina es un país que no está desarrollado industrialmente. Comprar insumos sencillos es muy difícil, por ejemplo conectores o tornillos de precisión. A veces se consigue algo en Buenos Aires, o hay que comprar cosas usadas. No hay demanda para ingeniería de precisión, por lo que no hay muchos comercios que vendan esos insumos. Determinados materiales tampoco se consiguen, así como servicios. Hay poco conocimiento en el país de determinadas implementaciones técnicas. Nosotros compramos muchas veces las cosas básicas, y al resto lo hacemos porque no lo conseguimos en el mercado, y este aprendizaje puede ser útil para problemas de bajas temperaturas en la industria, como en el desarrollo electrónico y en ámbitos más lejanos a los que nos movemos. De algún modo, esto también es un producto de nuestro Laboratorio, porque para armarlo es necesario desarrollar cierto conocimiento técnico que también es exportable. Nosotros vemos con buenos ojos esta iniciativa de CONICET de acercar los laboratorios a la industria, y que exista un ida y vuelta de demanda y oferta de soluciones.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CN&lt;/strong&gt;: -¿Cuál es la perspectiva para el Laboratorio en el largo plazo?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SG&lt;/strong&gt;: -El BT La Plata recién empieza. Cuando uno recibe algo tiene que justificar lo recibido, lo que significa que lo que se invirtió en nosotros tenemos que justificarlo con resultados. No queremos ampliarnos hasta no demostrar que estamos construyendo sobre una báse sólida, por lo que no vamos a incorporar más equipamiento hasta no tener suficientes resultados obtenidos con lo que ya tenemos funcionando. Esperamos poder hacer contribuciones interesantes a la ciencia básica, pero que además, que el BT La Plata funcione como punto de consulta para temas que estén relacionados con lo que hacemos.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nota del Editor&lt;/strong&gt;: La ironía de publicar una nota sobre temperaturas extremadamente bajas en La Plata justo cuando una ola de calor récord golpea la ciudad es producto del deseo del autor que tal inclemencia climática cese de inmediato.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Variables financieras y personas de a pie</title>
      <link>https://ciencianet.com.ar/post/variables-financieras-y-personas-de-a-pie/</link>
      <pubDate>Fri, 29 Nov 2013 00:29:32 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/variables-financieras-y-personas-de-a-pie/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Daniel Parisi.&lt;/strong&gt; Instituto Tecnológico Buenos Aires y CONICET.&lt;/p&gt;
&lt;p&gt;[&lt;img src=&#34;https://ciencianet.com.ar/images/2013/11/parisi.jpg&#34; alt=&#34;Bolsa de Nueva York&#34;&gt;&lt;/p&gt;
&lt;p&gt;La fotografía muestra el piso de la Bolsa de Comercio de Nueva York. La dinámica de las personas caminando por ese espacio físico tal vez esté más relacionada de lo que parece con la dinámica de los precios de las acciones que estos mismos agentes generan. En un estudio reciente mostramos que un sistema de peatones simulados exhibe características muy similares a las que se observan en las series de precios de activos financieros o divisas.&lt;/p&gt;
&lt;p&gt;Existen distintos modelos físico-computacionales que permiten simular y predecir el movimiento de peatones. Uno de estos es el conocido &amp;quot;Modelo de Fuerza Social&amp;quot;. Con este modelo se puede simular, por ejemplo, dos habitaciones conectadas por una puerta y dos grupos de personas que quieren atravesarla en sentidos opuestos. A este sistema se le llama flujo bi-direccional (o contrapuesto) a través de un cuello de botella (la puerta). Se ha observado ya (con este modelo y en la vida real) que el flujo en la puerta presenta oscilaciones: por momentos dominan las personas que cruzan en una dirección y por momentos las que cruzan en sentido opuesto. Estas variaciones (fluctuaciones) han sido comparadas cualitativamente con las subas y bajas de los precios de las acciones de empresas.&lt;/p&gt;
&lt;p&gt;Sin embargo, que dos cosas se parezcan a simple vista no significa que sean equivalentes desde un punto de vista más riguroso. Inspirados por esta semejanza, implementamos una simulación computacional que consideró dos grupos de peatones: un grupo siempre quiere atravesar la puerta de izquierda a derecha y el otro en el sentido opuesto. Se midió entonces el número de personas por m&lt;sup&gt;2&lt;/sup&gt; (la densidad) en las cercanías de la puerta. Se encontró una relación muy estrecha entre las variaciones de densidad y las variaciones de precio de las acciones en la bolsa. Las variaciones o &amp;quot;retornos&amp;quot; de una variable financiera son el porcentaje que subió o bajó una acción, por ejemplo. Dada una serie temporal de precios, o de densidades de personas cerca de la puerta con flujo contrapuesto, es fácil calcular la serie de los retornos haciendo la diferencia entre un valor y el anterior en el tiempo.&lt;/p&gt;
&lt;p&gt;Es conocido en el mundo de las finanzas que los retornos tienen ciertas características estadísticas que lo diferencian de cualquier otro sistema. Hay aproximadamente 10 de estas características y se las conoce como &amp;quot;hechos estilizados&amp;quot; (&lt;a href=&#34;http://en.wikipedia.org/wiki/Statistical_finance&#34;&gt;&lt;em&gt;stylized facts&lt;/em&gt;&lt;/a&gt;) de los sistemas financieros. Es muy difícil, imposible diría, encontrar un sistema que no sea financiero y que exhiba todos estos hechos estilizados. Es más, los modelos que se construyen específicamente para intentar simular mercados financieros solo reproducen algunos pocos de estos hechos estilizados.&lt;/p&gt;
&lt;p&gt;Lo sorprendente del sistema peatonal simulado es que, sin ser un modelo que intenta simular series financieras, logra reproducir por lo menos 8 de los principales hechos estilizados propios de las finanzas. Veamos un ejemplo de hecho estilizado financiero que se cumple débilmente en las simulaciones de los peatones en flujo contrapuesto arriba descripta. La volatilidad, es una medida del tamaño de la variación sin importar si la acción subió o bajó. Una forma de medir la volatilidad es tomando el valor absoluto del retorno. Es un echo estilizado que los sistemas financieros muestran una alta autocorrelación de la volatilidad a la vez que los retornos de las acciones no están autocorrelacionados.&lt;/p&gt;
&lt;p&gt;Esto significa que no hay relación entre la variación de precios de hoy con la de mañana (retorno no correlacionado) pero grandes fluctuaciones son generalmente seguidas por grandes fluctuaciones y pequeñas por pequeñas (volatilidad autocorrelacionada). Obviamente, no se sabe el signo de las variaciones (si subirán o bajarán). Dicho de otra manera, sabiendo la variación de hoy sabré que la variación de mañana será de tamaño similar, pero no sabré si será negativa o positiva (si supiera sería fácil ganar dinero operando en la bolsa,... y no lo es).&lt;/p&gt;
&lt;p&gt;El modelo descripto arriba de peatones, que siempre quieren ir en la misma dirección, sin tomar ninguna decisión es suficiente para reproducir casi todos los hechos estilizados. Sin embargo, no permite lo análogo a una crisis financiera, en la cual todos los agentes buscan masivamente deshacerse de las acciones (una corrida). Para poder lograr este comportamiento en el sistema peatonal, hemos permitido que los agentes puedan decidir si seguir con su destino original o si cambiar de sentido en su recorrido al atravesar la puerta. La decisión se toma al acercarse a la puerta y ver hacia donde se dirigen los vecinos. Esto permite que cada grupo tenga un número variable de personas pudiéndose dar el caso extremo que todos los agentes quieran ir hacia el mismo lado, lo que equivaldría a una crisis financiera.&lt;/p&gt;
&lt;p&gt;Con esta modificación, no sólo se obtienen &amp;quot;crisis peatonales&amp;quot; sino que además se observa una autocorrelación de la volatilidad tan grande como la de los sistemas financieros, mientras que la serie de los retornos de la densidad no tiene ninguna autocorrelación. Es decir, que este importante hecho estilizado requiere que los precios se vayan del equilibrio y que haya corridas, grandes o pequeñas, para que exista. Sólo para esto es relevante que los peatones puedan tomar decisiones al mirar que hacen otros agentes cercanos. Todos los demás hechos estilizados que caracterizan a los sistemas financieros pueden ser explicados simplemente por la competencia entre dos grupos de personas con intereses o expectativas contrapuestas, los que creen que una acción bajará y los que creen que subirá.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; D. R. Parisi, D. Sornette y D. Helbing. &amp;quot;&lt;a href=&#34;http://link.aps.org/doi/10.1103/PhysRevE.87.012804&#34;&gt;Financial price dynamics and pedestrian counterflows: A comparison of statistical stylized facts&lt;/a&gt;&amp;quot;. Phys. Rev. E 87, 012804 (2013).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; Daniel Parisi (&lt;a href=&#34;mailto:dparisi@itba.edu.ar&#34;&gt;dparisi@itba.edu.ar&lt;/a&gt;)&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Física y biología para develar misterios del control motor durante el canto de las aves</title>
      <link>https://ciencianet.com.ar/post/fisica-y-biologia-para-develar-misterios-del-control-motor-durante-el-canto-de-las-aves/</link>
      <pubDate>Thu, 25 Apr 2013 01:04:14 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/fisica-y-biologia-para-develar-misterios-del-control-motor-durante-el-canto-de-las-aves/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Ana Amador.&lt;/strong&gt; Departamento de Física, Facultad de Ciencias Exactas y Naturales, Universidad de Buenos Aires. CONICET.&lt;/p&gt;
&lt;p&gt;El modo en que las aves aprenden a cantar es uno de los pocos ejemplos que existen en el reino animal de aprendizaje vocal, siendo un proceso similar al que seguimos los humanos para aprender a hablar. Además del aprendizaje, la ejecución de un canto es en sí misma una tarea compleja ya que requiere una fina coordinación del sistema respiratorio y el sistema muscular que controla la fuente sonora (similar a las cuerdas vocales de humanos). Debido a esto, es de gran interés el estudio de las instrucciones neuronales utilizadas durante la ejecución de un comportamiento complejo como es el canto.&lt;/p&gt;
&lt;p&gt;Un trabajo de colaboración entre la Universidad de Buenos Aires (Yonatan Sanz Perl y Gabriel B. Mindlin) y la Universidad de Chicago (Ana Amador y Daniel Margoliash) permitió mostrar, midiendo cómo el sistema neuronal procesa la información recibida, que un ave puede reconocer un canto sintético (simulado por un modelo físico-matemático) como su propio canto.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2013/04/amador-300x211.jpeg&#34; alt=&#34;Diamante mandarín.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;La reducción del número de variables necesarias propuesta desde la física permitió generar una nueva hipótesis sobre el código neuronal utilizado para el procesamiento y generación del canto. En el Laboratorio de Sistemas Dinámicos de la Facultad de Ciencias Exactas y Naturales (UBA), Gabriel Mindlin y colaboradores generaron un modelo físico para la producción de canto de aves cuyo resultado es un canto sintético que puede compararse con el canto real del ave.&lt;/p&gt;
&lt;p&gt;Este modelo logró una reducción muy importante del número de variables incluidas en el problema: ajustando sólo dos parámetros fue posible reproducir el canto del diamante mandarín (Taeniopygia Guttata). Si bien los cantos sintéticos y reales eran muy similares, la pregunta del millón era si estos cantos sintéticos sonaban reales para el ave. Para evaluarlo, en este trabajo nos propusimos medir las respuestas neuronales.&lt;/p&gt;
&lt;p&gt;En el cerebro de las aves canoras, existen núcleos neuronales que responden específicamente al escuchar el canto propio, y no responden al escuchar otros estímulos como ruido o tonos, pero más importante, tampoco responden a cantos de otros individuos de la misma especie (conespecíficos) o al canto del propio ave si es modificado intencionalmente. Es por esto que estas neuronas se conocen como selectivas al canto propio.&lt;/p&gt;
&lt;p&gt;Para poner a prueba al modelo sintético de canto, se presentó el canto propio del ave y su versión sintética, y se midió la respuesta neuronal del núcleo HVC (un núcleo con neuronas selectivas al canto propio), obteniendo el mismo patrón de respuesta para ambos cantos. Se concluyó entonces que los cantos sintéticos, generados con este modelo matemático que requiere de pocas variables, es suficiente para captar las características relevantes del canto del ave.&lt;/p&gt;
&lt;p&gt;Este modelo físico así validado permitió identificar dos parámetros (“gestos motores”) que se usaron para estudiar el código neuronal utilizado por el ave para el procesamiento y generación del canto. Los resultados sugieren que la información auditiva del canto que llega a áreas de la corteza cerebral está codificada según coordenadas asociadas a los movimientos musculares necesarios para producir el canto.&lt;/p&gt;
&lt;p&gt;Además, debido a que la actividad neuronal está sincronizada con el canto, las neuronas no podrían estar actuando de manera promotora ya que desde la activación neuronal en la corteza hasta la generación del canto se estima que existe un retraso considerable debido a la cantidad de sinapsis (conexiones entre neuronas) intervinientes. De esta manera se propone una codificación neuronal novedosa donde los gestos motores representados en la corteza del cerebro son utilizados para realizar predicciones del comportamiento necesario para generar el canto. Esto lleva a un marco de referencia conceptual novedoso para estudiar el código neuronal.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Articulo original:&lt;/strong&gt; “&lt;a href=&#34;http://www.nature.com/nature/journal/v495/n7439/full/nature11967.html&#34;&gt;Elemental gesture dynamics are encoded by song premotor cortical neuons&lt;/a&gt;”, Nature 495, 59 (2013).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Comentario de Todd Troyer:&lt;/strong&gt; &lt;a href=&#34;http://www.nature.com/nature/journal/v495/n7439/full/nature11957.html&#34;&gt;Neuroscience: The units of a song&lt;/a&gt;, Nature 495, 56 (2013).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Otros enlaces relacionados:&lt;/strong&gt;  &lt;a href=&#34;http://www.scholarpedia.org/article/Models_of_birdsong_%28physics%29&#34;&gt;Models of birdsong&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Nanobiotecnología y nanomedicina</title>
      <link>https://ciencianet.com.ar/post/nanobiotecnologia-y-nanomedicina/</link>
      <pubDate>Thu, 11 Apr 2013 01:09:24 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/nanobiotecnologia-y-nanomedicina/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero.&lt;/strong&gt; Investigadora de CONICET en el INIFTA (Instituto de Investigaciones Fisicoquímicas Teóricas y Aplicadas). Docente en la UNLP.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reseña del artículo Nanotecnología, hacia un nuevo portal científico-tecnológico de&lt;/strong&gt; Fiona M. Britto y Guillermo R. Castro (&lt;em&gt;Laboratorio de Nanobiomateriales, CINDEFI (CONICET-UNLP, CCT La Plata), Dto. de Química, Facultad de Cs. Exactas, UNLP) publicado en la revista Química Viva de Diciembre de 2012, número 3 año 11&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2013/04/quimica_viva.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;En este artículo de revisión los autores plantean ofrecen un panorama general de las aplicaciones más prometedoras de la Nanomedicina –disciplina de reciente creación, a partir de los avances de la Nanobiotecnología-, como ingeniería de tejidos, liberación controlada de fármacos, y detección precoz de enfermedades. También comentan y discuten algunas de las tendencias y desafíos actuales del área.&lt;/p&gt;
&lt;p&gt;El artículo comienza describiendo el proceso que se denomina Nanotecnología, que esencialmente consiste en la producción de objetos tecnológicos debido a la manipulación de la materia a escala atómica y que ha generado un cambio de paradigma que involucra tanto la concepción y diseño de los objetos materiales como su producción.&lt;/p&gt;
&lt;p&gt;Existe una definición establecida por la &lt;em&gt;National Science Foundation&lt;/em&gt; según la cual un objeto es “nano” si alguna de sus tres dimensiones físicas está en una escala comprendida entre 1 y 100 nanómetros (un nanómetro es igual a 10&lt;sup&gt;-9&lt;/sup&gt; metros). Así, la Nanotecnología comprende el estudio, diseño, síntesis, manipulación y aplicación de materiales funcionales, dispositivos y sistemas a través del control de la materia a escala nanométrica, y también el uso de las nuevas propiedades en esa escala.&lt;/p&gt;
&lt;p&gt;La convergencia de la Nanotecnología con la Biología surge de pensar a las moléculas que participan en los procesos biomoleculares como lo que los autores denominan “complejas nano-bio-máquinas”. Las proteínas, ácidos nucleicos, lípidos y polisacáridos, entre otros componentes de la materia viva, tienen sus dimensiones dentro de la escala nanométrica, sugiriendo que su comportamiento está regido por las mismas reglas que se comportan los nanomateriales.&lt;/p&gt;
&lt;p&gt;Así surgen dos amplios campos para la investigación y desarrollo: la Nanobiotecnologia, y su aplicación al cuidado de la salud, la Nanomedicina. La Nanomedicina ofrece muchísimas aplicaciones potenciales y otras que encuentran ya en el mercado. Algunas de las más prometedoras son la liberación controlada y dirigida de fármacos, la ingeniería de tejidos, el diagnóstico por imágenes y la detección temprana de patologías, de las cuales el artículo ofrece detallada descripción y que reseñaremos brevemente aquí.&lt;/p&gt;
&lt;h3 id=&#34;drug-delivery&#34;&gt;Drug delivery&lt;/h3&gt;
&lt;p&gt;Es una metodología de administración de medicamentos que logra la reducción de los posibles efectos secundarios tóxicos y aumenta la eficiencia de las terapias, ya que los nanosistemas administran los fármacos de una manera programada, durante períodos prolongados, permitiendo niveles constantes de la droga en el organismo, e incluso con la habilidad de dirigir la droga hacia el sitio específico de acción.&lt;/p&gt;
&lt;p&gt;De esta forma, las dosis necesarias se reducen y los efectos nocivos también. Los nanocomponentes actúan entonces como portadores de la droga, permitiendo controlar: cuánta se libera, en qué órganos, en qué tipo de célula, en qué tiempo y por cuánto tiempo reside el fármaco en el organismo. De especial interés resultan los nano biopolímeros, pues al tratarse de sustancias naturales tienen buena compatibilidad y degradabilidad en el organismo, ocasionando pocas reacciones adversas. Como además responden a condiciones ambientales, como el pH, pueden ser “programados”.&lt;/p&gt;
&lt;h3 id=&#34;ingeniería-de-tejidos&#34;&gt;Ingeniería de tejidos&lt;/h3&gt;
&lt;p&gt;Conocida también como medicina regenerativa, consiste en el desarrollo de sustitutos biológicos, con la finalidad de resolver problemas clínicos y quirúrgicos asociados a la pérdida de tejido o al fallo funcional de órganos. Los materiales y terapias tradicionales que se usan en este campo presentan una serie de problemas de biocompatibilidad, degradabilidad, y como el rechazo por inflamación e infección, que serían resueltos por el desarrollo de nanosistemas híbridos biocompatibles, con propiedades mecánicas, eléctricas y de superficie que se acerquen a las de los tejidos reales.&lt;/p&gt;
&lt;p&gt;Actualmente el área de mayor desarrollo es la regeneración del tejido óseo, también está bajo el foco de la Nanoingeniería de tejidos la regeneración de vasos sanguíneos y neuronas, y los implantes dentales. Finalmente, el avance de esta rama de la Nanociencia dependerá del entendimiento que se logre de de las interacciones entre los nanomateriales y las células, a nivel molecular.&lt;/p&gt;
&lt;h3 id=&#34;detección-y-diagnóstico&#34;&gt;Detección y diagnóstico&lt;/h3&gt;
&lt;p&gt;El objetivo de este campo es el desarrollo de sistemas de análisis y de imagen utilizando nanobiomateriales, que superen la velocidad, sensibilidad y selectividad de los métodos existentes para el diagnóstico temprano de enfermedades. La clave para este desafío es potenciar las propiedades ópticas y eléctricas especiales de los nanomateriales con la capacidad de reconocimiento específico del material biológico. Los nanomateriales actúan aquí como biosensores, compuestos de un receptor biológico (por ejemplo, enzimas, ADN, anticuerpos) y dispositivo que envíe una señal medible, que –sin necesidad de otras sustancias que actúen de contraste- trabajan in vitro sobre pequeñas muestras de tejido.&lt;/p&gt;
&lt;p&gt;Por otra parte, para los estudios que sí requieren el empleo de sustancias de contraste, la utilización de nanopartículas metálicas, semiconductoras o magnéticas como agentes de contraste provee mejoras significativas en la precisión de imágenes obtenidas &lt;em&gt;in vivo&lt;/em&gt;. La aplicación más conocida son los “puntos cuánticos”: nanopartículas semiconductoras, muy útiles como marcadores biológicos gracias a su fluorescencia. Como esta propiedad es dependiente del tamaño de las partículas, pueden obtenerse puntos cuánticos que emiten de modo intenso y bien definido en una amplia gama de colores, mejorando la calidad de la imagen en resonancias magnéticas, tomografías computadas, imágenes de fluorescencia, etc.&lt;/p&gt;
&lt;p&gt;Sin embargo, su uso no está muy extendido pues estas sustancias resultan actualmente costosas, y tampoco se conoce completamente su efecto en el organismo.&lt;/p&gt;
&lt;h3 id=&#34;nanotoxicología&#34;&gt;Nanotoxicología&lt;/h3&gt;
&lt;p&gt;Como puede apreciarse a lo largo del trabajo de Britto y Castro, son numerosas las aplicaciones de interés, con vistas a un uso extendido. Sin embargo, aún no se conocen completamente los alcances y consecuencias del uso de los nuevos materiales y tecnologías, de modo que los estudios toxicológicos de los mismos presentan una gran importancia a nivel mundial. Así, la Nanotoxicología tiene como objetivo el desarrollo de protocolos para la fabricación, uso, reciclaje, y descontaminación de nano-objetos, teniendo en cuenta que muchas de las propiedades de los nanomateriales dependen del tamaño de su particulado. Por ejemplo, las propiedades ópticas del oro -en particular, su color- dependen vistosamente del tamaño de las nanopartículas.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2013/04/gold_nanoparticles.jpg&#34; alt=&#34;Cambios en las propiedades ópticas del oro en función del tamaño de las nanopartículas. Créditos de la imagen: http://www.malvern.com/LabEng/industry/nanotechnology/gold_silver_nanoparticles.htm&#34;&gt;&lt;/p&gt;
&lt;p&gt;En cuanto al impacto de los nanoproductos sobre el medio ambiente, aún resta resolver cuestiones como la evaluación del ciclo de vida, la cuantificación de su emisión, los efectos sobre la cadena alimentaria, entre otros.&lt;/p&gt;
&lt;h3 id=&#34;a-modo-de-conclusión&#34;&gt;A modo de conclusión&lt;/h3&gt;
&lt;p&gt;El trabajo plantea interesantes observaciones respecto de las nuevas disciplinas. Además de la necesidad de seguir investigando en ellas, los autores reafirman la necesidad del trabajo conjunto entre áreas académicas, gobiernos y empresas que promuevan la inversión en investigación y desarrollo. También resaltan su naturaleza interdisciplinar y su consecuente requerimiento de profesionales especialmente formados en el área.&lt;/p&gt;
&lt;p&gt;Con los potenciales de las nanociencias y nanotecnologías, en combinación con los factores antes mencionados, y bajo una correcta implementación, los autores auguran un salto cuali y cuantitativo en la solución de los problemas de los países en desarrollo, y más aún, en el futuro de la humanidad.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Artículo original disponible en &lt;a href=&#34;http://www.quimicaviva.qb.fcen.uba.ar/&#34;&gt;http://www.quimicaviva.qb.fcen.uba.ar/&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>De la difusión de partículas a la difusión de la información: visualizando datos geoestadísticos mediante cartogramas</title>
      <link>https://ciencianet.com.ar/post/de-la-difusion-de-particulas-a-la-difusion-de-la-informacion-visualizando-datos-geoestadisticos-mediante-cartogramas/</link>
      <pubDate>Sat, 09 Mar 2013 01:11:21 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/de-la-difusion-de-particulas-a-la-difusion-de-la-informacion-visualizando-datos-geoestadisticos-mediante-cartogramas/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Karina Mazzitello&lt;/strong&gt; (Universidad de Mar del Plata) y &lt;strong&gt;Julián Candia&lt;/strong&gt; (Universidad Nacional de La Plata).&lt;/p&gt;
&lt;p&gt;La forma tradicional que se utiliza para visualizar resultados de censos y datos estadísticos en general es a través de diagramas de barras, de tortas y de cajas. En un diagrama de tortas a cada dato de una variable estadística dada le corresponde una porción de tamaño proporcional a su valor. Si el número de valores a representar es muy grande el diagrama de tortas resulta ilegible, debido a la gran cantidad de porciones requeridas en que se debería dividir la torta.&lt;/p&gt;
&lt;p&gt;Por este motivo vamos a encontrar muchas veces en la literatura extensas tablas muy detalladas con datos estadísticos tediosas de leer, sin ser volcadas a ningún tipo de diagrama para no perder información. Si en lugar de tablas o los diagramas tradicionales, se utiliza un mapa que represente, por ejemplo, la incidencia de una epidemia dada con código de colores, inevitablemente se verá alta incidencia en ciudades y baja incidencia en zonas rurales debido a sus diferencias en la cantidad de habitantes.&lt;/p&gt;
&lt;p&gt;Parecería a primera vista, que las zonas rurales fueran mas seguras, pero no necesariamente es así. Una alternativa es graficar la incidencia per cápita, que resuelve el problema a costa de descartar toda la información acerca de dónde ocurre la mayoría de los casos. Afortunadamente se puede combinar la densidad de población con datos geográficos para crear lo que podríamos llamar “cartogramas de difusión”, una forma mucho más eficiente y rápida para visualizar datos estadísticos de diferentes regiones, desde censos a mapas electorales.&lt;/p&gt;
&lt;p&gt;A modo de ilustración de ésta técnica se ha aplicado recientemente el método de los cartogramas a la problemática de la criminalidad en los estados de Brasil (ver el &lt;a href=&#34;http://link.springer.com/article/10.1007%2Fs13538-012-0091-0%5D(http://link.springer.com/article/10.1007%2Fs13538-012-0091-0)&#34;&gt;artículo original&lt;/a&gt;. Veremos con este ejemplo la utilidad de los mismos y luego explicaremos cómo se construyen. Comenzaremos mostrando un mapa original de Brasil y luego lo compararemos con un cartograma de dicho país, conteniendo ambas gráficas información similar.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2013/03/fig1a-300x257.jpg&#34; alt=&#34;Fig. 1(a): Un mapa de Brasil con los estados coloreados en escala de grises representando la tasa de homicidios cada cien mil habitantes. Los estados mas oscuros tienen mayor tasa de homicidios que los claros.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;En la figura 1(a) se muestra un mapa original de Brasil que ha sido coloreado según las tasas de homicidio cada 100.000 habitantes, correpondientes al año 2008. Las regiones oscuras son más peligrosas que las claras. En la figura 1(b) se grafica un cartograma de Brasil con los estados deformados según la tasa de homicidios cada 100.000 habitantes (año 2008). Los estados más oscuros son los más poblados y los más claros los menos poblados. Las áreas deformadas del cartograma son proporcionales a las tasas de homicidio y revelan una alta tasa de crímenes en la región noreste. Se puede apreciar que no necesariamente los estados más poblados tienen una alta tasa de homicidios como por ejemplo San Pablo y Minas Gerais, cuyas regiones no son tan grandes y sin embargo están muy poblados.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2013/03/fig1b-300x300.jpg&#34; alt=&#34;Fig. 1(b): Un cartograma de Brasil con los estados deformados para reflejar el número de homicidios cada cien mil habitantes. Revela una taza elevada en la región noreste. La escala de grises está asociada a la densidad de población (más oscuro implica mayor población).:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Resulta mas fácil visualizar los datos en el mapa deformado de la figura 1(b) que en el mapa convencional de la figura 1(a). La utilidad de los cartogramas queda en evidencia: rápidamente se puede atribuir el tamaño de la región de un mapa a una variable elegida, que en nuestro ejemplo es la tasa de homicidios. Además, los mismos pueden colorearse siguiendo un código con otra variable, permitiendo así dos capas de visualización de datos. En la figura 1(b) se colorearon los estados en escala de grises de acuerdo a sus densidades de población y como se mencionó anteriormente la tasa de homicidios no está relacionada a la cantidad de habitantes. Claramente, la construcción de mapas deformados ayuda a develar correlaciones.&lt;/p&gt;
&lt;p&gt;Las mediciones estadísticas realizadas sobre una población están frecuentemente correlacionadas entre sí, pero estas correlaciones permanecen ocultas si se utilizan directamente tablas o las herramientas tradicionales de visualización. Por ejemplo, en la figura 2 se muestra el mismo cartograma que en la figura 1(b) pero incluyendo un índice socioeconómico para cada estado. Las regiones rojas tienen menor nivel socioeconómico y tienden a una mayor tasa de homicidios. Es decir, hay una clara relación entre tasa de homicidios y el nivel socioeconómico dado por el acceso a empleo e ingresos, educación y salud. Verdaderamente esta correlación es fácil de visualizar en el cartograma de la figura 2.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2013/03/fig2-300x300.jpg&#34; alt=&#34;Fig. 2: El mismo cartograma que en la fig. 1(b) incluyendo datos socio-económicos. Las regiones rojas tienen menor nivel socio-económico que las azules y tienden a razones más altas de homicidios. Las correlaciones son mas difíciles de detectar en otros tipos de gráficas tales como los scatter plots.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Los cartogramas son gráficos elaborados a partir de una idea simple de la física asociada a los procesos difusivos de partículas. Supongamos que en una caja dividida por la mitad tenemos un gas que ocupa ambos lados de la caja a tempertura constante. Supongamos además, que en uno de los lados hay mayor densidad que en el otro. Si quitamos la pared que divide la caja habrá un flujo neto de partículas desde la región de mayor densidad a la región de menor densidad. El proceso difusivo alcanza un estado de equilibrio cuando la redistribución de partículas se homogeiniza. ¿Cómo se puede utilizar este fenómeno en la elaboración de cartogramas?&lt;/p&gt;
&lt;p&gt;Si representamos una variable estadística (por ejemplo, las tasas de homicidios por provincia) como diferentes densidades de partículas podemos estudiar su difusión hasta alcanzar el estado homogéneo. Con su desplazamiento, estas partículas arrastran los contornos de cada región del cartograma. El resultado es un mapa deformado cuya distorsión refleja la información estadística que se proporcionó como punto de partida. Retornando a los homicidios se pueden construir cartogramas de barrios en lugar de estados y hacer el análisis más focalizado.&lt;/p&gt;
&lt;p&gt;Este artículo pretende ilustrar la potencialidad y riqueza del método en una temática concreta para llamar la atención sobre su empleo como algo novedoso para la visualización de datos geográficos multidimensionales. También se puede aplicar esta técnica a diversas problemáticas locales o regionales, como por ejemplo la incidencia y distribución de determinadas enfermedades, epidemias, resultados de elecciones, censos y otros datos estadísticos con información geográfica. Recientemente el CONICET ha aprobado un proyecto para lanzarlo como herramienta de divulgación de datos geo-estadísticos a escala nacional y de alcance latinoamericano a través de una página web que será construida en el presente año.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>La conjetura de Maldacena</title>
      <link>https://ciencianet.com.ar/post/la-conjetura-de-maldacena/</link>
      <pubDate>Wed, 06 Feb 2013 01:15:02 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/la-conjetura-de-maldacena/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Gastón Giribet.&lt;/strong&gt; Universidad de Buenos Aires y CONICET.&lt;/p&gt;
&lt;p&gt;Gastón Giribet comparte con &lt;strong&gt;CienciaNet&lt;/strong&gt; su columna para el diario La Prensa, publicada en la edición en papel del lunes 7/1/13, donde ofrece una descripción de la idea detrás de la llamada &#39;conjetura de Maldacena&#39;. Para leer la nota del mismo diario, donde Juan Maldacena comenta su trabajo, cliquear &lt;a href=&#34;http://www.laprensa.com.ar/401132-Una-explicacion-que-puede-unir-dos-grandes-teorias.note.aspx&#34;&gt;aquí&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Algunas conferencias del propio Maldacena sobre su valiosa contribución, que cumplió 10 años en 2007, pueden encontrarse aquí: &lt;a href=&#34;http://video.ias.edu/search/node/maldacena&#34;&gt;http://video.ias.edu/search/node/maldacena&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2013/02/ten_years_maldacena-300x225.gif&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;La conjetura de Maldacena, también conocida con el nombre técnico de “correspondencia AdS/CFT”, es sin lugar a dudas uno de los resultados más importantes de la física teórica de las últimas décadas. Esto se debe en particular a que esta conjetura ofrece una novedosa y promisoria forma de abordar problemas cuya resolución eludió a los físicos teóricos durante muchísimas décadas. Entre las preguntas que la conjetura de Maldacena permite responder se encuentra la pregunta acerca de por qué las partículas fundamentales tales como los quarks tienden a permanecer unidos cuando no tienen demasiada energía. Esta pregunta es crucial para entender la constitución de la materia.&lt;/p&gt;
&lt;p&gt;Otra pregunta que gracias a este resultado podemos abordar es la de cuál es la explicación para las exóticas propiedades térmicas de los astros conocidos como agujeros negros. Así, tal como estos dos ejemplos muestran, el rango de aplicación del resultado de Maldacena va desde el infinitamente pequeño mundo subatómico hasta las escalas astrofísicas y cosmológicas.&lt;/p&gt;
&lt;p&gt;El logro de Maldacena ha sido, precisamente, mostrar que ciertas teorías físicas que rigen la física microscópica de las partículas fundamentales son totalmente equivalentes a ciertas teorías físicas con las que describimos fenómenos macroscópicos tales como la fuerza de gravedad entre los astros. Más precisamente, la conjetura de Maldacena establece la equivalencia entre dos teorías físicas que, si bien eran previamente conocidas, hasta fines de 1997 nadie había notado que estaban intrínsecamente relacionadas.&lt;/p&gt;
&lt;p&gt;Una de esas dos teorías es la denominada “teoría de cuerdas en el espacio-tiempo curvo”, que puede considerarse como una generalización de la teoría de la Relatividad General que Einstein propuso para describir el campo gravitatorio. La otra teoría es una teoría cuántica de campos, similar a la que los físicos emplean para estudiar las interacciones nucleares entre las partículas subatómicas. Según Maldacena, estas dos teorías son, en realidad, dos formas de describir las mismas ecuaciones o, como se suele decir, dos caras de la misma moneda.&lt;/p&gt;
&lt;p&gt;Para acercar al lector un poco más al valor que la conjetura de Maldacena tiene, permítaseme recurrir a la siguiente metáfora: Imaginémonos recorriendo los pasillos de una biblioteca inmensa; podemos pensar en la Biblioteca de Babel que Jorge Luis Borges ideó, una biblioteca cuyos anaqueles albergan todos los libros que podrían haber sido escritos. Imaginemos también que cada uno de esos libros corresponde a una teoría física. Algunos de esos libros -algunas de esas teorías- tendrán sentido y otros no; algunos de esos libros contendrán las fórmulas para entender ciertos fenómenos de la naturaleza mientras que otros se ocuparán de otros rincones de la ciencia. Debemos imaginar a Maldacena como un brillante bibliotecario con quien nos ha sido dado toparnos en nuestro recorrido.&lt;/p&gt;
&lt;p&gt;Con intuición genial, este bibliotecario toma dos libros de anaqueles distantes y nos los entrega, y al hacerlo nos afirma que esos dos libros, de apariencia tan distinta, en realidad contienen exactamente la misma historia, la misma información, aunque está esa historia escrita en dos idiomas distintos en cada uno de los libros. Nuestra sorpresa ante un hallazgo tal es infinita ya que, entre tantos libros, encontrar dos ejemplares del mismo parece &lt;em&gt;a priori&lt;/em&gt; impracticable; pero la sorpresa es mayor cuando constatamos que, además, esos dos libros no contienen una historia cualquiera sino que contienen las fórmulas que describen todas las leyes fundamentales de la física, desde las leyes que rigen lo más pequeño hasta las que rigen lo más grande, desde los constituyentes últimos de la materia hasta la geometría del mismo universo.&lt;/p&gt;
&lt;p&gt;Maldacena nos proporciona, además, un diccionario para poder traducir el idioma de uno de esos libros en el idioma del otro, lo que nos permite constatar que, en efecto, lo que nos dice es cierto y que ambos libros hablan en realidad de lo mismo, de la misma historia, de las mismas leyes. Para ir más allá con la analogía y entender en mayor profundidad la importancia del aporte de Maldacena, imaginemos que ambos libros hubieran sido víctimas de vandalismo en el pasado y que se hubieran arrancado las primeras páginas de uno de ellos y las últimas páginas del otro. Entonces, la importancia del diccionario que Maldacena nos provee sería capital ya que con él podemos nosotros reconstruir totalmente la historia; podríamos traducir lo que no sabíamos leer de uno de esos libros en términos del lenguaje del otro y descifrar, así, finalmente las leyes del universo.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>El último elemento: El bosón de Higgs</title>
      <link>https://ciencianet.com.ar/post/el-ultimo-elemento-el-boson-de-higgs/</link>
      <pubDate>Wed, 05 Sep 2012 01:24:20 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/el-ultimo-elemento-el-boson-de-higgs/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Gaston Giribet&lt;/strong&gt;. Este artículo es una versión modificada del texto publicado en la Revista Noticias el 01/09/12. Gaston Giribet es Doctor en Física por la Universidad de Buenos Aires, es Profesor de la Facultad de Ciencias Exactas y Naturales de la Universidad de Buenos Aires e Investigador del CONICET.&lt;/p&gt;
&lt;p&gt;El pasado 4 de julio científicos de la Organización Europea para la Investigación Nuclear (CERN), en Ginebra, anunciaron el descubrimiento de una nueva especie de partícula, jamás observada hasta el momento, la cual exhibe las propiedades que, según predice la teoría, debería tener la largamente buscada “partícula de Higgs”. La partícula de Higgs, también llamada “bosón de Higgs” por pertenecer al sub-conjunto de partículas que los físicos denominan “bosones”, es la última partícula que quedaba por descubrir del edificio teórico denominado Modelo Estándar de las partículas fundamentales, y es la explicación del hecho de que la materia posea masa.&lt;/p&gt;
&lt;h3 id=&#34;el-modelo-estándar-de-las-partículas-elementales&#34;&gt;El modelo estándar de las partículas elementales&lt;/h3&gt;
&lt;p&gt;El Modelo Estándar es un ambicioso esquema teórico que propone una descripción minuciosa de cómo serían absolutamente todos los constituyentes microscópicos que componen la materia en el universo. Además de esto, el Modelo describe también las interacciones entre dichos constituyentes; es decir, lo que usualmente llamamos fuerzas. Según la teoría, toda la materia que observamos en el universo y las fuerzas actuando entre esa materia están constituidas por partículas fundamentales infinitamente pequeñas. El electrón, el fotón, los quarks, los neutrinos, son sólo algunas de esas veintitantas partículas del Modelo Estándar.&lt;/p&gt;
&lt;p&gt;De comprobarse que, en efecto, la partícula observada recientemente en el colisionador de partículas LHC del CERN se trata de la partícula de Higgs, entonces se habría hallado el último bloque de ese colosal edificio teórico y estaríamos en condiciones de afirmar que tenemos una descripción acabada de todos los fenómenos hasta hoy observados en el universo, con excepción de fenómenos que involucran a la fuerza de gravedad, para la cual la física parece reservar un capítulo a parte.&lt;/p&gt;
&lt;h3 id=&#34;el-origen-de-la-masa&#34;&gt;El origen de la masa&lt;/h3&gt;
&lt;p&gt;La importancia de la partícula de Higgs no viene dada sólo por su propiedad de ser la última partícula predicha por la teoría que quedaba por ser descubierta, sino también porque ella desempeña un papel crucial: es precisamente esta partícula la responsable de que las otras tengan masa. Por ello, descubrir el Higgs es, de suyo, encontrar la razón por la cual la materia tiene masa.&lt;/p&gt;
&lt;p&gt;En cuanto construcción teórica, el Modelo Estándar debe su éxito a un conjunto de hermosas propiedades matemáticas que uno encuentra en sus ecuaciones. Estas propiedades reciben el nombre de “simetrías de gauge” y son parte esencial de cómo hoy los físicos conciben la naturaleza. Sin ánimo de renunciar a esas propiedades, los físicos entendieron desde un comienzo que la simetría de gauge es posible sólo si la masa de las partículas no es una propiedad intrínseca de ellas sino, por el contrario, es una propiedad emergente debido a algún otro fenómeno. Fue así como en 1964 Peter Higgs junto a otros físicos (correspondería también mencionar los nombres de Brout, Englert, Hagen, Guralnik y Kibble) propusieron un mecanismo según el cual las partículas podrían adquirir masa.&lt;/p&gt;
&lt;p&gt;Esta idea fue posteriormente implementada por Salam y Weinberg en la formulación del Modelo Estándar. Según el mecanismo de Higgs, la “masa” que observamos de las partículas, lo que solemos asociar a la renuencia de éstas a ser frenadas o aceleradas, no es una propiedad intrínseca de ellas sino que es el producto de la interacción de ellas con otra partícula, la hoy llamada “bosón de Higgs”. Este bosón interacciona con otras partículas adosándoseles y de ese modo confiriéndoles la masa. Así, las partículas en realidad no tendrían masa en un sentido estricto sino que su comportamiento inercial se debería a que se propagan en el universo surcando un omnipresente campo plagado de partículas de Higgs que les entorpece su andar “como si fueran masivas”.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2012/09/higgs-300x201.jpg&#34; alt=&#34;Peter Higgs. Imagen: Universidad de Edinburgo.:left&#34; title=&#34;Higgs&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;la-manifestación-del-bosón-de-higgs&#34;&gt;La manifestación del bosón de Higgs&lt;/h3&gt;
&lt;p&gt;La historia se vuelve aún más interesante cuando uno advierte que, según la teoría, el bosón de Higgs no sólo puede manifestarse indirectamente a través de su propiedad de otorgarles la masa a otras partículas, sino que podría también hacerse presente per se. Para lograrlo, es necesario producir colisiones de partículas de muy alta energía, similares a las que se han alcanzado en el CERN.&lt;/p&gt;
&lt;p&gt;La razón por la cual los procesos de colisiones necesarios para exhortar al Higgs a salir de su cueva deben ser muy energéticos está relacionada con el hecho de que el Higgs tiene una masa muy grande. Por ejemplo, si la partícula observada recientemente en el CERN se tratara en efecto del bosón de Higgs entonces su masa se calcula entre 125 y 126 GeV (Giga-electronVoltios), lo que equivale a decir que sería 245.000 veces más pesado que un electrón.&lt;/p&gt;
&lt;p&gt;Para entender esto es conveniente recurrir a la siguiente analogía: Asumamos que el universo está lleno de ese “campo de Higgs”. Esto es, supongamos que el universo es un medio en el cual las partículas se mueven. Imaginemos a ese medio denso, como si fuera algún tipo de pegamento de esos que uno usa en la escuela. Ocurre que algunas de las partículas fundamentales tienen la cualidad de pegarse a ese medio gomoso más que otras, dependiendo esto de sus propiedades. La idea es que las partículas en realidad no tienen masa sino que tienen “diferente adhesión a ese medio gomoso en el que se propagan”. Así, las partículas que se pegan con mayor afinidad se comportan como si fueran pesadas, como si tuvieran mucha masa, con un andar lento y costoso. Por el contrario, aquéllas que se adhieren muy poco al medio gomoso son las que se mueven rápidamente, de manera ligera, como si estuvieran en el vacío; éstas son las que uno llama partículas sin masa, o con masa muy pequeña.&lt;/p&gt;
&lt;p&gt;Ahora bien, hasta ahí esta descripción sólo viene a reforzar lo que decíamos arriba, que el campo de Higgs es lo que les confiere la masa a las otras partículas. Pero uno puede ir más allá con la metáfora y pensar que, así como ese medio tiene la propiedad de pegarse a las partículas que transitan en él, tiene también la propiedad intrínseca de generar olas. Es decir, aun cuando no haya una partícula moviéndose en ese medio gomoso, el mismo puede moverse formando olas de pegamento, o grumos que se propagan en el mismo medio del que están hechos. Son precisamente esas “olitas” las que uno llama “partículas de Higgs”, y la razón por la que ha demandado tanto esfuerzo observarlas es que esas olas del medio gomoso no se generan tan fácilmente como lo hacen las olas en el agua o en un medios menos densos; por el contrario, para generarlas uno necesita “más energía”. Es necesario lograr procesos muy energéticos dentro de ese medio para que las repercusiones de los mismos devengan en una “olita” en el campo de Higgs.&lt;/p&gt;
&lt;p&gt;Lo que en el CERN parece haberse observado es precisamente esta manifestación más directa del Higgs, esas olitas. Hubo que esperar casi cinco décadas desde su predicción hasta poder alcanzar la energía suficiente para experimentar este fenómeno, y es esto lo que hace de este momento uno tan especial. De todos modos, y a pesar de estar embriagados con el entusiasmo propio de quienes probablemente estén frente a la última partícula que quedaba por descubrir, los físicos prefieren la cautela: lo que los dos experimentos (llamados ATLAS y CMS) que funcionan en el colisionador LHC han anunciado el 4 de julio es la observación de una nueva partícula, jamás observada anteriormente, y cuyas propiedades coinciden con las esperadas para el escurridizo bosón de Higgs.&lt;/p&gt;
&lt;p&gt;Investigaciones venideras nos dirán si efectivamente se trata del Higgs o si, por el contrario, se trata de algo aún más exótico. En lo personal, yo estaría feliz con lo primero.&lt;/p&gt;
&lt;h3 id=&#34;el-colisionador-de-partículas-lhc&#34;&gt;El colisionador de partículas LHC&lt;/h3&gt;
&lt;p&gt;Pero propongo que hagamos aquí un descanso. Abramos un paréntesis para saldar una deuda, la de explicar qué es precisamente el colisionador de partículas LHC que funciona en Ginebra y cómo una máquina de esas características nos permitiría aprender sobre la estructura microscópica de la materia. El colisionador LHC, acrónimo de la traducción al inglés de “gran colisionador de hadrones”, es el acelerador de partículas más grande que haya sido construido. Funciona acelerando protones e iones pesados. Los protones pertenecen al conjunto de partículas que los físicos denominan “hadrones” y es por eso que el LHC recibe ese nombre; por otro lado, los iones pesados son átomos cargados eléctricamente cuyos núcleos tienen una gran cantidad de protones y de neutrones.&lt;/p&gt;
&lt;p&gt;El LHC consiste en un túnel circular de 27 kilómetros de perímetro enterrado a cien metros de profundidad en la frontera entre Suiza y Francia. Aunque los detalles técnicos de una máquina de las características del LHC son infinitos y ciertamente exceden cualquier tipo de descripción que un profano como yo podría intentar, el principio de funcionamiento es curiosamente simple: gracias a un intenso campo magnético, los protones del LHC son acelerados a lo largo del túnel y finalmente forzados a colisionar entre sí a velocidades altísimas, generando de esa manera choques de partículas de muy alta energía.&lt;/p&gt;
&lt;p&gt;El producto de esas colisiones es colectado en complejos detectores y luego analizado por los físicos de partículas. Los expertos se dedican entonces a estudiar los detritos de esos choques sub-atómicas con la intención de entender cuáles son los constituyentes mínimos de la materia. Cuanto más energéticos son esos choques, más se adentra uno en los secretos íntimos de la materia.&lt;/p&gt;
&lt;p&gt;Para entender cómo es esto pensemos en un choque entre colectivos: si exploráramos los restos de un choque entre dos colectivos que venían a moderada velocidad, entonces sólo podríamos concluir que los colectivos están compuestos de guardabarros, parabrisas y neumáticos, ya que son ésos los vestigios que quedarían diseminadas en el pavimento luego del impacto. En cambio, si nos fuera dado estudiar los restos de una colisión entre colectivos que venían a muy alta velocidad, entonces aprenderíamos más de su composición, y al explorar sus restos comprobaríamos la existencia de tuercas, tornillos, carburadores y bujías. Esto ejemplifica lo que ocurre en los aceleradores de partículas: es necesario alcanzar velocidades muy altas si lo que se pretende es escudriñar en lo más profundo de la materia.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2012/09/cern-300x200.jpg&#34; alt=&#34;Detección de partículas en el LHC. Imagen: CERN.:left&#34; title=&#34;Cern&#34;&gt;&lt;/p&gt;
&lt;p&gt;En el caso particular del LHC las partículas allí aceleradas son protones, y al estar éstos compuestos a su vez por otras partículas más pequeñas (los quarks) el análisis de los restos de sus colisiones resulta ciertamente muy complejo. Participan en la realización y en el análisis de esos experimentos muchísimos científicos de muchísimos países, los cuales suman esfuerzos y recursos para llevar a cabo una de las empresas científicas más grandes de la historia.&lt;/p&gt;
&lt;p&gt;Entre los muchos expertos involucrados en los experimentos del LHC hay grupos de nuestro país realizando tareas de gran importancia; cabe mencionar el grupo de Ricardo Piegaia, de la Universidad de Buenos Aires, y el grupo de María Teresa Dova, de la Universidad Nacional de La Plata. De todos modos, la porción de la comunidad científica interesada en los resultados de las colisiones de protones en Ginebra excede ampliamente al grupo de físicos directamente involucrados con el experimento. Un ejemplo particular de esto es el entusiasmo, compartido ampliamente por la comunidad científica, ante anuncio del reciente descubrimiento de esa partícula “que tanto se le parece al Higgs”.&lt;/p&gt;
&lt;h3 id=&#34;predicción-teórica-y-experimento&#34;&gt;Predicción teórica y experimento&lt;/h3&gt;
&lt;p&gt;Un corolario que podemos -y acaso debemos- extraer de descubrimientos tales como el acaecido recientemente en el LHC es el que nos habla de la fecundidad de las construcciones teóricas a la hora de guiar las investigaciones en física. Es remarcable que la existencia de partículas como el Higgs u otros componentes del Modelo Estándar haya sido primero conjeturada sobre un pizarrón o sobre un pedazo de papel y sólo después -en muchos casos, décadas después- hayan los físicos asistido a sus descubrimientos.&lt;/p&gt;
&lt;p&gt;Este aspecto suele a veces quedar tristemente desplazado a un segundo plano, pero es uno de los logros más importantes de las ciencias naturales: ese poder de predicción, de anticipación de fenómenos, siendo la investigación a veces guiada sólo por intuiciones y argumentos que al comienzo pueden lucir tan frágiles y etéreos como aquella reticencia a abandonar la “simetrías de gauge” como forma de entender el mundo.&lt;/p&gt;
&lt;p&gt;Un caso paradigmático de cómo cogitaciones puramente teóricas pueden eventualmente desembocar en el descubrimiento de nuevas partículas en la naturaleza es la historia del descubrimiento de la partícula llamada Omega. Esta historia transcurre en los años 60s. En aquel momentos los físicos conocían ya un vasto catálogo de partículas y se abría con ello un gran abanico de preguntas: ¿Por qué había tantos tipos distintos de partículas sub-atómicas?, ¿había, acaso, una manera conveniente de clasificarlas? Los más osados llegaron a preguntarse si podría una tarea de clasificación tal llevarlos a predecir la existencia de nuevas partículas aún no detectadas.&lt;/p&gt;
&lt;p&gt;La respuesta por la afirmativa no tardaría en llegar. A comienzos de la década de 1960, Gell-Mann y Neemann arribaron independientemente a la conclusión de que, en efecto, existía una manera conveniente de clasificar las partículas conocidas hasta ese entonces. El método consistía en organizar el bestiario sub-atómico formando octágonos, y fue por ello que uno de sus padres bautizó al método con el nombre de “la manera óctuple”.&lt;/p&gt;
&lt;p&gt;Según la manera óctuple de organizar las partículas, éstas se ubican formando octágonos sobre un papel luego de que uno traza sobre éste ejes imaginarios etiquetando sus propiedades, de forma similar a la de esos gráficos con ejes que hacíamos en las clases de matemática en el colegio. Ante la sorpresa de aquellos dos físicos, las partículas no se ubicaban caprichosamente sobre el papel sino que lo hacían formando un patrón definido, armónico, formando octágonos simétricos. Así, aquellos pioneros concluyeron que debía haber alguna razón para tal simetría y que no podían ser esos prolijos octágonos mera casualidad.&lt;/p&gt;
&lt;p&gt;Al mismo tiempo advirtieron que otro grupo de partículas, primas cercanas a las que formaban octágonos, hacían una gracia similar pero formando triángulos en grupos y ya no solamente octágonos. Nuevamente, aparecía sobre el papel una figura simétrica que remedaba a un triángulo formado por nueve partículas. –Sería un triángulo perfecto si hubiera una décima partícula aquí en la punta– pensó uno de ellos, y así se conjeturó la existencia de Omega-. Esta partícula, cuya existencia fue predicha basándose sólo en la inquietud estética de dos genios que vieron aparecer figuras geométricas en sus cuadernos de notas, fue detectada pocos años después en el acelerador de partículas del Laboratorio Nacional de Brookhaven, en los Estados Unidos.&lt;/p&gt;
&lt;p&gt;Poco más tarde, la manera óctuple dio origen a la teoría de los quarks, que son las partículas de las que, según hoy sabemos, se componen otras partículas tales como la Omega-, los protones y los neutrones. Y la teoría de los quarks me invita a contar a otro ejemplo: recuerdo cuando, siendo yo un estudiante de segundo año de la universidad, se anunciaba oficialmente el descubrimiento del quark top. Hasta el reciente anuncio del pasado 4 de julio, el quark top detentaba el título de ser la última partícula fundamental en haber sido descubierta. Se trata del quark más pesado de los seis detectados en la naturaleza, y en la dinastía de partículas fundamentales se le reserva un lugar como miembro de una “familia” de partículas cuya existencia había sido predicha a comienzos de los años 70’s por Kobayayi y Maskawa.&lt;/p&gt;
&lt;p&gt;El quark top fue finalmente observado en los aceleradores de partículas en 1995, dos décadas después de su predicción teórica, regalándonos de esta manera otro ejemplo de cómo las construcciones teóricas no siempre se limitan a la mera descripción de los fenómenos naturales en términos matemáticos sino que, en muchas ocasiones, son estas construcciones las que se adelantan a la misma observación de dichos fenómenos. Esta digresión invita a preguntarnos cuáles de las predicciones de las más especulativas teorías físicas de la actualidad acabarán finalmente manifestándose en futuros experimentos y observaciones.&lt;/p&gt;
&lt;p&gt;Construcciones teóricas de la física-matemática tales como la teoría de cuerdas podrían, quizá, terminar prediciendo fenómenos que alguna vez sean observados. ¿Existen en el universo más dimensiones espaciales que las tres que experimentamos cotidianamente?, ¿existe para cada tipo de partícula del Modelo Estándar un alter ego como nos propone la teoría de “la supersimetría”?, ¿cuál es la razón por la cual es más abundante la materia que la anti-materia en el universo?, ¿qué tipo de “materia oscura” es la que ayuda a apelmazar a las galaxias y les impide dispersarse al rotar tan rápidamente?, ¿cuál es el origen de esa “energía oscura” que acicatea al universo a acelerar su expansión? Son éstas preguntas que aún permanecen abiertas y para las cuales los físicos teóricos han ensayado varias posibles respuestas desde hace décadas. El tiempo dirá si alguna de esas teorías resulta ser verdad. Vayamos de a poco.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>¿De dónde vienen las oscilaciones epidémicas?</title>
      <link>https://ciencianet.com.ar/post/de-donde-vienen-las-oscilaciones-epidemicas/</link>
      <pubDate>Mon, 14 May 2012 01:26:48 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/de-donde-vienen-las-oscilaciones-epidemicas/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Guillermo Abramson&lt;/strong&gt;. Grupo de Física Estadística e Interdisciplinaria del Centro Atómico Bariloche, CONICET e Instituto Balseiro.&lt;/p&gt;
&lt;p&gt;Desde épocas remotas las epidemias han causado enorme sufrimiento a la Humanidad. Cada civilización sobrellevó sus plagas de la mejor manera que pudo, y éstas fueron forjando en alguna medida el curso de la Historia. Hasta hace no muchas décadas las enfermedades infecciosas tenían una presencia en la vida cotidiana mucho mayor que hoy en día. Esto era así aún en las regiones más favorecidas del mundo. La malaria, la difteria, la sífilis, la meningitis meningocóccica, y mucha, mucha tuberculosis. Gracias a los avances en áreas que van desde los antibióticos a la plomería (y sobre todo a mucho dinero) gran parte de estas pestes ya casi no existen en las regiones más desarrolladas, así como en buena parte del mundo subdesarrollado. Cada tanto, sin embargo, la sombra de una vieja o de una nueva plaga aparece en el horizonte.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://es.wikipedia.org/wiki/Ronald_Ross&#34;&gt;Ronald Ross&lt;/a&gt; (nacido en Almora, India, en 1857) fue un médico genial, descubridor del agente causante de la malaria y de su transmisión por el mosquito, por lo cual recibió el Premio Nobel de Medicina en 1902. Era aficionado a la matemática, y publicó en 1917 un artículo &lt;a href=&#34;#1&#34; title=&#34;1&#34;&gt;[1]&lt;/a&gt; en el que manifiesta su asombro de que no existiera una teoría matemática de la propagación de epidemias, un campo en el cual se tenían, desde hacía tiempo, grandes cantidades de datos estadísticos esperando ser examinados. Dice, además, que las cuestiones fundamentales de la epidemiología, en las cuales se basan las medidas preventivas (tales como la tasa de contagio, la frecuencia de brotes o la pérdida de la inmunidad) no pueden ser resueltas por métodos que no sean analíticos. ¿A qué se debe que algunas enfermedades persistan en la población, mientras que otras aparecen y desaparecen? ¿Por qué existen las epidemias? Ross sospechaba que la respuesta a estas preguntas se esconde en principios fundamentales a los que puede accederse mediante el cálculo, del mismo modo que en la Astronomía, la Física y la Mecánica.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2012/05/sir-300x266.png&#34; alt=&#34;Esquema del modelo SIR:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;El campo interdisciplinario del estudio matemático de las epidemias se ha desarrollado considerablemente en el siglo transcurrido desde la propuesta pionera de Ross, pero queda sin embargo mucho por hacer. Los modelos tradicionales están basados en la división de la población en categorías, de acuerdo a su estado infeccioso. Poco después de Ross un par de científicos escoceses, William Kermack (químico) y Anderson McKendrick (médico) propusieron el que se ha convertido en el padre de todos los modelos epidemiológicos: el modelo &lt;strong&gt;SIR&lt;/strong&gt; &lt;a href=&#34;#2&#34; title=&#34;2&#34;&gt;[2]&lt;/a&gt;. Los individuos que componen la población pertenecen a las categorías susceptibles (&lt;strong&gt;S&lt;/strong&gt;), infectados (e infecciosos, &lt;strong&gt;I&lt;/strong&gt;) o recuperados (e inmunes, &lt;strong&gt;R&lt;/strong&gt;). Existen transiciones entre estos estados, controladas por fenómenos tanto epidemiológicos como sociales. Los susceptibles pueden contagiarse por contacto con infectados, pasando del estado &lt;strong&gt;S&lt;/strong&gt; al &lt;strong&gt;I&lt;/strong&gt;. A su vez los infectados se recuperan, pasando al estado &lt;strong&gt;R&lt;/strong&gt;. El balance entre la velocidad a la cual ocurren estos dos procesos determina la evolución de la epidemia. Usualmente se condensa esto en términos de un único parámetro, la tasa reproductiva de la infección, que expresa el número de infecciones secundarias producidas por cada caso de la enfermedad. Si este número es mayor que 1, entonces ocurre una epidemia. Si es menor que 1, el “brote” inicial decae. A medida que la infección se va propagando la cantidad de susceptibles disminuye, de manera que a la infección se le hace cada vez más difícil propagarse, eventualmente empieza a decaer, y finalmente se extingue.&lt;/p&gt;
&lt;p&gt;Es fácil ver que este modelo sencillo es una caricatura de lo que ocurre en la realidad. Por un lado, la historia natural de muchos agentes infecciosos es más complicada. Pueden existir estados latentes (infectados pero no infecciosos), una respuesta inmune compleja (inclusive dependiente de la edad del anfitrión), pérdida de la inmunidad (con regreso de los &lt;strong&gt;R&lt;/strong&gt; al estado &lt;strong&gt;I&lt;/strong&gt;), vacunación y tratamientos, una demografía con nacimientos, muertes y migraciones, y un largo etcétera. Por otro lado, existen hipótesis más sutiles en la formulación matemática de estos sistemas, cuya modificación podría producir un comportamiento dinámico distinto. En nuestro trabajo hemos estudiado estos problemas desde numerosos puntos de vista [[3](#3 &amp;quot;3·), - &lt;a href=&#34;#6&#34;&gt;6&lt;/a&gt; &amp;quot;6&amp;quot;], tratando de dilucidar el rol de los distintos mecanismos que se esconden detrás de esas hipótesis. Por ejemplo, ¿qué ocurre si la población, en lugar de estar bien mezclada, con contactos de &amp;quot;todos con todos&amp;quot;, está organizada en una red social, como ocurre en la realidad? &lt;a href=&#34;#3&#34;&gt;[3]&lt;/a&gt;. ¿Y qué pasa si, en lugar de analizar poblaciones grandes que evolucionan de manera suave, la población es pequeña y las fluctuaciones que ocurren a nivel individual son relevantes? &lt;a href=&#34;#4&#34; title=&#34;4&#34;&gt;[4]&lt;/a&gt;. O, por otro lado, ¿qué pasa si los fenómenos infecciosos no ocurren a tasas constantes como se supone habitualmente, sino que deben cumplir plazos más o menos bien definidos? En los próximos párrafos veremos con algún detalle este último caso, que es el objeto de nuestros trabajos más recientes [&lt;a href=&#34;#5&#34; title=&#34;5&#34;&gt;5,&lt;/a&gt;&lt;a href=&#34;#6&#34; title=&#34;6&#34;&gt;6&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2012/05/delay-300x254.png&#34; alt=&#34;Esquema comparativo entre recuperación a tasa constante y con tiempos característicos.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;¿Qué significa que los fenómenos infecciosos ocurren a tasas constantes? Pensemos en lo que ocurre al individuo apenas contagiado en un modelo &lt;strong&gt;SIRS&lt;/strong&gt; (es decir, con pérdida de la inmunidad). Acaba de ingresar a la categoría &lt;strong&gt;I&lt;/strong&gt;, en la que permanecerá hasta que se recupere. Una tasa constante de recuperación significa que el individuo tiene una cierta probabilidad de curarse hoy, la misma probabilidad de curarse mañana, la misma probabilidad de curarse pasado mañana... ¡y así sucesivamente hasta que se cure! Esto, claramente, está lejos de la realidad para muchas enfermedades infecciosas. Es mucho más razonable suponer que, lo que ocurrirá, es que el infectado se recuperará una vez transcurrido cierto tiempo característico de su enfermedad. A lo sumo un poquito antes, o un poquito después. Lo mismo puede decirse de la duración de la inmunidad en el estado &lt;strong&gt;R&lt;/strong&gt;. Estos mecanismos de recuperación y de pérdida de la inmunidad con tiempos característicos pueden también formularse matemáticamente. El análisis es un poco más complicado, razón por la cual no se lo usa habitualmente. En [5] hemos analizado un sistema &lt;strong&gt;SIRS&lt;/strong&gt; de este tipo, en situaciones en las que el análisis &amp;quot;clásico&amp;quot; (al estilo de Kermack y McKendrick) predice un comportamiento de tipo endémico, con una pequeña fracción de la población que permanece infectada. Encontramos que la hipótesis de tiempos característicos de recuperación y pérdida de inmunidad, en cambio, produce una dinámica oscilatoria, con rebrotes periódicos de la infección. La aparición de las oscilaciones depende de dos factores. Por un lado, el tiempo de pérdida de la inmunidad debe ser mayor que el de recuperación (cuánto mayor depende, además, de la tasa reproductiva). Parece un resultado inocente, pero puede ser muy relevante en la vida real. Imaginemos que estamos ante un agente infeccioso endémico en una población, con una cierta prevalencia. Ahora imaginemos que el tiempo de inmunidad se alarga, por ejemplo por mejoras en una vacuna. Podemos empezar a ver una oscilación, ¡inclusive con aumento de la prevalencia! Eso no significa que la vacuna no esté funcionando, claro está, ya que el valor medio de la prevalencia puede disminuir. Evidentemente es algo que hay que tener en cuenta.&lt;/p&gt;
&lt;p&gt;Por otro lado, como dijimos antes, los tiempos de infección y de inmunidad pueden no ser exactos, sino más o menos difusos alrededor de un valor medio. El modelo predice que, para que aparezcan las oscilaciones, ninguno de estos procesos puede estar muy desparramado alrededor de su valor medio. Existe un ancho crítico, por debajo del cual las recuperaciones y las pérdidas de inmunidad de los distintos individuos &amp;quot;no se mezclan&amp;quot;. Esto permite la sincronización de la infección en muchos de ellos, condición necesaria para la oscilación. En ambos casos se trata de una transición en el comportamiento dinámico de la epidemia en función de la historia natural del agente infeccioso, en lugar de depender de la tasa reproductiva que controla los modelos clásicos.&lt;/p&gt;
&lt;p&gt;Muy bien, tenemos un modelo matemático de epidemias que produce oscilaciones. ¿Existen las oscilaciones epidémicas en el mundo real? Claro que sí. El sarampión, antes de la introducción de la vacunación masiva a fines de los &#39;60, rebrotaba cada dos años en muchos países. La tos convulsa, una infección para la cual también existe vacuna pero que está reapareciendo, oscila con un período de 4 años. Se ha evidenciado un período de 11 años en los brotes de sífilis en algunas ciudades norteamericanas. Varias enfermedades respiratorias, muy ligadas a la estacionalidad anual, oscilan sin embargo con período bienal. Es el caso de la parainfluenza de tipos 1 y 3, y de la propia gripe, que parece tener picos en distintas semanas en años pares o impares en algunas ciudades. Nuestro modelo predice períodos de oscilación que son compatibles con los parámetros epidemiológicos de varias de estas enfermedades. En particular, cuando se supone adicionalmente que la tasa de contagio está sujeta a oscilaciones estacionales (como en el caso de las enfermedades tipo influenza), observamos 6 oscilaciones con dos picos anuales, pero que ocurren en momentos distintos del año, tal como se observa en situaciones reales.&lt;/p&gt;
&lt;p&gt;Podemos concluir, por un lado, que es posible formular modelos matemáticos que relajen las hipótesis fuertes que hacen que, muchas veces, los modelos clásicos de enfermedades infecciosas parezcan caricaturas de la realidad. Por otro lado, que existen fenómenos dinámicos novedosos cuando estudiamos cada una de estas suposiciones por separado. Finalmente, que estos modelos se caracterizan por un fenómeno de sincronización de la fase infecciosa, que se manifiesta en la aparición de oscilaciones de distinta complejidad, cuyas propiedades pueden estudiarse detalladamente. Existen muchos mecanismos por los cuales un sistema epidémico puede manifestar oscilaciones. Ciertamente, en la naturaleza pueden operar simultáneamente más de uno, de manera que es necesario estudiar cuidadosamente sus características e interacciones.&lt;/p&gt;
&lt;h2 id=&#34;para-saber-más&#34;&gt;Para saber más:&lt;/h2&gt;
&lt;p&gt;El artículo de Wikipedia sobre el &lt;a href=&#34;http://en.wikipedia.org/wiki/SIR_Model&#34;&gt;modelo SIR&lt;/a&gt; está muy bien (en inglés). Si quiere explorar su dinámica, hay un &lt;a href=&#34;http://jsxgraph.uni-bayreuth.de/wiki/index.php/Epidemiology:_The_SIR_model&#34;&gt;simulador on line&lt;/a&gt; en la Universidad de Bayreuth. El episodio 3 de la primera temporada de la serie televisiva NUMB3ERS trata sobre los modelos SIR. Visite la &lt;a href=&#34;http://fisica.cab.cnea.gov.ar/estadistica/abramson/&#34;&gt;página web del autor&lt;/a&gt; y la del &lt;a href=&#34;http://fisica.cab.cnea.gov.ar/estadistica/&#34;&gt;Grupo de Física Estadística e Interdisciplinaria&lt;/a&gt;. Y si le gusta la astronomía, no el blog del autor: &lt;a href=&#34;http://guillermoabramson.blogspot.com.ar/&#34;&gt;En el cielo las estrellas&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;referencias&#34;&gt;Referencias&lt;/h2&gt;
&lt;p&gt;&lt;a id=&#39;1&#39;&gt;1&lt;/a&gt;. Ronald Ross, An application of the theory of probabilities to the study of a priori Pathometry, Proc. Roy. Soc. Lond. 93A, 225–240 (1917).&lt;/p&gt;
&lt;p&gt;&lt;a id=&#39;2&#39;&gt;2&lt;/a&gt;. William Kermack and Anderson McKendrick, A contribution to the mathematical theory of epidemics, Proc. Roy. Soc. Lond. 115A, 700–721 (1927).&lt;/p&gt;
&lt;p&gt;&lt;a id=&#39;3&#39;&gt;3&lt;/a&gt;. Marcelo Kuperman and Guillermo Abramson, Small world effect in an epidemiological model, Phys. Rev. Lett. 86, 2909–2912 (2001).&lt;/p&gt;
&lt;p&gt;&lt;a id=&#39;4&#39;&gt;4&lt;/a&gt;. Sebastián Risau-Gusman and Guillermo Abramson, Bounding the quality of stochastic oscillations in population, Eur. Phys. J. B 60 , 515–520 (2007).&lt;/p&gt;
&lt;p&gt;&lt;a id=&#39;5&#39;&gt;5&lt;/a&gt;. Sebastián Gonçalves, Guillermo Abramson and Marcelo F. C. Gomes, Oscillations in SIRS model with distributed delays, Eur. Phys. J. B 81, 363–371 (2011).&lt;/p&gt;
&lt;p&gt;&lt;a id=&#39;6&#39;&gt;6&lt;/a&gt;. Sebastián Gonçalves, Guillermo Abramson and Marcelo F. C. Gomes, The interaction between seasonality and delays in epidemic models, en preparación.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>TOP TEN: La revista Physics World presenta su selección de los 10 principales avances del 2011</title>
      <link>https://ciencianet.com.ar/post/top-ten-la-revista-physics-world-presenta-su-seleccion-de-los-10-principales-avances-del-2011/</link>
      <pubDate>Wed, 22 Feb 2012 01:33:34 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/top-ten-la-revista-physics-world-presenta-su-seleccion-de-los-10-principales-avances-del-2011/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero&lt;/strong&gt;.  Facultad de Ciencias Exactas (UNLP).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2012/02/physw.jpg&#34; alt=&#34;:left&#34; title=&#34;physw&#34;&gt;La selección estuvo a cargo del equipo editorial de la revista y se realizó entre 350 artículos publicados allí. La elección estuvo guiada por algunos marcadores, como la relevancia del aporte para todos los físicos, la importancia de la investigación y la conexión entre el experimento y la teoría. El artículo que los recopila fue escrito por Hamish Johnston y puede leerse en &lt;a href=&#34;http://physicsworld.com/cws/article/news/48126&#34;&gt;http://physicsworld.com/cws/article/news/48126&lt;/a&gt;. Como toda lista, es parcial y sesgada, pero de todos modos resulta interesante. Allí vamos.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Primer puesto: Medidas cuánticas “inmorales”&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;El primer lugar fue para Aephraim Steinberg y sus colegas de la Universidad de Toronto en Canadá, por su trabajo experimental en mecánica cuántica. Usando una nueva técnica llamada &amp;quot;weak measurement&amp;quot;, el equipo fue el primero en realizar un seguimiento de las trayectorias promedio de fotones individuales que pasan a través del experimento de Young de la doble rendija. Steinberg y su equipo han sido capaces de obtener información acerca de los caminos tomados por los fotones sin destruir el patrón de interferencia. En el experimento, la doble rendija se sustituye por un divisor de haz y un par de fibras ópticas. El fotón llega al divisor de haz y se desplaza por una de las fibras. Después de emerger de la fibra óptica, pasa a través de un trozo de calcita, que lo polariza levemente de acuerdo a su momento. Finalmente, crea un patrón de interferencia en una pantalla. Los fotones son entonces seleccionados según dónde impacten en la pantalla, lo cual permite a los investigadores determinar la dirección media de viaje de los mismos. El experimento revela, por ejemplo, que un fotón detectado en el lado derecho del diagrama de difracción es más probable que haya surgido de la fibra óptica que está a la derecha. Lo cual no es poco. En palabras de Steinberg, según el paradigma cuántico, &amp;quot;asking where a photon is before it is detected is somehow immoral&amp;quot;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Segundo puesto: Mediciones de la función de onda&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;El segundo lugar está ligado al anterior. Fue asignado a Jeff Lundeen del Consejo Nacional de Investigación de Canadá, en Ottawa. Este ex colega de Steinberg también utilizó la &amp;quot;weak measurement&amp;quot; para trazar la función de onda de un conjunto de fotones idénticos, sin perder información sobre su estado. Además de mejorar la comprensión de los fundamentos de la mecánica cuántica, estas mediciones podrían ser útiles en los casos en que la tomografía no se pueda utilizar**.**&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Tercer puesto: &amp;quot;Encubrimiento&amp;quot; de eventos&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Este lugar es para un aporte teórico y un dispositivo experimental relacionados. El puesto es compartido por dos equipos, uno en la Universidad de Cornell en los EE.UU. encabezado por Alexander Gaeta, y el otro en el Imperial College de Londres, dirigido por Martin McCall. El equipo de McCall publicó a principios de 2011 un análisis teórico sobre cómo puede “encubrirse” un evento en el espacio y tiempo para volverlo indetectable (descrito en &lt;a href=&#34;http://physicsworld.com/cws/article/print/46376)&#34;&gt;http://physicsworld.com/cws/article/print/46376)&lt;/a&gt;. Unos meses después, Gaeta y sus colaboradores construyeron un dispositivo que usa dos lentes temporales para lograr ese efecto al que llaman “cloaking”.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cuarto puesto: Mediciones del universo a partir de agujeros negros&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;El cuarto lugar en la lista fue otorgado a Darach Watson y sus colegas de la Universidad de Copenhague, Dinamarca, y la Universidad de Queensland, Australia. Los investigadores han elaborado una forma de usar los agujeros negros supermasivos para hacer mediciones precisas de distancias cósmicas. Estos agujeros negros supermasivos se pueden encontrar en casi todas partes en el universo, y a diferencia de las supernovas que se utilizan actualmente, son detectables por largos períodos de tiempo.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Quinto puesto: Efecto Casimir dinámico, luz a partir del vacío&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;En la mitad de la lista se encuentra otro experimento. Christopher Wilson y sus colegas de la Universidad Tecnológica de Chalmers en Suecia, junto con físicos de Japón, Australia y los EE.UU. comparten el mérito de ser los primeros en detectar el efecto Casimir dinámico en el laboratorio. Este efecto, predicho teóricamente en 1970, consiste en la aparición de pares de fotones a partir del vacío cuántico generada por un cuerpo acelerado. Además de arrojar nueva luz sobre el efecto Casimir, el uso de un dispositivo superconductor de interferencia cuántica (SQUID) que simula un espejo oscilando al 5% de la velocidad de la luz hacen, de este un experimento extremadamente inteligente. Este efecto puede ser vinculado con la posibilidad de que las fluctuaciones de energía en el vacío hayan sido las responsables de la expansión del Universo durante los primeros instantes de su creación.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Sexto puesto: Temperatura del Universo primitivo&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;El sexto puesto es para un cálculo. Fue otorgado a un equipo de físicos de EE.UU., India y China que ha hecho el mejor cálculo de la temperatura de condensación del Universo primitivo: dos billones de grados Kelvin. El trabajo es además un aporte en la comprensión de la cromodinámica cuántica (QCD), que describe las propiedades de los neutrones, protones y otros hadrones.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Séptimo puesto: Oscilaciones de neutrinos&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;El séptimo puesto es para un trabajo experimental. Corresponde al equipo internacional de físicos que trabajan en el experimento Tokai-to- Kamioka (T2K) en Japón. Los investigadores dispararon un haz de neutrinos de muón a un detector y encontraron con que había cambiado u &amp;quot;oscilado&amp;quot; a neutrinos de electrón. Aunque estos resultados no son suficientes para pretender el descubrimiento de la oscilación neutrino muón-electrón, resulta la mejor evidencia disponible en la actualidad de que un &amp;quot;sabor&amp;quot; de neutrinos pueda oscilar en otro.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Octavo puesto: Láser viviente&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Este puesto es para un experimento en Biofísica. Malte Gather and Seok Hyun Yun, de la Harvard Medical School en EE. UU. fueron los primeros en fabricar un láser a partir de una célula viva. Iluminando con una intensa luz azul una proteína fluorescente de una célula embrionaria de riñón se logró que las moléculas emitieran una luz intensa, monocromática y direccional. Este fenómeno no destruye las células y se especula que podría ser utilizado para distinguir las células cancerosas de las sanas.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Noveno puesto: Computadora cuántica de un solo chip&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;El noveno lugar fue asignado a Matteo Mariantoni y sus colegas en la Universidad de California en Santa Bárbara por implementar una versión cuántica de la arquitectura &amp;quot;Von Neumann&amp;quot; de computadoras personales. Basado en circuitos superconductores e integrados en un solo chip, el nuevo dispositivo ya ha sido utilizado para realizar dos importantes cálculos cuánticos. Su desarrollo da un paso más hacia la creación de ordenadores cuánticos prácticos capaces de resolver problemas reales.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Décimo puesto: Reliquias del Big Bang&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Este lugar fue adjudicado a Michele Fumagalli y Xavier Prochaska de la Universidad de California, Santa Cruz junto a John O&#39;Meara de Saint Michael&#39;s College en Vermont. Ellos fueron los primeros en avistar nubes de gas generadas durante el Big Bang. A diferencia de otras nubes en el Universo (que parecen estar formadas por elementos creados en las estrellas) las nubes en cuestión solo están formadas por hidrógeno, helio y litio, los elementos más livianos que fueron creados por el Big Bang. Además de confirmar predicciones de la teoría del Big Bang, estas nubes proveen información sobre los elementos que conformaron las primeras estrellas y galaxias.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Hielo solar. Una heladera que funciona con el calor del Sol. Segunda parte</title>
      <link>https://ciencianet.com.ar/post/hielo-solar-una-heladera-que-funciona-con-el-calor-del-sol-segunda-parte/</link>
      <pubDate>Wed, 09 Nov 2011 01:42:02 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/hielo-solar-una-heladera-que-funciona-con-el-calor-del-sol-segunda-parte/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero.&lt;/strong&gt; Facultad de Ciencias Exactas (UNLP).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2011/11/8df9796-300x225.jpg&#34; alt=&#34;El equipo 2011.:left&#34; title=&#34;El equipo 2011&#34;&gt; Aunque abundan los anuncios de avances científicos con posibles aplicaciones beneficiosas para la sociedad, no es tan frecuente que su desarrollo se sostenga a lo largo de los años y finalmente redunde en innovaciones concretas. Y mucho menos frecuente es que el trabajo incluya la adaptación de las tecnologías para que puedan ser apropiadas más fácilmente por la comunidad a quien fueran destinadas.&lt;/p&gt;
&lt;p&gt;Enmarcado en el perfil de la Universidad Nacional de General Sarmiento (UNGS) -institución fuertemente conectada con las necesidades sociales- el trabajo del equipo del Dr. Rodolfo Echarri (físico, investigador del CONICET, docente) es uno de estos casos. En Diciembre de 2007 en la UNGS se hacían las primeras pruebas de este desarrollo. Bajo la mirada de alumnos y docentes, el prototipo instalado en el Campus de Los Polvorines fabricó 300 gramos de hielo (ver la primera parte de esta nota &lt;a href=&#34;https://ciencianet.com.ar/post/hielo-solar-una-heladera-que-funciona-con-el-calor-del-sol/&#34;&gt;aquí&lt;/a&gt;). Para fines de este año, casi 4 después, una versión mejorada de la heladera estará instalada en el noroeste de Córdoba, y se espera que produzca 5 kilos de hielo por día.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2011/11/Archivo7-300x225.jpg&#34; alt=&#34;Montado del prototipo. Versión 2011.:left&#34; title=&#34;Prototipo 2011&#34;&gt;&lt;/p&gt;
&lt;p&gt;El proyecto contó en sus inicios con la colaboración del INTEC-Instituto Tecnológico de Santo Domingo, República Dominicana; los primeros resultados pueden encontrarse &lt;a href=&#34;http://redalyc.uaemex.mx/pdf/870/87011539007.pdf&#34;&gt;aquí&lt;/a&gt;. Actualmente Echarri está acompañado por los investigadores Andrés Sartarelli, Sergio Vera y Ernesto Cyrulies, también de UNGS.&lt;/p&gt;
&lt;h3 id=&#34;los-destinatarios&#34;&gt;Los destinatarios&lt;/h3&gt;
&lt;p&gt;La versión 2011 de la heladera solar será instalada en Los Talas, una pequeña población del noroeste de Córdoba, cuya principal actividad es la producción caprina. En esta región, sin tendido eléctrico y con un clima riguroso, la dificultad para mantener los productos lácteos refrigerados es un factor limitante para los pequeños productores, para quienes resulta inaccesible económicamente el uso de un equipo tradicional de enfriamiento por gas envasado.&lt;/p&gt;
&lt;p&gt;En busca de una solución, el Instituto de investigación y desarrollo tecnológico para la Pequeña Agricultura Familiar (IPAF) de la Región Pampeana del Instituto Nacional de Tecnología Agropecuaria (INTA) y la Asociación de Productores del Noroeste de Córdoba (APENOC) convocaron en 2010 al equipo de Echarri.
Como consecuencia del trabajo conjunto entre los diversos actores, se prevé la instalación de dos equipos para fines de este año.&lt;/p&gt;
&lt;p&gt;Uno de los logros a destacar es que la instalación de las heladeras correrá por cuenta de los mismos productores, quienes serán asesorados por los investigadores para el armado. Un análisis detallado de la situación de los productores cordobeses y del impacto de la propuesta a nivel local pueden encontrarse &lt;a href=&#34;http://adiungs.com.ar/wp-content/uploads/2011/07/ADIUNGS-JULIO-2011-web.pdf&#34;&gt;aquí&lt;/a&gt; en palabras del investigador Sergio Vera.&lt;/p&gt;
&lt;h3 id=&#34;sin-enchufe-ni-motor&#34;&gt;Sin enchufe ni motor&lt;/h3&gt;
&lt;p&gt;La heladera funciona mediante la adsorción y desorción de metanol (alcohol metílico) por carbón activado, que es un material muy poroso y adsorbente. El prototipo básico está formado por un &lt;strong&gt;recipiente colector&lt;/strong&gt; donde está el carbón activado empapado en alcohol metílico, un &lt;strong&gt;condensador&lt;/strong&gt;, que convierte en líquido los vapores del alcohol, un &lt;strong&gt;evaporador&lt;/strong&gt; que recoge el alcohol líquido para que puedan volver a evaporarse y una &lt;strong&gt;cámara fría&lt;/strong&gt;, donde el agua se enfría convirtiéndose en hielo. En la primera parte de la nota se encuentra una descripción más detallada del mecanismo de enfriamiento.&lt;/p&gt;
&lt;h3 id=&#34;los-ingredientes-necesarios&#34;&gt;Los ingredientes necesarios&lt;/h3&gt;
&lt;p&gt;Una Universidad con la mirada puesta en las necesidades de los ciudadanos, un grupo de científicos comprometidos con la investigación y abiertos al diálogo, instituciones que releven las necesidades de la sociedad en que están inmersas y puedan hacer de vínculo y finalmente ciudadanos dispuestos a sumarse a propuestas en principio tan delirantes como conseguir frío a partir del Sol.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>La supuesta detección de velocidades superlumínicas de los neutrinos</title>
      <link>https://ciencianet.com.ar/post/la-supuesta-deteccion-de-velocidades-superluminicas-de-los-neutrinos/</link>
      <pubDate>Tue, 08 Nov 2011 01:44:34 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/la-supuesta-deteccion-de-velocidades-superluminicas-de-los-neutrinos/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Gaston E. Giribet.&lt;/strong&gt; Departamento de Física, Facultad de Ciencias Exactas y Naturales, Universidad de Buenos Aires y CONICET.&lt;/p&gt;
&lt;p&gt;El Laboratorio Nacional Gran Sasso de Italia anunció el pasado 23 de septiembre la detección de partículas que, aparentemente, tendrían una velocidad mayor a la de la luz. Se trata de las partículas llamadas neutrinos, las cuales se conocen desde hace ya varias décadas, pero de las cuales hemos aprendido mucho recién en los últimos años.&lt;/p&gt;
&lt;p&gt;La existencia de los neutrinos fue predicha en la década de 1930 por W. Pauli, y éstos fueron observados recién a mediados de la década de 1950. Hace tan solo una década que se consiguió evidencia significativa de que los neutrinos tienen masa (durante mucho tiempo se creyó que no era así) y que debido a la pequeñísima masa que tienen (a lo sumo unos pocos electronvoltios, i.e. cerca de una milmillonésima parte de la masa de un átomo de hidrógeno) se comportan de manera muy curiosa, cambiando sus propiedades con alternancia, oscilando su identidad a medida que viajan.&lt;/p&gt;
&lt;p&gt;Y como si este deambular esquizofrénico no fuera ya suficiente para merecer nuestra sorpresa, se suma hoy el notable anuncio de los científicos en el Gran Sasso. Si sus mediciones resultaran ciertas, algunos neutrinos viajarían más rápidamente que la luz, excediendo la velocidad de ésta en una parte en cienmil. La forma en la que los físicos dicen haber medido este efecto es sencillo de explicar, aunque muy difícil de implementar experimentalmente dada la gran cantidad de detalles a los que es menester atender: los físicos del laboratorio italiano se disponen a medir con gran exactitud la distancia recorrida por un rayo de neutrinos y medir también el tiempo empleado en recorrer dicha distancia. Luego, la velocidad a la que los neutrinos viajan se obtiene, como sabemos desde el preescolar, al dividir la distancia por el tiempo empleado en recorrerla.&lt;/p&gt;
&lt;p&gt;Los neutrinos son partículas que interaccionan muy débilmente con la materia, razón por la cual es imposible evitar que rayos de estas partículas escapen tras las paredes de los aceleradores de partículas como producto residual de tales experimentos. Los físicos, lejos de desaprovechar esos neutrinos escapistas, aprovechan los rayos para realizar otros experimentos y entender así sus propiedades. De hecho, fue este el truco empleado para detectarlos en los 50s. Y es esto lo que ocurre también con los neutrinos producidos en el acelerador del CERN, en la frontera franco-suiza.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2011/11/OPERAdetector-300x201.jpg&#34; alt=&#34;Detector del experimento OPERA.:left&#34; title=&#34;Detector del experimento OPERA&#34;&gt;&lt;/p&gt;
&lt;p&gt;Luego de abandonar los detectores de muones del CERN, los neutrinos producidos allí viajan cerca de 732 kilómetros hasta la cordillera de los Apeninos, donde son detectados por el experimento llamado OPERA, que está ubicado bajo 1400 metros de piedra, en el Laboratorio Nacional Gran Sasso de Italia.&lt;/p&gt;
&lt;p&gt;Debido a la mencionada propiedad de interaccionar débilmente con la materia, los neutrinos ven al macizo Gran Sasso casi transparente, y es esa la razón por la que el experimento se encuentra en las profundidades, cobijado por la piedra, donde prácticamente sólo los neutrinos pueden llegar. Cabe mencionar que no es esta la primera vez que se observa este tipo de comportamiento super-lumínico de los neutrinos. Resultados similares habían ya sido obtenidos por el experimento denominado MINOS, ubicado en Medio Oeste de los Estados Unidos. No obstante, el grado de exactitud de OPERA es mucho mayor y es esto lo que hace tan importante el anuncio.&lt;/p&gt;
&lt;p&gt;También sería necesario explicar cómo el hecho de que estos neutrinos producidos en el CERN y detectados en el Gran Sasso superen la velocidad de la luz no entra en tajante contradicción con las observaciones de neutrinos provenientes de supernovas lejanas, como es el caso de la supernova SN1987a cuya emisión de anti-neutrinos fue detectada en el momento esperado, en concordancia con los modelos de evolución estelar. Los neutrinos son emitidos pocas horas antes del estallido de luz en una supernova de esas características, y los neutrinos de SN1987a fueron observados precisamente con esa antelación, y no una mayor, a la emisión de luz asociada. Si aquellos neutrinos producidos en la explosión SN1987, acaecida a una distancia de 168.000 años luz de la tierra, hubieran tenido la velocidad que se les adjudica a los neutrinos italianos, entonces habrían aquéllos arribado a la tierra varios años antes.&lt;/p&gt;
&lt;p&gt;Ahora bien, ¿por qué sería tan importante si se descubriera que una partícula diminuta y reticente a interaccionar con la materia viaja más rápido que la luz? La respuesta es que un descubrimiento de esta índole pondría en tela de juicio uno de los pilares fundamentales de la física: la existencia de partículas que viajen más rápidamente que la luz está en tajante contradicción con la teoría de la relatividad especial de Einstein, la cual establece la velocidad de la luz como un límite infranqueable. Violar dicho límite representaría una crisis fundamental en la física teórica ya que llevaría en germen el problema de poner en riesgo la noción de causalidad.&lt;/p&gt;
&lt;p&gt;Es así como si los resultados del experimento italiano llegaran a ser confirmados, sería necesario rever de cuajo varios aspectos fundamentales de la física que conocemos. No obstante, en estos casos la actitud que prevalece en la comunidad científica es siempre la cautela, y aún más en este caso. Lo más probable es que la supuesta observación de velocidades super-lumínicas de los neutrinos en el Gran Sasso se deba, en realidad, a algún error sistemático del experimento. Es por esto necesario que otros laboratorios confirmen las mediciones de manera independiente, y que se evalúe si no se ha cometido ningún error en el experimento efectuado ni en la interpretación del mismo.&lt;/p&gt;
&lt;p&gt;No sería, pues, la primera vez que la teoría de la relatividad saliera triunfante luego de haber sido desafiada.&lt;/p&gt;
&lt;p&gt;Versión ampliada de *¿Einstein se equivocó?*, por Gaston Giribet, Revista Noticias Nº 1815.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Actualización - 19 de Marzo de 2012 -&lt;/strong&gt; Después de que OPERA reconociera la existencia de errores, el experimento ICARUS informa que los neutrinos viajan a la velocidad de la luz. Los resultados están en: &lt;a href=&#34;http://arxiv.org/pdf/1203.3433v1.pdf&#34;&gt;http://arxiv.org/pdf/1203.3433v1.pdf&lt;/a&gt; Measurement of the neutrino velocity with the ICARUS detector at the CNGS beam The CERN-SPS accelerator has been briefly operated in a new, lower intensity neutrino mode with ~10^12 p.o.t. /pulse and with a beam structure made of four LHC-like extractions, each with a narrow width of ~3 ns, separated by 524 ns. This very tightly bunched beam structure represents a substantial progress with respect to the ordinary operation of the CNGS beam, since it allows a very accurate time-of-flight measurement of neutrinos from CERN to LNGS on an event-to-event basis. The ICARUS T600 detector has collected 7 beam-associated events, consistent with the CNGS delivered neutrino flux of 2.2 10^16 p.o.t. and in agreement with the well known characteristics of neutrino events in the LAr-TPC. The time of flight difference between the speed of light and the arriving neutrino LAr-TPC events has been analysed. The result is compatible with the simultaneous arrival of all events with equal speed, the one of light. This is in a striking difference with the reported result of OPERA that claimed that high energy neutrinos from CERN should arrive at LNGS about 60 ns earlier than expected from luminal speed.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>La expansión acelerada del Universo y el Premio Nobel de Física de 2011</title>
      <link>https://ciencianet.com.ar/post/la-expansion-acelerada-del-universo-y-el-premio-nobel-de-fisica-de-2011/</link>
      <pubDate>Fri, 04 Nov 2011 01:47:01 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/la-expansion-acelerada-del-universo-y-el-premio-nobel-de-fisica-de-2011/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Gastón E. Giribet&lt;/strong&gt;. Departamento de Física, Facultad de Ciencias Exactas y Naturales, Universidad de Buenos Aires y CONICET.&lt;/p&gt;
&lt;h3 id=&#34;la-expansión-del-universo-y-la-relatividad&#34;&gt;La expansión del Universo y la Relatividad&lt;/h3&gt;
&lt;p&gt;Una de las predicciones más sorprendentes de la Teoría de la Relatividad, y que al comienzo fue recibida con sorpresa por el mismo Einstein, es que si uno asume que el Universo es homogéneo (que en todas partes es similar) e isótropo (que en todas direcciones es similar) entonces indefectiblemente éste puede expandirse o contraerse con el tiempo, pero nunca permanecer estático.&lt;/p&gt;
&lt;p&gt;Este aspecto dinámico del Universo es inexorable, una conclusión ineluctable de las ecuaciones de Einstein. Muchos físicos advirtieron tempranamente sobre esta implicancia de la Teoría de la Relatividad General; el mismo Albert Einstein recibió el resultado con reluctancia al comienzo y muchos otros como Erwin Schrödinger intentaron conciliar la idea de un universo estático con la teoría. Incluso, Einstein creyó por un momento haber logrado detener el Universo en sus cuadernos de notas. Para lograrlo había el creador de la Relatividad deformado su propia teoría introduciendo un nuevo término en sus célebres ecuaciones, el cual parecía hacer el trabajo.&lt;/p&gt;
&lt;p&gt;No obstante, rápidamente se advirtió que ese nuevo y extraño término, llamado &amp;quot;el término cosmológico&amp;quot;, no era de suficiente provecho y que, aunque lograba detener el Universo ejerciendo una presión negativa para lograrlo, éste terminaba desestabilizándose ante el menor aleteo de un mosquito y continuaba así su expansión (o su contracción.) ¿Debían entonces Einstein y sus contemporáneos aceptar la idea de un universo en movimiento?, ¿o acaso el Universo se hallaba entonces en su inestable equilibrio gracias al término cosmológico que venía a corregir sus ecuaciones?&lt;/p&gt;
&lt;p&gt;Otra posibilidad, a la que no muchos parecían subscribir, era abandonar la Relatividad General como el marco teórico para describir el Universo a gran escala. Afortunadamente no fue este último el camino que tomaron los físicos sino uno que, como sabemos hoy, resultaría más provechoso. Uno se ve tentado entonces a concluir que la creencia en la Relatividad prevaleció sobre la concepción cosmogónica estática, pero esta visión de los sucesos sería incompleta si no reconociéramos los méritos que las observaciones astronómicas tuvieron a la hora de confirmar el movimiento del cosmos.&lt;/p&gt;
&lt;h3 id=&#34;la-observación-de-la-expansión-cósmica&#34;&gt;La observación de la expansión cósmica.&lt;/h3&gt;
&lt;p&gt;Muchas preguntas permanecían abiertas en esos tiempos tempranos de la cosmología. Aunque la idea de nuestro Universo como un escenario dinámico se encontraba en consonancia con la revisión de la naturaleza que la revolución darwiniana ya había iniciado como línea de pensamiento -aunque en un contexto muy distinto-, seguía siendo una concepción difícil de digerir. Y aún aceptando el movimiento del cosmos quedaban preguntas básicas por responder tales como si el Universo se expandía o se contraía, o si en caso de que ocurriere lo primero se expandiría por siempre o sólo por un tiempo.&lt;/p&gt;
&lt;p&gt;Hoy, a casi cien años de la formulación de la Relatividad General, nos es dado saber la respuesta a muchas de estas preguntas. Pero, antes de adentrarnos en lo que hoy sabemos del cosmos, volvamos a los años 10s y 20s del pasado: Einstein insistió durante un tiempo considerable en su idea de conciliar la Relatividad con una imagen estática del cosmos, y para eso había agregado aquel nuevo &amp;quot;término cosmológico&amp;quot; en sus ecuaciones deformando ligeramente la versión original de su propia teoría –en parte esto se debió a que el término cosmológico satisfacía otras propiedades matemáticas que él buscaba en su teoría-.&lt;/p&gt;
&lt;p&gt;No obstante, la historia cambiaría radicalmente poco tiempo después. Tan sólo trece años después de la formulación de la Relatividad General, el astrónomo Edwin Hubble obtendría en 1929 la primera evidencia observacional de que, en efecto, el universo se movía y que éste lo hacía expandiéndose. Vale aclarar aquí que la Relatividad General predecía el movimiento del Universo, pero no decidía si ese movimiento era en expansión o en contracción ya que ello dependía, además, de conocer de la cantidad de materia total que hay en el cosmos.&lt;/p&gt;
&lt;p&gt;Hubble observó que las estrellas distantes presentaban un tono más rojizo que el que se esperaba de ellas, y que ese tinte inesperado de la luz que nos enviaban podía entenderse como signo de que dichas estrellas estaban alejándose de la Tierra, alejándose más rápidamente aquellas estrellas que más lejos se encontraban de nosotros. Era ésa evidencia substancial de la expansión del Universo.&lt;/p&gt;
&lt;p&gt;Además, las observaciones efectuadas por Hubble indicaban que la expansión cósmica se daba en todas direcciones de la misma manera y a la misma velocidad, una observación que inmediatamente invita a aclarar que esa isotropía no se debe a que seamos nosotros el centro del universo, como algún rezagado partícipe de la visión eclesiástica podría aventurar, sino a que el Universo parece satisfacer aquellas hipótesis que Einstein había sugerido desde el comienzo: El universo es en todas partes y en todas direcciones similar, homogéneo e isótropo. Así, todo punto del cosmos es su centro y todo punto se separa de los otros de igual manera.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2011/11/TypeIaSupernova.jpg&#34; alt=&#34;Supernova tipo Ia 1994D (el punto brillante abajo a la izquierda) en la región externa de la galaxia NGC4526. Crédito: High-Z Supernova Search Team, HST, NASA and V. Rubin/CIW.&#34; title=&#34;TypeIaSupernova&#34;&gt;&lt;/p&gt;
&lt;p&gt;El mismo Einstein tuvo acceso a las observaciones de Hubble y encontró la evidencia contundente. Las estrellas se alejan de nosotros y lo hacen con más velocidad conforme más lejos de nosotros se hallan. Fue entonces cuando advirtió que su teoría de la Relatividad General iba más lejos que sus propias convicciones y preconceptos: La Relatividad había predicho la mismísima expansión del Universo antes de que ésta hubiera sido observada en el telescopio.&lt;/p&gt;
&lt;p&gt;Einstein se sintió compelido a reconocer que la inclusión de su famoso &amp;quot;término cosmológico&amp;quot; en sus ecuaciones, el que entonces parecía ser ya innecesario, había sido &amp;quot;el desatino más grande de su vida&amp;quot;. Sin ése la teoría explicaba la expansión cósmica, una predicción que habría merecido otro premio Nobel.&lt;/p&gt;
&lt;h3 id=&#34;la-aceleración-y-el-premio-nobel&#34;&gt;La aceleración y el premio Nobel&lt;/h3&gt;
&lt;p&gt;Ahora bien, aún asumiendo que el Universo se expandía, y aún teniendo evidencia observacional de ello, quedaba la pregunta abierta de si sería esa expansión eterna, o si, por el contrario, luego de una larga excursión cósmica, todas las estrellas revertirían sus carreras y recolapsarían.&lt;/p&gt;
&lt;p&gt;La respuesta a esta pregunta tardó en llegar ya que, como mencionamos antes, saber el destino del Universo depende de conocer la cantidad de materia y energía que haya en él, un dato del que sólo en las dos últimas décadas hemos aprendido lo suficiente: Desde comienzos de la década de los 90s contamos con evidencia de que no parece haber suficiente materia en el Universo como para que éste recolapse. Mucho más sorprendente fue el descubrimiento, a fines de esa misma década, de que el Universo no sólo se expande sino que, además, lo hace cada vez más rápido, acelerándose.&lt;/p&gt;
&lt;p&gt;Fue la observación de esto último lo que le valió a Saul Perlmutter, Brian Schmidt y Adam Riess el premio Nobel de física de este año. En 1997 y 1998 ellos, junto a otros coautores, publicaron las observaciones de supernovas tipo Ia lejanas (explosiones de estrellas muy distantes debidas típicamente a enanas blancas que ganan mucha masa y superan el llamado límite de Chandrasekar) que presentaban un alto vicio hacia el color rojo cuando se las observaba. Esto, combinado con su propiedad de emitir luz con una intensidad que permite ser calculada con precisión, permitió a Perlmutter et al. emplear ese tipo de supernovas como &amp;quot;candelas patrón&amp;quot; para establecer a qué velocidad se expande el Universo en un dado momento de la historia del mismo.&lt;/p&gt;
&lt;p&gt;Fue así como se logró determinar que el Universo, hoy, se expande aceleradamente. Esta expansión acelerada del Universo es desconcertante en varios aspectos teóricos, y hubo quien señaló que entender dicha aceleración es &amp;quot;el problema más desconcertante de la física teórica actual&amp;quot;. En particular, es difícil entender de una manera satisfactoria cómo es que el valor del término cosmológico es tan pequeño.&lt;/p&gt;
&lt;p&gt;Los modelos más naturales desde el punto de vista estético-matemático, si es que tal criterio existe en verdad, sugieren que el término cosmológico debería no estar presente o, si lo estudviera, su valor debería ser muchísimos órdenes de magnitud superior al observado. No obstante, sin ánimo de entrar en los detalles técnicos del caso, alcanza con decir que la mejor descripción teórica con la que hoy contamos para explicar la expansión acelerada del Universo es, paradójicamente, aquella que Einstein había ensayado en 1917; es decir, deformar la Teoría de la Relatividad General como él mismo lo había sugerido, introduciendo su &amp;quot;término cosmológico&amp;quot; -aunque en una medida mucho menor a la originalmente propuesta para detener el cosmos-.&lt;/p&gt;
&lt;p&gt;Es así como hoy asistimos al momento en el que la cosmología le da la derecha al genio alemán mostrando que incluso &amp;quot;el más grande desatino de su vida&amp;quot; fue en realidad un gracioso acierto. El término cosmológico de Einstein parece estar allí, cumpliendo el papel de acelerar la expansión.&lt;/p&gt;
&lt;h3 id=&#34;el-futuro-del-universo&#34;&gt;El futuro del Universo&lt;/h3&gt;
&lt;p&gt;Surge entonces la pregunta sobre el futuro del universo: Si no recolapsará el cosmos sobre sí mismo, si éste seguirá expandiéndose por siempre, enfriándose, ¿qué será del futuro de éste?, ¿qué ocurrirá cuando las estrellas agoten el combustible nuclear que las hace brillar?, ¿qué ocurrirá cuando la coalescencia gravitatoria de las diferentes galaxias lleve a la materia estelar a apelmazarse?&lt;/p&gt;
&lt;p&gt;Estas preguntas sobre el futuro del Universo ocupan también la mente de los cosmólogos, quienes ensayan respuestas que, aunque diversas, siempre contrastan con el pasado tumultuoso del Universo temprano y caliente. Algunas especulaciones sugieren que, en el futuro remoto, grandes agujeros negros serán creados tras la coalescencia de astros y galaxias, y que esos gigantes, oscuros y fríos terminarán radiando su masa al ritmo lento que Stephen Hawking predijo para tal fenómeno en la década de los 70s.&lt;/p&gt;
&lt;p&gt;Otras especulaciones sobre el futuro del universo tienen en cuenta la expansión acelerada del mismo, y predicen que esta expansión terminaría por alejar unas regiones del universo de otras hasta que ya no sea posible enviar luz de un lado al otro, y que el universo iría así desconectándose causalmente, desapareciendo tras el escurridizo concepto de “horizonte cosmológico”.&lt;/p&gt;
&lt;p&gt;Sea cual fuere el destino de nuestro Universo en el futuro remoto, hay algo en lo que los cosmólogos parecen estar de acuerdo: Muy probablemente será una muerte fría y solitaria.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>GRB110328A no sería un GRB - Curiosa emisión de rayos gama proveniente de la Constelación del Dragón</title>
      <link>https://ciencianet.com.ar/post/grb110328a-no-seria-un-grb-curiosa-emision-de-rayos-gama-proveniente-de-la-constelacion-del-dragon/</link>
      <pubDate>Sat, 14 May 2011 01:49:28 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/grb110328a-no-seria-un-grb-curiosa-emision-de-rayos-gama-proveniente-de-la-constelacion-del-dragon/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Gastón Giribet.&lt;/strong&gt; Departamento de Física, Instituto de Física Buenos Aires, UBA &amp;amp; CONICET.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;En este artículo el autor comenta y discute una reciente observación, relevante en el campo de la astrofísica, sobre una curiosa emisión de rayos gama proveniente de la Constelación del Dragón. Esta emisión se debería a la destrucción parcial de una estrella debido a la cercanía de la misma a un agujero negro supermasivo. Además de la información, interesante por sí misma, nos ofrece un ejemplo actual de la construcción de conocimiento en ciencia.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Hace pocas semanas, el 28 de marzo de 2011, el satélite Swift de NASA observó en la Constelación del Dragón un evento de emisión en rayos gama muy curioso y cuya interpretación ulterior es muy interesante. El evento en rayos gama, bautizado como Swift J164449.3+573451, fue originalmente interpretado como un &amp;quot;gamma ray burst&amp;quot; (GRB) &lt;a href=&#34;#1&#34;&gt;[1]&lt;/a&gt;, a lo que se debe su nombre alternativo GRB110328A.&lt;/p&gt;
&lt;p&gt;No obstante, rápidamente se advirtió que el evento no presentaba las características usualmente observadas para un GRB, por lo que la interpretación usual en términos del colapso de una estrella como mecanismo generador de tal emisión gama no parecía funcionar. En particular, se observó que la fuente de rayos gama, fuera cual fuere su naturaleza, siguió emitiendo esporádicamente destellos los días siguientes a la observación del fenómeno inicial.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2011/05/GRB110328A-1-300x300.jpg&#34; alt=&#34;Imagen del evento GRB110328A obtenida por el satélite Swift de la NASA.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Entonces, inmediatamente surge la pregunta acerca de qué tipo de fenómeno astrofísico daría lugar a un evento tal. Una respuesta a esta pregunta llegó rápidamente, y parece haber consenso al respecto: La inusual radiación gama no se debería a un GRB, sino a la &amp;quot;disrupción&amp;quot; de una estrella debida a la cercanía de la misma a un agujero negro supermasivo.&lt;/p&gt;
&lt;p&gt;La masa de este agujero negro se calcula entre uno y diez millones de masas solares. Este tipo de agujeros negros existe en los centros de las galaxias y, aunque algunas veces están cubiertos por materia circundante congregada en torno a ellos, otras veces (como en este caso, o como probablemente ocurre en el caso de Sagitario A*, la fuente asociada al agujero negro que estaría en el centro de nuestra Via Láctea).&lt;/p&gt;
&lt;p&gt;Estos gigantescos monstruos están desnudos, invisibles, expectantes, acechando a alguna estrella que pase cerca de ellos para, como en el caso del evento GRB110328A del que hablamos aquí, eventualmente destruirla con su fuerza gravitatoria (más precisamente, por el efecto centrífugo que genera la misma dinámica) y comer parte de ella. Esto parece ser lo que ocurrió en la galaxia lejana de la que GRB110328A nos llega.&lt;/p&gt;
&lt;p&gt;La razón por la que llamo la atención sobre este hallazgo es que se trata de un evento curioso, difícilmente observado, y que permite tener más información para estudiar la física de los agujeros negros supermasivos, objetos sobre los que aún hay varias preguntas abiertas.&lt;/p&gt;
&lt;p&gt;Para aquellos que quisieren saber más detalles acerca del evento GRB110328A, pueden recurrir a dos artículos muy interesantes que aparecieron en estas semanas, en los cuales se discute el evento y se adhiere a la interpretación del mismo como una &amp;quot;&lt;em&gt;tidal force disruption&lt;/em&gt;&amp;quot;; a saber: &lt;a href=&#34;https://arxiv.org/abs/1104.32577&#34;&gt;Bloom et al.&lt;/a&gt; y &lt;a href=&#34;https://arxiv.org/abs/1104.2528&#34;&gt;Barres et al.&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referencias.&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;J.S. Bloom, at al., &amp;quot;A relativistic jetted outburst from a massive black hole fed by a tidally disrupted star&amp;quot;, arXiv:1104.3257.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;U. Barres de Almeida and A. De Angelis, &amp;quot;Enhanced emission from GRB 110328A could be evidence for tidal disruption of a star&amp;quot;, arXiv:1104.2528.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a id=&#34;1&#34;&gt;1.&lt;/a&gt; Los “gamma ray burst” o GRB son emisiones puntuales de radiación electromagnética en el rango de los rayos gama, que han sido detectados –aunque poco frecuentemente- en explosiones asociadas a supernovas y colapsos estelares.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Sobre la pista del grafeno en Argentina: efectos electromecánicos en la nanoescala</title>
      <link>https://ciencianet.com.ar/post/sobre-la-pista-del-grafeno-en-argentina-efectos-electromecanicos-en-la-nanoescala/</link>
      <pubDate>Tue, 14 Dec 2010 02:05:01 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/sobre-la-pista-del-grafeno-en-argentina-efectos-electromecanicos-en-la-nanoescala/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Héctor Riojas Roldán&lt;/strong&gt;. Universidad Nacional de La Plata.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Este artículo ha sido financiado por el proyecto INVOFI de la Asociación Física Argentina.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Este año han sido galardonados con el premio Nobel de Física 2010 los rusos Andre Geim (51) y Konstantin Novoselov (36) por sus estudios sobre el grafeno que posibilitaría nuevos avances en la física cuántica como así también numerosas aplicaciones tecnológicas. En este artículo reseñamos algunos trabajos realizados por investigadores argentinos.&lt;/p&gt;
&lt;h3 id=&#34;grafeno-en-argentina&#34;&gt;Grafeno en Argentina&lt;/h3&gt;
&lt;p&gt;En nuestro país el investigador Luis Foa Torres (32) Dr. en Física, investigador adjunto del Instituto de Física Enrique Gaviola (CONICET) y profesor adjunto de la FaMAF, en la Universidad Nacional de Córdoba, nos dice “Nuestro trabajo se centra principalmente en el estudio de las propiedades eléctricas de materiales nanoestructurados, notablemente materiales basados en carbono como los nanotubos de carbono y el grafeno. Mis primeras experiencias con estos materiales se remontan al 2005 cuando trabajaba para la Comisión de Energía Atómica de Grenoble (Francia), una línea que continué luego en Dresden (Alemania) y posteriormente aquí en Córdoba, donde desde el 2009 trabajo junto a un pequeño equipo de investigadores”.&lt;/p&gt;
&lt;h3 id=&#34;pero-primero-qué-es-el-grafeno&#34;&gt;Pero primero, ¿qué es el grafeno?&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/12/Fig1.jpg&#34; alt=&#34;Figura 1:left&#34; title=&#34;Figura 1&#34;&gt; El grafeno es una estructura de átomos de carbono formando una red bidimensional de tan sólo un átomo de espesor. La red cristalina que los contiene le da la forma de un panal de abejas. Cuando esta plancha de átomos tiene una de sus dimensiones en el orden de las decenas de nanometros se dice que es una “cinta” (&lt;em&gt;ribbon&lt;/em&gt;) de grafeno y se suele usar las siglas en inglés GNRs por &lt;em&gt;graphene nanoribbons&lt;/em&gt;. Dependiendo de la forma de los bordes de la cinta, las mismas se denominan tipo zig-zag o tipo sillón (&lt;em&gt;armchair&lt;/em&gt;), como se puede ver en la figura 1.&lt;/p&gt;
&lt;h3 id=&#34;veamos-algunas-de-sus-características&#34;&gt;Veamos algunas de sus características&lt;/h3&gt;
&lt;p&gt;Tienen alta conductividad térmica y eléctrica debido a que los electrones tienen una alta movilidad y baja dispersión. Combina alta elasticidad y ligereza, lo que lo convierte en unos de los materiales más resistentes ya que su dureza es extrema (varias veces más fuerte que el acero). Puede reaccionar con otros elementos y compuestos químicos y además es transparente. Doblando estas láminas se tienen los nanotubos (buckytubos o &lt;em&gt;buckytubes&lt;/em&gt;) y nanoesferas (buckyesferas o &lt;em&gt;buckyballs&lt;/em&gt; ) como se ilustra en la figura 2, aunque como nos aclara nuestro investigador “los nanotubos se obtuvieron mucho antes que el grafeno. Sólo recientemente lograron &#39;cortar&#39; tubos para fabricar cintas de grafeno, en un proceso similar a la apertura de un cierre, logrando así producir una estructura a partir de la otra.[&lt;a href=&#34;#ref1&#34;&gt;1&lt;/a&gt;]&lt;/p&gt;
&lt;h3 id=&#34;posibilidades-tecnólogicas-y-aplicaciones&#34;&gt;Posibilidades tecnólogicas y aplicaciones&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/12/Fig21.jpg&#34; alt=&#34;Figura 2.:left&#34; title=&#34;Figura 2&#34;&gt; En teoría el grafeno es más estable a nanoescalas que el silicio y dado que que la tendencia en la eléctrónica es cada vez hacer cosas más pequeñas, todo indica que es el mejor candidato a revolucionar el campo tecnológico electrónico. Pantallas táctiles flexibles, sensores de gases y otras sustancias y nueva generación de baterías ultracompactas. La dificultad fundamental para algunas aplicaciones es que el grafeno no tiene una brecha en la banda de energías permitidas (&lt;em&gt;energy band-gap&lt;/em&gt;). Solamente cuando el sistema se confina en una de las direcciones, formando cintas de grafeno se encuentra que algunas de ellas exhiben una brecha energética que depende de la topología. Aun así, el desafío es crear una brecha energética mediante un estímulo externo. Esto permitiría controlar las propiedades eléctricas y generar aplicaciones útiles.&lt;/p&gt;
&lt;h3 id=&#34;aportes-teóricos-nacionales&#34;&gt;Aportes teóricos nacionales&lt;/h3&gt;
&lt;p&gt;En una publicación científica reciente el Dr Luis Foa Torres [&lt;a href=&#34;#ref2&#34;&gt;2&lt;/a&gt;] , conjuntamente con otros científicos, estudiaron las propiedades de transporte de cargas cuando las láminas de grafeno se someten a esfuerzos mecánicos como se indica en la figura 3 (en este caso la fuerza es uniaxial). Se encuentra que la conductancia eléctrica tiene altas influencias debido a la simetría de los bordes de las láminas de grafeno. Se comparan los bordes brazos de silla y zigzag.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/12/Fig3-112x300.jpeg&#34; alt=&#34;Figura 3.:left&#34;&gt;] Mientras que los bordes zigzag resultan altamente resistentes a los esfuerzos de corte por fuerzas mecánicas, en la configuración tipo brazo de silla se induce una brecha energética generando así una transición metal-semiconductor. Estos novedosos resultados ubican a las tipo brazos de silla en una opción mucho mejor para aplicaciones electromécanicas. En palabras del autor “En algunas condiciones, nuestras simulaciones computacionales predicen que la tensión puede hacer que el sistema, originalmente conductor, se vuelva un semiconductor o viceversa. Esto podría ser de gran utilidad para la generación de nuevos dispositivos nanoelectromecánicos”.&lt;/p&gt;
&lt;p&gt;El Dr Luis Foa Torres dice además “su obtención en el laboratorio no requiere de elementos costosos ni exóticos: el grafeno puede aislarse a partir del grafito, el material de la mina de lápiz, el mismo que todos usamos en la escuela. Reexaminando un material conocido, con tenacidad y voluntad para ir más allá del camino marcado, Geim y Novoselov lograron un descubrimiento revolucionario que podría marcar el inicio de una nueva era.” También podemos encontrar en la reunión de la AFA de este año en Malargüe, Mendoza, dos resúmenes de sus últimos trabajos en progreso bajo los títulos “Bombeando electrones en materiales basados en carbono: la influencia de los defectos” (P584) y “Nanodispositivos en el límite cuántico: transporte alterno en un mundo de carbono” (P210).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referencias.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;ref1&#34;&gt; 1. &lt;/a&gt; Ver &lt;a href=&#34;https://doi.org/10.1038/nature07872&#34;&gt;Nature Vol 458, 16 April 2009.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;ref2&#34;&gt; 2. &lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1103/PhysRevB.81.193404&#34;&gt;Physical Review B 81, 193404 (2010)&lt;/a&gt;.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Avances en la lucha contra la placa dental</title>
      <link>https://ciencianet.com.ar/post/avances-en-la-lucha-contra-la-placa-dental/</link>
      <pubDate>Mon, 06 Dec 2010 20:28:15 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/avances-en-la-lucha-contra-la-placa-dental/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro&lt;/strong&gt;. Instituto de Física de Líquidos y Sistemas Biológicos.&lt;/p&gt;
&lt;p&gt;Los profesores Bauke Dijkstra y Lubbert Dijkhuizen de la Universidad de Groningen (Holanda) analizaron la enzima glucansucarasa obtenida de la bacteria &lt;em&gt;Lactobacillus reuteri&lt;/em&gt;, que se encuentra en la boca y en el tracto digestivo. Las bacterias usan la enzima glucansucarasa para convertir el azúcar de los alimentos en largas cadenas de azúcar &amp;quot;pegajosa&amp;quot;. Las bacterias usan este &amp;quot;pegamento&amp;quot; para adherirse al &lt;a href=&#34;http://es.wikipedia.org/wiki/Esmalte_dental&#34;&gt;esmalte dental&lt;/a&gt;. La principal causa del decaimiento dental, la bacteria &lt;em&gt;Streptococcus mutans&lt;/em&gt;, también utiliza esta enzima. Una vez adheridas al esmalte dental, estas bacterias fermentan &lt;a href=&#34;http://es.wikipedia.org/wiki/Az%C3%BAcar&#34;&gt;azúcares&lt;/a&gt; liberando ácidos que disuelven el calcio de los &lt;a href=&#34;http://es.wikipedia.org/wiki/Diente&#34;&gt;dientes&lt;/a&gt;. Así es como se desarrolla la &lt;a href=&#34;http://es.wikipedia.org/wiki/Caries&#34;&gt;caries&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/12/sonrisas-300x300.jpg&#34; alt=&#34;Fuente: Wikimedia:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Por medio de métodos cristalográficos, los investigadores fueron capaces de elucidar la estructura tridimensional (3D) de la enzima, siendo los primeros en lograr cristalizar glucansucarasa. La estructura cristalina reveló un mecanismo único de plegamiento: los distintos dominios de la enzima no están formados por una única cadena lineal de aminoácidos, sino por dos cadenas que se unen vía una estructura en forma de U. Este es el primer reporte en la literatura de un mecanismo de plegamiento de estas características. La determinación de la estructura 3D permitió a los investigadores obtener evidencias del mecanismo funcional de la enzima. La glucansucarasa divide la &lt;a href=&#34;http://es.wikipedia.org/wiki/Sacarosa&#34;&gt;sacarosa&lt;/a&gt; en &lt;a href=&#34;http://es.wikipedia.org/wiki/Fructosa&#34;&gt;fructosa&lt;/a&gt; y &lt;a href=&#34;http://es.wikipedia.org/wiki/Glucosa&#34;&gt;glucosa&lt;/a&gt;, y luego adhiere la molécula de glucosa en la cadena azucarada que de este modo va creciendo.&lt;/p&gt;
&lt;p&gt;Hasta ahora, la comunidad científica asumía que ambos procesos eran realizados por diferentes partes de la enzima. Sin embargo, el modelo creado por los investigadores de Groningen reveló que ambas actividades ocurren en el mismo sitio activo de la enzima. Dijkhuizen confía en que inhibidores específicos de la glucansucarasa puedan ayudar a prevenir que las bacterias se adhieran al esmalte dental, y los resultados obtenidos son cruciales para desarrollar estos inhibidores. No obstante, tal desarollo no ha sido exitoso, ya que los inhibidores de glucansucarasa también inhiben la enzima amilasa presente en la saliva, que es necesaria para degradar el almidón de los alimentos.&lt;/p&gt;
&lt;p&gt;El presente estudio ayuda a comprender por qué sucede esto, ya que es muy probable que las proteínas glucansucarasa hayan evolucionado a partir de la enzima amilasa. &amp;quot;Sabemos que las dos enzimas son similares&amp;quot;, dice Dijkhuizen, &amp;quot;ya que la estructura cristalográfica muestra que los sitios activos son virtualmente idénticos. Los futuros inhibidores necesitan ser dirigidos a blancos muy específicos debido a que ambas enzimas están muy cercanas evolutivamente&amp;quot;.&lt;/p&gt;
&lt;p&gt;Dijkhuizen menciona que en un futuro, los inhibidores de glucansucarasa pueden agregarse a la pasta dental y enjuages bucales. &amp;quot;Pueden ser incluso agregados a golosinas&amp;quot;, sugiere. &amp;quot;Un inhibidor puede prevenir que el azúcar liberada en la boca pueda causar daño&amp;quot;. Sin embargo, Dijkhuizen no espera que los cepillos dentales sean abandonados: &amp;quot;será siempre necesario que limpies tus dientes&amp;quot;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fuente:&lt;/strong&gt; &lt;a href=&#34;http://www.rug.nl/corporate/nieuws/archief/archief2010/persberichten/190_10&#34;&gt;Nota de prensa&lt;/a&gt; de la Universidad de Groningen.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; Remarkable fold of a 117 kDa glucansucrase fragment: Insights into evolution and product specificity of GH70 enzymes. Autores: Andreja Vujicić-Žagar, Tjaard Pijning, Slavko Kralj, Cesar A. López, Wieger Eeuwema, Lubbert Dijkhuizen y Bauke W. Dijkstra. PNAS, 30 de noviembre de 2010. El artículo se encuentra disponible en: &lt;a href=&#34;http://www.pnas.org/content/early/2010/11/24/1007531107&#34;&gt;www.pnas.org/cgi/doi/10.1073/pnas.1007531107&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Supernova SN 1979C: ¿El agujero negro más joven?</title>
      <link>https://ciencianet.com.ar/post/supernova-sn-1979c-el-agujero-negro-mas-joven/</link>
      <pubDate>Tue, 23 Nov 2010 20:32:59 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/supernova-sn-1979c-el-agujero-negro-mas-joven/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Cecilia Garraffo&lt;/strong&gt;, Harvard-Smithsonian Center for Astrophysics y &lt;strong&gt;Gastón Giribet&lt;/strong&gt;, Universidad de Buenos Aires - Conicet.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/11/lanzamiento_chandra_0.jpg&#34; alt=&#34;Lanzamiento del Chandra. El Observatorio Espacial fue lanzado en Agosto de 1999 por el transbordador Columbia. Crédito: NASA y el Centro de Ciencias del Chandra.:left&#34; title=&#34;lanzamiento_chandra_0&#34;&gt; Según entendemos hoy la astrofísica de los objetos compactos, los agujeros negros [&lt;a href=&#34;#1&#34;&gt;1&lt;/a&gt;] astrofísicos (aquéllos cuyas masas superan a la masa de nuestro sol en unos pocos factores) tienen su origen en algunas de las llamadas supernovas. Las supernovas son las explosiones de algunas estrellas al final de sus vidas. En muchos casos, estas explosiones dejan como detrito una estrella que aún tiene suficiente masa como para colapsar debido a la fuerza de su propia gravedad y convertirse finalmente en un agujero negro.&lt;/p&gt;
&lt;p&gt;El recientemente anunciado descubrimiento de lo que, según se cree, sería el agujero negro más jóven detectado en nuestro universo cercano se relaciona con este tipo de “nacimiento”. El observatorio de rayos X Chandra de la Nasa descubrió recientemente evidencia de que un agujero negro en la galaxia M100, ubicada a 50 millones de años luz de nuestro sistema solar, habría nacido de la explosión de supernova SN 1979C, observada hace 31 años. Si este descubrimiento se confirmara, estaríamos frente al agujero negro negro más jóven que hayamos observado en nuestro universo cercano.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/11/chandra_01.jpg&#34; alt=&#34;Imagen de SN 1979C: Imagen compuesta que muestra una supernova en la Galaxia M100. Crédito: X-ray: NASA/CXC/SAO/D.Patnaude et al, Optical: ESO/VLT, Infrared: NASA/JPL/Caltech.:left&#34; title=&#34;chandra_0&#34;&gt; Este es un hallazgo de gran importancia ya que nos permitiría observar un agujero negro y el comportamiento de la materia en sus inmediaciones desde sus primeros años de vida. Por otro lado, ésta puede considerarse la primera observación directa de un agujero negro en el centro de una supernova, por lo que representa una pieza fundamental para comprobar nuestras teorías acerca de la formación de objetos compactos en el cosmos.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Artículo original:&lt;/strong&gt; &lt;a href=&#34;http://www.cfa.harvard.edu/news/2010/pr201023.html&#34;&gt;http://www.cfa.harvard.edu/news/2010/pr201023.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referencia:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;1&#34;&gt; 1. &lt;/a&gt; ¿Qué es un agujero negro? Alrededor de 1930 se probó matemáticamente que si la masa de una estrella supera dos veces y media la de nuestro Sol, la presión del gas que la forma no puede equilibrar el efecto de la gravedad. Comienza entonces un proceso de contracción que la convierte en un objeto super denso, con un campo gravitatorio muy intenso. Ninguna radiación o partícula puede escapar del campo de una estrella colapsada, pues necesitaría superar la velocidad de la luz en el vacío para hacerlo. Como la luz que incide sobre estas estrellas colapsadas no es reflejada se les dio el nombre de &amp;quot;agujeros negros&amp;quot;.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Estabilidad como un mecanismo de selección natural en redes interactuantes</title>
      <link>https://ciencianet.com.ar/post/estabilidad-como-un-mecanismo-de-seleccion-natural-en-redes-interactuantes/</link>
      <pubDate>Tue, 26 Oct 2010 20:34:18 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/estabilidad-como-un-mecanismo-de-seleccion-natural-en-redes-interactuantes/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Pablo Gleiser&lt;/strong&gt;. Centro Atómico Bariloche.&lt;/p&gt;
&lt;p&gt;Una gran variedad de sistemas en la naturaleza pueden describirse como redes, donde las unidades del sistema están representadas por nodos, y la interacción entre estas unidades está representada por conexiones entre los nodos. Así, podemos pensar al cerebro como una red de neuronas, a una organización social como una red de personas, o a Internet como una red de computadoras.&lt;/p&gt;
&lt;p&gt;Muchas de estas redes poseen propiedades específicas que no se observan en sistemas aleatorios donde los nodos son conectados al azar. Estas redes se denominan redes complejas. Sorprendentemente, se ha observado que muchas propiedades cuantitativas de las redes complejas son compartidas por sistemas muy diversos, incluyendo sistemas sociales, biológicos y tecnológicos. Esto sugiere que existen mecanismos muy básicos que gobiernan la formación y evolución de estas redes.&lt;/p&gt;
&lt;p&gt;Descubrir cuáles son estos mecanismos genéricos es un desafío para la ciencia actual. Existen diferentes formas en las que la física intenta modelar estos sistemas para comprender su funcionamiento. Una forma consiste en tener en cuenta la mayor cantidad de elementos presentes en el sistema. Esto lleva a la formulación de modelos con un gran número de parámetros que pueden ser ajustados mediante datos experimentales, y dan una descripción realista de un sistema específico.&lt;/p&gt;
&lt;p&gt;En el otro extremo, un enfoque posible es el de considerar un modelo abstracto con un número mínimo de parámetros. Con este enfoque es posible obtener resultados muy generales, que permiten describir cuáles son los ingredientes fundamentales necesarios para que se pueda observar un fenómeno dado. Éste es precisamente el enfoque que Juan Perotti, Orlando Billoni, Francisco Tamarit y Sergio Cannas, de la Facultad de Matemática, Astronomía y Física de la Universidad Nacional de Córdoba, han seguido en el artículo titulado &amp;quot;Estabilidad como un mecanismo de selección natural en redes interactuantes&amp;quot; que ha sido publicado recientemente en la revista &amp;quot;Papers in Physics&amp;quot;.&lt;/p&gt;
&lt;p&gt;En este trabajo ellos proponen un mecanismo para la formación de redes complejas basado en un algoritmo con dos ingredientes fundamentales que se encuentran acoplados entre sí: crecimiento y estabilidad de la red. En el modelo, los nodos de la red tienen estados dinámicos que cambian con el tiempo. A través de las interacciones de la red los nodos llegan a un estado estable. Esto es, un estado en el que una pequeña perturbación no altera el estado del sistema. Una alteración posible es la incorporación de un nuevo nodo a la red, que trae como consecuencia la presencia de nuevas conexiones que modifican el estado de los otros nodos en la red y pueden desestabilizar al sistema.&lt;/p&gt;
&lt;p&gt;Por ejemplo, en un sistema biológico podría pensarse en un ecosistema donde una nueva especie se hace presente. El efecto de esta nueva especie podría ser sólo el incremento del número de especies, o ésta podría alterar al ecosistema llevando a la extinción de muchas especies. Estos ingredientes son tenidos en cuenta en el modelo, por lo que la incorporación de un nuevo nodo sólo se acepta si se dan las condiciones matemáticas para que el sistema permanezca estable.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/10/estabilidad-red.post_.jpg&#34; alt=&#34;:left&#34; title=&#34;estabilidad-red.post&#34;&gt;&lt;/p&gt;
&lt;p&gt;Los estudios realizados sobre el modelo muestran que las reglas que tienen en cuenta el crecimiento y la estabilidad de la red establecen una limitación sobre las posibles estructuras de red que se pueden obtener. Los autores analizaron las redes obtenidas con el modelo y observaron características presentes en redes reales, tales como redes biológicas de células. Por ejemplo, las redes obtenidas con el modelo presentan un grán número de nodos con pocas conexiones y unos pocos nodos con un gran número de conexiones. Los nodos con pocas conexiones forman pequeños módulos, donde los nodos están muy conectados entre sí, y cumplen un rol funcional de estabilizar la red. Por otro lado los nodos con un gran número de conexiones establecen un nexo entre los módulos, lo que permite reducir la distancia entre nodos en diferentes módulos.&lt;/p&gt;
&lt;p&gt;La generalidad del modelo también permite hacer un análisis de los resultados obtenidos en el contexto de redes ecológicas. Los autores señalan que la incorporación de ingredientes específicos tales como el envejecimiento de los nodos y una capacidad limitada de establecer conexiones con otro nodos permitirá extender el alcance del modelo a nuevos sistemas tales como las redes tróficas.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo Original:&lt;/strong&gt; Juan I. Perotti, Orlando V. Billoni, Francisco A. Tamarit, Sergio A. Cannas, &lt;a href=&#34;https://doi.org/10.4279/pip.020005&#34;&gt;Stability as a natural selection mechanism on interacting networks&lt;/a&gt;. Papers in Physics 2, 020005 (2010).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; S. A. Cannas (Email: &lt;a href=&#34;mailto:cannas@famaf.unc.edu.ar&#34;&gt;cannas@famaf.unc.edu.ar&lt;/a&gt; )&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Modelo matemático para la propagación de la tos convulsa en Argentina</title>
      <link>https://ciencianet.com.ar/post/modelo-matematico-para-la-propagacion-de-la-tos-convulsa-en-argentina/</link>
      <pubDate>Mon, 25 Oct 2010 20:37:52 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/modelo-matematico-para-la-propagacion-de-la-tos-convulsa-en-argentina/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero&lt;/strong&gt;. Facultad de Ciencias Exactas (UNLP).&lt;/p&gt;
&lt;p&gt;La tos convulsa, también llamada coqueluche o pertussis, es una enfermedad infecciosa respiratoria inmunoprevenible causada por la bacteria Bordetella pertussis. Se caracteriza por episodios muy intensos de tos que pueden prolongarse por varias semanas, y en el caso de niños pequeños puede causar complicaciones y hasta la muerte. En los adultos los síntomas suelen ser más leves. La enfermedad se transmite de persona a persona a través de gotitas de aerosol dispersadas por la tos de un infectado. La tasa de comunicabilidad es alta: pueden contagiarse entre el 80 y 90 % de las personas susceptibles en contacto con un individuo infectado.&lt;/p&gt;
&lt;p&gt;Antes de la existencia de vacunas específicas, era la principal causa de muerte en niños. El desarrollo y la inclusión de vacunas en los calendarios nacionales de vacunación redujeron de manera significativa la morbi mortalidad de la enferemdad. En nuestro país el esquema consiste en tres dosis que se administran a los 2, 4 y 6 años de vida, más dos refuerzos que se dan a los 18 meses y al ingreso escolar. Desde el año 2009 se incluye un refuerzo más a los 11 años de edad.&lt;/p&gt;
&lt;p&gt;Pese a años de utilización de vacunas, la enfermedad no se ha podido erradicar. Más aún, en las últimas dos décadas se ha reportado en varios países un notable aumento en el número de enfermos registrados por año. Mundialmente se estima que hay por año 40 millones de casos, de los cuales 300.000 son fatales, con un 70% de incidencia en menores de 5 años, un 90% en menores de 1 año de los cuales 75% son menores de 6 meses. El 50% de los afectados necesita internación. En nuestro país el número de casos reportados, según la Organización Mundial de la Salud, pasó de 267 en el año 2002 a 1742 en el 2009. Además del aumento de casos se ha registrado un cambio en la proporción de casos según la edad de los pacientes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/10/Graph1-s.jpg&#34; alt=&#34;:left&#34; title=&#34;Graph1-s&#34;&gt;&lt;/p&gt;
&lt;p&gt;Para explicar el resurgimiento de la tos convulsa se han propuesto varias hipótesis, entre las cuales se cuentan: coberturas de vacunación por debajo de las óptimas; baja efectividad de la vacuna (se estima alrededor del 70% de efectividad luego de la 3ra dosis); corta duración de la inmunidad conferida por vacunación; presencia de nuevas variantes de B. pertussis contra las que las vacunas en uso podrían no ser tan eficaces y cuadros clínicos atípicos sin diagnóstico o con diagnóstico tardío. La importancia de cada uno de estos factores no es igual en todos los países por lo que no se ha podido arribar a una conclusión que permita sentar las bases para mejorar estrategias de control contra la enfermedad.&lt;/p&gt;
&lt;p&gt;En este marco, los modelos matemáticos predictivos surgen como una herramienta importante que puede contribuir a la toma de decisiones en el área de la salud pública. Si bien se han desarrollado modelos para esta patología en otros países, los mismos no contemplan nuestras particularidades epidemiológicas locales. En esta dirección se encamina el trabajo encabezado por el físico Gabriel Fabricius –especializado en mecánica estadística y física computacional- y la bioquímica Daniela Hozbor, quien dirige un grupo de investigación que aborda distintos aspectos de la epidemiología de la enfermedad y del desarrollo de vacunas. También han participado de este trabajo el bioquímico Aníbal Lodeiro, el físico Augusto Melgarejo y el matemático Alberto Maltz.&lt;/p&gt;
&lt;h3 id=&#34;el-modelo&#34;&gt;El modelo&lt;/h3&gt;
&lt;p&gt;El modelo matemático diseñado por el grupo plantea que la población está dividida en tres grupos de individuos: los susceptibles de adquirir la infección, los que están infectados y los que poseen algún grado de inmunidad. Este último grupo se subdivide, a su vez, según sea el grado de inmunidad de los individuos: adquirido naturalmente (por haber estado infectados) o adquirido artificialmente (por haber recibido distintas dosis de vacuna). Como la tos convulsa reviste distinta gravedad según la edad, y como el contagio es mayor en ciertas etapas de la vida -como ocurre en el período escolar- cada uno de los grupos de individuos es a su vez dividido en clases etarias.&lt;/p&gt;
&lt;p&gt;Por otra parte, tener los resultados discriminados por edades permite saber por ejemplo qué fracción de los infectados corresponden a las clases etarias más bajas (menores de un año), los cuales son los que pueden perecer a causa de la enfermedad. La transferencia de individuos entre los distintos grupos poblacionales es descripta por un conjunto de ecuaciones diferenciales no lineales acopladas que se resuelven numéricamente.&lt;/p&gt;
&lt;p&gt;Al modelo se incorporan los datos reales: a partir de información epidemiológica, poblacional y clínica deben establecerse algunos parámetros como las tasas de contagio entre individuos susceptibles e infectados de distintas clases etarias, el tiempo que dura la infección, la velocidad a la que se pierde la inmunidad, las tasas de nacimiento y muerte.&lt;/p&gt;
&lt;h3 id=&#34;algunos-resultados&#34;&gt;Algunos resultados&lt;/h3&gt;
&lt;p&gt;El modelo descrito permite estudiar el efecto de un retraso en las dosis de vacunación que es simulado alterando las coberturas de las distintas dosis vacunales y observando los nuevos valores obtenidos para la incidencia de la pertussis en los distintos grupos etarios. El objetivo de este trabajo fue evaluar si es posible establecer alguna conexión entre el aumento registrado en casos de pertussis a partir de 2002 en Argentina con la ausencia/retraso de algunas dosis de vacuna registradas en la población. Los resultados indican que un retraso en la primera dosis produce un aumento significativo en la incidencia global de la enfermedad y en particular para los niños de hasta 1 año de edad. Los retrasos en las siguientes dosis también producen efectos significativos, pero siempre menores que el producido por un retraso en la primera dosis.&lt;/p&gt;
&lt;p&gt;Estos resultados enfatizan la importancia de aplicar a tiempo la primera dosis vacunal y muestran que este tipo de estudios pueden resultar muy útiles a la hora de evaluar estrategias y definir políticas de prevención y control de pertussis en nuestro país.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original.&lt;/strong&gt; &lt;em&gt;Fabricius, G., Lodeiro, A., Melgarejo,A., Graieb, A. y Hozbor,D.A. Deterministic epidemiology model to study pertussis in Argentina. 27th Annual Meeting of the European Society for Paediatric Infectious Disease, Brussels, Belgium, June 9-13, 2009.&lt;/em&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>¿La dimensión, importa?</title>
      <link>https://ciencianet.com.ar/post/la-dimension-importa/</link>
      <pubDate>Thu, 21 Oct 2010 20:36:18 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/la-dimension-importa/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Luis A. Pugnaloni.&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET La Plata, UNLP).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/10/dimensionalidad-768x328.jpg&#34; alt=&#34;Foto grupal de los participantes al evento.&#34; title=&#34;dimensionalidad.preview&#34;&gt;&lt;/p&gt;
&lt;p&gt;Hace poco participé de un taller (&lt;em&gt;workshop&lt;/em&gt;) sobre coloides y granulares. El evento se llamó &amp;quot;&lt;a href=&#34;http://www.mpipks-dresden.mpg.de/~pardim10/&#34;&gt;Particulate matter: Does dimensionality matter?&lt;/a&gt;&amp;quot;. Cuya significación es: Es importante la dimensión en sistemas formados por partículas? Aquí &amp;quot;partículas&amp;quot; se refiere a objetos visibles, al menos en un microscopio convencional.&lt;/p&gt;
&lt;p&gt;Participaron colegas de Europa (la mayoría), América (casi todos del norte) y Asia (muy pocos). Durante el evento se propuso la pregunta, usada como título del encuentro, multiples veces. Diferentes especialistas ofecían diferentes respuestas. La semana de charlas y exposiciones terminó y no se llegó a conclusión alguna: unos insistían en que la dimensión no es importante y otros en que si.&lt;/p&gt;
&lt;p&gt;En esta nota intento explicar las razones de lo inútil que es responder preguntas como estas desde un aspecto técnico. Usaré este evento científico como &amp;quot;mal&amp;quot; ejemplo. Pero al final comentaremos la verdadera función de preguntas como estas en contextos como el taller mencionado.&lt;/p&gt;
&lt;p&gt;Una pregunta del estilo: ¿La dimensión, importa? es tan vaga que cada interlocutor puede interpretar de forma diferente la consigna. Quiero destacar que esta pregunta usa dos palabras cuyo significado es extremadamente ambiguo.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dimensión.&lt;/strong&gt; Esta palabra parece ser algo muy bien definido. Sin embargo en esta conferencia había concepciones diferentes. Lamentablemente, se discutía sin darse cuenta de que en ocasiones se referían a cosas distintas. Para algunos (en especial para los experimentalistas) la dimensión de un sistema se puede controlar confinando las partículas en espacios planos o lineales. Por ejemplo, unas cuantas bolitas entre dos placas de vidrio bien cercanas constituiría un sistema en dos dimensiones (2D).&lt;/p&gt;
&lt;p&gt;Así, es posible cambiar lentamente a un sistema tridimansional (3D) alejando poco a poco las placas de vidrio una de otra. Para otros (los más teóricos) la dimensión podía cambiar de uno a infinito pero sólo de uno en uno. Claramente unos piensan en &amp;quot;dimensión&amp;quot; como sinónimo de &amp;quot;confinamiento&amp;quot;. Esto es, ¿en cuántas direcciones dejo que las partículas comunes y tridimensionales de nuestro mundo se muevan?. Otros pensaban en partículas que eran de otro mundo, bidimensionales, que ni podían imaginar la tercera dimensión, o n-dimensionales incluso.&lt;/p&gt;
&lt;p&gt;Quiero aclarar que es posible describir un mundo estrictamente 2D. Pero en ese caso, las fuerzas de interacción entre objetos en ese mundo deben ser diferentes al mundo 3D. La fuerza electrostática de Coulomb, por ejemplo, en un mundo estrictamente unidimensional (1D) no es inversamente proporcional al cuadrado de la distancia entre las cargas eléctricas sino independiente de la distancia. Mucho más complejo sería conocer las complicadas fuerzas de fricción entre objetos macroscópicos en un mundo extrictamente 1D.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Importa&lt;/strong&gt; Lo que a mi me importa quizá no te importa. Preguntar si algo importa en una conferencia como esta implica que queremos saber la opinión subjetiva de los participantes, no se puede esperar una respuesta factual. La importancia de las cosas es de caracter relativo y subjetivo. Así, escuché científicos (Frederic Lechenault) decir que sus experimentos eran más simples en 2D (confinados a un plano, para hablar con propiedad) y por eso es importante la dimensión; porque complica o simplifica los experimentos. Lo mismo puede decir un teórico porque sus cálculos son más simples en unas dimensiones que en otras.&lt;/p&gt;
&lt;p&gt;Otros (Corey O&#39;Hern), pensando de un modo más general, decían que sus resultados no cambiaban en forma &amp;quot;importante&amp;quot; con la dimensión y por lo tanto ésta no importaba. Este último argumento es más fuerte pero merece ser atacado. ¿Qué significa que un resultado cambia en forma &amp;quot;importante&amp;quot;? La respuesta puede ser variada.&lt;/p&gt;
&lt;p&gt;Doy como ejemplo tres visiones estereotipadas posibles según un ingeniero, un físico y un matematico (y no intento contar un chiste). Ingeniero: Si la propiedad A como función del parámetro B cambia en un factor 2 al pasar de 2D a 3D entonces la dimensión importa. El valor cuantitativo de una propiedad es importante. Físico: Si la propiedad A como función del parámetro B cambia al menos su forma funcional al pasar de 2D a 3D entonces la dimensión importa. La forma funcional es valorada por encima de los detalles cuantitativos. Matemático: Si la propiedad A como función del parámetro B cambia al menos su topología al pasar de 2D a 3D entonces la dimensión importa. No deslumbra un cambio de forma funcional, hace falta la aparición de discontinuidades para que resulte importante.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lo que importa a los que importan&lt;/strong&gt; Como es de esperar, no se puede responder en forma definitiva la pregunta propuesta en la conferencia de ejemplo. Pero si se puede aprovechar para escuchar opiniones y aprender qué le importa a los importantes. Estos pueden decidir donde poner dinero y donde publicar resultados. Varios científicos influyentes estaban presentes. No pude intuir que alguno de estos se dieran cuenta de lo inapropiada de la pregunta dado su afán por responderla según su leal entender.&lt;/p&gt;
&lt;p&gt;Queda claro que sus respuestas guían a los menos influyentes sobre que tipo de investigaciones debieran seguir en el futuro si desean recibir la aprobación de aquellos otros. Quizá ese era el objetivo de los organizadores.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Siempre se aprenden cosas interesantes&lt;/strong&gt;. Más allá de esto, siempre se aprenden cosas en las reuniones científicas. Cuento aquí un par de cosas que escuche (no por vez primera), que considero suficientemente generales para que le interese a cualquiera.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Teorías locas:&lt;/strong&gt; Un matemático (Henry Cohn) dictó una conferencia sobre cuál es la máxima densidad con que se pueden acomodar esferas en cualquier dimensión (especialmente de 4D a infinitoD). Comentó sobre las peculiaridades de la 8D que la hacen especialmente interesante (para él). Cómo Henry parece estar acostumbrado a que le digan que eso de n-dimensiones es una locura matemática sin utilidad, empezó por explicar que un famoso matemático en la década de 1940 explicó que las señales que intentaban mandar en una línea de transmisión de información (radio, teléfono, etc) estaban limitadas por el número de &amp;quot;esferas de ruido&amp;quot; en el espacio multidimensional de los parámetros que controlan las propiedades de la señal que podían acomodarse. Esto es una aplicación muy concreta del problema de acomodar esferas en n-dimensiones para la industria de las telecomunicaciones. Aún así, me pregunto que tan impostante es que sean hiperesferas (nombre para referirse a esferas matemáticas en dimensiones mayores a 3) en lugar de hipercubos (cubos matemáticos en n-dimensiones) cuyo ordenamiento intuyo es tan fácil como el de cubos convencionales (3D), cuadrados (2D) y segmentos (1D).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;¿Para qué investigar en 1D y en infinitoD?&lt;/strong&gt; Un prestigioso científico de la audiencia (Bob Behringer), preguntó qué sentido tenían estudios teóricos sobre 1D o infinitoD que nunca se podrán llevar a la realidad. Atinadamente uno de los organizadores (Patrick Charbonneau) le aclaró que esas teorías admiten soluciones matemáticas exactas y que pueden servir como modelos de prueba para simulaciones numéricas. Dado un algoritmo numérico para modelado de sistemas 3D, se lo puede probar en 1D e infinitoD para ver si cumplen con los teoremas demostrados por los matemáticos. Si lo hacen uno gana en confianza sobre lo que estos cálculos numéricos podrían decirnos en casos 3D.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Escuela Internacional de Gravedad Cuántica</title>
      <link>https://ciencianet.com.ar/post/escuela-internacional-de-gravedad-cuantica/</link>
      <pubDate>Sat, 04 Sep 2010 20:42:43 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/escuela-internacional-de-gravedad-cuantica/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Nicolás Grandi.&lt;/strong&gt; Instituto de Física La Plata.&lt;/p&gt;
&lt;p&gt;Entre el 19 y el 27 de julio de 2010, en el Departamento de Física de la Universidad Nacional de La Plata, tuvo lugar la &lt;em&gt;International School in Quantum Gravity,&lt;/em&gt; que reunió a un centenar de asistentes entre estudiantes e investigadores, provenientes de prestigiosos institutos de más de diez países. La escuela consistió en una serie de &lt;em&gt;lectures&lt;/em&gt; concentradas en uno de los problemas abiertos más intrigantes de la física teórica contemporánea: el comportamiento cuántico de la gravedad. Se abordaron temas relacionados con la teoría de cuerdas, la gravedad cuántica de lazos, y la conjetura de Maldacena, entre otros.&lt;/p&gt;
&lt;p&gt;El objetivo de la escuela fue poner al alcance de los estudiantes de doctorado argentinos y latinoamericanos los avances más recientes en estas áreas, de modo de facilitar el desarrollo de investigación de frontera en nuestros países. Las exposiciones estuvieron a cargo de científicos de primer nivel internacional, entre los que se contaron Joseph Polchinski (Santa Barbara, USA), Martín Kruczenski (Purdue University, USA), Carlos Núñez (Swansea University, UK), Eva Silverstein (Santa Barbara y Stanford, USA), George Thompson (Abdus Salam ICTP, Italia), Sean Hartnoll (Harvard, USA), Rodolfo Gambini (UdelaR, Uruguay), Jorge Russo (ICREA Barcelona, España), Nathan Berkovits (IFT, Brasil) y Jorge Zanelli (CECS, Chile).&lt;/p&gt;
&lt;p&gt;La escuela estuvo dedicada a la memoria de Carlos G. Bollini, uno de los científicos más destacados que dio nuestro país, quien en colaboración con J.J. Giambiagi publicó el artículo más citado de la física argentina. En dicho trabajo se propuso un forma novedosa de controlar los infinitos que parecían plagar la teoría cuántica de campos, la llamada “regularización dimensional”. Una propuesta similar fué aplicada por los físicos holandeces G. &#39;t Hooft y M. Veltman a la teoría de las interacciones fuertes, por lo que recibieron el premio Nobel de Física en 1999. Durante la escuela se realizó un emotivo homenaje a Bollini, en el marco del cual lo recordaron sus colegas y discipulos Daniel Bes, Hector Vucetich y Carlos García Canal, y su viuda Susana Lévy de Bollini.&lt;/p&gt;
&lt;p&gt;La escuela fue organizada por la red argentina Strings@ar, que nuclea a más de cincuenta investigadores argentinos que trabajan en institutos del país y del exterior en temas de teoría de cuerdas, teoría de campos, cosmología y gravitación. Fue financiada parcialmente con aportes del &lt;em&gt;Abdus Salam International Centre for Theoretical Physics&lt;/em&gt; dependiente de la UNESCO, del Centro Latinoamericano de Física, del Instituto de Física de La Plata, del Instituto Balseiro, del programa R@aices dependiente del Ministerio de Ciencia y Técnica, y de la empresa Micro Automación. Al finalizar la escuela, entre el 28 y el 30 de julio, tuvo lugar en la Biblioteca Nacional la conferencia &lt;em&gt;Quantum Gravity in the Southern Cone V&lt;/em&gt;, donde se discutieron los avances más recientes en el área.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Celdas solares con nanohilos</title>
      <link>https://ciencianet.com.ar/post/celdas-solares-con-nanohilos/</link>
      <pubDate>Tue, 17 Aug 2010 20:47:16 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/celdas-solares-con-nanohilos/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Marcelo Cappelletti&lt;/strong&gt;. Facultad de ingeniería. Universidad Nacional de La Plata.&lt;/p&gt;
&lt;p&gt;El avance a pasos agigantados de la nanotecnología brinda la posibilidad de nuevas investigaciones y desarrollos con aplicaciones en diversos campos tales como la informática, medicina o fuentes de energía alternativas, que prometen mejorar la calidad de vida de la humanidad y el medio ambiente en general.&lt;/p&gt;
&lt;p&gt;En este sentido, en el campo de almacenamiento, producción y conversión de energía, los dispositivos fotovoltaicos (dispositivos capaces de convertir energía solar en energía eléctrica) de unión &lt;a href=&#34;http://es.wikipedia.org/wiki/Uni%C3%B3n_PN&#34;&gt;p-n&lt;/a&gt; que emplean nanohilos semiconductores, presentan una clara ventaja respecto de los dispositivos convencionales de película delgada, en cuanto a tener una mayor eficiencia en la conversión de energía y un menor costo.&lt;/p&gt;
&lt;p&gt;Sin embargo, actualmente, los dispositivos que emplean nanohilos basados en semiconductores compuestos III-V (semiconductores formados por dos elementos de los grupos III y V de la tabla periódica), presentan una pobre característica de rectificación y una eficiencia de conversión de energía solar muy inferior a lo que predice la teoría. Esto puede ser debido a los &lt;a href=&#34;http://es.wikipedia.org/wiki/Portador_de_carga&#34;&gt;portadores de carga&lt;/a&gt; atrapados en las superficies de los nanohilos, a las elevadas resistencias de los contactos y a un control insuficiente del &lt;a href=&#34;http://es.wikipedia.org/wiki/Dopaje_%28semiconductores%29&#34;&gt;dopaje&lt;/a&gt; a través de los nanohilos.&lt;/p&gt;
&lt;p&gt;Jorge Caram, Claudia Sandoval, Mónica Tirado y David Comedi de la Universidad Nacional de Tucumán, junto a Josef Czaban, David Thompson y Ray LaPierre de la Universidad de McMaster (Canadá) han estudiado este problema a partir de dispositivos fotovoltaicos de unión p-n que emplean nanohilos de un semiconductor compuesto, el arseniuro de galio (GaAs).&lt;/p&gt;
&lt;p&gt;Un método típico para evaluar el rendimiento de los dispositivos fotovoltaicos consiste en realizar mediciones de la característica Tensión-Corriente continua. Sin embargo, en este trabajo, los autores proponen una caracterización del dispositivo mucho más completa, mediante un estudio de &lt;a href=&#34;http://es.wikipedia.org/wiki/Espectroscopia_de_impedancia&#34;&gt;espectroscopía de impedancia&lt;/a&gt; de estructuras de nanohilos coraza(shell)-corazón(core) p-n de GaAs, el cual es un método que analiza la dispersión de las propiedades eléctricas en un intervalo amplio de frecuencias (usando corriente alterna).&lt;/p&gt;
&lt;p&gt;Los dispositivos de nanohilos para aplicaciones fotovoltaicas que han sido investigados, fueron fabricados por el grupo de investigación LaPierre de la Universidad de McMaster, y consisten de coraza-corazón p-n alineados de manera perpendicular al GaAs. El material empleado para el dopaje tipo n fue el telurio (Te) en lugar de silicio (Si), debido al comportamiento &lt;a href=&#34;http://es.wikipedia.org/wiki/Anf%C3%B3tero&#34;&gt;anfótero&lt;/a&gt; que el Si puede presentar en nanohilos de GaAs.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/08/cappelletti.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Se diseñaron dos clases bien diferentes de dispositivos dependiendo del grado de superposición espacial, coraza-corazón p-n, a lo largo del eje del nanohilo. Estas diferentes distribuciones espaciales resultaron ser un punto trascendente para la obtención de importantes conclusiones en cuanto a las características eléctricas en el rango de frecuencias 103–107 Hz.&lt;/p&gt;
&lt;p&gt;Los resultados obtenidos indican que para frecuencias menores a 104 Hz, cuando se invierte la tensión de polarización desde 1.5 V hasta -1.5 V, la impedancia medida de los dispositivos con mayor superposición coraza-corazón (muestra D en la figura), se comporta de acuerdo a lo esperado para una unión p-n.&lt;/p&gt;
&lt;p&gt;Por el contrario, los dispositivos con pequeña superposición (muestra A en la figura) no siguen este comportamiento. Además, la respuesta del dispositivo decae abruptamente para frecuencias mayores de 104 Hz. Los autores atribuyeron este efecto a los portadores capturados y liberados desde los niveles de energía profundos de la &lt;a href=&#34;http://es.wikipedia.org/wiki/Banda_prohibida&#34;&gt;banda prohibida&lt;/a&gt; del semiconductor.&lt;/p&gt;
&lt;p&gt;Este estudio permitió conseguir una mejor comprensión de los fenómenos físicos que actualmente limitan la eficiencia de estos dispositivos. Los autores creen que sus hallazgos permitirán mejorar la eficiencia en la conversión de energía solar a través de dispositivos fotovoltaicos basados en nanohilos.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; &lt;a href=&#34;http://iopscience.iop.org/0957-4484/21/13/134007/&#34;&gt;Electrical characteristics of core–shell p–n GaAs nanowire structures with Te as the n-dopant&lt;/a&gt; , Nanotechnology, 21, 134007 (2010)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; David Comedi (E-mail: &lt;a href=&#34;mailto:dcomedi@herrera.unt.edu.ar&#34;&gt;dcomedi@herrera.unt.edu.ar&lt;/a&gt; ) &lt;a href=&#34;http://www.herrera.unt.edu.ar/nano&#34;&gt;http://www.herrera.unt.edu.ar/nano&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Reciente avance en física de medios granulares</title>
      <link>https://ciencianet.com.ar/post/reciente-avance-en-fisica-de-medios-granulares/</link>
      <pubDate>Thu, 05 Aug 2010 20:48:31 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/reciente-avance-en-fisica-de-medios-granulares/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Andrés D. Medus.&lt;/strong&gt; Estudiante de doctorado del Departamento de Física, Facultad de Ciencias Exactas y Naturales, Universidad de Buenos Aires.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Este artículo ha sido financiado por el proyecto INVOFI de la Asociación Física Argentina.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Investigadores del Grupo de Medios Porosos de la Facultad de Ingeniería de la UBA en colaboración con pares de la Universidad de la Plata y de la Universidad de Lyon (Francia), demostraron experimentalmente que el flujo de material granular a través de un orificio resulta independiente de la presión en la base del recipiente contenedor, derrumbando de este modo un antiguo postulado de la Física de Medios Granulares.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Por el ápice abierto el cono inverso &lt;br&gt;
Deja caer la cautelosa arena,  &lt;br&gt;
Oro gradual que se desprende y llena &lt;br&gt;
El cóncavo cristal de su universo. &lt;br&gt;&lt;/p&gt;
&lt;p&gt;-- &lt;em&gt;El reloj de arena&lt;/em&gt; de Jorge Luis Borges.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Muchos de nosotros, en algún momento de nuestras vidas, hemos sido hipnotizados por el constante fluir de los granos de arena a través del orificio que une los bulbos en un reloj de arena. Este fluir casi imperturbable no se daría si de un líquido se tratase. En ese caso la velocidad de salida del líquido sería dependiente de la presión en el orificio, que a su vez depende de la altura de la columna de líquido restante en el bulbo superior. Sin embargo, para el caso de medios granulares como la arena, se da un extraño fenómeno denominado “efecto Janssen”, por el cual la presión ejercida por los granos que se encuentran encima del orificio, se transmite por fricción hacia los laterales del recipiente contenedor.&lt;/p&gt;
&lt;p&gt;Como resultado, cuando la altura de la columna de arena es superior al doble del tamaño de la apertura de salida, la presión ejercida en la región próxima al orificio es casi constante e independiente de la cantidad restante. Luego, el flujo de arena a través del orificio será también aproximadamente constante e independiente de la cantidad que reste en el bulbo superior.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/08/cinta-transportadora-CN.png&#34; alt=&#34;Modelo experimental armado.&#34; title=&#34;cinta-transportadora-CN&#34;&gt;&lt;/p&gt;
&lt;p&gt;De lo dicho hasta aquí se desprende un importante postulado de la Física de los Medios Granulares: si la presión en el fondo de un silo es constante, entonces también lo será el flujo de material granular a través del orificio. “&lt;em&gt;En los últimos 50 años numerosos trabajos de diversas disciplinas utilizan este efecto (presión constante en la base del recipiente) para justificar que el caudal permanece constante durante la descarga. Es decir, presión constante, implica caudal constante. Hasta tal punto está generalizada esta idea, que esta explicación ha sido sostenida aún cuando la columna de granos no es suficiente para alcanzar una presión constante en la base&lt;/em&gt;” nos cuenta la Dra. María Alejandra Aguirre, integrante del equipo que ha demostrado recientemente que este postulado es falso.&lt;/p&gt;
&lt;p&gt;Para ello, construyeron el dispositivo que puede observarse en la figura, consistente en una caja contenedora de Plexiglas con una abertura de tamaño variable, conteniendo discos del mismo material que son impulsados por una cinta móvil de velocidad regulable. A diferencia de lo que ocurre para un reloj de arena, en este caso el dispositivo está dispuesto en forma horizontal y los discos son impulsados por una cinta móvil y no por la gravedad. Las experiencias realizadas muestran que “&lt;em&gt;el caudal permanece constante aunque la presión en la base varíe y que sólo puede cambiarse si se modifica la velocidad con que los granos atraviesan la apertura&lt;/em&gt;” concluye la Dra. Aguirre, refutando aquel antiguo postulado.&lt;/p&gt;
&lt;p&gt;Consultada acerca de las posibles aplicaciones de esta investigación, la Dra. Aguirre indica que “&lt;em&gt;puede servir de fundamento para buscar diferentes maneras y/o mecanismos que aumenten la velocidad de salida de granos a fin de incrementar el caudal de descarga en aplicaciones industriales&lt;/em&gt;”.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo Original:&lt;/strong&gt; M. A. Aguirre, J. G. Grande, A. Calvo, L. A. Pugnaloni, J.-C Géminard; Phys. Rev. Lett. 104, 238002 (2010) (&lt;a href=&#34;http://prl.aps.org/abstract/PRL/v104/i23/e238002&#34;&gt;http://prl.aps.org/abstract/PRL/v104/i23/e238002&lt;/a&gt;, &lt;a href=&#34;http://arxiv.org/abs/1005.2884&#34;&gt;PDF&lt;/a&gt;).&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Sobre el movimiento democrático a escala molecular en líquidos a baja temperatura</title>
      <link>https://ciencianet.com.ar/post/sobre-el-movimiento-democratico-a-escala-molecular-en-liquidos-a-baja-temperatura/</link>
      <pubDate>Tue, 23 Mar 2010 20:53:28 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/sobre-el-movimiento-democratico-a-escala-molecular-en-liquidos-a-baja-temperatura/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;J. Ariel Rodriguez Fris.&lt;/strong&gt; Universidad Nacional del Sur.&lt;/p&gt;
&lt;p&gt;El movimiento de los átomos, moléculas e iones que conforman un líquido cuando, en promedio, se desplazan el equivalente a su tamaño a partir de su posición inicial se la denomina relajación estructural. La dinámica de relajación estructural de líquidos a baja temperatura es heterogénea en tiempo y espacio. Es decir que dicha dinámica es muy diferente de una región a otra y además cambia en el tiempo. Cabe aclarar que estas heterogeneidades se hacen más conspicuas con la disminución de la temperatura.&lt;/p&gt;
&lt;p&gt;El hallazgo de heterogeneidades dinámicas, tanto teóricas como experimentales, en líquidos a baja temperatura brindó una clave para comprender el mecanismo por el cual relajan dichos sistemas. En esta tesis demostramos, mediante simulación por computadoras, que dicha relajación, al menos para los tres líquidos estudiados, se debe a la ocurrencia de eventos que denominamos &lt;em&gt;d-clusters&lt;/em&gt; (por &lt;em&gt;democratic clusters&lt;/em&gt;, o agregados democráticos), los cuales son una clase de heterogeneidades dinámicas no estudiadas hasta el momento.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/03/fris.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Para caracterizar la dinámica de relajación utilizamos tres modelos de líquidos: uno arquetípico de constituyentes que interaccionan como los gases nobles (como el Argón), otro que emula agua sobreenfriada y, por último, uno que emula dióxido de silicio. Al seguir la dinámica de pequeñas porciones de estos sistemas en función del tiempo, descubrimos que los constituyentes pasan largos períodos de &amp;quot;inactividad&amp;quot;, sin moverse demasiado en promedio (por lo que en estos tiempos el sistema no relaja ya que se halla aproximadamente inmóvil o congelado). Se analizan pequeñas porciones del líquido ya que cada región se comporta diferente debido a las heterogeneidades dinámicas.&lt;/p&gt;
&lt;p&gt;Por otro lado, vimos que a ciertos tiempos sobreviene una rápida reorganización de aproximadamente el 35 % de los constituyentes formando una agrupación (ó cluster) relativamente compacta. Posteriormente reaparece el período inmóvil y así sucesivamente, alternan. A este tipo de movimiento colectivo, el cual es una nueva clase de heterogeneidades dinámicas, que dura sólo una fracción respecto del gran período inmóvil, lo denominamos &lt;em&gt;d-cluster&lt;/em&gt;. La d por democrático ya que es una importante fracción del total del sistema que se reorganiza.&lt;/p&gt;
&lt;p&gt;Se destaca el hecho que nuestro trabajo permitió a otros autores, estudiando un polímero a baja temperatura y comparando simulaciones con experimentos, dar soporte experimental a la ocurrencia de estos eventos. En conclusión, nuestro trabajo ayudó a mejorar la comprensión del mecanismo de relajación estructural a baja temperatura de tres líquidos de muy distinta naturaleza, lo cual presupone que dicho mecanismo podría ser entonces común a todos los líquidos.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;CienciaNet&lt;/strong&gt; consultó a Ezequiel Albano, de la Universidad de La Plata, quien fue jurado evaluador de la tesis. Ezequiel comenta que &amp;quot;La tesis de Ariel constituye un aporte teórico fundamental para el entendimiento de los complejos procesos de relajación dinámica que tienen lugar durante la formación de vidrios. Asimismo, los trabajos surgidos de la tesis han logrado una gran repecución internacional y motivado la realización de experimentos con el objeto de corroborar sus predicciones.&amp;quot;&lt;/p&gt;
&lt;p&gt;Esta tesis recibió el premio J.J. Giambiagi 2009 a la mejor tesis de física teórica, otorgado por la Asociación Física Argentina.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo Original:&lt;/strong&gt; &amp;quot;Dinámica de relajación de líquidos formadores de vidrios&amp;quot; presentada por Ariel Rodriguez Fris para optar por el grado de Doctor en la Universidad Nacional del Sur, bajo la dirección de Gustavo Appignanesi.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; Ariel Rodriguez Fris (E-mail: &lt;a href=&#34;rodriguezfris@plapiqui.edu.ar&#34;&gt;rodriguezfris@plapiqui.edu.ar&lt;/a&gt;).&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>La evolución de los lenguajes y la física contemporánea</title>
      <link>https://ciencianet.com.ar/post/la-evolucion-de-los-lenguajes-y-la-fisica-contemporanea/</link>
      <pubDate>Tue, 16 Feb 2010 20:55:51 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/la-evolucion-de-los-lenguajes-y-la-fisica-contemporanea/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Gabriel Baglietto.&lt;/strong&gt; Instituto de investigaciones Fisicoquímicas Teóricas y Aplicadas (CONICET, UNLP).&lt;/p&gt;
&lt;p&gt;La amplitud de los temas tratados por la física hoy en día es sorprendente. Además de la temática tradicionalmente asociada a esta disciplina, existe una gran variedad de artículos en revistas de física sobre economía, música, ecología, inteligencia artificial, demografía, redes sociales, etc. ¿Qué tipo de aporte puede realizar un físico en estos campos? En este artículo comentamos el trabajo realizado por un físico del Instituto Balseiro para entender la evolución de los idiomas.&lt;/p&gt;
&lt;p&gt;En 1889 el matemático francés Henry Poincaré descubrió y demostró que un sistema tan simple como el constituido por tres partículas que interactúan gravitatoriamente no podía resolverse exactamente. A partir de ese momento, muchos físicos comenzaron a interesarse en lo que hoy se denominan &lt;strong&gt;sistemas complejos&lt;/strong&gt;, sistemas en los que las interacciones entre sus partes hacen difícil, sino imposible, su resolución exacta.&lt;/p&gt;
&lt;p&gt;Así comenzó a generarse una gran cantidad de herramientas matemáticas y conceptuales para extraer información útil de problemas complicados. Ahora estas herramientas se están utilizando en muchos campos del conocimiento. Uno de ellos es el de la lingüística evolutiva, que se encarga del estudio de los orígenes y desarrollos de los diferentes idiomas. Una de las preguntas que trata de responder esta ciencia es mediante qué mecanismo se alcanzó la distribución de idiomas que existe en la actualidad. Se sabe que hay pocos idiomas hablados por muchas personas y muchos hablados por muy pocas. Más precisamente, la relación entre cuántos idiomas son hablados por una dada cantidad de personas sigue una &lt;a href=&#34;http://es.wikipedia.org/wiki/Distribuci%C3%B3n_log-normal&#34;&gt;distribución log-normal&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2010/02/evolucion-lenguas.preview-300x144.png&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;La mayoría de los modelos que se han propuesto para tratar de explicar por qué los idiomas se distribuyen de acuerdo con esta ley hacen mucho hincapié en los efectos de las mutaciones de varias características lingüísticas que dan origen al surgimiento de nuevos lenguajes y además consideran la posibilidad de la extinción de los lenguajes.&lt;/p&gt;
&lt;p&gt;Todo esto parece muy razonable. Sin embargo, los modelos propuestos hasta el presente no lograban reproducir fielmente la distribución real de lenguajes. Damián Zanette del Instituto Balseiro se dio cuenta de que “estos modelos no consideraban el hecho de que durante períodos de tiempo que son cortos comparados con las escalas típicas de la evolución de los lenguajes, los hablantes de una dada lengua pueden variar sustancialmente en número simplemente por los efectos de su propia dinámica poblacional” antes de que por interacciones o alteraciones en la lengua misma. “Por ejemplo, en los últimos cinco siglos –un período que incluye la devastadora invasión cultural de Europa al resto del globo- quizás el 50% de los lenguajes del mundo se extinguieron (entre ellos, dos tercios de los 2000 lenguajes nativos preexistentes en América) o cambiaron drásticamente.&lt;/p&gt;
&lt;p&gt;En el mismo período, sin embargo, la población del mundo creció doce veces o más.” O sea que si bien en este período resultó considerable la disminución en el número de lenguas debida a la interacción entre culturas, el efecto en el número de hablantes en las diferentes lenguas producido simplemente por el crecimiento poblacional fue mucho mayor.&lt;/p&gt;
&lt;p&gt;Sobre la base de esta observación construyó un modelo que logra explicar la distribución de lenguas existente tomando en cuenta sólo los efectos demográficos. Esto no significa que los otros factores no deban ser tenidos en cuenta, pero parece ser que hasta el momento, los modelos existentes habían menospreciado un factor determinante.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo Original&lt;/strong&gt;: D. H. Zanette, &amp;quot;Demographic growth and the distribution of language sizes&amp;quot;, International Journal of Modern Physics C, vol. 19, pp. 237–247 (2008). &lt;a href=&#34;http://arxiv.org/pdf/0710.1511&#34;&gt;http://arxiv.org/pdf/0710.1511&lt;/a&gt; Instituciones: Instituto Balseiro &lt;a href=&#34;http://www.ib.edu.ar/&#34;&gt;http://www.ib.edu.ar/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; Damián Zanette (E-mail: &lt;a href=&#34;mailto:zanette@cab.cnea.gov.ar&#34;&gt;zanette@cab.cnea.gov.ar&lt;/a&gt;)&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Líquido que desafía las reglas</title>
      <link>https://ciencianet.com.ar/post/liquido-que-desafia-las-reglas/</link>
      <pubDate>Mon, 30 Nov 2009 21:01:34 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/liquido-que-desafia-las-reglas/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro&lt;/strong&gt;. Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP) y Universidad Tecnológica Nacional.&lt;/p&gt;
&lt;p&gt;Físicos franceses descubrieron un líquido que se &amp;quot;congela&amp;quot; cuando se calienta. Marie Plazanet y colegas de la &lt;a href=&#34;http://www.ujf-grenoble.fr/36392593/0/fiche___pagelibre_accueil/&#34;&gt;Université Joseph Fourier&lt;/a&gt; y del &lt;a href=&#34;http://www.ill.eu/&#34;&gt;Institut Laue-Langevin&lt;/a&gt;, ubicados en Grenoble, encontraron un solución simple formada por dos compuestos orgánicos que se convierte en sólido cuando se calienta a temperaturas entre 45 y 75°C, y se convierte en líquido cuando se enfría nuevamente. El equipo sostiene que los responsables de este comportamiento novedoso son los &lt;a href=&#34;http://es.wikipedia.org/wiki/Puentes_de_hidr%C3%B3geno&#34;&gt;puentes de hidrógeno&lt;/a&gt; .&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/11/puentes.post_.jpg&#34; alt=&#34;Estructuras moleculares de αCD: Estructura de energía mínima con 12 puentes de hidrógeno intramolecular (izquierda) y otra con 4 puentes rotos (derecha).:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Por lo general, un sólido se funde cuando se calienta, y los líquidos se transforman en gas, aunque existen algunas excepciones cuando al calentar se producen algunas reacciones químicas que no son reversibles, tal como la &lt;a href=&#34;http://es.wikipedia.org/wiki/Polimerizaci%C3%B3n&#34;&gt;polimerización&lt;/a&gt;. Sin embargo, una transición reversible en la cual un líquido se solidifica al calentarse nunca había sido observada hasta ahora.&lt;/p&gt;
&lt;p&gt;Plazanet y sus colegas prepararon una solución líquida conteniendo α-ciclodextrina (αCD), agua y 4-metilpiridina (4MP). Las &lt;a href=&#34;http://es.wikipedia.org/wiki/Ciclodextrina&#34;&gt;ciclodextrinas&lt;/a&gt; son estructuras cíclicas que contienen grupos terminales hidroxilos que pueden formar puentes de hidrógeno con 4MP o moléculas de agua. A temperatura ambiente, hasta 300 gramos de αCD pueden disolverse en un litro de 4MP. La solución resultante es homogénea y transparente, pero se transforma en un sólido blanco lechoso cuando se calienta. La temperatura a la que cambia de fase disminuye a medida que la concentración de αCD aumenta.&lt;/p&gt;
&lt;p&gt;Estudios de dispersión de neutrones revelaron que la fase sólida es un sistema &amp;quot;sol-gel&amp;quot; en el que la formación de puentes de hidrógeno entre αCD y 4MP producen una estructura rígida ordenada. A menores temperaturas, sin embargo, los puentes de hidrógeno tienden a romperse lo que resulta en que la solución vuelve a la fase líquida. Simulaciones de &lt;a href=&#34;http://es.wikipedia.org/wiki/Din%C3%A1mica_molecular&#34;&gt;dinámica molecular&lt;/a&gt; realizadas por Plazenet y sus colaboradores confirmaron que los anillos de ciclodextrinas se distorsionan a medida que la temperatura se aproxima al punto de solidificación. Los puentes de hidrógeno internos en αCD se rompen y los grupos hidroxilos rotan hacia afuera, lo que permite la formación de una red de uniones entre diferentes moléculas. El equipo ha encontrado que un número de sistemas ciclodextrinas/piridinas también se solidifican al calentarse, y ahora se encuentra estudiando con más detalle el sistema sol-gel para entender mejor el mecanismo de solidificación.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; &lt;a href=&#34;http://www.ncbi.nlm.nih.gov/pubmed/15352791&#34;&gt;M Plazanet et al. 2004 J. Chem. Phys., 121 5031.&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Uno de los derechos humanos es conocer la composición de nuestros alimentos</title>
      <link>https://ciencianet.com.ar/post/uno-de-los-derechos-humanos-es-conocer-la-composicion-de-nuestros-alimentos/</link>
      <pubDate>Fri, 09 Oct 2009 21:03:55 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/uno-de-los-derechos-humanos-es-conocer-la-composicion-de-nuestros-alimentos/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Guillermo Bibiloni.&lt;/strong&gt; Departamento de Física, Facultad de Ciencias Exactas, UNLP.&lt;/p&gt;
&lt;p&gt;La radiación ambiental se origina a partir de un gran número de fuentes naturales y antropogénicas o creadas por el hombre. Los elementos radiactivos naturales que se hallan en el aire, el agua y el suelo y pueden clasificarse en: primordiales, creados antes de la formación de la Tierra, y cosmogénicos, formados como resultados de reacciones entre los núcleos de los gases que constituyen la atmósfera y los rayos cósmicos que inciden sobre ella. Ejemplos del primer tipo son el 40K, el 238U, el 235U y el 232Th y sus hijos, productos de los procesos de decaimiento. Por otro lado, el 7Be, 14C y el 3H, son ejemplos de radioisótopos de origen cosmogénico.&lt;/p&gt;
&lt;h3 id=&#34;vivimos-en-un-mundo-radioactivo-y-también-lo-son-nuestros-alimentos&#34;&gt;Vivimos en un mundo radioactivo y también lo son nuestros alimentos&lt;/h3&gt;
&lt;p&gt;La radioactividad de la corteza terrestre es principalmente debida a la presencia de 40K y a los que se suman los radionucleídos de las tres series naturales (238U, 235U y 232Th). Estos nucleidos están en todo tipo de suelos y aguas. Los niveles específicos de radiación terrestre ambiental están relacionados con la composición geológica de cada área separada litológicamente y al contenido natural de radionucleídos en rocas a partir de los cuales los suelos fueron originados. De modo que, diferencias sobresalientes en la radioactividad natural de muestras ambientales, tales como agua y suelos, existen en relación al origen geológico de los suelos.&lt;/p&gt;
&lt;p&gt;La distribución de equilibrio de los isótopos naturales puede ser perturbada cuando materiales naturales que contienen isótopos radioactivos son utilizados como materia prima por diversas industrias convencionales (concentración o refinamiento de minerales, petróleo y agua para aplicaciones, centrales de carbón, etc.), o nucleares. De esta manera, los elementos radioactivos presentes, tales como Ra, U y Th pueden ser concentrados en los productos, subproductos, residuos y efluentes debido a la actividad humana. Así, los efluentes y residuos modificarían el nivel de actividad por sobre los valores naturales.&lt;/p&gt;
&lt;p&gt;El advenimiento de la era nuclear, a mediados del siglo pasado, trajo aparejada la producción de nuevos radioisótopos, ahora creados por el hombre, como por ejemplo el 137Cs y el 90Sr. Estos radioisótopos antropogénicos agregaron globalmente cantidades pequeñas al inventario de radiactividad. Cuando tienen lugar ensayos de armas nucleares o accidentes vinculados a la industria nuclear, enormes cantidades de partículas radiactivas son liberadas en la atmósfera, las cuales, arrastradas por los vientos, recorren largas distancias para luego precipitar y diseminarse sobre grandes extensiones de la Tierra.&lt;/p&gt;
&lt;p&gt;Este fenómeno, denominado lluvia radioactiva, produce el depósito de elementos radiactivos sobre los pastos, las hojas de las plantas, los suelos, las aguas y los sedimentos. Esta contaminación fue objeto de interés internacional, lo que devino en una variedad de mediciones de los niveles de radiactividad depositados sobre la superficie de la Tierra. Vivimos entonces en un mundo radioactivo en el cual los niveles de radiación natural son diferentes, en general, en cada región del planeta. Los seres vivos también son radioactivos ya que el aire que respiran y los alimentos que consumen contienen radionucleídos los cuales quedan alojados en los tejidos.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/10/vaca.jpg&#34; alt=&#34;:left&#34; title=&#34;vaca&#34;&gt; &lt;strong&gt;Nuestros alimentos son radioactivos pero, ¿están contaminados radioactivamente?&lt;/strong&gt; ¿Cómo saber, entonces, si un alimento, hoy y aquí, tiene niveles de nucleídos por encima de los niveles naturales o si contiene algún isótopo antropogénico? Lo ideal es determinar los constituyentes radiactivos en condiciones “normales” para determinar la línea de base y disponer de “un observatorio de control” capaz de comparar estos resultados. En el Grupo de Investigación y Servicios de Radioactividad Natural y Medio Ambiente, Departamento de Física-Facultad de Ciencias Exactas-UNLP, IFLP-CONICET (GISDRAMA) se llevan adelante un conjunto de proyectos destinados a la construcción de estas líneas de base. En particular, los autores del trabajo que aquí comento, &lt;em&gt;Activity levels of gamma-emitters in Argentinean cow milk&lt;/em&gt; (&lt;em&gt;Journal of Food Composition and Analysis, Volume 22, Issue 3, May 2009, Pages 250-253&lt;/em&gt;), pretende justamente esto.&lt;/p&gt;
&lt;p&gt;Los autores, integrantes del GISDRAMA, hacen un relevamiento de la radioactividad presente en leche vacuna de las diferentes cuencas lecheras de la Argentina y los registran para establecer niveles de base. Como dato interesante estudian también muestras de leche de nuestros vecinos, Chile y Uruguay. Para ello determinaron sistemáticamente la concentración de núcleos emisores de radiación electromagnética (radiación gamma) en más de treinta muestras de leche tomadas entre los años 2000 y 2007 de diferentes cuencas lecheras. Estos experimentos se llevaron a cabo utilizando un espectrómetro de radiación gamma de alta resolución.&lt;/p&gt;
&lt;p&gt;Los resultados mostraron que en condiciones normales, las leches argentinas contienen en promedio 60 Bq/l del nucleído natural 40K. La dosis efectiva comprometida, determinada considerando el consumo anual de leche en Argentina (219 l por persona) junto con el coeficiente de dosis y la pirámide poblacional, indican que los valores de dosis efectiva es cercana a los valores recomendados por el Comité Internacional de Radioprotección y la Comité Científico sobre los Efectos de Radiación Atómica de las Naciones Unidas. Por otra parte, las actividades de los elementos de los nucleídos pertenecientes a las cadenas naturales del U y del Th están por debajo del límite de detección (2 Bq/l y 1 Bq/l, respectivamente).&lt;/p&gt;
&lt;p&gt;Con referencia a los nucleídos antropogénicos, sólo se detectaron niveles de actividad de 137Cs por encima del límite de detección (0.06 Bq/l) en la muestra de leche chilena. Estos valores son compatibles con la lluvia radioactiva proveniente de los ensayos nucleares realizados en el Pacífico Sur durante décadas pasadas.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;¿Cómo seguir adelante y detectar eventuales contaminaciones radioactivas?&lt;/strong&gt; Pero como siempre, detrás de una publicación hay una idea rectora. Y que mejor que transcribir palabras de la propia Judith Desimoni y sus colaboradores: “La radioactividad no se detecta por ninguno de los cinco sentidos humanos pero es omnipresente. Sin embargo, la sola mención de la palabra radioactividad genera temor en la población debido a la mala prensa que se ha ganado por los accidentes en reactores nucleares y a las explosiones de Nagasaki e Hiroshima, por ejemplo&amp;quot;.&lt;/p&gt;
&lt;p&gt;&amp;quot;Entonces, cómo convivir sin temor con la producción de energía nucleoeléctrica, las aplicaciones industriales y médicas de los nucleídos y el Plan Estratégico Nuclear de nuestro país? Por un lado, alfabetizando científicamente a la población y por otro haciendo controles medioambientales y alimenticios sistemáticos y periódicos. El primer punto es un trabajo que conecta a los docentes-investigadores y la sociedad mediante el mejoramiento del conocimiento de la población de los efectos de la exposición a bajas dosis de radiaciones ionizantes. En efecto, nuestro laboratorio es un &lt;strong&gt;&#39;laboratorio de puertas abiertas&#39;&lt;/strong&gt; dedicado a la recepción de alumnos y docentes de escuelas, y de las distintas carreras de grado y postgrado de la UNLP, organizaciones barriales y no gubernamentales. Estos actores son utilizados como multiplicadores de la información. En lo que respecta al monitoreo, tratamos de contribuir mediante el desarrollo de programas de control gerenciados por laboratorios constituidos en universidades nacionales, tal como en Europa. Nos enorgullece afirmar que somos el primer laboratorio universitario con estos objetivos del país! Pero obviamente somos conscientes de que nuestra contribución es sólo un granito de arena en una playa.&amp;quot;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Artículo original:&lt;/strong&gt; J. Desimoni, F. Sives, L. Errico, G. Mastrantonio, M.A. Taylor, &lt;a href=&#34;https://doi.org/10.1016/j.jfca.2008.10.024&#34;&gt;Activity levels of gamma-emitters in Argentinean cow milk&lt;/a&gt;, Journal of Food Composition and Analysis, vol 22, pp 250-253&lt;/p&gt;
&lt;p&gt;**Instituciones:**Departamento de Física, Facultad de Ciencias Exactas, Universidad Nacional de La Plata, Instituto de Física La Plata (CONICET), LaSeISiC, Facultad de Ciencias Exactas, Universidad Nacional de La Plata (CIC–CONICET), Toxicología de Alimentos, Departamento de Química, Universidad Nacional de La Pampa.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; Leo Errico (E-mail: &lt;a href=&#34;mailto:errico@fisica.unlp.edu.ar&#34;&gt;errico@fisica.unlp.edu.ar&lt;/a&gt;), Guillermo Bibiloni (E-mail: &lt;a href=&#34;mailto:bibiloni@fisica.unlp.edu.ar&#34;&gt;bibiloni@fisica.unlp.edu.ar&lt;/a&gt;)&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Monopolos magnéticos y cuerdas de Dirac en un material</title>
      <link>https://ciencianet.com.ar/post/monopolos-magneticos-y-cuerdas-de-dirac-en-un-material/</link>
      <pubDate>Sun, 13 Sep 2009 21:05:17 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/monopolos-magneticos-y-cuerdas-de-dirac-en-un-material/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro&lt;/strong&gt;, Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP) y Universidad Tecnológica Nacional.&lt;/p&gt;
&lt;p&gt;Cualquiera que ha jugado con un imán sabe que tiene dos polos, norte y sur, y que no importa en cuantos pedazos lo rompa, cada uno de ellos también tendrá un polo norte y uno sur. Todos los imanes que conocemos –todas las formas de magnetismo que conocemos- están basadas en dipolos magnéticos, es decir, en componentes elementales que tienen los dos polos. Sin embargo, sabemos que para el caso de la electricidad hay cargas positivas y negativas (protones y electrones por ejemplo) que podemos encontrar en forma completamente separada.&lt;/p&gt;
&lt;p&gt;Esta asimetría –la aparente inexistencia de cargas magnéticas separadas, “norte” o “sur” (mono-polos en lugar de di-polos)- ha sido un interrogante para la física moderna. Varias teorías predicen partículas elementales con carga magnética única, los monopolos magnéticos. En 1931, el físico Paul Dirac llego a la conclusión de que los monopolos magnéticos elementales deben existir, y los describió asociados a una líneas solenoidales, las cuerdas de Dirac, que llevan flujo magnético. Estas cuerdas se extienden al infinito, o conectan dos monopolos de carga opuesta.&lt;/p&gt;
&lt;p&gt;En forma más reciente, en un trabajo teórico liderado por Roderich Moessner, de Dresden (Alemania), se propuso la aparición de monopolos como excitaciones colectivas en una clase de sistemas magnéticos frustrados, los llamados hielos de spin. Estos monopolos “emergentes” difieren de los elementales de Dirac en que no tienen existencia fuera del material magnético, en que sus cargas son menores que las de Dirac y en que sus cuerdas asociadas son observables. Hasta ahora no existía ninguna evidencia experimental reproducible de la existencia de monopolos, ni en forma elemental, ni como partículas emergentes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/09/monopolosM.jpg&#34; alt=&#34;:left&#34;&gt;
El experimento liderado por Tennant y Grigera se basa en el estudio por distintas técnicas de un material, el titanato de disprosio, que a temperaturas por debajo de 1 K (aproximadamente -272 °C) se comporta como un hielo de spin. Usando la técnica de difracción de neutrones, se determinó que los momentos magnéticos dipolares de este material se reorganizan formando una especie de spaghetti magnético (ver figura). El nombre viene de la manera en la que los dipolos se ordenan formando tubos contorsionados por las que se transporta flujo magnético (las cuerdas de Dirac).&lt;/p&gt;
&lt;p&gt;Los neutrones tienen momento magnético, y por lo tanto sufren la influencia de campos magnéticos externos. Si un haz de neutrones atraviesa un material magnético, los neutrones sufren distintas deflexiones dependiendo de los campos magnéticos que encuentran en su camino. De las características del haz resultante, y mirando varios haces apuntados en distintas direcciones, se puede reconstruir la distribución de campo magnético dentro del material (ver figura). De esta manera, analizando en pantallas detectoras los neutrones inyectados a través de titanato de disprosio a temperaturas debajo de 1 K, este grupo encontró evidencia de la existencia de las cuerdas de campo magnético. La aplicación de un campo magnético externo permitió “peinar” estas cuerdas, estirándolas en una dirección. De esta manera es posible reducir su densidad y promover la disociación de los monopolos que existen en sus puntas. Como resultado, fue posible observar monopolos magnéticos unidos por cuerdas de flujo.&lt;/p&gt;
&lt;p&gt;Otras características de este gas de monopolos fueron observadas con medidas de magnetización y calor específico. Estas proveyeron confirmación de la existencia de los monopolos y mostraron que interactúan en una manera similar a las cargas eléctricas (un equivalente a la ley de Coulomb). Hay aspectos del problema que van más allá de la ciencia básica. Como explica Jonathan Morris, uno de los miembros del grupo de Berlin, y primer autor en el trabajo: &amp;quot;Estamos escribiendo sobre propiedades nuevas y fundamentales de la materia. Estas propiedades son válidas en general para materiales de la misma topología, es decir, para momentos magnéticos en una red de pirocloro. Esto puede tener implicancias importantes en el desarrollo de tecnologías magnéticas. Sobre todo, para nosotros, significa que por primera vez hemos observado fraccionalización en tres dimensiones&amp;quot;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; &lt;a href=&#34;https://doi.org/10.1126/science.1178868&#34;&gt;Science&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; S. A. Grigera, &lt;a href=&#34;mailto:sag@iflysib.unlp.edu.ar&#34;&gt;sag@iflysib.unlp.edu.ar&lt;/a&gt; Instituto de Física de Líquidos y Sistemas Biológicos, La Plata, +54 (0)221 4233283.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>La música de las esferas</title>
      <link>https://ciencianet.com.ar/post/la-musica-de-las-esferas/</link>
      <pubDate>Fri, 28 Aug 2009 21:08:39 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/la-musica-de-las-esferas/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Nicolás Grandi:&lt;/strong&gt; Instituto de Física La Plata.&lt;/p&gt;
&lt;p&gt;Texto publicado en el libro &lt;strong&gt;&amp;quot;Cero absoluto- Curiosidades de Física&amp;quot;&lt;/strong&gt;, escrito por docentes del Museo de Física UNLP (Editorial IFLP - Conicet, 2005). Publicado con autorización de la Editorial.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/08/MUSICA_LLORET-355x500.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Es posible que el origen del pensamiento científico se encuentre en la visión del cielo nocturno. La perturbación causada por el movimiento de los planetas a través de la de otro modo inmutable belleza de la Vía Láctea, provocó en los antiguos observadores la necesidad de encontrar alguna regularidad o “ley” que ordenara esos movimientos. Esto llevo a &lt;a href=&#34;http://es.wikipedia.org/wiki/Pit%C3%A1goras&#34;&gt;Pitágoras&lt;/a&gt; y sus seguidores en la antigua Grecia a la idea de la “Música de las Esferas”, una visión del mundo que equiparaba al Universo con una melodía tocada por los cuerpos celestes, a cuyas “notas” obedecía todo lo existente, desde las estrellas y los planetas hasta la más humilde de las gotas de lluvia.&lt;/p&gt;
&lt;p&gt;Podríamos decir que los Pitagóricos buscaron en los cielos las reglas que regían el mundo cotidiano. Sin embargo, la posterior comprensión de las leyes del movimiento de los objetos, desarrollada por &lt;a href=&#34;http://es.wikipedia.org/wiki/Galileo_Galilei&#34;&gt;Galileo&lt;/a&gt; y luego por &lt;a href=&#34;http://es.wikipedia.org/wiki/Isaac_Newton&#34;&gt;Newton&lt;/a&gt;, y su aplicación a los cuerpos celestes, dejaron la idea de Pitágoras en el olvido. Así surgió una visión completamente opuesta, donde son los constituyentes elementales o “más pequeños” los que siguen determinadas leyes sencillas, cuyas consecuencias afectan al universo todo. En esta visión, el Universo se parece a un edificio, a cuyos “ladrillos” llamamos partículas elementales y cuya “arquitectura” sigue reglas bien determinadas.&lt;/p&gt;
&lt;p&gt;De esta manera, la explicación de la complejidad de la química mediante constituyentes en movimiento dio sustento a la antigua idea de átomo. Más adelante, la comprensión de la estructura y variedad de los átomos mismos llevó a la introducción de los neutrones, protones y electrones, los cuales combinados en distintas formas y números forman los diversos elementos. Dando “un paso más”, la estructura de protones y neutrones se explicó en términos de partículas aún más elementales: quarks, gluones, etc.&lt;/p&gt;
&lt;p&gt;La riqueza de este “edificio” es intrigante. La pregunta de por qué existen tantos tipos diferentes de “ladrillos” y qué es finalmente lo que determina las reglas adecuadas para “apilarlos” no parece tener respuesta. Es por esto que en las últimas décadas una idea innovadora ha ganado lugar en el pensamiento científico. Según esta nueva visión, los constituyentes últimos o “más elementales” del universo, serían microscópicas “cuerdas” idénticas, y la única manera de construir cosas con ellas sería “cortarlas y unirlas por sus extremos”.&lt;/p&gt;
&lt;p&gt;Sorprendentemente, en contra de lo que se podría imaginar, este esquema tan simple permite explicar y comprender en profundidad una gran cantidad de fenómenos, desde las interacciones nucleares hasta la gravedad. Este escenario plantea la siguiente pregunta: si nos hemos convencido de que el mundo está hecho de un enorme zoológico de partículas elementales diferentes, ¿Cómo podríamos construirlas con un solo tipo de cuerdas? La respuesta es tan simple como sugestiva: cada tipo de partícula elemental no sería más que cuerdas idénticas a las otras, pero vibrando en un tono diferente. De este modo cada “nota” corresponde a una de las especies de partículas que constituyen el mundo, desde los electrones y quarks, pasando por los fotones que constituyen la luz, hasta los gravitones que guían el movimiento de la Vía Láctea.&lt;/p&gt;
&lt;p&gt;De este modo, la idea Pitagórica de Música de las Esferas parece haber retornado de manera inesperada.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Todo lo que Ud. siempre quiso saber sobre el Dengue y no se animaron a contarle</title>
      <link>https://ciencianet.com.ar/post/todo-lo-que-ud-siempre-quiso-saber-sobre-el-dengue-y-no-se-animaron-a-contarle/</link>
      <pubDate>Tue, 31 Mar 2009 21:40:00 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/todo-lo-que-ud-siempre-quiso-saber-sobre-el-dengue-y-no-se-animaron-a-contarle/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero:&lt;/strong&gt; Departamento de Física, Facultad de Ciencias Exactas y Naturales, Universidad de Buenos Aires.&lt;/p&gt;
&lt;p&gt;El físico Hernán Solari, investigador del Conicet en el Departamento de Física de la Facultad de Ciencias Exactas y Naturales (UBA) y autor de varios artículos sobre epidemiología matemática -en particular sobre modelos de propagación del mosquito &lt;em&gt;Aedes aegypti&lt;/em&gt; y de fiebre amarilla y dengue, elaborados junto al Grupo de Estudio de Mosquitos-, responde aquí una serie de preguntas al respecto.&lt;/p&gt;
&lt;h3 id=&#34;sobre-el-mosquito&#34;&gt;Sobre el mosquito&lt;/h3&gt;
&lt;p&gt;- &lt;strong&gt;¿Quién es el transmisor del Dengue?&lt;/strong&gt; - &lt;strong&gt;H.S.&lt;/strong&gt;: El dengue pasa del hombre al mosquito; en este último el virus que causa la enfermedad se reproduce sin afectarlo. Pasado el período de reproducción el mosquito puede infectar a los humanos que pica. Esto es lo que diferencia a un transmisor mecánico -como podría ser cualquier mosquito no portador o un tábano o una jeringa- de un vector, donde el virus se reproduce. El resultado es que solo unos pocos mosquitos tienen esta capacidad. De entre ellos, &lt;em&gt;Aedes aegypti&lt;/em&gt;, por su carácter urbano es el vector más eficiente. En la misma familia &lt;em&gt;Aedes albopictus&lt;/em&gt; también transmite la enfermedad y está presente en paisajes más agrestes. Y ya está invadiendo el norte del país. Al menos en Misiones se lo ha detectado.&lt;/p&gt;
&lt;p&gt;- &lt;strong&gt;¿Qué hábitos tiene el vector? ¿Dónde vive? ¿Cuándo pica?&lt;/strong&gt; - &lt;strong&gt;H.S.&lt;/strong&gt;: &lt;em&gt;Aedes aegypti&lt;/em&gt;, como todo mosquito, pasa por tres fases que se desarrollan en la cercanía del agua y una fase de adulto en la que vuela. Los huevos son puestos en las paredes de receptáculos con agua clara, particularmente donde hay levaduras producto de la descomposición de vegetales. Estas levaduras suelen dar un tinte amarillento al agua. Por sobre el nivel de agua se depositan los huevos y cuando estos se sumergen eclosionan las larvas. Más tarde se forma la pupa o crisálida y finalmente emerge el adulto. Todo el ciclo de vida gira alrededor de este ambiente.&lt;/p&gt;
&lt;p&gt;El adulto no resiste altas temperaturas por lo que se refugia del sol a la sombra de las plantas y prefiere lugares húmedos. Suele vivir en el interior de las casas. En épocas de grandes epidemias los pobladores comentaban que al abrir cajones salían nubes de mosquitos. Su característica es picar al amanecer y al atardecer. Los pulmones de manzana suelen ser los mejores lugares para buscarlos, pero un clásico es encontrarlos compartiendo el hábitat con plantas de potus mantenidas en agua.&lt;/p&gt;
&lt;p&gt;- &lt;strong&gt;¿Cómo es posible reconocerlo?&lt;/strong&gt; - &lt;strong&gt;H.S.&lt;/strong&gt;: Si uno tiene buena vista, puede observar las rayas blancas en las patas del adulto que son características. También tiene escamas plateadas en su costado. Las larvas son muy nerviosas y se esconden en el fondo del recipiente al detectar nuestra presencia, aunque finalmente tienen que subir a respirar. Como mencionamos, sus movimientos son característicamente nerviosos y su tamaño no es muy grande, digamos de 3 milímetros. En la zona de Ensenada (Punta Lara) suelen verse mosquitos con marcas blancas en sus patas y de gran tamaño, pero pertenecen a otra especie.&lt;/p&gt;
&lt;p&gt;- &lt;strong&gt;¿Cuán infalible es la transmisión de la enfermedad?&lt;/strong&gt; - &lt;strong&gt;H.S.&lt;/strong&gt;: Hay muchos factores a tener en cuenta. Tres de cada cuatro mosquitos que pican a un humano en período de viremia se infectan, y tres de cada cuatro veces que un mosquito con virus ya maduro pica a un adulto le transmite la enfermedad. El mosquito sigue siendo contagioso y puede picar a otra persona, aunque también se puede morir antes de ser contagioso. Todo es falible, en todo hay elementos que podríamos considerar azarosos. ¿Cómo se explica entonces que con la abundancia que actualmente existe del mosquito lleguen casos de dengue importados y no se inicien epidemias? El problema consiste en que los síntomas clínicos se dan al final del período contagioso. Durante unos días (por dar un valor, digamos 5) luego de ser infectada la persona, el virus se multiplica en el huésped (humano), y a este período le siguen unos 3 días donde se da la mayor probabilidad de contagiar a un mosquito, apareciendo los síntomas clínicos sobre el final.&lt;/p&gt;
&lt;p&gt;Hay personas que tienen solo molestias leves, son casos subclínicos. Un 60% o más de las infecciones son subclínicas, no se reportan. Cuando un caso evoluciona clínicamente a formas graves de la enfermedad, esas formas graves se dan después del período en que es contagioso. Ocurre exactamente igual con la fiebre amarilla. El virus de la fiebre amarilla es transmitido por los mismos mosquitos y pertenece a la misma familia -flavivirus- que el del dengue. Lo que históricamente se ha reconocido como fiebre amarilla, son solo el 15% de los casos. El resultado es que cuando nos alarmamos y tomamos medidas, la epidemia ya está en marcha silenciosamente.&lt;/p&gt;
&lt;p&gt;Por el momento no existe una transmisión vertical del virus en el mosquito (de madres a hijos -huevos-), de existir, de producirse una mutación del virus en ese sentido, entonces se estaría en condiciones de catástrofe: cada hembra pone unos 60 huevos con un período que en verano puede llegar a ser tan corto como 4 días. Este es el peligro global del dengue, con la expansión permanente de los vectores y detrás de ellos la enfermedad.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/03/DENGUE_03-20-03-2007-768x622.jpg&#34; alt=&#34;Prevención: Folleto del Gobierno de la Provincia de Corrientes.&#34;&gt;&lt;/p&gt;
&lt;p&gt;- &lt;strong&gt;¿Es útil la fumigación?&lt;/strong&gt; - &lt;strong&gt;H.S.&lt;/strong&gt;: La fumigación es un recurso extremadamente limitado en la lucha contra este mosquito. Hay muchos problemas, como por ejemplo acceder a donde está el adulto (interior de las casas y debajo de las plantas) ya que se encuentra protegido de la niebla del fumigador. El tamaño de la gota es crítico para la efectividad del veneno y los aparatos de fumigación deben estar calibrados. Se elimina solo parte de los adultos, pero cada hembra deposita unos 60 huevos por oviposición, lo que le da un potencial reproductivo formidable. Si uno observa la evolución de la epidemia de Tartagal (2004), donde se fumigaba 400 metros alrededor de cada caso de dengue detectado, se ve que la política de fumigación no pudo impedir la formación de focos, es decir, la propagación de la epidemia por el mosquito.&lt;/p&gt;
&lt;p&gt;- &lt;strong&gt;¿Cómo controlarlos?&lt;/strong&gt; - &lt;strong&gt;H.S.&lt;/strong&gt;: Lo más sensato es revisar la casa a conciencia, buscando recipientes abandonados que acumulen agua donde se puedan criar los mosquitos. Cubiertas de auto en desuso están entre sus favoritos, acumulaciones de agua en los desagües pluviales, frascos con agua, aun en la cocina o interior de la casa, bebederos de los animales y un larguísimo etcétera. La ventaja adicional del método es que cuando, después de tomar sangre, la hembra se dispone a poner los huevos, busca sitios de cría y al no encontrarlos, finalmente emigra. Por eso es que la eliminación de criaderos debe realizarse preventivamente, antes de que se produzca un foco epidémico en el lugar. Comenzar a eliminar criaderos donde se da el foco es una medida controvertida, ya que promueve que la enfermedad se traslade a zonas vecinas.&lt;/p&gt;
&lt;h3 id=&#34;sobre-la-enfermedad&#34;&gt;Sobre la enfermedad&lt;/h3&gt;
&lt;p&gt;- &lt;strong&gt;¿Qué tipos de Dengue hay?&lt;/strong&gt; - &lt;strong&gt;H.S.&lt;/strong&gt;: Se suele hablar de dengue hemorrágico y dengue común. Ocurre que hay cuatro cepas virales de dengue, nombradas DEN1 a DEN4. Si nunca tuvimos dengue, cualquiera de estas cepas nos produce la enfermedad. Si tuvimos dengue, tendremos anticuerpos para esa cepa, digamos DEN1, pero la infección de otra cepa nos producirá un cuadro caracterizados por hemorragias. Este cuadro, cuando no es detectado a tiempo o no es atendido adecuadamente, es el que produce casi toda la mortalidad. En el año 2008, alguna cepa circulante por Paraguay era capaz de producir hemorragias y cuadros severos en la primera infección.&lt;/p&gt;
&lt;p&gt;- &lt;strong&gt;¿Cuál es el diagnóstico diferencial? ¿Cómo distinguir entre dengue y gripe?&lt;/strong&gt; - &lt;strong&gt;H.S.&lt;/strong&gt;: No soy médico, pero sí he tenido oportunidad de escuchar a los expertos del Hospital Muñiz explicándoselo a médicos de Capital Federal. En rasgos generales la cuestión es así: Los síntomas iniciales son similares a los de la gripe, como dolor de cabeza, fiebre alta y náuseas. Suele aparecer un dolor característico “detrás” del ojo. El dolor en las articulaciones suele ser intenso, por lo que a la enfermedad se la conoce como “quebranta huesos”.&lt;/p&gt;
&lt;p&gt;La mayor diferencia consiste en que el dengue no produce desarrollo de mucosidad. Por eso, se recomienda a los médicos, que ante un cuadro compatible con dengue, se indique al paciente que regrese a la consulta para un control. En caso de confirmarse los síntomas de dengue, así se puede actuar a tiempo. A los fines de controlar la epidemia, yo agrego que todo caso tipo gripe debe ser tratado como un caso de dengue. Es decir, debemos extremar las medidas como el uso de tules y mosquiteros (práctica hospitalaria) y repelentes (espirales, vaporizadores eléctricos, etcétera) para evitar que el mosquito sano pique al enfermo de dengue. De esta forma se corta el ciclo de la epidemia. Esperar a la confirmación del caso es actuar tarde. No sirve.&lt;/p&gt;
&lt;p&gt;- &lt;strong&gt;¿Y entre dengue y fiebre amarilla?&lt;/strong&gt; - &lt;strong&gt;H.S.&lt;/strong&gt;: Como ya dije, dengue y fiebre amarilla son extremadamente parecidos en su forma de transmisión (mismos vectores, mismo sistema) y los virus pertenecen a la misma familia. La evolución clínica inicial es similar en ambos casos, los tests de laboratorios más accesibles como el llamado ELISA (realizados ya por centros especializados), solo confirman flavivirus y no distinguen entre ellos. Hay tests específicos que solo se realizan en el Instituto Maiztegui de Pergamino, que ante una epidemia como la presente esencialmente colapsa.&lt;/p&gt;
&lt;p&gt;- &lt;strong&gt;¿Existen vacunas?&lt;/strong&gt; - &lt;strong&gt;H.S.&lt;/strong&gt;: Para el dengue no hay vacunas, uno enfrenta un virus complicado para el sistema inmune. El dengue hemorrágico es una clara manifestación de que el pensamiento más simplista respecto de generar anticuerpos al tener la enfermedad, no es adecuado.&lt;/p&gt;
&lt;p&gt;- &lt;strong&gt;¿Qué hacer ante la sospecha de la enfermedad?&lt;/strong&gt; - &lt;strong&gt;H.S.&lt;/strong&gt;: Ante un cuadro clínico compatible se debe ir al médico. No se debe tomar aspirina, pues es anticoagulante y puede agravar el cuadro, otros antipiréticos no tienen este problema. Concurrir al médico obviamente. El médico deberá hacer un seguimiento del cuadro para poder distinguirlo de otras enfermedades de igual comienzo, no solo cuadros gripales, es común confundirlo con la leptospirosis, por ejemplo. El caso del paciente fallecido de fiebre amarilla en Misiones (2008) se trató de un diagnóstico equivocado de leptospirosis. El paciente fue dado de alta y en su casa, sin asistencia médica, se desarrolló el cuadro tóxico de la fiebre amarilla. La hija del muerto, también diagnosticada con leptospirosis fue re-examinada y se confirmó fiebre amarilla. El caso no solo ilustra el problema del diagnóstico, sino cómo el contagio se suele dar en núcleos familiares que “comparten mosquitos”.&lt;/p&gt;
&lt;p&gt;- &lt;strong&gt;¿Cuál es el tratamiento?&lt;/strong&gt; - &lt;strong&gt;H.S.&lt;/strong&gt;: No hay más tratamiento que el control de los síntomas, la estabilización del paciente (en caso de que no esté estabilizado) y, en el caso de dengue hemorrágico, la internación y la hidratación. Los médicos del Muñiz a cargo de enfermedades tropicales enfatizaban que la mortalidad en dengue hemorrágico se controla manteniendo al paciente hidratado, entiendo que por vía intravenosa. Habría que preguntarle a los expertos.&lt;/p&gt;
&lt;p&gt;- &lt;strong&gt;¿Cuál es la mortalidad?&lt;/strong&gt; - &lt;strong&gt;H.S.&lt;/strong&gt;: 5 por 1.000 de los casos de dengue diagnosticados clínicamente se convierten en cuadros graves, y aproximadamente la mitad de los graves en mortales. Pero estadísticas como estas no son confiables, por lo general existen problemas metodológicos al realizarlas. La estadística es un arte muy difícil y suele haber mucho de arbitrario y antojadizo en la interpretación, aun en publicaciones reputadas como científicas.&lt;/p&gt;
&lt;p&gt;- &lt;strong&gt;¿Quedan secuelas?&lt;/strong&gt; - &lt;strong&gt;H.S.&lt;/strong&gt;: No, que yo sepa, tanto de fiebre amarilla como de dengue, la recuperación es total.&lt;/p&gt;
&lt;p&gt;- &lt;strong&gt;¿Es el dengue una enfermedad de los pobres?&lt;/strong&gt; - &lt;strong&gt;H.S.&lt;/strong&gt;: Este es un concepto peligroso. La falta de medios suele exponernos más a todas las enfermedades, ¿qué no decir de una mala alimentación o un sistema inmunológico deprimido? Las condiciones de vida que impone la falta de recursos siempre son negativas. Pero el dengue no es una enfermedad exclusiva de los pobres. Imaginemos un escenario posible. Juan y su familia viven en una casa quinta o en un barrio privado. Este año decidieron tomar las vacaciones en Brasil, donde existe una importante epidemia de dengue que involucra la ciudad costera que ellos visitarán, “naturalmente” nadie les avisó. Para el final de la estadía Juan es picado por un mosquito infectado y regresa a su casa incubando el dengue. En la casa, Juan tiene unos hermosos potus con agua, unos desagües pluviales excelentes, etc. Todo esta pulcro y cuidado. Hay algunos mosquitos, pero ¿quién puede librarse de ellos? Molestan al amanecer y al atardecer, pero nada del otro mundo ¿quién no soporta entre 3 y 5 picaduras por día?&lt;/p&gt;
&lt;p&gt;El no imagina que tiene una probabilidad mayor al 98% de infectar esos mosquitos y una probabilidad cercana al 90% de que alguien se contagie dengue en su casa. Sin embargo, en esos potus, en esos desagües construidos con criterio hidráulico pero sin criterio epidemiológico Juan cría &lt;em&gt;Aedes aegypti&lt;/em&gt;, en número suficiente como para iniciar una epidemia o un brote desde su propio hogar. Juan no hizo nada por librar su casa de criaderos de mosquitos, porque al final, el dengue es una enfermedad de los pobres, ¿no? Y Juan sabe bien que él no es pobre.&lt;/p&gt;
&lt;p&gt;- &lt;strong&gt;¿Qué condiciones deben darse para que se desencadene una epidemia?&lt;/strong&gt; - &lt;strong&gt;H.S.&lt;/strong&gt;: Las condiciones básicas son la presencia de mosquitos &lt;em&gt;Aedes aegypti&lt;/em&gt; en forma adulta (durante el invierno no están más que en la forma de huevos), el arribo de un enfermo contagioso que sea picado por ellos y un poco de mala suerte. Cuando hay una epidemia cercana, como ocurría en el norte del país con el dengue circulando por Bolivia, Paraguay y Brasil, donde el mosquito se encuentra establecido, la única barrera que quedaba era la suerte. El dengue prueba y prueba. ¿Hoy no prendió? Mala suerte, mañana mandamos otro infectado y así hasta que prende. Las comunicaciones internas, la rapidez de los viajes en estos tiempos, la presencia del mosquito en la zona de La Plata, y el Gran Buenos Aires, por mencionar solo dos regiones, la extensión de la temporada cálida, el aparente fin de la sequía, son todos factores que hacen probable o más probable un brote de dengue en la región.&lt;/p&gt;
&lt;p&gt;A favor de la salud juega en general el invierno, ya que al bajar las temperaturas disminuye la velocidad de propagación de la enfermedad y la presencia de mosquitos adultos. Como la evolución de la epidemia toma tiempo, las probabilidades de una importante epidemia en la zona este año son bajas. Lo que hagamos nosotros destruyendo sitios de cría ayudará a que bajen aún más y se mantengan así el próximo verano. Donde el acceso al agua es difícil (no hay agua corriente) se deben modificar las costumbres de almacenar agua de lluvia, pues los depósitos suelen ser enormes criaderos de estos mosquitos.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contra el dengue, lo único que funciona es la prevención con participación comunitaria. El mosquito que me transmitirá el mal, si eso ocurre, casi con seguridad se crió en mi casa o en la de mis vecinos.&lt;/strong&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>BNCT: Neutrones, Aceleradores y Oncología</title>
      <link>https://ciencianet.com.ar/post/bnct-neutrones-aceleradores-y-oncologia/</link>
      <pubDate>Sun, 15 Mar 2009 21:44:21 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/bnct-neutrones-aceleradores-y-oncologia/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Diego Vallejo&lt;/strong&gt;: Facultad de Ingeniería - Universidad Nacional de La Plata. En colaboración con &lt;strong&gt;Sandra Maguid&lt;/strong&gt;, Universidad Nacional de Quilmes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Este artículo ha sido financiado por el proyecto INVOFI de la Asociación Física Argentina.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cáncer&lt;/strong&gt;: un conjunto de enfermedades que desequilibran el control celular. Aparecen células que se multiplican en exceso y adquieren carácter maligno. Al cáncer lo sufren los que lo padecen y los que están cerca de los que lo padecen. Google devuelve casi 300 millones de páginas relacionadas con esa palabra. El cáncer es siempre noticia, a veces es susurro y otras veces es política: en 1971 Richard Nixon (entonces presidente de los Estados Unidos) se tomó un momento para declararle “norteamericanamente” la guerra al cáncer, con las siguientes palabras:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;“Ha llegado el momento en Norteamérica en que el mismo tipo de esfuerzo mancomunado que logró dividir el átomo, y llevar al Hombre a la Luna, debe dirigirse a la conquista de esta terrible enfermedad”&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/03/DV-fig1-300x200.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Pero aquí, en Argentina, en la Comisión Nacional de Energía Atómica, lejos de las declaraciones rimbombantes, el Dr. Andrés Kreiner, Jefe del Grupo de Espectroscopía Nuclear, trabaja junto con otros científicos en una terapia alternativa para la enfermedad. Nos recibe amablemente en su laboratorio del Centro Atómico Constituyentes y nos introduce en un sector amplio, con mobiliario electrónico, multitud de llaves, botones y controles que deslizan sobre escalas graduadas. Y todo ocurre casi tranquilamente, en medio de un trabajo que no se detiene.&lt;/p&gt;
&lt;h3 id=&#34;terapias-y-límites&#34;&gt;Terapias y límites&lt;/h3&gt;
&lt;p&gt;¿Cuál es el ideal de un tratamiento oncológico? Sin duda, destruir o detener todas las células malignas sin causar daño a las sanas. Las terapias usuales por radiación gama funcionan con tumores localizados. Sin embargo, en el caso de los melanomas y los glioblastomas, el éxito de las técnicas convencionales es limitado. En palabras de Kreiner: –&lt;em&gt;Hay cánceres geométricamente difusos, hay células &amp;quot;guerrilleras&amp;quot; que infiltran el tejido normal circundante. El volumen afectado no está bien definido. Si el tumor es infiltrante y difuso, esa estrategia [la de los rayos gama enfocados] fracasa&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/03/DV-fig2-300x193.jpg&#34; alt=&#34;:right&#34;&gt;&lt;/p&gt;
&lt;p&gt;Un tratamiento alternativo en el que trabaja el equipo de Kreiner es la Terapia por Captura Neutrónica en Boro (BNCT). –&lt;em&gt;La idea fue propuesta por Gordon Locher en 1936, poquito tiempo después del descubrimiento del neutrón, que fue en 1932&lt;/em&gt; –refiere. Hay dos etapas en el BNCT: primero se inyecta al paciente una sustancia, que contiene boro 10, un elemento no tóxico, no radiactivo. Esta sustancia debe ser afín a las células cancerosas. En el caso ideal, el boro es absorbido por todas las células tumorales y no por las células sanas.&lt;/p&gt;
&lt;p&gt;En la segunda etapa se irradia con neutrones, los que son capturados con alta eficiencia por el boro. Entonces ocurre una reacción nuclear y se emiten dos partículas: un núcleo de litio y una partícula alfa. Al recorrer su camino dentro del tejido vivo y frenarse el litio y la particula alfa producen un gran daño biológico. Nos dice Kreiner: –&lt;em&gt;Esa es la ventaja del BNCT: que toda la radiación queda localizada en la célula que tiene boro. [...] Los rangos [donde queda la radiación] son entre 5 y 10 micrones, del tamaño típico de una célula.&lt;/em&gt; Y agrega: –&lt;em&gt;Uno busca producir daño al ADN para impedir que prolifere, para esterilizar [la célula cancerosa]. Estas partículas producen un daño muy complejo. Para ejemplificarlo, en vez de romper una cadena, rompe las dos del ADN. Y el tejido vivo [que incluye a las células malignas] no tiene medios eficientes para reparar este daño. Y así se destruyen o dejan de proliferar las células malignas, que es lo que uno quiere.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;–¿Qué es lo que demora tanto la implementación del BNCT? –&lt;em&gt;La idea es genial, pero la implementación práctica está llevando muchísimo tiempo. Por muchos motivos. Uno fue obtener una droga suficientemente selectiva [la sustancia que transporta al boro]. Y además se irradiaba a los pacientes con haces de neutrones térmicos, que se absorben desde la superficie, en el hidrógeno y el nitrógeno, entonces todo el tejido anterior recibía mucha más dosis que el propio tumor. Eso hizo que esta técnica fracasase. Después de unos cuantos años la gente dijo &amp;quot;no usemos neutrones térmicos sino epitérmicos&amp;quot;, que tienen 10 keV y eso hace que pueda penetrar el haz sin ser absorbido, que se termalice en el propio tejido y llegue térmico al tumor donde está el boro. Son varias cosas que la gente trabajosamente fue aprendiendo.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;entender-el-efecto&#34;&gt;Entender el efecto&lt;/h3&gt;
&lt;p&gt;¿De qué depende el éxito del BNCT? ¿Cuáles son las condiciones para que sea efectivo? –Cuéntenos cómo se evalúa el efecto específico del litio y de la particula alfa. –&lt;em&gt;Irradiamos cultivos celulares con haces de litio, cuya energía podemos fijar. Podemos fijar la dosis, podemos fijar claramente las condiciones de trabajo. Y medir la eficacia biológica relativa, que es el parámetro que sirve para comparar el efecto de diferentes iones relativamente a la radiación gama, que es la radiación de referencia porque se usa en radioterapia convencional.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;–¿Y respecto a la distribución de boro? ¿La sustancia que lo transporta logra llevarlo a las células cancerosas? –&lt;em&gt;Hemos utilizado el microhaz de iones pesados del Laboratorio, y hemos estudiado dentro del tejido cómo se microlocaliza la droga.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/03/DV-fig3-300x272.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Andrés Kreiner hace una pausa. Escucha los comentarios de otros investigadores, se excusa brevemente y se reúne con su equipo. Argumenta, evalúa la marcha de la experiencia que está ocurriendo. Luego vuelve y continúa explicándonos. –&lt;em&gt;La muestra es un corte muy delgado de un tejido tumoral de un hámster previamente inyectado con el compuesto de boro cuya microdistribución queremos estudiar. Con un haz de oxígeno uno barre la muestra. Esto produce rayos X característicos, que se generan en el cobre del compuesto químico que contiene al boro. Esto me permite detectar trazas y saber dónde está y cuánto boro hay en cada punto de la muestra. Son intentos de ir entendiendo el problema. Tratar de predecir el efecto radiobiológico del tratamiento.&lt;/em&gt; Una vez que los especialistas pueden evaluar y entender el efecto en tejidos de animales de laboratorio, disponen de modos específicos de trasladar ese conocimiento a los efectos en tejidos humanos.&lt;/p&gt;
&lt;h3 id=&#34;neutrones-y-aceleradores&#34;&gt;Neutrones y Aceleradores&lt;/h3&gt;
&lt;p&gt;–Háblenos del problema de los neutrones. –&lt;em&gt;Hasta ahora el BNCT se hace sólo en reactores porque en nuestra civilización es la única fuente de neutrones de que disponemos&lt;/em&gt; –explica Kreiner y agrega: –&lt;em&gt;Llevar pacientes a un reactor no es lo ideal... y tampoco lo es llevar reactores a un hospital. Entonces lo que nosotros estamos tratando de hacer es desarrollar un relativamente pequeño acelerador capaz de ser instalado en un hospital y que sea generador de neutrones. Un haz de protones incidiendo sobre un blanco conveniente produce una reacción nuclear que genera neutrones. Y esos neutrones son los que queremos utilizar para el BNCT.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;–Está el tema del costo, no es lo mismo hacer un reactor que un acelerador, ¿no? –&lt;em&gt;Sí, y muy a favor del acelerador. Un reactor nuclear es un aparato enormemente complejo, con un costo de inversión muy grande, con problemas de seguridad radiológica importantes, con uranio... y un acelerador es un aparato que uno desenchufa y se acabó la radiación.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/03/DV-fig4-300x208.jpg&#34; alt=&#34;:right&#34;&gt;&lt;/p&gt;
&lt;p&gt;–¿Y si uno quisiera comprarlo, de qué valor estamos hablando? –&lt;em&gt;&amp;quot;Cuando&amp;quot; se desarrollen... porque hoy en día no se puede comprar. No hay ningún acelerador en ningún lugar del mundo que produzca la cantidad y la calidad de neutrones para BNCT. Hay gente que está trabajando hace tiempo en esto. Por ejemplo en Japón, en Italia, en Rusia&lt;/em&gt; –y agrega con entusiasmo– &lt;em&gt;y nosotros estamos corriendo esa carrera&lt;/em&gt;. Kreiner nos explica que últimamente hay cambios en el paradigma del BNCT. Tradicionalmente se pensaba que el boro debía dopar todas las células malignas. Sin embargo se está evaluando que si el boro se localiza en los vasos que irrigan al tejido tumoral, que tienen elevada radiosensibilidad, también podría prevenirse su proliferación. Este conocimiento fue adquirido en nuestro país por la Dra. A. Schwint, colega de la CNEA.&lt;/p&gt;
&lt;h3 id=&#34;aplicación-exitosa&#34;&gt;Aplicación exitosa&lt;/h3&gt;
&lt;p&gt;–¿En algún caso se aplicó BNCT exitosamente? –&lt;em&gt;Está en desarrollo. La droga está en el límite de selectividad aceptable. En Japón [...] algunos pacientes se han tratado con efectos impresionantes. He visto fotografías de personas con tumores de cabeza y cuello inoperables. Recibieron una sola aplicación de menos de una hora, frente a las repetidas aplicaciones en radiación gama. Después de unos meses la cara estaba totalmente limpia.&lt;/em&gt; Pero agrega con cautela: –&lt;em&gt;Tiene una efectividad limitada, porque la droga no es óptima, también en cada paciente el efecto es diferente, la concentración en tejido no es la misma. Hay una variabilidad muy grande propia de cada organismo en particular. Acá en la Argentina se está aplicando con reactores, sobre melanomas de extremidades, brazos y piernas, que no implica riesgo para el paciente, y el control local de los nódulos melanóticos es muy bueno. Lo que pasa es que a los melanomas, una vez que proliferan, no hay manera de pararlos. Es decir, el control local es muy bueno, pero la enfermedad reaparece en otros lugares.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/03/DV-fig6-300x141.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;–&lt;em&gt;En síntesis&lt;/em&gt; –concluye Kreiner– &lt;em&gt;el juego es: primero cargar todas las células tumorales con boro y lo menos posible el tejido sano, y luego irradiar con neutrones. Es una terapia binaria. El tema es: utilizar una droga que tenga una selectividad particular en relación al tumor, y después una máquina chiquita, instalable en un hospital, disponible las 24 horas para poder desarrollar BNCT en el medio idóneo, que es una institución dedicada al cáncer, donde están los médicos y la infraestructura médica adecuada. Trabajamos en relación con el Centro Médico Vidt y el Instituto Roffo.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Para mayor información:&lt;/strong&gt; &lt;a href=&#34;http://www.tandar.cnea.gov.ar/&#34;&gt;http://www.tandar.cnea.gov.ar/&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>¿Es posible controlar la luz difractada por una red de difracción?</title>
      <link>https://ciencianet.com.ar/post/es-posible-controlar-la-luz-difractada-por-una-red-de-difraccion/</link>
      <pubDate>Fri, 27 Feb 2009 21:55:10 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/es-posible-controlar-la-luz-difractada-por-una-red-de-difraccion/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro&lt;/strong&gt;. Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Este artículo ha sido financiado por el proyecto INVOFI de la Asociación Física Argentina.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;¡Sí!...y se logra empleando elementos dispersores distribuidos de forma determinada. Científicos argentinos han estudiado en forma numérica este fenómeno y lograron dicho &lt;em&gt;dominio&lt;/em&gt; utilizando un conjunto de cilindros de sección circular. Lo que es aún más sorprendente es que la predicción de las direcciones en que se intensificarán la luz reflejada y la luz transmitida es independiente del material de los cilindros y de la polarización de la luz incidente.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/02/interf-300x116.jpg&#34; alt=&#34;Esquema de la estructura dispersora que forma una red finita de período d.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Este control es posible a partir de un efecto meramente geométrico. La estructura en estudio posee lo que los investigadores denominan “doble período”, la cual está formada por un número determinado de cilindros circulares de igual radio uno al lado del otro, cuya distancia entre centros se mantiene constante. Este sub-arreglo se repite un cierto número de veces y conforma una especie de “red de cables” inmersa en aire como se observa en la figura. La presencia de esta geometría en particular, modifica tanto la respuesta de la intensidad de la luz reflejada como la de la luz transmitida. Este trabajo de investigación no sólo es innovador desde el punto de vista teórico sino que también tiene un acercamiento “realista” pues la estructura con la que se trabaja tiene dimensiones finitas y puede estar formada tanto de un material aislante como de un metal.&lt;/p&gt;
&lt;p&gt;El radio de los cilindros modelados fue de 50 nm (un nanómetro, nm, es la milmillonésima parte de un metro) y se los consideró formados tanto de un material metálico, plata, como de un material aislante, sílice. Para un cierto número de cilindros en cada sub-arreglo, iluminando con luz infrarroja y para distintos ángulos de observación, se puede lograr la &lt;strong&gt;cancelación&lt;/strong&gt; o bien la &lt;strong&gt;minimización&lt;/strong&gt; de algunos de los órdenes de la luz difractada en la reflexión y en la transmisión; y si se modifica la cantidad de cilindros en el sub-arreglo es posible lograr la &lt;strong&gt;intensificación&lt;/strong&gt; de otros órdenes, lo cual no ocurre para el caso de una red simple (formada por un único cilindro en cada sub-arreglo). Por lo tanto, la geometría de la estructura dispersora es la causante de, por ejemplo, obtener en ciertas direcciones que la intensidad de la luz reflejada sea despreciable; y lo que es más importante es que estas características se mantienen para cualquier ángulo de incidencia.&lt;/p&gt;
&lt;p&gt;El gran interés en el estudio de la respuesta electromagnética de este tipo de estructuras complejas radica en las posibles y prometedoras aplicaciones; algunas de ellas son el diseño de polarizadores y de filtros o bien de superficies que permitan saber de antemano las características de la luz difractada; entre otras.&lt;/p&gt;
&lt;p&gt;Este estudio también es aplicable a otras regiones del espectro electromagnético, como por ejemplo las microondas o las ondas milimétricas. Además, en este trabajo se verificó que la obtención de una respuesta específica de cierta clase de estructuras dispersoras es independiente del material con el cual fueron construidas, hecho que facilitaría notablemente su diseño y manufactura.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo Original:&lt;/strong&gt; Marcelo Lester, Diana C. Skigin, y Ricardo A. Depine. &lt;a href=&#34;https://doi.org/10.1364/AO.47.001711&#34;&gt;Control of the diffracted response of wire arrays with double periods&lt;/a&gt;. Appl. Opt. 47, 1711-1717 (2008).&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>¿Puede la fricción generar rayos X?</title>
      <link>https://ciencianet.com.ar/post/puede-la-friccion-generar-rayos-x/</link>
      <pubDate>Tue, 13 Jan 2009 21:59:08 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/puede-la-friccion-generar-rayos-x/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Diego Maza.&lt;/strong&gt; Grupo de Medios Granulares. Universidad de Navarra.&lt;/p&gt;
&lt;p&gt;La tribología es una rama de la ciencia poco conocida y escasamente explorada por la comunidad científica internacional. Su objeto de estudio son los procesos de fricción y desgaste que aparecen entre diferentes cuerpos cuando estos interactúan entre sí mediante fuerzas de contacto.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2009/01/RX.jpg&#34; alt=&#34;:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Un caso típico de esa interacción viene dado por la separación de dos superficies que interaccionan mediante un medio adhesivo como ocurre comúnmente con las cintas tipo Scotch que todos tenemos en casa. Durante este proceso, comúnmente conocido como “&lt;em&gt;peeling&lt;/em&gt;”- su traducción literal al castellano sería “decapado”- la variedad de procesos físico-químicos que tienen lugar pueden dar lugar a sorprendentes respuestas como la generación de rayos X (RX).&lt;/p&gt;
&lt;p&gt;Tal extremo había sido reportado por científicos rusos durante la época soviética en idioma ruso, razón por la cual, su existencia era prácticamente ignorada por el resto de la comunidad científica. Recientemente sin embargo, este fenómeno ha sido “redescubierto” por científicos del Departamento de Física y Astronomía de la Universidad de California, quienes han demostrado la generación de fotones de altas energías cuando desenrollamos un trozo de cinta Scotch (¡el resultado no depende del fabricante!) de su carrete.&lt;/p&gt;
&lt;p&gt;Este estudio [&lt;a href=&#34;#1&#34;&gt;1&lt;/a&gt;], ha puesto de manifiesto que aún a velocidades moderadas de separación de la cinta del rollo, se pueden generan RX -además de fotones de un amplio espectro de energía- si el proceso tiene lugar en un vacío moderado (1 mTorr). El mecanismo de generación de esta radiación no se conoce aún con precisión, existiendo de hecho, un par de teorías contrapuestas que intentan dar una explicación del mismo.&lt;/p&gt;
&lt;p&gt;Sin entrar en detalles, resulta claro que la generación de rayos X requiere de altas energías. Así, generar “triboelectrones” -como se conoce a los electrones generados mediante un proceso de fricción- lo suficiente energéticos como para generar radiación de frenado deben ser producidos por una diferencia de potencial muy alta. Esta diferencia de potencial podría aparecer - especulan los autores- entre el material de la cinta (de base acrílica y cargado positivamente) y el rollo de polietileno y cargado negativamente. Así las cosas, no sería de extrañar que en un país tan dado a los extremos legales como los EE UU, pronto veamos en las etiquetas de cinta Scotch “Advertencia: riesgo de exposición a radiación...”.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referencia:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;1&#34;&gt; 1. &lt;a href=&#34;http://www.nature.com/nature/journal/v455/n7216/full/nature07378.html&#34;&gt;Correlation between nanosecond X-ray flashes and stick-slip friction in peeling tape&lt;/a&gt; . C.G. Camara, J.V. Escobar, J. R. Hird &amp;amp; S.J. Putterman. Nature, vol 455.  &lt;/p&gt;
&lt;p&gt;En la dirección: &lt;a href=&#34;http://www.nature.com/nature/videoarchive/x-rays/&#34;&gt;http://www.nature.com/nature/videoarchive/x-rays/&lt;/a&gt; Puede encontrarse un video donde se muestra como bajo condiciones de vacío moderado esta radiación puede ser tan intensa como para radiografiar un dedo humano.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>La naturaleza nos invita a jugar con la luz</title>
      <link>https://ciencianet.com.ar/post/la-naturaleza-nos-invita-a-jugar-con-la-luz/</link>
      <pubDate>Tue, 02 Dec 2008 22:00:45 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/la-naturaleza-nos-invita-a-jugar-con-la-luz/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro.&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP) y Universidad Tecnológica Nacional.&lt;/p&gt;
&lt;p&gt;Científicos de la Universidad de Utah lograron desentrañar la estructura fotónica tridimensional de las escamas presentes en el caparazón de un escarabajo autóctono de Brasil.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/12/aluna-fig1-216x300.png&#34; alt=&#34;Lamprocyphus augustus.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Su color verde iridiscente es consecuencia de la estructura periódica de cada una de las escamas que conforman su caparazón y no de su pigmentación. Cuando la luz incide sobre él se producen interferencias múltiples dando como resultado sólo la reflexión del color verde. Cada una de sus escamas actúa como un cristal fotónico natural.&lt;/p&gt;
&lt;p&gt;Pero ¿qué es un cristal fotónico? La comunidad científica ha denominado con este nombre a los materiales compuestos por varios elementos periódicamente distribuidos que dispersan la luz. Hoy en día hay un gran interés en el estudio de estos nuevos materiales debido a la búsqueda del control de las propiedades ópticas, es decir de la manipulación de la luz y del potencial y amplio espectro de sus aplicaciones, como por ejemplo en nuevas tecnologías de comunicación puramente ópticas o en computadoras ópticas ultrarrápidas diseñadas con circuitos integrados ópticos o chips que funcionen con luz y no con electricidad.&lt;/p&gt;
&lt;p&gt;La especie que inspiró a Galusha y a Bartl para modelar un cristal fotónico artificial fue el escarabajo Lamprocyphus augustus, cuya característica particular es que su coloración es independiente del ángulo de observación. Este hecho se explica analizando y entendiendo la geometría y composición de una de sus escamas, que está formada por 200 piezas de quitina y cada una de ellas ésta orientada en diferentes direcciones haciendo que la luz que incide con distintos ángulos sobre el caparazón del escarabajo dé como resultado que la luz reflejada sea de un color verde nítido y brillante.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/12/aluna-fig2-300x126.png&#34; alt=&#34;Imagen de un corte transversal de la escama.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;El siguiente paso consistió en determinar con gran precisión la estructura tridimensional de la escama. Para ello se usó un microscopio electrónico de barrido con el fin de adquirir y procesar imágenes, y por otro lado, se recurrió a la focalización de un haz de iones de galio para retirar capas extremadamente delgadas de la escama. Repitiendo sucesivas veces este procedimiento se obtuvieron 150 imágenes de distintas secciones de una misma escama que sirvieron para reproducir, en forma teórica, su estructura 3D.&lt;/p&gt;
&lt;p&gt;La compleja estructura fotónica tridimensional, que opera en el rango de longitudes de onda visibles, no se obtuvo a través de un problema “pensado” sino emulando la arquitectura de los sistemas biológicos presentes en la naturaleza y tal hallazgo abre las puertas a la creación de nuevos dispositivos ópticos que podrían revolucionar nuestra tecnología actual.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo Original:&lt;/strong&gt; Jeremy W. Galusha, Lauren R. Richey, John S. Gardner, Jennifer N. Cha, and Michael H. Bartl. &lt;a href=&#34;https://doi.org/10.1103/PhysRevE.77.050904&#34;&gt;Discovery of diamond-based photonic crystal structure in beetle scales&lt;/a&gt;. Physycal Review E 77, 050904(R) (2008).&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Formación de opinión: Líderes vs medios de comunicación</title>
      <link>https://ciencianet.com.ar/post/formacion-de-opinion-lideres-vs-medios-de-comunicacion/</link>
      <pubDate>Thu, 16 Oct 2008 22:08:44 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/formacion-de-opinion-lideres-vs-medios-de-comunicacion/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Norma Canosa:&lt;/strong&gt; Instituto de Física de La Plata (UNLP-CONICET).&lt;/p&gt;
&lt;p&gt;En este trabajo se investiga un modelo para la formación de opinión, basado en la teoría del impacto social, mediante métodos estadísticos que permiten estudiar el comportamiento dinámico de un grupo social bajo la influencia de un líder fuerte y de los medios masivos de comunicación. La denominada teoría del impacto social es un modelo matemático desarrollado por el psicólogo estadounidense Bibb Latané en 1981, que describe la forma en que una persona responde a la influencia social de un grupo de individuos en base a factores tales como: (a) Las relaciones de fuerza, es decir que tan importante es el grupo de individuos que influye sobre un único individuo; (b) El grado de proximidad social del individuo al grupo; (c) El número de integrantes del grupo.&lt;/p&gt;
&lt;p&gt;Resulta más probable que la opinión de un individuo coincida con las opiniones del grupo cuando aumenta su proximidad social al grupo, o cuando la importancia del grupo crece. La teoría original estudiaba únicamente el impacto de los grupos sobre los individuos pero luego fue generalizada por el mismo Latané a un modelo estadístico para poder describir la evolución dinámica del impacto de los grupos sociales.&lt;/p&gt;
&lt;p&gt;Esta teoría ha servido de base para desarrollar los llamados &lt;em&gt;modelos para la formación de opinión&lt;/em&gt;, los cuales han despertado un gran interés entre los físicos debido a que exhiben una rica variedad de fenómenos similares a los que se dan en diversos sistemas físicos muy estudiados en mecánica estadística o materia condensada. Uno de estos fenómenos ocurre cuando la competencia entre un líder fuerte y un medio de comunicación masivo provoca una transición brusca entre dos estados de opinión distintos; la opinión del líder y la del medio.&lt;/p&gt;
&lt;p&gt;En un sistema físico esto corresponde a una transición de fase abrupta (conocida como de primer orden) que se produce cuando, debido a la variación de uno o varios parámetros de control, el sistema pasa abruptamente de un estado a otro. Tanto estos como otros interesantes aspectos se pueden investigar mediante métodos estadísticos para lo cual se recurre a representar a los &lt;em&gt;modelos para la formación de opinión&lt;/em&gt; como sistemas físicos.&lt;/p&gt;
&lt;p&gt;Uno de los más simples consiste de una cadena cíclica de partículas interactuantes, las que representan a individuos, donde cada partícula posee dos posibles orientaciones, hacia arriba o hacia abajo, que representan las opiniones. Cada individuo del grupo experimenta una fuerza promedio similar y puede cambiar de opinión. El líder está representado por una partícula con un parámetro de fuerza mucho mayor que la fuerza promedio, indicando que su impacto sobre la sociedad será mayor que el promedio y cuya orientación (opinión) puede no cambiar. El efecto de la proximidad entre los integrantes del grupo está cuantificado por un factor de peso, que modifica la fuerza promedio y que se mide en base a la inversa de la distancia entre las partículas que interactúan entre sí.&lt;/p&gt;
&lt;p&gt;Este sistema se puede colocar en un campo externo variable que estará representando la influencia de un medio masivo de comunicación sobre la opinión de los individuos. En este trabajo se emplea un &lt;em&gt;modelo para la formación de opinión&lt;/em&gt; investigando el comportamiento dinámico de un grupo social en relación con la ocurrencia de una transición de fase dinámica, como las que se presentan en sistemas magnéticos cuando un parámetro de control, es decir el campo magnético, oscila. En el caso investigado, el grupo está sometido a la influencia de un líder fuerte y a la de los medios masivos de comunicación en dos posibles escenarios:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(1) Solo el líder está actuando y cambia periódicamente su opinión.&lt;/strong&gt; En este caso la influencia del líder conduce a una transición de fase dinámica entre dos estados. Uno de ellos corresponde a una interacción débil entre el líder y el grupo, de modo que el grupo no sigue la opinión del líder y el otro a un estado dinámico donde el grupo sigue la opinión del líder. Esta transición de fase es abrupta, y se asemeja a una transición de fase de primer orden. Estas transiciones abruptas ocurren cuando el grupo posee un número muy grande de integrantes. A medida que el número de integrantes disminuye se pueden observar efectos de tamaño finito que dan cuenta de los cambios en el comportamiento de comunidades más reducidas.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(2) Los medios masivos cambian periódicamente su mensaje y deben competir con el líder, quien no cambia de opinión.&lt;/strong&gt; En este caso se observa que los medios masivos requieren superar cierto umbral a fin de imponerse sobre la opinión del líder y llevar al grupo social a un estado dinámicamente desordenado. Estos resultados muestran la utilidad de emplear métodos de la física estadística para describir el comportamiento dinámico de un grupo social bajo la influencia de distintos factores que compiten entre sí.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; &lt;a href=&#34;https://doi.org/10.1103/PhysRevE.76.061125&#34;&gt;Dynamic behavior of a social model for opinion formation&lt;/a&gt; , Clelia M. Bordogna, Ezequiel V. Albano, &lt;em&gt;Physical Review E&lt;/em&gt; 76, 061125 (2007).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instituciones:&lt;/strong&gt; &lt;a href=&#34;http://www.ing.unlp.edu.ar/fismat/imapec/imapec2/index.html&#34;&gt;IMApEC&lt;/a&gt; (Fac. Ing., UNLP), &lt;a href=&#34;http://www.inifta.unlp.edu.ar/&#34;&gt;INIFTA&lt;/a&gt; (UNLP-CONICET).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; C. Bordogna (E-mail: &lt;a href=&#34;mailto:cleliabordoga@yahoo.com.ar&#34;&gt;cleliabordoga@yahoo.com.ar&lt;/a&gt; ), E. Albano (E-mail: &lt;a href=&#34;mailto:ezequielalb@yahoo.com.ar&#34;&gt;ezequielalb@yahoo.com.ar&lt;/a&gt; )&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>El trabajo científico en los grandes emprendimientos</title>
      <link>https://ciencianet.com.ar/post/el-trabajo-cientifico-en-los-grandes-emprendimientos/</link>
      <pubDate>Mon, 29 Sep 2008 22:09:49 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/el-trabajo-cientifico-en-los-grandes-emprendimientos/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Luis Pugnaloni&lt;/strong&gt;. Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP).&lt;/p&gt;
&lt;p&gt;Valeria Perez Reale hizo su licenciatura en Fisica en la Universidad Nacional de La Pampa y luego de realizar su tesis doctoral en la Universidad de Berna se incorporo al proyecto ATLAS que es parte del gran colisionador de protones (LHC, por sus siglas en ingles). En esta entrevista no disimula su entusiasmo por su trabajo en Ginebra que ha alcanzado niveles insospechados de repercusion mundial.&lt;/p&gt;
&lt;p&gt;-&lt;strong&gt;¿Cuántas entrevistas te han hecho en los últimos días? ¿Esperabas esta reacción del público al lanzamiento del LHC?&lt;/strong&gt; Muchas, no me imaginaba la repercusion que genero la noticia del primer haz de proton injectado en el LHC el 10 de septiembre, me dio mucha alegria! Yo estaba en la sala de control de la experiencia ATLAS y nunca vi tantas camaras de television y reporteros siguiendo tambien con emocion este momento historico. Como Pampeana la nota que me emociono mas y me lleno de orgullo fue la del Diario La Arena. Y ahora me dicen que tambien estaremos en la revista Gente! de chiquita nunca me imaginaria tal magnitud de repercusion! Estos es gracias a la WWW (que fue inventada en el CERN) )y la globalizacion de la informacion.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/09/valeria-290x300.jpg&#34; alt=&#34;Valeria Perez Reale: En su oficina en el CERN (Ginebra).:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;-&lt;strong&gt;¿Cuál es tu trabajo específico en el colisionador?&lt;/strong&gt; El LHC (gran colisionador de protones) es la maquina mas potente del mundo- se encuentra 100 m bajo tierra en un tunel de 27 km de circunferencia, esta compuesto de casi 2000 imanes superconductores enfriados a las temp mas baja del universo: 2 K. Los protones viajaran casi a la velocidad de la luz a energias jamas alcanzadas (comparados a dos tren bala) en sentidos opuestos donde colisionaran en 4 puntos donde se encuentran los detectores. Uno de los detectores- que uno se debe imaginar como un gran microscopio que &amp;quot;ve&amp;quot; lo que ocurre en cada colision- es ATLAS. La experiencia ATLAS para la cual participo hace casi 7 años desde el 2002, es una colaboracion internacional demas de 2500 scientificos de 38 paises del mundo.&lt;/p&gt;
&lt;p&gt;En la experiencia ATLAS yo trabaje 6 años para la seleccion de datos. En antiguos experimentos toda colision era registrada en disco, en el LHC existen mil millones de colisiones por segundo (casi 200 CDs de informacion que se producen por segundo en ATLAS), necesitamos un sistema de flitro llamado &amp;quot;&lt;em&gt;trigger&lt;/em&gt;&amp;quot; que solo seleccione eventos interesantes de la colision. Yo fui la coordinadora de un grupo de 30 personas de distintos lugares del mundo que seleccionaba electrones y photones. Desde este año llevo el sombrero &amp;quot;Pixel&amp;quot; el subdetector mas cerca del haz y mas preciso, mide con tecnologia semiconductora la dirección de trazas de particulas electricas, en cada colision se producen casi 100.000 trazas por evento, y tenemos que buscar la interesante. Es casi como buscar un aguja en un pajar...&lt;/p&gt;
&lt;p&gt;-&lt;strong&gt;Después de todos estos años en Ginebra, contanos un punto débil y un punto fuerte que hayas descubierto de tu formación en la Universidad Nacional de La Pampa.&lt;/strong&gt; La Universidad de La Pampa me dio una educación de fisica basica al mismo nivel que cualquier otra Universidad argentina. La educacion de La pampa ha sido de alto nivel.&lt;/p&gt;
&lt;p&gt;-&lt;strong&gt;Entonces no hay nada que mejorar en la educacion universitaria? Ningun punto debil?&lt;/strong&gt; Quizas un punto de mejoria seria de introducir en el ultimo año materias mas prácticas y enfatizar en la programacion. El systema de educacion mas corta &amp;quot;sistema de Bologna&amp;quot; que se adopto en Europa sigue un poco estas lineas, UK en cuatro años gradúa físicos con un conocimiento global teórico pero tambien práctico. Esto facilita la inserción al PhD o laboral.&lt;/p&gt;
&lt;p&gt;-&lt;strong&gt;Creo adivinar la respuesta pero... ¿Regresarías a vivir y trabajar en Argentina? ¿Por qué?&lt;/strong&gt; Nunca descarté la posibilidad de volver a trabajar a la Argentina. Si me dan la posibilidad de dirigir un grupo de investigacion en física de altas energías y ATLAS lo tendría que pensar seriamente, tambien en la vida está el aspecto personal y familiar que pesa mucho en la decisión de donde uno vive y trabaja.&lt;/p&gt;
&lt;p&gt;-&lt;strong&gt;Digamos que te ofrecen armar de cero un grupo en física de altas energias en Argetnina. Te dan un puesto en una univerisdad o CONICET en la ciudad de tu elección y los derechos de cualquier investigador argentino a solicitar subsidios y dirigir estudiantes. ¿Volverias?&lt;/strong&gt; Todo depende del grupo, proyecto y lugar/ciudad de trabajo, igual aplicaría a Europa, USA o Asia.&lt;/p&gt;
&lt;p&gt;-&lt;strong&gt;La mayoría de los científicos están acostumbrados a trabajar en soledad o en pequeños grupos de investigación, ¿qué cualidades se necesitan para trabajar en un proyecto con miles de científicos involucrados?&lt;/strong&gt; La ciencia del futuro es de investigacion en grupo, se dió en la física de altas energias hace años y se está dando en otras ramas como astronomía, medicina, etc. Los proyectos grandes son como una gran empresa, se necesita saber trabajar en grupo, tener una visión global del experimento y ser muy práctico en las decisiones y prioridades del proyecto.&lt;/p&gt;
&lt;p&gt;-&lt;strong&gt;¿Entonces en el futuro los pequenios grupos (menos de 10 investigadores) no seran muy bien considerados? ¿o no tendran chances de hacer aportes importantes a la ciencia?&lt;/strong&gt; No fue mi respuesta, a lo que me refiero es que la globalizacion tambien afectó a la ciencia y que cada vez más tendrá que haber colaboraciones o mas grandes o mas estrechas entre grupos.&lt;/p&gt;
&lt;p&gt;-&lt;strong&gt;El reconocimiento por un descubrimiento es un factor importante para todo científico, ¿de qué modo te llega el reconocimiento personal cuando el descubrimiento lo hacés junto a tantos colegas?&lt;/strong&gt; El reconocimiento es una satisfaccion personal, cuando hay mas de 200 nombres en un paper o casi mil para los que vendrán del LHC, el reconocimiento será dentro del grupo de trabajo directo.&lt;/p&gt;
&lt;p&gt;-&lt;strong&gt;¿Qué pasa si no encuentran el bosón de Higgs? ¿Le echarás las culpas a tus programas o al modelo estándar?&lt;/strong&gt; Si el boson de Higgs existe el LHC lo tendrá que ver. El problema será que debe haber al menos dos años de toma de datos a la luminosidad deseada y la búsqueda de muchos canales complementarios por parte de las dos experiencias ATLAS y CMS. Si no descubrimos el Higgs siempre tendremos nueva física para ver SUSY, exotica o algo imaginado, asi que siempre será interesante y siempre estaremos ocupados!&lt;/p&gt;
&lt;p&gt;-&lt;strong&gt;Intuyo que no te seria fácil descartar el modelo estandar solo porque con el LHC no puedan &amp;quot;probar&amp;quot; la existencia de partículas aún no observadas experimentalemnte.&lt;/strong&gt; El LHC tiene un rango de energia y luminosidad que podrá ver el SM Higgs o nueva física (varios higgs,etc). Espero que la naturaleza nos revele alguna pista para saber que camino seguir (en este momento son los teóricos que proponen varios caminos y los experimentales debemos seguir).&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Cuando investigadores en física piensan en la educación y la divulgación de la ciencia</title>
      <link>https://ciencianet.com.ar/post/cuando-investigadores-en-fisica-piensan-en-la-educacion-y-la-divulgacion-de-la-ciencia/</link>
      <pubDate>Fri, 26 Sep 2008 22:12:05 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/cuando-investigadores-en-fisica-piensan-en-la-educacion-y-la-divulgacion-de-la-ciencia/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Sandra Murriello:&lt;/strong&gt; LABJOR, Universidade Estadual de Campinas (Brasil).&lt;/p&gt;
&lt;p&gt;En las últimas décadas las actividades de divulgación científica revelan un creciente auge y muchas veces son los mismos centros de investigación quienes las llevan a cabo. De este modo los investigadores, con gran experiencia en la composición escrita de artículos científicos, se transforman en productores de textos de divulgación sobre el tema en que ellos son expertos.&lt;/p&gt;
&lt;p&gt;Esta investigación se propone entender las formas en que científicos del área de Física conciben y, eventualmente, asumen la actividad de composición de textos de divulgación científica o transferencia de conocimientos a una comunidad más amplia y heterogénea, en lo demográfico, evolutivo, cultural y social.&lt;/p&gt;
&lt;p&gt;Basadas en el modelo de teorías implícitas del aprendizaje, Astrid Bengtsson del Instituto Balseiro, Nora Scheuer de la Universidad Nacional del Comahue y Mar Mateos Sanz de la Universidad Autónoma de Madrid diseñaron y aplicaron en la comunidad de físicos argentinos un cuestionario que sondea concepciones sobre divulgación y educación científica.&lt;/p&gt;
&lt;p&gt;Este modelo sostiene que las actividades de producción y comprensión realizadas por las personas están mediadas por concepciones de carácter relativamente implícito que esta investigación busca explicitar. Se conocen como teorías implícitas del aprendizaje: la teoría directa, la interpretativa y la constructiva. La directa establece una relación lineal entre condiciones y resultados del aprendizaje, sin atender a los procesos. Asume que el conocimiento, considerado una copia del modelo, se alcanza por exposición a una fuente autorizada. La teoría interpretativa es una evolución de la anterior pero incorpora al sujeto y sus condiciones personales o las condiciones del ambiente como factores que influyen en el resultado. Por su parte, la teoría constructiva implica un salto cualitativo, considerando el aprendizaje como una redescripción de los propios conocimientos del aprendiz. Jerarquiza las situaciones de aprendizaje que tienen en cuenta los contextos de producción del conocimiento. Considera los procesos representacionales de los aprendices y promueve explicitar los distintos puntos de vista, así como la integración de saberes.&lt;/p&gt;
&lt;p&gt;Siendo que esta investigación está orientada a analizar particularmente la divulgación de la física, el cuestionario fue enviado por correo electrónico a los asociados a la Asociación Física Argentina de los cuales respondieron 71. En esta muestra se analizaron las posibles asociaciones entre las opciones de respuesta seleccionadas, preguntas y datos de caracterización de los participantes por medio de diversos cálculos estadísticos.&lt;/p&gt;
&lt;p&gt;Los resultados muestran que los científicos que contestaron el cuestionario tienen preferencia por las opciones de la teoría constructiva en divulgación, sin embargo, para educación científica aumentan las elecciones de opciones correspondientes a la teoría directa. La combinación de teorías predominante es la dupla constructiva-interpretativa.&lt;/p&gt;
&lt;p&gt;Sólo un grupo reducido (18) mostró gran consistencia en la elección de opciones constructivas y parecería estar mostrando tener una teoría respecto a la adquisición y transmisión de conocimientos científicos. Los demás investigadores (53) presentan diversos abordajes, definida en este marco como &amp;quot;baja consistencia&amp;quot;, respecto a cómo debe ser transmitido el conocimiento científico a personas no expertas.&lt;/p&gt;
&lt;p&gt;&amp;quot;Estos resultados nos permitirían sostener la importancia de la inclusión de estas cuestiones en la formación de los científicos, en este caso los físicos, para que puedan desempeñar en forma más potente y efectiva su participación, cada vez más necesaria, en actividades de comunicación científica a comunidades más amplias y heterogéneas&amp;quot;, sostiene Bengtsson. Desde este punto de vista la promoción de la divulgación científica podría pensarse como una de las nuevas alfabetizaciones necesarias para el siglo XXI, no sólo en lo que hace a sus destinatarios, sino también a sus propios agentes, quienes podrían beneficiarse de oportunidades para extender y revisar sus formas de componer textos para destinatarios más alejados de la comunidad científica.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; A. Bengtsson, &lt;em&gt;Concepciones en investigadores del área de la física sobre divulgación y educación científicas a través de textos&lt;/em&gt;. &lt;a href=&#34;http://www.fceia.unr.edu.ar/fceia/sief9/PAGINA_WEB/index.htm&#34;&gt;Noveno Simposio de Investigación en Educación en Física&lt;/a&gt; (2008). A. Bengtsson, &lt;em&gt;Concepciones sobre divulgación y aprendizaje de la ciencia en autores de textos de divulgación científica&lt;/em&gt;, Diploma de Estudios Avanzados, Facultad de Psicología, Universidad Autónoma de Madrid, (2004)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instituciones:&lt;/strong&gt; &lt;a href=&#34;http://www.ib.edu.ar/&#34;&gt;Instituto Balseiro&lt;/a&gt; (Universidad Nacional de Cuyo), &lt;a href=&#34;http://www.uncoma.edu.ar/&#34;&gt;Universidad Nacional del Comahue&lt;/a&gt;, &lt;a href=&#34;http://www.uam.es/&#34;&gt;Universidad Autónoma de Madrid&lt;/a&gt; (España)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; Astrid Bengtsson (E-mail: &lt;a href=&#34;mailto:astrid@cab.cnea.gov.ar&#34;&gt;astrid@cab.cnea.gov.ar&lt;/a&gt;)&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>El LHC ¿la maquina del fin del mundo?</title>
      <link>https://ciencianet.com.ar/post/el-lhc-la-maquina-del-fin-del-mundo/</link>
      <pubDate>Sun, 14 Sep 2008 22:12:49 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/el-lhc-la-maquina-del-fin-del-mundo/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Nicolás Grandi&lt;/strong&gt;. Instituto de Física La Plata&lt;/p&gt;
&lt;p&gt;Empecemos por el principio ¿Qué es el LHC? El LHC o “&lt;em&gt;Large Hadron Collider&lt;/em&gt;” (“Gran Colisionador de Hadrones”) es un acelerador de partículas. Una máquina que toma partículas del tipo de las que se encuentran en el núcleo atómico (hadrones) y las pone a girar en un círculo de 27 kilómetros de longitud. Con la ayuda de enormes electroimanes, acelera ese movimiento de modo que las partículas terminen moviéndose a gran velocidad a lo largo del círculo. En un dado momento se toman dos haces de partículas que giran en sentidos contrarios y se los desvía haciéndolos chocar de frente.&lt;/p&gt;
&lt;p&gt;¿Por qué producir tal choque? Desde Demócrito, una de las ideas que ha guiado el desarrollo de la física, es que la complejidad del mundo visible puede ser explicada en términos de leyes simples que rigen el funcionamiento de sus componentes elementales. Los primeros químicos llevaron la idea atomista a su etapa de madurez: la infinidad de substancias diferentes que constituyen nuestra experiencia inmediata, pudo ser explicada en términos de sólo un centenar de tipos diferentes de átomos, que se agrupan siguiendo ciertas reglas para formar las moléculas de cada una de las substancias. El catálogo de todas esas reglas y su estudio constituyeron la base de la ciencia que hoy llamamos Química.&lt;/p&gt;
&lt;p&gt;Esa asombrosa conclusión se alcanzó mediante la técnica de “romper” las moléculas en sus átomos componentes y luego permitir que tales átomos vuelvan a asociarse en nuevas moléculas. Para producir esta “rotura” los investigadores calentaban sus muestras, es decir daban energía a las moléculas encerradas en sus recipientes de modo de que se movieran muy rápidamente y chocaran, destrozándose en sus átomos componentes, que al volver a chocar se asociaban en nuevas moléculas. El éxito de este programa de investigación motivó su aplicación subsiguiente a los átomos mismos. Si golpeando y rompiendo las moléculas edificamos la Química ¿Qué podríamos lograr golpeando y rompiendo los átomos? De nuevo, el método se mostró exitoso. Golpeando los átomos con partículas de luz (o fotones) y con partículas de electricidad (o electrones), se descubrió que están compuestos por dos componentes básicos: un pequeñísimo núcleo que se esconde en el interior, y una nube difusa de electrones que forma su cubierta exterior.&lt;/p&gt;
&lt;p&gt;El estudio de la forma de esta nube exterior de electrones y de su interacción con los fotones y los electrones incidentes, permitió desarrollar un enfoque nuevo y revolucionario de la física: la Mecánica Cuántica. Como premio a este esfuerzo, las reglas de la Mecánica Cuántica explican completamente las leyes de la Química, haciéndolas consecuencia del modo en que los electrones se acomodan en la cubierta exterior de los átomos. Es decir que ahora comprendemos más cosas (la forma en que los electrones rodean al núcleo para formar los átomos y la manera en que los diferentes átomos se agrupan en moléculas) con menos reglas (las de la Mecánica Cuántica, la Química siendo sólo una consecuencia lógica de ellas).&lt;/p&gt;
&lt;p&gt;Durante todo el siglo XX, la Física persistió con ese programa, aplicándolo a escalas cada vez más pequeñas. El siguiente escalón fue la investigación del núcleo atómico, bombardeándolo con electrones o protones, o colisionándolo con otros núcleos, para así romperlo en sus componentes elementales y comprender las reglas que rigen su interacción. Descubrimos que el mundo subnuclear es increíblemente rico, existiendo un enorme zoológico de partículas elementales descripto por lo que se conoce como Modelo Standard de las Interacciones Fundamentales. Durante este proceso, en más de una ocasión se infirió la existencia de un componente aún desconocido a una dada escala, a partir de una aparente inconsistencia de las reglas de interacción a esa escala.&lt;/p&gt;
&lt;p&gt;Un posterior experimento, siempre del tipo de “romper e investigar los pedazos”, confirmó la existencia del componente propuesto. Por ejemplo, la existencia de “huecos” en la tabla original de Mendeleev, sugirió que debía existir un nuevo tipo de átomo con las propiedades necesarias para llenar cada hueco. La posterior investigación permitió aislar esos elementos confirmando tal hipótesis. Similar es la historia de los neutrinos, propuestos por Fermi como explicación a la aparente falla en la regla de “conservación de la energía” en una reacción conocida como “decaimiento beta”. También la de algunos quarks, cuya existencia se sabía necesaria antes de su observación, como modo de evitar una inconsistencia conocida como “anomalía” en el Modelo Standard.&lt;/p&gt;
&lt;p&gt;Sin embargo, una de tales partículas hipotéticas ha resultado elusiva, no dejándose observar hasta el presente. Es el llamado “bosón de Higgs”, responsable de dar masa a las partículas intervinientes en la interacción nuclear, asegurando que las fuerzas involucradas sean de corto alcance. Una de las razones, tal vez la principal, para la construcción del LHC es la búsqueda del bosón de Higgs.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/09/1-768x500.jpg&#34; alt=&#34;Instalación del calorímetro del experimento ATLAS. Fuente: http://atlas.ch&#34;&gt;&lt;/p&gt;
&lt;p&gt;Pero hay muchas otras cosas interesantes que pueden suceder en el LHC. Por ejemplo, se ha propuesto que, además de las tres dimensiones que observamos forman el mundo (a saber: alto, ancho y espesor, o bien arriba-abajo, izquierda-derecha, adelante-atras), podrían existir otras dimensiones hasta ahora inobservadas. La pregunta inmediata de ¿por qué no las vemos? se puede responder diciendo que las direcciones adicionales son compactas y pequeñas. Una dirección compacta es una que, si caminamos sin volvernos a lo largo de ella, terminamos en el punto de partida. Para fijar ideas, pensemos en una hoja de papel extendida sobre la mesa. Esta hoja tiene sólo dos dimensiones, las cuales no son compactas. Una hormiga puesta sobre la hoja puede caminar en dos direcciones perpendiculares entre sí y, si no se vuelve, no retornará jamás a su punto de partida. Si ahora enrollamos la hoja de papel para formar un cilindro, una de las dimensiones, aquella que se extiende a lo largo del cilindro, sigue sin ser compacta, pero en cambio la otra, la que se extiende alrededor del cilindro, se vuelve compacta. Una hormiga que caminara en esa dirección llegaría al cabo de un tiempo, después de dar una vuelta completa alrededor del cilindro, a su punto de partida. Si el cilindro fuese muy pequeño, la hormiga podría no notar la existencia de esa dirección, ya que al caminar a lo largo de ella retorna casi inmediatamente al punto de partida.&lt;/p&gt;
&lt;p&gt;Una explicación alternativa a por qué no vemos las dimensiones adicionales, es que puede existir algún tipo de fuerza que nos impida movernos en esas direcciones. Por ejemplo si tomamos la hoja de papel y en lugar de enrollarla la doblamos formando una zanja profunda, nuestra hormiga podría quedar atrapada en dicha zanja, siéndole imposible trepar por las paredes. Para ella, es sólo posible moverse a lo largo de la zanja y no en la dirección transversal, es decir que puede resultarle natural asumir que dicha dirección no existe. Existe la posibilidad de que algunas de las partículas resultantes de una colisión en el LHC simplemente desaparezcan. Si tal cosa sucede, es natural inferir que dichas partículas se movieron en alguna de las direcciones adicionales que no podemos ver. En otras palabras, el LHC puede ser la maquina que descubra las dimensiones extra.&lt;/p&gt;
&lt;p&gt;Finalmente, una de las posibilidades más interesantes del LHC es la creación de microscópicos agujeros negros. Un agujero negro es un punto del espacio donde la materia está tan apretada que no puede salir. Para entender la idea, recodemos que todos los cuerpos se atraen entre sí por medio de la gravedad. Eso es también cierto para las partículas elementales. Por lo tanto, cuando una partícula se aleja de otra debe, al igual que un cohete que se aleja de la tierra, alcanzar una cierta “velocidad de escape” que le permita deshacerse del efecto atractivo del campo gravitatorio de la otra partícula. Cuanto más cerca estén las partículas, más intenso es el campo gravitatorio y más grande será la velocidad de escape necesaria.&lt;/p&gt;
&lt;p&gt;En algún punto, para partículas muy cercanas, la velocidad de escape se hace mayor que la de la luz. Como nada puede moverse mas rápido que la luz, dichas partículas están definitivamente ligadas por la gravedad, no pudiendo jamás separarse. Mas aun, si alguna de dichas partículas emitiera fotones, ni siquiera ellos podrían escapar del campo gravitatorio de la partícula emisora. Por eso los agujeros negros son negros, ¡ni siquiera la luz puede escapar de ellos!&lt;/p&gt;
&lt;p&gt;Una consecuencia natural de lo expuesto es que cualquier partícula adicional que sea atraída por el campo gravitatorio del agujero negro y se acerque demasiado, caerá en él y no podrá jamás salir. Es decir que los agujeros negros son objetos voraces: tragan todo lo que tienen alrededor.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/09/9-768x658.jpg&#34; alt=&#34;Simulación de la creación de un agujero negro. Fuente: http://atlas.ch&#34;&gt;&lt;/p&gt;
&lt;p&gt;Existe la posibilidad de que durante las colisiones producidas en el LHC, las partículas lleguen tan cerca unas de otras que produzcan un agujero negro. Esta es ciertamente una de las posibilidades más excitantes del LHC, y una de las razones por las que ha estado en los medios en estos días. Como con casi todo lo que dicen, los medios deforman, desinforman y, cuando pueden, mienten, respecto de los peligros de un evento de esa naturaleza. Dicen que un agujero negro comenzaría inmediatamente a tragar todo lo que haya a su alrededor (cierto) y que por lo tanto engulliría la ciudad de Ginebra, los Alpes suizos y finalmente la Tierra, en pocos segundos (falso).&lt;/p&gt;
&lt;p&gt;Casi sin excepción dicen que para hacer tales afirmaciones se han asesorado por científicos (cierto tal vez en algún caso) que son expertos en el área (falso con seguridad en todos los casos). Mas allá de la intención sensacionalista de estos informes, también se esconde en ellos el ya omnipresente complejo de Frankenstein (el miedo del hombre a su propia creación), y el estereotipo post segunda guerra del científico amoral a quien solo le importa el conocimiento y no se preocupa por los efectos de sus descubrimientos. Un estereotipo bastante oscurantista, que propaga solapadamente la máxima “mejor no saber ciertas cosas”. Una imagen que es además irresponsable, porque pone el énfasis de la bomba atómica no en los políticos que la construyeron, el pueblo que los votó y los militares que la arrojaron, sino en los científicos cuyos descubrimientos la hicieron posible.&lt;/p&gt;
&lt;p&gt;Pero ¿es realmente peligroso el LHC? La respuesta es: simple, total y absolutamente NO. Los agujeros negros tienen una propiedad adicional, que fue descubierta por Hawking en el trabajo que constituyó la base de su fama, y que los vuelve completamente inofensivos: ¡se evaporan! En efecto, los agujeros negros tienen una temperatura que depende inversamente de su tamaño. Un agujero negro gigante, como el que se supone que existe en el centro galáctico, está relativamente frío. En cambio un agujero negro pequeño, como los que podrían llegar a producirse en el LHC, está extremadamente caliente. Como todo objeto caliente, los agujeros negros emiten calor en forma de radiación, brillan como brilla una pieza de metal al ser calentada. En otras palabras ¡no son tan negros después de todo!&lt;/p&gt;
&lt;p&gt;Junto con la radiación, el agujero negro pierde energía, y dado que la energía es lo mismo que la masa, el agujero negro pierde masa mientras brilla, haciéndose cada vez más pequeño. Los agujeros negros grandes, fríos, emiten muy poca radiación y por lo tanto pierden masa muy lentamente, mientras que a la vez atraen fuertemente y engullen todo lo que los rodea, por lo que ganan masa muy rápidamente. El efecto resultante de esta competencia es que los agujeros negros grandes crecen. En cambio los agujeros negros pequeños, calientes, emiten muchísima radiación por lo que pierden masa muy rápidamente, y mientras tanto atraen y engullen los objetos de su entorno muy lentamente, por lo que ganan muy poca masa. Es decir que el efecto resultante es que se evaporan muy rápidamente. De hecho en unos pocos microsegundos. En tan corto tiempo no son capaces de absorber ni siquiera el átomo más cercano.&lt;/p&gt;
&lt;p&gt;En conclusión, aquéllos quienes nos dedicamos a la Física de las Interacciones Fundamentales vivimos el momento más estimulante de la última década. Todas las puertas están por abrirse, lo que sea que encontremos del otro lado será desconocido y maravilloso. Casi cualquier escenario posible de descubrimiento en el LHC es extremadamente interesante. La posibilidad más aburrida posible es que encontremos sólo el bosón de Higgs ¡y eso no tiene nada de aburrido! Significaría que el Modelo Standard que hemos desarrollado a lo largo de los últimos 30 años es esencialmente correcto. Pero es probable que encontremos además muchas nuevas partículas elementales, dimensiones extra y hasta efímeros agujeros negros. Incluso si contra todas las expectativas el bosón de Higgs no apareciera, las implicaciones de tal ausencia serían enormes y el proceso ulterior de reformulación del Modelo Standard sería de lo más estimulante. La cantidad de trabajo inminente recuerda la antigua maldición china “ojala vivas en tiempos interesantes”.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Capilaridad, diferencias de presión, transpiración, y.... Nuevo mecanismo para el transporte de agua en plantas vasculares</title>
      <link>https://ciencianet.com.ar/post/capilaridad-diferencias-de-presion-transpiracion-y-nuevo-mecanismo-para-el-transporte-de-agua-en-plantas-vasculares/</link>
      <pubDate>Wed, 10 Sep 2008 22:15:12 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/capilaridad-diferencias-de-presion-transpiracion-y-nuevo-mecanismo-para-el-transporte-de-agua-en-plantas-vasculares/</guid>
      <description>
        
          &lt;h6 id=&#34;este-artículo-ha-sido-financiado-por-el-proyecto-invofi-de-la-asociación-física-argentina&#34;&gt;Este artículo ha sido financiado por el proyecto INVOFI de la Asociación Física Argentina.&lt;/h6&gt;
&lt;p&gt;Mediante un modelo matemático simple, científicos argentinos proponen un mecanismo basado en consideraciones geométricas, que acercaría un poco más de luz sobre el aún incompleto conocimiento del fenómeno de transporte de agua en plantas vasculares.&lt;/p&gt;
&lt;p&gt;El transporte de agua y otras sustancias desde las raíces hasta la hoja es un problema de la fisiología vegetal que aún hoy en día deja más dudas que certezas a la hora de establecer los mecanismos que intervienen en este complejo e interesante proceso. Algunos de estos mecanismos han sido estudiados, pero con el estado actual del conocimiento en esta materia no puede explicarse acabadamente este fenómeno que permite la vida y el crecimiento de estos organismos.&lt;/p&gt;
&lt;p&gt;El sistema vascular xilemático (capilares micrométricos interconectados) es el encargado del transporte de agua y nutrientes de la raíz hasta las hojas. Estos capilares están interconectados, lateral y axialmente, por orificios denominados pits. Estos orificios poseen en su interior una membrana conformada por una red de micro fibras elásticas que actúan como una válvula capilar. Debido a la alta evaporación, la presión dentro de los tubos desciende hasta valores negativos (tensión) y, por esto, a través de la membrana del pit, se generan burbujas (proceso denominado &lt;em&gt;air-seeding&lt;/em&gt;), obstruyéndose así el flujo de la savia ascendente.&lt;/p&gt;
&lt;p&gt;No esta claro si la formación de burbujas en los capilares (cavitación) perjudica al árbol, evitando el flujo de agua, o beneficia su estructura eliminando las tensiones que podrían romper las paredes de los tubos. El proceso es factible cuando un tubo lleno de savia y otro de aire son adyacentes. Debido a la elevada tensión en el tubo con savia, la interfase liquido-aire se desplaza hasta que una burbuja vence la fuerza capilar y entra en el sistema disparando la cavitación. Esa burbuja se expande relajando el líquido y deteniendo el flujo de agua en ese tubo. Cuando la transpiración disminuye, durante la noche, ese tubo se rellena y el sistema recupera así su conductividad inicial.&lt;/p&gt;
&lt;p&gt;En el artículo publicado en la revista &lt;em&gt;Tree Physiology&lt;/em&gt;, Ariel Meyra, Victor Kuz y Guillermo Zarragoicoechea proveen una posible explicación de este mecanismo de &lt;em&gt;air-seeding&lt;/em&gt;, propuesto por Martin Zimmermann, y de cómo funcionan las membranas que dan origen al mismo. También afirman que la la posibilidad de variar la curvatura de las válvulas no sólo es importante para producir el &lt;em&gt;air-seeding&lt;/em&gt;, sino también en el control del transporte de savia, al producir un exceso de presión capilar.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo Original:&lt;/strong&gt; &lt;a href=&#34;https://academic.oup.com/treephys/article/27/10/1401/1659188&#34;&gt;Meyra, Ariel G; Kuz, Victor A; Zarragoicoechea, Guillermo J. &lt;em&gt;Geometrical and physicochemical considerations of the pit membrane in relation to air seeding: the pit membrane as a capillary valve&lt;/em&gt;. Tree Physiology 27, 1401-1405 (2007).&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instituciones:&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (IFLySiB - CCT CONICET La Plata), UNLP, CICPBA.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; &lt;a href=&#34;mailto:vasco@iflysib.unlp.edu.ar&#34;&gt;vasco@iflysib.unlp.edu.ar&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Experimentos con rayos que hicieron historia</title>
      <link>https://ciencianet.com.ar/post/experimentos-con-rayos-que-hicieron-historia/</link>
      <pubDate>Thu, 22 May 2008 23:46:49 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/experimentos-con-rayos-que-hicieron-historia/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero&lt;/strong&gt;. Instituto de Investigaciones Fisicoquímicas Teóricas y Aplicadas (CONICET-UNLP-CIC).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Relato presentado en el Museo de Física de la Universidad Nacional de La Plata durante la Noche de los Museos 2008.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;En la década de 1870, el británico William Crookes, reconocido químico de la época, creyó que había encontrado un nuevo estado de la materia que se sumaba a los tres ya conocidos (sólido, líquido y gaseoso): la materia radiante.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/05/florence2.jpg&#34; alt=&#34;William Crook con la medium Florence Cook.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Como muchos otros investigadores, tenía diversos intereses: creía que podía estudiar científicamente la “fuerza psíquica” que ejercían los mediums mediante experimentos. Fue uno de los más importantes investigadores de lo que se llama Espiritismo Científico. Llegó incluso a publicar un artículo en la revista &lt;em&gt;Quarterly Journal of Science&lt;/em&gt; –de la cual fue editor–, donde clasificaba los 13 tipos de fenómenos que había observado.&lt;/p&gt;
&lt;p&gt;En aquel entonces los científicos estaban completando la Tabla Periódica, descubriendo nuevos elementos químicos. Crookes era experto en la identificación de sustancias químicas a partir de sus espectros de emisión y había descubierto un nuevo elemento, el Talio. Entre otras cosas, generaba y estudiaba descargas eléctricas en tubos con gases a baja presión. Alrededor de 1875, Crookes mejoró los tubos de vacío inventados por Geissler. Estos tubos tenían dos placas metálicas (ánodo y cátodo) y cuando se conectaban a una fuente eléctrica mostraban zonas luminosas, diferentes según la presión del gas.&lt;/p&gt;
&lt;p&gt;Crookes consiguió alcanzar presiones aún más bajas, obteniendo descargas que se propagaban en línea recta, en forma de rayos. Cuando estos misteriosos “rayos catódicos” impactaban contra las paredes del vidrio generaban un llamativo resplandor verde pálido. Motivado por su descubrimiento, hizo más experimentos. Haciendo girar molinillos de mica dentro de los tubos, se convenció de que estaba observando materia, pero en un nuevo estado, que llamó radiante. Pensaba que en el alto vacío del tubo, el gas llegaba a un inconcebible estado de división, y sus átomos eran rechazados por el cátodo, generando los rayos. Además, los rayos podían producir también efectos térmicos y ser desviados por campos magnéticos, sugiriendo de que se trataba de partículas eléctricamente cargadas emitidas por el cátodo.&lt;/p&gt;
&lt;h3 id=&#34;controversia&#34;&gt;Controversia&lt;/h3&gt;
&lt;p&gt;Otros investigadores se sumaron al estudio del nuevo fenómeno y pronto se generó un debate. El físico alemán Lenard era el principal opositor a la hipótesis de Crookes. Había observado que los rayos catódicos podían atravesar láminas metálicas delgadas sin ser desviados de su trayectoria recta. Sostenía entonces que no podía tratarse de partículas sino de “perturbaciones ondulatorias del éter” (actualmente, ondas electromagnéticas). Muchos ingleses se sumaron al bando de Crookes; entre otros Thomson y FitzGerald. Pero los alemanes, entre quienes se encontraban Hertz y Goldstein, se alineaban detrás de Lenard. No es la materia que viaja –decían- es el éter que vibra. Por otra parte, la crítica a sus investigaciones “del otro mundo” fue unánime. En 1907 recibió el Premio Nobel de Química.&lt;/p&gt;
&lt;h3 id=&#34;consecuencias-de-los-experimentos-de-crookes&#34;&gt;Consecuencias de los experimentos de Crookes&lt;/h3&gt;
&lt;h4 id=&#34;rayos-x&#34;&gt;Rayos X&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/05/Rontgen-en-su-laboratorio.png&#34; alt=&#34;Wilhem Röntgen en su laboratorio.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;En 1895 Wilhem Röntgen se entusiasmó con la fluorescencia observada por Crookes. Se preguntaba si los rayos catódicos atravesaban el vidrio de los tubos, y para comprobarlo, cubrió con cartón uno de los mismos. No observó ningún resplandor, pero sí vio luminiscencia en una pantalla de platinocianuro de bario que tenía en su laboratorio.&lt;/p&gt;
&lt;p&gt;Durante las siguientes semanas, repitió el experimento interponiendo diferentes materiales entre la pantalla y el tubo, notando que sólo el plomo podía impedir la luminiscencia. La conclusión era inevitable: el tubo emitía algún tipo de radiación, invisible pero penetrante en la materia. Cuando intentó en fotografiar este fenómeno encontró otra sorpresa: las placas fotográficas que tenía estaban veladas. Para comprobar el alcance de la radiación en la emulsión, colocó el tubo y la placa fotográfica en distintas habitaciones, obteniendo una imagen de la puerta que las separaba. Obtuvo también imágenes del paso de la radiación a través del cuerpo humano. La primera radiografía fue una imagen de la mano de su esposa Bertha luego de una exposición de 15 minutos.&lt;/p&gt;
&lt;p&gt;Röntgen se convirtió en el científico del momento. Había descubierto los rayos X. Posteriormente a su conferencia de 1896 cosechó múltiples reconocimientos y en 1901 recibió el Nobel de Física. A pesar de las posibles aplicaciones, Röntgen se negó a comercializar o patentar su descubrimiento, argumentando que el beneficio pertenecía a la Humanidad.&lt;/p&gt;
&lt;h4 id=&#34;radiactividad&#34;&gt;Radiactividad&lt;/h4&gt;
&lt;p&gt;Motivado por las investigaciones de Crookes y Röntgen, Henri Becquerel, en 1896, retomó el estudio iniciado por su padre sobre minerales fluorescentes. Como en los tubos de Crookes la emisión de rayos X estaba acompañada de la fluorescencia, Becquerel se preguntó si sus materiales luminosos emitirían también rayos X.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/05/Becquerel-curie--300x189.jpg&#34; alt=&#34;Henri Bequerel juento a Pierre y Marie Curie.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Comenzó a experimentar: exponía una muestra de sal de uranio al sol y luego la depositaba sobre una placa fotográfica cubierta por un grueso envoltorio. Observaba que la placa se velaba. Un día nublado alteró su rutina: sin previa exposición colocó la muestra sobre la placa, y la guardó a la espera de días soleados.&lt;/p&gt;
&lt;p&gt;Poco después, con un presentimiento, reveló la placa y encontró la veladura provocada por la muestra, notando que la radiación se emitía sin necesidad de la exposición a la luz. Becquerel había descubierto la propiedad de ciertas sustancias de emitir por sí mismas radiación penetrante, posteriormente nombrada radiactividad por Mme. Curie. Cuando Becquerel (y el resto de la comunidad) observó que no podía obtener imágenes de huesos como ocurría con los rayos X, se desinteresó del asunto. Más tarde, propuso a la joven estudiante Marie Curie que continuara la investigación. Junto al matrimonio Curie, Becquerel recibió el Premio Nobel de Física en 1903.&lt;/p&gt;
&lt;h4 id=&#34;descubrimiento-del-electrón&#34;&gt;Descubrimiento del electrón&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/05/thomson.jpg&#34; alt=&#34;Joseph Thomson en su laboratorio.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;En la prolongada controversia onda-partícula, Joseph J. Thomson dio la respuesta definitiva, al menos hasta el advenimiento de la Mecánica Cuántica. Diseñó un dispositivo para hacer pasar los rayos por un campo magnético o eléctrico, desviando sus trayectorias. Aplicando un campo electromagnético, y mediante argumentos teóricos, pudo determinar tanto la velocidad de las partículas como el cociente entre su carga eléctrica y su masa.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/05/mano-lp-683x1024.jpg&#34; alt=&#34;Mano humana: una de las primeras imágenes de radiografías obtenidas en Argentina (Instituto de Física La Plata).&#34;&gt;&lt;/p&gt;
&lt;p&gt;En aquel tiempo, las únicas partículas cargadas negativamente que se conocían eran los iones negativos de los átomos. Pero las partículas de los rayos catódicos no podían identificarse con tales iones, pues para ser desviadas tan marcadamente, debían de poseer una carga eléctrica inimaginablemente elevada, o bien tratarse de partículas muy ligeras, mil veces más livianas que el átomo más ligero. Esta última interpretación encajaba mejor, y por otra parte, los físicos habían intuido ya que la corriente eléctrica era transportada por partículas cargadas.&lt;/p&gt;
&lt;p&gt;Los rayos catódicos fueron entonces identificados como las trayectorias de partículas subatómicas, que además eran las unidades elementales de la electricidad, dándoseles el nombre de electrones. Aunque algunos grandes científicos de la época, como Lord Kelvin, menospreciaron el hallazgo, en 1906 Thomson recibió el Nobel de Física.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>¿Las Acuaporinas favorecen un aumento del Calcio intracelular?</title>
      <link>https://ciencianet.com.ar/post/las-acuaporinas-favorecen-un-aumento-del-calcio-intracelular/</link>
      <pubDate>Sat, 29 Mar 2008 00:20:13 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/las-acuaporinas-favorecen-un-aumento-del-calcio-intracelular/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Osvaldo Chara&lt;/strong&gt;: Instituto de Física de Líquidos y Sistemas Biológicos (CONICET-UNLP-CIC)&lt;/p&gt;
&lt;p&gt;Luciano Galizia y colaboradores de la Facultad de Medicina (UBA) demostraron recientemente que las Acuaporinas (proteínas ubicadas en las membranas de las células y que actúan como canales que permiten pasar agua) serían importantes en el aumento de la concentración de Calcio intracelular provocado bajo ciertos procesos osmóticos.&lt;/p&gt;
&lt;p&gt;Cuando las células son bañadas con una solución de osmolaridad menor que su interior celular, estas responden incrementando su volumen, por ósmosis. A continuación, se activan proteínas en la membrana celular que producen salida de solutos, los cuales producen, nuevamente por ósmosis, salida de agua. Esto reduce el volumen de la célula. Dicha respuesta se denomina Regulación de Volumen Decreciente (RVD). El pasaje de agua en este proceso tiene lugar o bien a través de la bicapa fosfolipídica, o alternativamente mediante &lt;a href=&#34;http://es.wikipedia.org/wiki/Acuaporina&#34;&gt;Acuaporinas&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/03/celulas-768x278.jpg&#34; alt=&#34;Figura: células cargadas con un compuesto fluorescente antes (izquierda) y después (derecha) de ser bañadas con medio hipotónico.&#34;&gt;&lt;/p&gt;
&lt;p&gt;Ya se conocían, previamente, los siguientes hechos: 1) una de estas Acuaporinas, la AQP2, era importante en el RVD de células renales, 2) el ingreso de agua por ósmosis provocaba un aumento en la concentración intracelular de Calcio y 3) algunos mecanismos relacionados con el RVD dependían de la concentración de Calcio dentro de la célula. En un intento de conectar estos tres hechos, Galizia y colaboradores, del &lt;a href=&#34;http://www.fmed.uba.ar/depto/fisiologia/biomem.htm&#34;&gt;Laboratorio de Biomembranas&lt;/a&gt; de la Facultad de Medicina (Universidad de Buenos Aires) formularon la siguiente pregunta: ¿Cumple la AQP2 algún papel en el aumento de la concentración intracelular de Calcio durante el ingreso de agua por ósmosis? Los resultados de sus estudios fueron publicados recientemente en &lt;em&gt;American Journal of Physiology&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Trabajando con células con y sin AQP2 cargadas con compuestos fluorescentes los autores pudieron medir el volumen celular y la concentración de Calcio intracelular durante un proceso de ósmosis como el descrito arriba. Interesantemente, los autores observaron que sólo en presencia de AQP2, las células muestran un incremento en la concentración intracelular de Calcio durante la ósmosis. Los autores concluyeron que la AQP2 en la membrana de las células sería crítica para incrementar la concentración de Calcio, lo cual es necesario para activar el RVD.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referencia:&lt;/strong&gt; Galizia L, Flamenco MP, Rivarola V, Capurro C, Ford P. Role of AQP2 in activation of calcium entry by hypotonicity: implications in cell volume regulation. &lt;a href=&#34;http://www.ncbi.nlm.nih.gov/pubmed/18094031?ordinalpos=1&amp;amp;itool=EntrezSystem2.PEntrez.Pubmed.Pubmed_ResultsPanel.Pubmed_RVDocSum&#34;&gt;Am J Physiol Renal Physiol. 2008. 294:F582-F590&lt;/a&gt;.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Bolas de acero hacen salpicaduras en la arena</title>
      <link>https://ciencianet.com.ar/post/bolas-de-acero-hacen-salpicaduras-en-la-arena/</link>
      <pubDate>Wed, 19 Mar 2008 00:30:24 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/bolas-de-acero-hacen-salpicaduras-en-la-arena/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro:&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET-UNLP)&lt;/p&gt;
&lt;p&gt;El aire entre los granos de arena hacen que la misma actúe como un fluido, de acuerdo con físicos holandeses que arrojaron bolas sobre arena para simular impacto de meteoritos. Realizando experimentos en una cámara al vacío, los investigadores descubrieron que las bolas penetraron más profundo en la arena y arrojaban más material a mayores presiones ambientes.&lt;/p&gt;
&lt;p&gt;Estos resultados arrojan nueva luz sobre la relación entre sistemas granulares y fluidos. Dos años atrás, Detlef Lohse y un equipo de físicos de la Universidad de Twente, idearon un mecanismo para la formación de cráteres arrojando bolas metálicas en camas de arena. Encontraron que el impacto arroja arena hacia afuera, en una salpicadura con forma de corona, dejando que la bola penetre más profundamente en la superficie, creando un hoyo. Entonces, la presión de la arena fuerza a los granos a llenar el hoyo, causando un chorro vigoroso de arena que se dispara hacia el aire desde el centro.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/03/pres-sand-235x300.jpg&#34; alt=&#34;Chorro de arena: Fotografías de alta velocidad que muestran el efecto de la presión en la altura del chorro de arena. Fuente: PhysicsWeb.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Los investigadores concluyeron que este mecanismo es muy similar a lo que sucede en un líquido. Esto significa que, bajo ciertas circunstancias, las conocidas ecuaciones de la dinámica de fluidos podrían ser utilizadas para describir los poco comprendidos sistemas granulares como la arena.&lt;/p&gt;
&lt;p&gt;Ahora, Lohse y colaboradores han avanzado un paso más y encontraron que la presión del aire sobre la arena está relacionada con la altura del chorro y la profundidad a la que penetra la bola. En un nuevo conjunto de experimentos, arrojaron bolas de acero de 1,4 cm en una cama de arena de 40 cm de profundidad. Pero esta vez pusieron todo el aparato en un contenedor en el que podían hacer vacío con una bomba de aire y midieron la profundidad a la que penetraron las bolas. A menor presión de aire, encontraron que el chorro es menos vigoroso, y que las bolas no penetran tan profundo.&lt;/p&gt;
&lt;p&gt;Los investigadores afirman que esto se debe a que una menor presión significa que hay menos aire alrededor de la bola y de los granos de arena, incrementando el arrastre y haciendo que la arena se comporte menos como un fluido. El resultado es una menor penetración y un chorro más corto. &amp;quot;Esto revela la importancia del aire en la materia granular fina&amp;quot;, afirma Lohse. Gabriel Caballero, co-autor del trabajo, dijo que el estudio tendría eventuales aplicaciones que van desde el diseño de sondas espaciales que deben aterrizar en otros planetas, al mezclado de polvos químicos en la industria farmacéutica. El equipo se encuentra ahora investigando cómo la salpicadura de arena crea el cráter originado.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fuente:&lt;/strong&gt; &lt;a href=&#34;http://physicsweb.org/articles/news/11/7/13/1?rss=2.0&#34;&gt;PhysicsWeb&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referencia:&lt;/strong&gt; &lt;a href=&#34;https://doi.org/10.1103/PhysRevLett.99.018001&#34;&gt;Physical Review Letters, &lt;strong&gt;99&lt;/strong&gt;, 018001 (2007)&lt;/a&gt;.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>El grafeno continua sorprendiendo</title>
      <link>https://ciencianet.com.ar/post/el-grafeno-continua-sorprendiendo/</link>
      <pubDate>Tue, 18 Mar 2008 00:43:03 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/el-grafeno-continua-sorprendiendo/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro:&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET-UNLP) y Universidad Tecnológica Nacional.&lt;/p&gt;
&lt;p&gt;¿Hay algo que el &lt;a href=&#34;http://en.wikipedia.org/wiki/Graphene&#34;&gt;grafeno&lt;/a&gt; -láminas de carbono de solo un átomo de espesor- no pueda hacer? Desde que este material fuera descubierto en 2004, ha mostrado ser un conductor eléctrico extremadamente bueno; un semiconductor que puede ser utilizado para crear transistores; un material muy resistente que puede utilizarse para hacer membranas ultra delgadas. Ahora, investigadores en Estados Unidos han confirmado que el grafeno puede ser también un muy buen conductor del calor.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/03/Graphene_xyz-300x254.jpg&#34; alt=&#34;Grafeno: lámina de carbono de un átomo de espesor. Fuente: Wikipedia.&#34;&gt;&lt;/p&gt;
&lt;p&gt;El equipo, que tuvo que inventar una nueva manera de medir la conductividad térmica para poder estudiar el material, está ahora investigando cómo pueden utilizarse las propiedades térmicas del grafeno para enfriar chips de silicio ultra rápidos. Los físicos sospechaban que el grafeno puede conducir muy bien el calor debido a que nanotubos de carbono, conformados por grafeno enrollado en tubos muy delgados, son muy buenos conductores térmicos. Sin embargo, el grafeno puede ser muy difícil de trabajar, y los investigadores han pugnado por determinar sus propiedades térmicas utilizando las técnicas tradicionales que involucran adherirle calentadores y otros dispositivos al material.&lt;/p&gt;
&lt;h3 id=&#34;dispersión-de-raman&#34;&gt;Dispersión de Raman&lt;/h3&gt;
&lt;p&gt;Alexander Balandin y colegas de la Universidad de California-Riverside han diseñado una nueva técnica de medición que usa un láser para calentar el grafeno y medir su temperatura. El equipo suspendió láminas de grafeno sobre zanjas de micrómetros de ancho excavadas en una superficie de óxido de silicio. Las láminas tenían varios micrometros de longitud y fueron fijadas en ambos extremos por capas de grafito, que actuaron como sumideros de calor.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/03/esquemaGrafeno-300x235.jpg&#34; alt=&#34;Esquema del experimento: Diagrama de la técnica utilizada para medir la conductividad térmica. Fuente: nanotechweb.org.&#34;&gt;&lt;/p&gt;
&lt;p&gt;El centro de la lámina es entonces expuesta al haz de luz láser, que calienta el grafeno y cambia las frecuencias a las cuales vibran los átomos de carbono. Parte de la luz láser cambia su frecuencia debido a que sufre una dispersión Raman, y la magnitud del cambio en la frecuencia es proporcional a la temperatura de la región iluminada.&lt;/p&gt;
&lt;h3 id=&#34;cambio-en-la-frecuencia&#34;&gt;Cambio en la frecuencia&lt;/h3&gt;
&lt;p&gt;Midiendo el cambio en la frecuencia -y por lo tanto la temperatura del grafeno- como función de la potencia del láser, los investigadores pudieron calcular la conductividad térmica del grafeno, cuyo valor resultó ser de 5300 W/(m K) a temperatura ambiente. Este es el valor más alto conocido para un sólido: 50% más alto que el de los nanotubos de carbono y más de 10 veces mayor que el de los metales como cobre y aluminio. Balandin dijo a &lt;a href=&#34;https://physicsworld.com/a/graphene-continues-to-amaze/&#34;&gt;physicsworld.com&lt;/a&gt; que el equipo se sorprendió de encontrar que el grafeno es mucho mejor conductor del calor que los nanotubos de carbono, aún cuando algún trabajo teórico había sugerido que esto era posible.&lt;/p&gt;
&lt;p&gt;La gran conductividad térmica del grafeno es probablemente resultado de la relativa facilidad que tienen los átomos de carbono para moverse en el grafeno, comparada con otros materiales. Balandin y sus colegas están ahora trabajando en una teoría que explique por qué esto es así. Balandin cree que la alta conductividad térmica del grafeno, su forma plana y su capacidad para integrarse con el silicio podrían jugar un papel importante en la disipación de calor de dispositivos electrónicos. El equipo está trabajando actualmente en el diseño de transistores ultra rápidos enfriados por grafeno.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fuente:&lt;/strong&gt; &lt;a href=&#34;https://physicsworld.com/a/graphene-continues-to-amaze/&#34;&gt;physicsworld&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referencia:&lt;/strong&gt; Alexander A. Balandin, Suchismita Ghosh, Wenzhong Bao, Irene Calizo, Desalegne Teweldebrhan, Feng Miao, and Chun Ning Lau. &amp;quot;Superior Thermal Conductivity of Single-Layer Graphene&amp;quot;. &lt;a href=&#34;https://doi.org/10.1021/nl0731872&#34;&gt;ASAP Nano Lett., ASAP Article, 10.1021/nl0731872&lt;/a&gt; , Febrero 20, 2008.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Identificación automática de objetos en imágenes con ruido</title>
      <link>https://ciencianet.com.ar/post/identificacion-automatica-de-objetos-en-imagenes-con-ruido/</link>
      <pubDate>Wed, 12 Mar 2008 00:46:51 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/identificacion-automatica-de-objetos-en-imagenes-con-ruido/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Luis Pugnaloni&lt;/strong&gt;. Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP).&lt;/p&gt;
&lt;p&gt;Hacer que una computadora detecte objetos en una imagen es un problema central. Pensemos en la necesidad de diferenciar y medir los espacios cultivados, forestados y urbanos a partir de imágenes aéreas o satelitales de todo el país. Investigadores de la Universidad de Buenos Aires presentaron un estudio donde comparan varios métodos de detección de bordes de objetos en imágenes con un tipo de ruido llamado &amp;quot;speckle&amp;quot; que aparece, por ejemplo, al usar luz láser.&lt;/p&gt;
&lt;p&gt;El trabajo demuestra que un método basado en el conocimiento del tipo de ruido que afecta a la imagen permite detectar rápida y correctamente las áreas de diferentes partes de la imagen. El trabajo de Juliana Gambini, Marta E. Mejail y Julio Jacobo-Berlles de la Facultad de Ciencias Exactas y Naturales de la Universidad de Buenos Aires en colaboración con Alejandro C. Frery (de la Universidade Federal de Alagoas, Brasil) presenta 5 métodos de detección de bordes en imágenes con ruido &amp;quot;speckle&amp;quot;. Algunos de estos métodos fueron desarrollados por ellos.&lt;/p&gt;
&lt;p&gt;El ruido &amp;quot;speckle&amp;quot; aparece en imágenes iluminadas con fuentes de iluminación coherentes. Tal es el caso del láser, el radar de apertura sintética (SAR), el sonar y el ultrasonido (usado para las ecografías). Las imágenes se ven moteadas a causa de la interferencia de reflejos provenientes de diferentes partes del objeto iluminado y de otros objetos cercanos. Detectar los bordes de un contorno que separa dos regiones de una imagen con tal tipo de ruido es particularmente complejo.&lt;/p&gt;
&lt;p&gt;Gambini y colaboradores usaron un modelo estadístico sobre la distribución aleatoria particular que presenta el ruido &amp;quot;speckle&amp;quot; para así distinguir entre el ruido proveniente de áreas de la imagen con diferente textura. Un algoritmo que les permite detectar los puntos donde el cambio de textura es abrupto les posibilitó identificar en forma eficiente y certera los bordes de cada región de una imagen.&lt;/p&gt;
&lt;p&gt;Puesto a prueba con otros métodos de detección éste resultó ser el más eficaz. Los autores del trabajo muestran las capacidades de este método de detección usando imágenes aéreas tomadas con un radar de apertura sintética. El trabajo podría significar una importante mejora en la evaluación de áreas cultivadas, crecimiento urbano, desastres naturales, etc. que son examinados a través de imágenes que necesariamente llevan ruido &amp;quot;speckle&amp;quot;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; &amp;quot;Accuracy of edge detection methods with local information in speckled imagery&amp;quot;, &lt;a href=&#34;https://doi.org/10.1007/s11222-007-9034-y&#34;&gt;Statistics and Computing&lt;/a&gt; , vol. 18, pp 15 (2008).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instituciones:&lt;/strong&gt; &lt;a href=&#34;http://exactas.uba.ar/&#34;&gt;Facultad de Ciencias Exactas y Naturales&lt;/a&gt; (UBA), &lt;a href=&#34;http://www.ic.ufal.br/&#34;&gt;Instituto de Computación&lt;/a&gt; (Universidade Federal de Alagoas, Brasil)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; Juliana Gambini (E-mail: &lt;a href=&#34;mailto:jgambini@dc.uba.ar&#34;&gt;jgambini@dc.uba.ar&lt;/a&gt; )&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Más información:&lt;/strong&gt; &lt;a href=&#34;http://es.wikipedia.org/wiki/Speckle&#34;&gt;Speckle&lt;/a&gt;, &lt;a href=&#34;http://es.wikipedia.org/wiki/Radar_de_apertura_sint%C3%A9tica&#34;&gt;SAR&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Hielo solar. Una heladera que funciona con el calor del Sol</title>
      <link>https://ciencianet.com.ar/post/hielo-solar-una-heladera-que-funciona-con-el-calor-del-sol/</link>
      <pubDate>Tue, 12 Feb 2008 00:53:29 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/hielo-solar-una-heladera-que-funciona-con-el-calor-del-sol/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Paula Bergero.&lt;/strong&gt; Facultad de Ciencias Exactas (UNLP).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2008/02/heladera3-198x300.jpg&#34; alt=&#34;:left&#34;&gt;
En tiempos en que la crisis energética es un tema de conversación habitual y la preocupación por el medio ambiente nos desvela, algunos investigadores argentinos dedican tiempo y esfuerzo al estudio de un mejor aprovechamiento de las llamadas energías sustentables. Por otra parte, amplios sectores de la sociedad, por ejemplo los asentamientos de emergencia del conurbano bonaerense, pobladores rurales en la Patagonia o comunidades seminómades como los kollas Tinkunaku de Salta, no tienen acceso a recursos básicos como el gas natural o la electricidad.&lt;/p&gt;
&lt;p&gt;Desde hace varios años y en diversos lugares del mundo se vienen desarrollando prototipos de cocinas solares, y ya existen diversos diseños sencillos y de bajo costo. En este &lt;a href=&#34;http://bioingenieria.edu.ar/grupos/puertociencia/documentos/actualizado/cocinas%20solares.PDF&#34;&gt;link&lt;/a&gt; se puede encontrar un interesante compendio elaborado en la Universidad Nacional de Entre Ríos.&lt;/p&gt;
&lt;p&gt;Pero mucho más innovador resulta el proyecto que se viene desarrollando desde 2003 en la Universidad Nacional de General Sarmiento, en Los Polvorines, para la fabricación de una heladera que funciona con el Sol. Para más sorpresa, esta heladera solar enfría de noche.&lt;/p&gt;
&lt;p&gt;El proyecto es dirigido por el Dr. en Física Rodolfo Echarri, y su equipo está formado por el Lic. Andrés Sartarelli y el Prof. Sergio Vera. Además cuenta con la colaboración del INTEC -Instituto Tecnológico de Santo Domingo, República Dominicana- donde la Ingeniera Inna Samson dirige un equipo de las mismas características.&lt;/p&gt;
&lt;p&gt;Los resultados del proyecto ya están a la vista. El dispositivo creado en la UNGS que funciona mediante la adsorción y desorción de metanol por carbón activado, ya ha llegado a producir 300 gramos de hielo. Suficiente para enfriar una cerveza. El prototipo de heladera está formado por un colector, un condensador, un evaporador y una cámara fría. No tiene enchufe ni motor, es más, no tiene partes móviles.&lt;/p&gt;
&lt;p&gt;El colector es un recipiente que contiene carbón activado (compuesto de carbono, muy poroso y adsorbente), que al principio del ciclo se encuentra “empapado” con metanol (alcohol metílico o alcohol de madera). El condensador convierte en líquido los vapores del metanol. El evaporador recoge este líquido y permite que se evapore nuevamente. En la cámara fría es donde el agua se transforma en hielo. Hilando más finamente, dentro del colector, las moléculas de metanol se encuentran pegadas a la superficie del carbón activado, en un estado que químicamente se define como “adsorbidas”. Para despegar las moléculas de la superficie –es decir, desorberlas- necesitamos entregarle energía al sistema.&lt;/p&gt;
&lt;p&gt;También necesitamos energía para convertir un líquido en vapor. Tanto el proceso de desorción como el de evaporación requieren que el sistema disponga de energía; por ejemplo, aumentando su temperatura. ¿Cómo es el funcionamiento de este dispositivo? Inicialmente, el colector con el carbón activado y el metanol está a temperatura ambiente. Cuando es expuesto a la radiación solar, la temperatura del mismo va aumentando, de modo que el alcohol se va desorbiendo, y pasa al condensador. Allí se acumula en estado gaseoso hasta que alcanza una presión a la cual ocurre la condensación, y vuelve entonces al estado líquido. Se acumula en el evaporador. A medida que el día transcurre y la radiación va disminuyendo, la temperatura desciende.&lt;/p&gt;
&lt;p&gt;Durante la noche el proceso se invierte. El metanol líquido que se encuentra en el evaporador se empieza a evaporar; para este proceso toma energía de la cámara fría haciendo que la temperatura en ella descienda. Esto último fenómeno podemos experimentarlo mojándonos la piel con alcohol: para evaporarse toma energía del cuerpo, haciendo descender la temperatura en la superficie.&lt;/p&gt;
&lt;h3 id=&#34;entrevista-a-rodolfo-echarri-investigador-del-conicet-y-profesor-de-la-ungs&#34;&gt;Entrevista a Rodolfo Echarri, Investigador del Conicet y profesor de la UNGS.&lt;/h3&gt;
&lt;p&gt;-¿Cómo surgió el proyecto de crear una heladera solar? - &lt;strong&gt;R.E.&lt;/strong&gt;: Dadas las características de nuestra Universidad, que intenta una fuerte articulación entre lo académico y el aporte directo al mejoramiento del nivel de vida de la comunidad, decidimos dar un paso en este sentido. El tema de la heladera cumple con ese rol al mismo tiempo que desde el punto de vista académico presenta un desafío mucho mayor que el de un calentador solar, ya que implica el desarrollo de una bomba de calor.&lt;/p&gt;
&lt;p&gt;-¿Cómo está compuesto el equipo de trabajo? - &lt;strong&gt;R.E.&lt;/strong&gt;: el equipo está formado por dos “sub-equipos”. Uno en la UNGS, que cuenta por el momento con tres personas (y otra más a incorporarse en marzo) que tienen una característica muy pronunciada en común: la preocupación por hacer de la física una herramienta de acercamiento a la comunidad. Tanto Andrés como yo, hemos sido formados en la Facultad de Ciencias Exactas y Naturales de la UBA, mientras que Sergio y Ernesto (el próximo integrante) se formaron en esta Universidad.&lt;/p&gt;
&lt;p&gt;La otra parte del equipo, que desarrolla el trabajo en República Dominicana, está formado por la Ingeniera Inna Samson que posee una maestría en Física del Calor y una serie de alumnos de ingeniería que van pasando para desarrollar partes puntuales del proyecto. En total ya han pasado once, gozando de una pequeña beca cada uno.&lt;/p&gt;
&lt;p&gt;-¿Cuáles fueron las dificultades que encontraron en el desarrollo del prototipo? - &lt;strong&gt;R.E.&lt;/strong&gt;: Hubo dificultades propias de cualquier proyecto de investigación (errores y aprendizajes de los mismos), pero también hubo dificultades no tan comunes, como por ejemplo, la falta de un lugar físico donde realizar el armado del prototipo (el primero fue armado en la terraza de mi casa), o la falta de dinero. Sin embargo, dada la buena voluntad de la Universidad, en poco tiempo más contaremos con un lugar más adecuado.&lt;/p&gt;
&lt;p&gt;-¿Cuáles serían las vías para que la heladera solar llegue efectivamente a los potenciales usuarios? - &lt;strong&gt;R.E.&lt;/strong&gt;: Nuestro deseo es lograr un modelo apto para su uso y que logremos obtener financiamiento estatal o de otras organizaciones para producir heladeras a ser colocadas en lugares de alta necesidad. Dicho de otra forma, creemos que el estado tiene que jugar un papel fundamental en la distribución de los bienes.&lt;/p&gt;
&lt;p&gt;-En tu opinión, ¿por qué no hay más proyectos con compromiso social como éste en Argentina? - &lt;strong&gt;R.E.&lt;/strong&gt;: En realidad, creo que la ciencia argentina (y en particular la física) está pensada en un contexto que no mira la realidad de nuestro país. Muchos científicos piensan que la verdadera ciencia es la totalmente aséptica, sin una relación directa con el contexto social, y no tienen en cuenta que el dinero de las investigaciones lo aporta toda la comunidad. Por eso, cuando se trata de un desarrollo tecnológico, a los científicos les da la impresión de estar haciendo “ciencia de segunda”&lt;/p&gt;
&lt;p&gt;-El sistema científico evalúa la producción de los investigadores principalmente contando el número de publicaciones, ¿Qué reconocimientos tiene un proyecto de este tipo? - &lt;strong&gt;R.E.&lt;/strong&gt;: El reconocimiento es el mismo que en otros casos, se cuenta el número de publicaciones, pero no se tienen en cuenta las dificultades adicionales de este tipo de proyectos, donde lo más importante no es publicar sino producir un equipo lo más eficiente posible.  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Más información&lt;/strong&gt; Explicación detallada del funcionamiento del dispositivo: &lt;a href=&#34;http://intec.edu.do/biblioteca/cienciaysociedad/2004/Vol%FAmen%2029-%20N%FAmero%201/Una%20alternativa%20para%20producci%F3n%20de%20fr%EDo%20con%20energ%EDa%20solar%20+%20Inna%20Samson;%20Rodolfo%20Echarri.PDF&#34;&gt;Descargar »&lt;/a&gt; &lt;a href=&#34;http://www.ungs.edu.ar/&#34;&gt;Universidad Nacional de General Sarmiento&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Transportando aminoácidos en las células del corazón</title>
      <link>https://ciencianet.com.ar/post/transportando-aminoacidos-en-las-celulas-del-corazon/</link>
      <pubDate>Wed, 31 Oct 2007 01:15:38 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/transportando-aminoacidos-en-las-celulas-del-corazon/</guid>
      <description>
        
          &lt;p&gt;Los aminoácidos representan los ladrillos a partir de los cuales se construyen las proteínas en los organismos biológicos, pero adicionalmente tienen asociadas funciones biológicas más complejas que las de solo ser la mínima unidad estructural de las proteínas. Daniel Peluffo, en un estudio reciente sobre células cardíacas de rata ha descripto corrientes eléctricas generadas por el aminoácido Arginina al atravesar la membrana celular utilizando un transportador específico de aminoácidos catiónicos (CAT).&lt;/p&gt;
&lt;p&gt;Por otro lado, este investigador estudió la directa vinculación de este transporte con la producción de Oxido Nítrico, un potente vasodilatador y agente regulador de la contractibilidad y la proliferación de las células cardíacas. A nivel fisiológico el aminoácido Arginina es esencial para organismos en fase de crecimiento, esta asociado con la liberación de hormonas como insulina, glucagón y prolactina en diferentes órganos y es precursor de diversos metabolitos, entre ellos el Oxido Nítrico de particular interés por su importante función regulatoria a nivel del sistema nervioso central y del sistema cardiovascular. Gases como oxígeno y dióxido de carbono así como moléculas no polares difunden pasivamente al (o desde el) interior de las células atravesando la membrana celular.&lt;/p&gt;
&lt;p&gt;Por otro lado, las sustancias cargadas (iones) deben atravesar la barrera interpuesta por la membrana celular por medio de proteínas inmersas en dichas membranas. Estas proteínas tienen una gran variedad de composiciones, selectividad iónica y mecanismos de transporte y se clasifican, según el caso, como canales, transportadores y bombas. En el caso del transporte de Arginina (un aminoácido con carga neta positiva) el transportador es una proteína de membrana conocida como Transportador de Aminoácidos Catiónicos (CAT). Estas proteínas transportan aminoácidos al interior celular transportando al exterior una o más moléculas cargadas (iones) por cada molécula de aminoácido que ingresa a la célula. La entrada de estas moléculas cargadas a través de la membrana produce verdaderas corrientes eléctricas que pueden ser medidas y evaluadas en respuesta a diferentes estímulos.&lt;/p&gt;
&lt;p&gt;En este trabajo se evidenció experimentalmente que las corrientes generadas se deben exclusivamente al transporte del aminoácido estudiado y no a otras especies cargadas que pudieran estar presentes en el medio de medición. Adicionalmente se demostró que la producción de Oxido Nítrico esta directamente vinculada con la cantidad de L-Arginina transportada al interior celular y que el transporte del aminoácido no requiere una fuente energética como el ATP sino que utiliza el transportador propulsado por la diferencia de concentración entre el interior y el exterior celular.&lt;/p&gt;
&lt;p&gt;Finalmente también se comprobó que el transportador es estéreoselectivo, es decir que solo transporta Arginina con una orientación particular (L) en su esqueleto de carbonos, así como otros aminoácidos que posean orientaciones del mismo tipo que la L-Arginina. Toda la información recabada por Peluffo y la descripción de las características del proceso de transporte es muy valiosa ya que las células cardíacas al igual que muchas otras células del organismo son incapaces de sintetizar Arginina y dependen exclusivamente de su transporte a través de la membrana. Está demostrado que el Oxido Nítrico juega un importante papel protector a nivel cardíaco, y se ha probado que altas dosis intravenosas u orales de L-Arginina mejoran la recuperación de pacientes con falla cardíaca congestiva. La continuación de estos estudios permitirá una mejor comprensión de los efectos de la Arginina a nivel cardíaco así como el desarrollo de terapéuticas más específicas.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; R. Daniel Peluffo, &lt;a href=&#34;https://physoc.onlinelibrary.wiley.com/doi/full/10.1113/jphysiol.2006.125054&#34;&gt;L-Arginine currents in rat cardiac ventricular myocytes, Journal of Physiology (London), 2007; 580: 925-936.&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Modificación de superficies de polímeros con haces de iones pesados</title>
      <link>https://ciencianet.com.ar/post/modificacion-de-superficies-de-polimeros-con-haces-de-iones-pesados/</link>
      <pubDate>Wed, 24 Oct 2007 01:19:05 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/modificacion-de-superficies-de-polimeros-con-haces-de-iones-pesados/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Verónica Chappa:&lt;/strong&gt; Unidad de Actividad Física, Centro Atómico Constituyentes, Comisión Nacional de Energía Atómica y Departamento de Física, Facultad de Ciencias Exactas, Universidad Nacional de La Plata.&lt;/p&gt;
&lt;p&gt;Un polímero es una sustancia compuesta por la unión de cientos de miles de moléculas pequeñas denominadas monómeros, que forman enormes cadenas de las formas más diversas. Las interacciones existentes entre estas grandes cadenas hacen que los materiales poliméricos presenten propiedades físicas y químicas muy diferentes a las que poseen aquellos constituídos por moléculas más sencillas.&lt;/p&gt;
&lt;p&gt;La irradiación de polímeros con el objeto de producir materiales avanzados, los cuales responden a distintos requerimientos, es un campo en permanente desarrollo. Por ejemplo, las irradiaciones utilizando radiación gamma o electrones de altas energías constituyen actualmente técnicas convencionales utilizadas a nivel comercial. No así es la aplicación de iones pesados con estos mismos fines ya que obtener y manipular haces de iones es más complejo y por lo tanto más costoso.&lt;/p&gt;
&lt;p&gt;Los iones pesados son átomos más o menos grandes a los que les faltan o sobran electrones y por lo tanto están cargados electricamente. Luego del descubrimiento de los materiales poliméricos se hizo evidente que eran muy sensibles a todo tipo de radiación. Se observó una degradación de los mismos al iluminarlos con luz y con radiación ultra violeta, también con rayos gamma y con partículas energéticas cargadas, como electrones e iones. Los cambios producidos en el material no necesariamente tienen un carácter negativo, muchas de las nuevas propiedades que presentan los polímeros irradiados hacen que sean aptos para diversas aplicaciones tecnológicas. Esto incrementó el interés de los investigadores que iniciaron estudios sistemáticos en este campo.&lt;/p&gt;
&lt;p&gt;Desde las décadas del 50 y 60, muchos científicos se dedicaron a estudiar los procesos radioquímicos generados en los polímeros luego de irradiarlos con rayos gamma y electrones. A diferencia de las radiaciones gamma, que afectan el material como un todo, o la irradiación con un haz de electrones, los cuales penetran unos pocos milímetros de profundidad con una dirección errática, los iones pesados depositan una altísima densidad de energía en un rango de sólo unos pocos micrones. Debido a este hecho se inducen cambios físico-químicos muy complejos en la superficie del material irradiado.&lt;/p&gt;
&lt;p&gt;El polietileno de ultra alto peso molecular -UHMWPE- constituye un excelente biomaterial, ya que es biocompatible y por lo tanto no es rechazado por el cuerpo después de su implantación. Su estructura única le da excelentes propiedades, tales como bajo coeficiente de fricción al rozar contra una superficie metálica, excelente resistencia al desgaste y dureza. Debido a estas características el UHMWPE ha sido elegido como el material para las prótesis de cadera y rodilla utilizándolo durante más de 30 años.&lt;/p&gt;
&lt;p&gt;Uno de los efectos producidos en el UHMWPE por la irradiación con iones pesados es el aumento local del peso molecular mediante la ligadura de cadenas poliméricas adyacentes -&lt;em&gt;crosslinking&lt;/em&gt;-, originando un aumento de la dureza superficial del material. A diferencia de otro tipo de radiación, la acción del ion pesado es muy eficiente y localizada en la formación de crosslinking y el efecto contrario, el corte de cadenas -&lt;em&gt;scission&lt;/em&gt;- es muy reducido en el caso particular del UHMWPE. Lograr una mejora en la resistencia al desgaste de los materiales utilizados en el reemplazo total de articulaciones mediante técnicas de ingeniería de superficies permite aumentar la vida útil de estas prótesis y por lo tanto evitar intervenciones quirúrgicas necesarias para su reemplazo.&lt;/p&gt;
&lt;p&gt;En este estudio se determinó que existe un número particular e iones por unidad de área, denominado fluencia óptima, para a cual se maximiza la formación de dobles enlaces entre carbonos de una misma cadena polimérica en el UHMWPE. Esta fluencia óptima depende fuertemente no sólo del material irradiado sino también de las características de la radiación utilizada, como el tipo de ion y su energía. Este comportamiento se había encontrado previamente al estudiar la resistencia al desgaste de este mismo material al irradiarlo. Se observó una mejora en la resistencia al desgaste que aumenta con a radiación hasta un cierto valor, y luego disminuye si se continua irradiando aun más.&lt;/p&gt;
&lt;p&gt;A partir del modelado y del estudio cuidadoso de la forma de las curvas experimentales de la formación de los enlaces dobles carbono-carbono -la unión básica para formar las cadenas poliméricas- en función de la fluencia, se obtuvo información relevante sobre los efectos producidos por el ión en el polímero a nivel microscópico.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;CienciaNet&lt;/strong&gt; consultó a Guillermo Marti del Centro Atómico Constituyentes (CNEA), quien comenta &amp;quot;El estudio encarado por Verónica Chappa como tema de su tesis doctoral resulta particularmente interesante, porque en este trabajo, centrado en el estudio de las interacciones de iones pesados con la materia, además de ser muy original, utiliza una herramienta (el acelerador de iones pesados TANDAR), generalmente pensada y usada para hacer investigación básica, para desarrollar una moderna aplicación tecnológica. Por otra parte, en la misma, se balancean perfectamente aspectos teóricos y experimentales de las reacciones con iones pesados y el desarrollo del código específico de análisis de datos obtenidos en base a la técnica denominada ERDA, es un aporte significativo que puede encontrar en el futuro otras aplicaciones importantes.&amp;quot; Guillermo Marti fue uno de los jurados del trabajo de Verónica Chappa.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; Este estudio forma parte de la tesis defendida por la autora de esta nota el 27 de marzo de 2007 en la Facultad de Ciencias Exactas (UNLP), para optar al grado de Doctor. Título: &amp;quot;Aplicación de haces de iones pesados en el análisis y modificación de la superficie de materiales&amp;quot;, Director: Gerardo García Bermúdez, Copias de la tesis pueden solicitarse directamente por email a su autora, Contacto: V. Chappa (Email: &lt;a href=&#34;mailto:chappa@tandar.cnea.gov.ar&#34;&gt;chappa@tandar.cnea.gov.ar&lt;/a&gt;)&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Sempiterna aqua</title>
      <link>https://ciencianet.com.ar/post/sempiterna-aqua/</link>
      <pubDate>Wed, 03 Oct 2007 01:26:09 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/sempiterna-aqua/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Roberto Fernández Prini.&lt;/strong&gt; INQUIMAE-DQIAQF, Facultad de Ciencias Exactas y Naturales, Universidad de Buenos Aires y Unidad de Actividad Quimica, Comisión Nacional de Energía Atómica.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2007/10/agua.jpg&#34; alt=&#34;Foto de Aaron Burden en Unsplash.&#34;&gt;&lt;/p&gt;
&lt;p&gt;Las propiedades del agua líquida son consideradas anómalas. Lo son porque se comportan de manera distinta a los demás líquidos. Por ello resulta paradójico que el agua sea el único líquido puro con el que el hombre se encuentra cotidianamente y que se lo piense anómalo. El ejemplo más destacado de esa anomalía puede resumirse en que el sólido (hielo) es menos denso –flota– que el líquido.&lt;/p&gt;
&lt;p&gt;El agua debe sus propiedades &lt;em&gt;curiosas&lt;/em&gt; a su pequeño tamaño molecular y a la fuerza relativa de las interacciones que existen entre sus moléculas. Mucho se ha especulado sobre el comportamiento de esta importante sustancia dado que H2O es sinónimo de probabilidad de vida. Entonces no es tan llamativo que el coportamiento del agua genere muchas especulaciones y hallazgos que no se han demostrardo fehacientemente.&lt;/p&gt;
&lt;p&gt;La cantidad de investigaciones que se han realizado sobre los sistemas acuosos, es decir agua que contiene otras moléculas –sales, sustancias orgánicas como la urea o los alcoholes, sustancias de interés biológico como proteínas, acido desoxirribonucleico, etc.–, son numerosísimas y de muy buena calidad. Por lo tanto, queda claro que algo se puede decir con certidumbre sobre esta sustancia. En el agua hay importantes interacciones entre las moléculas, relaciones de atracción y repulsión que forman agregados de existencia efímera que perduran algunos pico segundos (billonésimos de segundo), pero no hay estructuras que permanezcan estáticas, invariables en el tiempo, más allá de ese lapso fugaz.&lt;/p&gt;
&lt;p&gt;Algunos informes científicos han sugerido que el agua tiene memoria de las moléculas han sido disueltas en ella. Este tema siempre atrae porque se lo vincula inexorablemente con la homeopatía. Que el agua no conserva estructuras en su seno depués que las moléculas que se dice que las puedan generar ya no están más en ella, está claro. Una evidencia cotidiana lo da el efecto que tienen las microondas que se utilizan para calentar agua o soluciones acuosas. El campo de electromagnético de microondas oscila unas 100.000 millones de veces por segundo y si las moléculas no tuvieran movimientos de ese mismo orden de magnitud, el agua no se calentaría.&lt;/p&gt;
&lt;p&gt;En la actualidad hay mucho interés en conocer el comportamiento del agua en sistemas confinados. Esto es importante porque su comportamiento cambia, fenómeno que también ocurre con otras sustancias, pero el agua, dadas sus características, tiene comportamientos inesperados y que es necesario conocer porque sistemas confinados que contienen H2O son comunes en la biología, en sistemas naturales y en la tecnología. Sistemas confinados son los que tienen por lo menos una dimensión que es muy pequeña, por ejemplo en el orden de algunos mil millonésimos de metro. Ejemplos son alambres o tubos muy delgados, planos o superficies muy delgadas, en estos ejemplos &lt;em&gt;muy delgado&lt;/em&gt; remite a que una de sus dimensiones, el diámetro en los cables o el espesor en las superficies, son muy pequeñas involucrando pocas moléculas.&lt;/p&gt;
&lt;p&gt;También son sistemas confinados los que están constituidos por agregados de pocas moléculas, cristalitos o gotículas. A esto debe agregarse que la fuerte interacción entre las moléculas de agua, que cuando tienen espacio (sistemas macroscópicos) se disponen &lt;em&gt;en promedio&lt;/em&gt; con un hábito tetraédrico como el hielo, constituye una razón para que el agua confinada no admita fácilmente en su seno sustancias que tienen una interacción distinta, especialmente cuando perturban su estructura. La consecuencia es que son repelidas o segregadas lo más posible para que el resto no se vea perturbado.&lt;/p&gt;
&lt;p&gt;La gran afinindad de H2O por H2O y el sostenimiento de la estructura tetraédrica cuando es posible también se observa en la superficie de sistemas macroscópicos, esa zona se llama interfaz y también confina al H2O en una dirección. Así las propiedades y las posibles reacciones químicas de H2O podrán ser distintas en las superficies y en los sistemas confinados. La gran capacidad del agua para tratar de adaptarse al medio confinante en que se encuentra tiene consecuencias interesantes. Así cuando se introduce H2O en nanotubos de carbono que tienen un diámetro de 14 nm, es decir catorce mil-millonésimas de un metro que es como 30.000 veces más pequeño que el diámetro de un cabello, las moléculas se acomodan en la superficie formando como un &lt;em&gt;forro&lt;/em&gt; interno pero, debido al confinamiento no pueden estructurarse como en el hielo, sino que se unen (por uniones hidrógeno) fomando cuadrados.&lt;/p&gt;
&lt;p&gt;Ahora bien, si se agrega más agua cuando ya está completo el forro, las nuevas moléculas H2O presentan propiedades muy distintas a las que forman el forro dado que &lt;em&gt;no ven&lt;/em&gt; a la superficie de átomos de carbono; esas moléculas centrales, que son como el relleno de un churro, tienen una velocidad de desplazamiento a lo largo del eje del nanotubo mucho mayor que las del forro. Es como si hubiera dos tipos de moléculas H2O distintas, esto porque el ambiente en que se encuentran las dos poblaciones son muy distintos. El agua pura tiene tendencia a formar espontáneamente, aunque en muy pequeña cantidad, iones OH&lt;sup&gt;-&lt;/sup&gt; y H3O&lt;sup&gt;+&lt;/sup&gt; (iones hidroxilo e hidronio o protones hidratados). Ocurre que en conglomerados de pocas moléculas de H2O que forman como gotículas el ion H3O&lt;sup&gt;+&lt;/sup&gt; sólo vive en la superficie interior del conglomerado. Esto se atribuye a que la geometría del ion hidronio es muy distinta a la de H2O y por lo tanto el hidronio es rechazado. Lo mismo se observó ya hace tiempo con la gotículas que contienen iones esféricos, los iones negativos no están en el centro de las gotículas sino que tienden a posicionarse fuera del centro, esto a pesar que tanto los iones como el conglomerado de H2O son esféricos.&lt;/p&gt;
&lt;p&gt;En el caso de que en lugar de iones, los que tienen fuerte interacción electrostática con H2O, hubiera en la gotícula un átomo neutro, como uno de gas interte, después de un breve tiempo la molécula de gas es totalmente expulsada de la gota y sólo se adhiere a su superficie exterior.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Sempiterna aqua.&lt;/em&gt; El tema es apasionante y estoy seguro que siempre estaremos preocupados por el agua; por eso esta nota, como &lt;em&gt;Finnegans Wake&lt;/em&gt;, termina como comenzó.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>El problema de Fermi, Pasta y Ulam: “Un pequeño descubrimiento”</title>
      <link>https://ciencianet.com.ar/post/el-problema-de-fermi-pasta-y-ulam-un-pequeno-descubrimiento/</link>
      <pubDate>Fri, 24 Aug 2007 01:30:37 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/el-problema-de-fermi-pasta-y-ulam-un-pequeno-descubrimiento/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Fernando Vericat:&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2007/08/maniac-1-300x224.jpg&#34; alt=&#34;MANIAC.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;A principios de 1952 la computadora MANIAC-I (Mathematical Analyzer Numerical Integrator And Calculator) fue puesta en servicio en Los Alamos. A mediados de ese mismo año, el físico Enrico Fermi decidió poner a prueba la potencialidad de la misma y propuso considerar la posibilidad de utilizarla como herramienta para investigar problemas dinámicos no lineales mediante “experimentos” numéricos. Con ese fin se asoció con el especialista en computación John Pasta y el matemático Stanislaw Ulam.&lt;/p&gt;
&lt;p&gt;El problema que eligieron para comenzar puede calificarse de modesto para los estándares actuales de computación: un conjunto de N = 32 masas puntuales iguales que pueden moverse a lo largo de una recta. Cada masa está unida a la anterior y a la siguiente mediante un resorte y los extremos de esta cadena están fijos. Los resortes fueron considerados como cuasi ideales, superponiendo a la fuerza lineal, característica de resortes que se comportan idealmente como osciladores armónicos, una pequeña fuerza perturbativa, cuadrática con la distancia entre las correspondientes masas vecinas.&lt;/p&gt;
&lt;p&gt;Sabemos de los textos de Física básica que para el sistema no-perturbado, las N acciones, además de la energía total, son también constantes de movimiento. En consecuencia, para un conjunto de acciones dadas, el sistema no puede recorrer toda la superficie de energía total constante sino que la trayectoria estará restringida a una curva específica (la curva que es la intersección de todas las superficies admitidas, una para cada acción).&lt;/p&gt;
&lt;p&gt;El sistema es completamente integrable. No existe ninguna incertidumbre acerca del movimiento. El movimiento claramente es no-ergódico. Esto significa que si distribuimos inicialmente la energía total, de manera que esté toda concentrada en un solo modo normal, ésta permanecerá siempre en él, que será por lo tanto el único modo excitado. Por supuesto que todas estas cuestiones eran bien conocidas por Fermi, Pasta y Ulam. Pero ellos esperaban que, al introducir la menor perturbación al sistema descrito por el Hamiltoniano no-perturbado, con el tiempo la energía se distribuiría equitativamente entre todos los modos normales transformándose el movimiento en ergódico.&lt;/p&gt;
&lt;p&gt;Cabe señalar a esta altura, que a fines del siglo XIX y principios del siglo XX, estaba claro que los métodos analíticos desarrollados durante los siglos anteriores, por Lagrange, Laplace, Hamilton, Jacobi, Liouville y otros ilustres físico-matemáticos tenían limitaciones de tipo operativo y que problemas aparentemente tan sencillos como el movimiento de tres cuerpos interactuando entre sí, no eran integrables y no admitían en consecuencia soluciones analíticas cerradas.&lt;/p&gt;
&lt;p&gt;Fue en esas circunstancias que el matemático francés Henri Poincaré reconoció la necesidad de utilizar un enfoque diferente para tratar sistemas dinámicos que eran, debido a su complejidad, no integrables. En lugar de tratar de obtener en forma explícita y cuantitativa las trayectorias de los sistemas dinámicos consideró la posibilidad de estudiar las propiedades de las mismas más cualitativamente desde un punto de vista geométrico y topológico.&lt;/p&gt;
&lt;p&gt;Sin embargo este enfoque, en particular, y los estudios de sistemas dinámicos clásicos, en general, se vieron relegados de la atención de los físico-matemáticos durante prácticamente toda la primera mitad del siglo XX en razón del arrasador éxito de la Mecánica Cuántica, al cual, dicho sea de paso, el propio Fermi contribuyó significativamente.&lt;/p&gt;
&lt;p&gt;Esa era, en líneas generales, la situación cuando Fermi, Pasta y Ulam realizaron la simulación numérica de la cadena de resortes. Según ya comentamos, ellos esperaban que la adición del término perturbativo, aún para una intensidad pequeña, se tradujera en la ergodicidad del sistema y la equipartición de la energía entre sus N modos normales. Sin embargo, y para su sorpresa, lo que observaron fue que, partiendo de un estado en el que toda la energía estaba concentrada en el armónico fundamental, la energía comenzaba, efectivamente a distribuirse entre los demás modos, pero esto ocurría hasta solamente el cuarto o quinto. Luego, con el tiempo, empezaba a concentrase nuevamente en el primer modo para luego recomenzar a distribuirse nuevamente entre esos pocos primeros armónicos, siguiendo un comportamiento cuasi-periódico que “modulaba” al comportamiento periódico de los modos normales no perturbados.&lt;/p&gt;
&lt;p&gt;Donde esperaban ver el desorden de la ergodicidad, ellos encontraron en realidad orden. Esto resultó inesperado para FPU a tal punto que Fermi llegó a hablar de “un pequeño descubrimiento”. Esto constituye un ejemplo, quizás el primero, de una regla bastante aceptada en los estudios modernos de sistemas no lineales y complejos: que lo interesante generalmente está en encontrar no lo que uno esperaría sino lo inesperado. Fermi murió en noviembre de 1954, y los resultados de esa primera simulación en dinámica nunca fueron formalmente publicados. Sin embargo un borrador con los mismos circuló entre unos pocos físicos y matemáticos especialistas, contribuyendo a incentivar significativamente los estudios en dinámica clásica que, incipientemente, eran retomados por algunos de ellos.&lt;/p&gt;
&lt;p&gt;Muchos de los nuevos esfuerzos fueron orientados a explicar el comportamiento del modelo de FPU lo cual a su vez generó, por retroalimentación, nuevos desarrollos. En general los intentos para resolver el problema de FPU se pueden dividir en dos grandes grupos: Uno de ellos considera al problema FPU como un claro caso de la llamada estabilidad KAM (por Kolmogorov, Arnold y Moser); el otro como un ejemplo de solitones KdV (por Korteweg-deVries). Tanto la demostración de la conjetura de Kolmogorov por Arnold y Moser (teoría KAM), dentro de la línea geométrico-topológica de Poincaré, así como la aparición de solitones como solución de la ecuación KdV, fueron publicadas en la década del 60 e inmediatamente se pensó en aplicarlos para explicar el problema de FPU.&lt;/p&gt;
&lt;p&gt;No es el espíritu de esta nota mostrar los aciertos y limitaciones de cada uno de estos (y otros) enfoques para explicar los resultados de FPU. Esto requeriría de una serie de detalles técnicos más allá de las pretensiones de la misma. Simplemente hemos querido señalar, en una perspectiva más bien histórica, la trascendencia del problema FPU dentro de una rama de la Ciencia que, desde la segunda mitad del siglo XX, viene creciendo en forma notable: la dinámica no-lineal o, en general, la física de sistemas complejos.&lt;/p&gt;
&lt;p&gt;Términos como ergódico, caos determinístico, atractores, puntos periódicos, ciclos, estabilidad, sensibilidad a condiciones iniciales, fractales, etc, se han vuelto comunes en la jerga científica. También las simulaciones numéricas mediante nuevas computadoras, con capacidades cada vez mayores, son hoy en día rutina, así como las colaboraciones entre físicos, matemáticos, programadores y científicos de los más diversos campos. El “pequeño descubrimiento” de Fermi, Pasta y Ulam puede considerarse, en muchos aspectos, como pionero en relación a esos conceptos y metodologías.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Fí­sicos juegan a los flippers para mejorar procesos industriales</title>
      <link>https://ciencianet.com.ar/post/fisicos-juegan-a-los-flippers-para-mejorar-procesos-industriales/</link>
      <pubDate>Fri, 13 Jul 2007 01:48:17 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/fisicos-juegan-a-los-flippers-para-mejorar-procesos-industriales/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Luis Pugnaloni&lt;/strong&gt;. Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP).&lt;/p&gt;
&lt;p&gt;Un grupo de investigadores publicó un estudio sobre el camino aleatorio que sigue una pelotita que rueda por una mesa inclinada con obstáculos; un fenómeno que importa a los fanáticos de los flippers. Se conoce desde hace más de un siglo que la posición donde cae la pelotita al final de la mesa tiene una distribución de probabilidad con la forma de una campana de Gauss si la pelotita nunca toca los lados de la mesa.&lt;/p&gt;
&lt;p&gt;Este nuevo estudio muestra que para mesas angostas, donde la pelotita inevitablemente choca con los bordes laterales, esta distribución es uniforme. El fenómeno puede ser usado para la mezcla de materiales granulares en la industria.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2007/07/galton-439x600.jpg&#34; alt=&#34;Tablero de Galton: Esta fotografía corresponde al tablero usado por los investigadores de la Universidad de Buenos Aires. Puede observase la disposición regular de los obstáculos.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;El trabajo de J. G. Benito, G. Meglio, I. Ippolito, M. Re y A. M. Vidales, una colaboración entre la Universidad Nacional de San Luis, la Universidad de Buenos Aires y la Universidad Nacional de Córdoba, presenta estudios experimentales, de simulación por computadora y teóricos de un dispositivo conocido desde hace más de un siglo como &amp;quot;el tablero de Galton&amp;quot; (ver foto).&lt;/p&gt;
&lt;p&gt;Un cierto número de clavos clavados a intervalos regulares sobre la mesa entorpecen el camino de una bolita que rueda cuesta abajo sobre la misma. Ya en el siglo XIX el científico británico Sir Francis Galton habí­a mostrado que si uno arroja un gran número de bolitas (una por vez) y las junta en casilleros dispuestos al final de la mesa los casilleros centrales estarían más llenos que los laterales. Esto es, hay mayor probabilidad de que una bolita caiga en un casillero en el centro que el los costados. La forma exacta de la distribución de las bolitas corresponde a una campana de Gauss.&lt;/p&gt;
&lt;p&gt;J. G. Benito y colaboradores estudiaron el efecto que produce el uso de mesas angostas donde las bolitas rebotan contra los laterales. Todos los estudios anteriores fueron hechos con mesas muy anchas. Tanto los experimentos como las simulaciones muestran que para mesas suficientemente angostas (en comparación con la longitud de la misma) las bolitas se distribuyen en forma uniforme al llegar a la base. De este modo la probabilidad de que una bolita caiga en el centro es la misma de que caiga en el borde de la mesa.&lt;/p&gt;
&lt;p&gt;Los autores del trabajo proponen que este efecto puede ser usado para mezclar granos de diferentes tipos en un proceso industrial. Es sabido que conseguir una mezcla uniforme de granos (cereales, piedras, peletes, etc.) es muy difí­cil si los granos difieren un poco en tamaño, forma o caracterí­sticas superficiales. Una de estas mesas podría usarse para hacer fluir simultáneamente granos de diferentes tipos de modo que las colisiones con los obstáculos los mezclen unos con otros.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; &lt;a href=&#34;https://doi.org/10.1007/s10035-007-0036-4&#34;&gt;Granular Matter, vol. 9, pp 159 (2007)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instituciones:&lt;/strong&gt; Laboratorio de Ciencia de Superficies y Medios Porosos, Departamento de Fí­sica (UNSL), Grupo de Medios Porosos, Facultad de Ingenierí­a (UBA), Facultad de Matemá¡tica, Astronomí­a y Fí­sica, (UNC).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; A. Vidales (E-mail: &lt;a href=&#34;mailto:avidales@unsl.edu.ar&#34;&gt;avidales@unsl.edu.ar&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Más información:&lt;/strong&gt; &lt;a href=&#34;http://es.wikipedia.org/wiki/Distribuci%C3%B3n_de_probabilidad&#34;&gt;Distribución de probabilidad&lt;/a&gt;.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Modelo matemático para el tratamiento de quimioterapia sobre tumores cancerosos</title>
      <link>https://ciencianet.com.ar/post/modelo-matematico-para-el-tratamiento-de-quimioterapia-sobre-tumores-cancerosos/</link>
      <pubDate>Wed, 27 Jun 2007 01:54:52 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/modelo-matematico-para-el-tratamiento-de-quimioterapia-sobre-tumores-cancerosos/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Luis Pugnaloni.&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP).&lt;/p&gt;
&lt;p&gt;El cáncer es originado por la mutación en una única célula que exhibe un crecimiento descontrolado, rompiendo la cooperación que mantiene la integridad del organismo. Un modelo matemático permite simular el crecimiento de un tumor de células cancerosas y los efectos que la quimioterapia tiene sobre él.&lt;/p&gt;
&lt;p&gt;El trabajo de S. Menchón y C. Condat, de la Facultad de Matemática, Astronomía y Física de la Universidad Nacional de Córdoba, presenta un modelo mesoscópico de comportamiento de un tumor. Los casilleros de una gran grilla se llenan con números que representan la cantidad de células cancerosas vivas, células cancerosas muertas y nutrientes disponibles en una región del tejido afectado. Una simulación por computadora se ocupa de ir cambiando estos valores según las células se alimentan reproducen, migran de una región a otra y mueren. Las células se reproducen cuando la cantidad de alimentos es abundante y migran o mueren cuando los alimentos escasean. El tejido así simulado es alimentado por un vaso sanguíneo próximo que suministra nutrientes en forma constante.&lt;/p&gt;
&lt;p&gt;La simulación muestra que, en determinadas condiciones, durante su crecimiento, un tumor desarrolla una forma redondeada con células proliferativas (que se reproducen mucho) sobre su superficie dado que allí hay más nutrientes. Hacia en el centro del tumor las células disponen de menos alimento dado que las de la periferia lo consumen rápidamente y por lo tanto no consiguen reproducirse e incluso mueren. El tamaño del tumor crece inicialmente en forma exponencial.  &lt;/p&gt;
&lt;p&gt;Más interesante aún, los autores simulan el efecto de la introducción de una droga mezclada entre los nutrientes que tiene el efecto de inducir la muerte de las células cancerosas. Aquí resulta fundamental el hecho de que algunas células cancerosas suelen ser intrínsecamente resistentes a la droga. Cuando esto sucede, las simulaciones muestran que estas células regeneran el tumor atacado por la droga que ya no será sensible al tratamiento dado que las células que lo forman nacieron de madres resistentes. También es posible que algunas células adquieran resistencia después de que el tumor sea expuesto a la droga a causa de mutaciones genéticas. Las simulaciones muestran que la resistencia adquirida suele ser un mejor mecanismo de defensa del tumor que la resistencia intrínseca ya que los tumores crecen más rápidamente en el primer caso.&lt;/p&gt;
&lt;p&gt;Las dosis de drogas antitumorales se aplican generalmente espaciadas por varios días para minimizar los efectos colaterales que éstas conllevan. En el período comprendido entre dos aplicaciones, si un tumor presenta resistencia (intrínseca o adquirida), el cáncer puede volver a crecer. Aún cuando la siguiente dosis vuelve a matar muchas células, el tumor se ha vuelto más resistente y la terapia comienza a ser menos efectiva. La alternancia de tipos de drogas es una estrategia muy común para combatir la resistencia intrínseca o adquirida.&lt;/p&gt;
&lt;p&gt;La autora de la tesis sugiere que, luego de un desarrollo más completo (como la inclusión de mezclas de drogas), estas simulaciones podrían ser usadas para estudiar diferentes planes de tratamientos quimioterapéuticos según el tipo de tumor de cada paciente para analizar cuál resultaría más efectivo antes de realizar la terapia.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; S. A. Menchón, &lt;a href=&#34;https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=2ahUKEwiP_c3UlfTtAhX5IbkGHerlAJYQFjAAegQIAhAC&amp;amp;url=https%3A%2F%2Fwww.famaf.unc.edu.ar%2Fdocuments%2F1037%2FDFis126.pdf&amp;amp;usg=AOvVaw3_86RGOwaMHYDffuHO2jnt&#34;&gt;&lt;em&gt;Modelado de las diversas etapas del crecimiento del cancer y de algunas terapias antitumorales&lt;/em&gt;&lt;/a&gt;, Tesis doctoral, Universidad Nacional de Córdoba.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instituciones:&lt;/strong&gt; &lt;a href=&#34;http://www.famaf.unc.edu.ar/&#34;&gt;Facultad de Matemática, Astronomía y Física&lt;/a&gt; (UNC)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; S. Menchón (E-mail: &lt;a href=&#34;mailto:silmenchon@gmail.com&#34;&gt;silmenchon@gmail.com&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Más información:&lt;/strong&gt; &lt;a href=&#34;http://es.wikipedia.org/wiki/C%C3%A1ncer_%28enfermedad%29&#34;&gt;Cáncer&lt;/a&gt;, &lt;a href=&#34;http://es.wikipedia.org/wiki/Quimioterapia&#34;&gt;Quimioterapia&lt;/a&gt; .&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>¿Wi Tricity? Transferencia de potencia sin cables</title>
      <link>https://ciencianet.com.ar/post/wi-tricity-transferencia-de-potencia-sin-cables/</link>
      <pubDate>Sat, 16 Jun 2007 01:57:04 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/wi-tricity-transferencia-de-potencia-sin-cables/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Ramiro Irastorza.&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP).&lt;/p&gt;
&lt;p&gt;Imagine un futuro en el que sea posible la transmisión de potencia a través del aire: teléfonos celulares (o móviles), robots hogareños, reproductores de mp3, computadoras personales y otros dispositivos electrónicos portátiles que sean capaces de recargar sus baterías por sí solos sin que tengamos que conectar cable ni cargador alguno. Un equipo del MIT (integrado por el departamento de Física, el de Ingeniería Eléctrica y el de Ciencias de la Computación) en conjunto con el &lt;em&gt;Institute for Soldier Nanotechnologies&lt;/em&gt; (ISN) desarrolló experimentalmente un importante paso en esa dirección. Usando bobinados resonantes fuertemente acoplados, demostraron una transferencia no radiante y eficiente (40%) a distancias de hasta ocho veces el radio de los resonadores (ellos mismos la denominaron WiTricity). Desarrollaron un modelo cuantitativo capáz de describir la transferencia de potencia con errores de solo el 5%.&lt;/p&gt;
&lt;p&gt;La transmisión de potencia sin cable es conocida hace ya varias décadas, quizas el mejor ejemplo sea la radiación electromagnética (ondas de radio). Si bien este tipo de transferencia es buena para la información no es eficiente para la transmisión de potencia. Gran parte de de la misma se perdería en el espacio porque la radiación se emite en todas direcciones. Por el contrario, WiTricity se basa en el uso de objetos resonantes acoplados. Dos objetos resonantes acoplados con la misma frecuencia intercambian energía de manera eficiente (por ejemplo: un transformador), si en cambio no tienen la misma frecuencia de resonancia la interacción es débil. Un ejemplo acústico: imagine una habitación con vasos de vino cada llenados con distintos niveles. Si un cantante de opera emite una nota lo suficientemente alta, será capaz de romper el vaso &amp;quot;afinado&amp;quot; a esa nota mientras que los otros vasos permanecen inadvertidos.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2007/06/witricity.png&#34; alt=&#34;Esquema del dispositivo experimental.&#34;&gt;&lt;/p&gt;
&lt;p&gt;Este grupo de investigadores se concentró en un sistema de dos bobinados de cobre (uno conectado a la fuente y el receptor a una bombilla de 60W, ver Imagen) acoplados fuertemente a través de sus campos magnéticos (a una frecuencia del orden de los MHz).&lt;/p&gt;
&lt;p&gt;Lo interesante de esto es que los medios biológicos interactúan muy debilmente con los campos magnéticos , así como también los objetos no acoplados, por lo que sería una forma segura de transmisión. Además, al ser no radiativa, la energía que no se transfiere al bobinado receptor queda limitada en el bobinado fuente.&lt;/p&gt;
&lt;p&gt;Si bien el principio es conocido hace más de un siglo la demanda de este tipo de transmisión es reciente. No existían dispositivos electrónicos portátiles, laptops, teléfonos celulares, robots, etc. Este tipo de invenciones serviría para recargar las batería de forma automática o, eventualmente, ni si quiera utilizarlas.&lt;/p&gt;
&lt;p&gt;Este artículo fue reportado el 7 de junio en Science Express, la publicación online del journal Science.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fuente:&lt;/strong&gt; &lt;a href=&#34;http://www.sciencedaily.com/releases/2007/06/070607171130.htm&#34;&gt;ScienceDaily&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; André Kurs, Aristeidis Karalis, Robert Moffatt, J. D. Joannopoulos, Peter Fisher, Marin Soljačić. &lt;a href=&#34;https://science.sciencemag.org/content/317/5834/83&#34;&gt;Wireless Power Transfer via Strongly Coupled Magnetic Resonances&lt;/a&gt;. Science, 06 Jul 2007 : 83-86.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Las señales de Calcio intracelular: Un lenguaje en la intimidad de las células. Un estudio matemático</title>
      <link>https://ciencianet.com.ar/post/las-senales-de-calcio-intracelular-un-lenguaje-en-la-intimidad-de-las-celulas-un-estudio-matematico/</link>
      <pubDate>Fri, 27 Apr 2007 02:21:49 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/las-senales-de-calcio-intracelular-un-lenguaje-en-la-intimidad-de-las-celulas-un-estudio-matematico/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Osvaldo Chara.&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP).&lt;/p&gt;
&lt;p&gt;La dinámica del Calcio intracelular provee un muy interesante ejemplo en el cual la controversia “simple” versus “complejo” puede ser investigada. En la ciencia pueden verse dos casos límites en la exploración del conocimiento. Estos dos paradigmas pueden encontrarse en la física y en la biología. Clásicamente, la física suele llevar a cabo aproximaciones que tienen una tendencia hacia la simplicidad. Por el contrario, las aproximaciones de la biología suelen tener una tendencia a la complejidad.&lt;/p&gt;
&lt;p&gt;Una manera de establecer una suerte de puente entre estos dos tipos de aproximaciones (y entre estas dos disciplinas) es llevar a cabo modelización matemática inspirándose en problemas físicos pero en el contexto de un problema biológico. La idea, simplemente, es llevar a cabo un modelo matemático, descrito por alguna o algunas expresiones matemáticas, que de cuenta del fenómeno biológico en estudio. Recientemente, Alejandra Ventura y colaboradores (2006), en el grupo de Silvina Ponce Dawson, en el Departamento de Física de la Facultad de Ciencias Exactas y Naturales, UBA estudiaron el calcio intracelular siguiendo esta línea de pensamiento.&lt;/p&gt;
&lt;p&gt;El calcio es un ión que aparece como un mensajero intracelular. Ciertos eventos extracelulares generan cambios en la concentración de este ión que son interpretados por la maquinaria celular promoviendo comportamientos diversos. De este modo, el Calcio participa de un “lenguaje intracelular”. Una forma de comprender este lenguaje que es hablado permanentemente por las estructuras de nuestras células es hacer un modelo matemático que de cuenta del comportamiento celular observado.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2007/04/ventura.jpg&#34; alt=&#34;Calcio intracelular: Se muestran las fuentes y sumideros del catión.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Calcio intracelular:&lt;/strong&gt; La concentración de este ión libre resulta de la combinación de los aportes provenientes de bombas, transportadores, reservorios o &lt;em&gt;buffers&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Ahora bien, el calcio puede ingresar a las células a través de canales de calcio insertados en la membrana plasmática o en membranas intracelulares. Una vez dentro del citoplasma el calcio puede difundir o unirse a estructuras móviles o inmóviles que pueden atraparlo por un tiempo breve y luego soltarlo nuevamente. Estas estructuras son generalmente proteínas y se denominan en inglés &lt;em&gt;buffers&lt;/em&gt; (podríamos traducirlos como amortiguadores, pues en virtud de su capacidad de almacenar calcio podrían amortiguar los cambios de la concentración intracelular de calcio). Por otro lado pueden ser exportadas hacia fuera de las células mediante otras proteínas llamadas transportadores o bombas. De este modo, hacer un modelo matemático de la concentración intracelular de calcio en todo el espacio interior de la célula en función del tiempo implica incorporar todos estos procesos. Para ello se puede emplear un sistema de ecuaciones llamadas de Reacción-Difusión, como ya fuera llevado a cabo previamente, incorporando con sumo detalle todos y cada uno de los procesos comentados.&lt;/p&gt;
&lt;p&gt;El problema, como puede imaginarse, es que el modelo puede tornarse enormemente complicado. La pregunta que intentan responder los autores es ¿Cuánto detalle deberían incorporar en un modelo para extraer información cuantitativa de los experimentos? Los autores proponen que modelos muy simples – que no incorporen demasiado detalle de los procesos arriba mencionados – podrían reproducir no sólo información cualitativa sino también cuantitativa. Ahora bien, ¿cómo construir modelos “simples”? Una forma que ya fuera explorada previamente, es reducir un modelo complicado a uno más sencillo. Otra forma es llevar a cabo modelos conducidos por experimentos (&lt;em&gt;data-driven models&lt;/em&gt; en inglés). Ventura y colaboradores llevaron a cabo este tipo de modelización para estudiar las “señales de Calcio”, es decir, el cambio de la concentración intracelular del calcio como respuesta a la entrada del propio Calcio a la célula.&lt;/p&gt;
&lt;p&gt;Las señales de calcio pueden tener distintas formas y los modelos conducidos por experimentos deben incorporar estas formas con funciones o, al menos, valores de parámetros diferentes a partir de soluciones individuales. La ventaja práctica de esta aproximación a las señales de calcio es obtener la amplitud y las cinéticas de la concentración de calcio intracelular que corresponde a cada observación experimental. De este modo la función que describe la entrada de Calcio a la célula no se propone &lt;em&gt;a priori&lt;/em&gt;, sino que se obtiene por ajuste de los datos experimentales.&lt;/p&gt;
&lt;p&gt;Los autores muestran que un sencillo modelo, cuyos valores de los parámetros se ajustan a partir de los datos experimentales, puede reproducir ciertas señales de calcio observadas experimentalmente. Interesantemente, este modelo no solo describe cualitativamente sino también cuantitativamente resultados experimentales, intentando conciliar la aparente dicotomía de “simple” versus “complejo”.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; Ventura, A.C., Bruno, L. &amp;amp; Ponce Dawson, S. 2006. Simple data-driven models of intracellular calcium dynamics with predictive power. &lt;a href=&#34;https://doi.org/10.1103/PhysRevE.74.011917&#34;&gt;Phys. Rev. E 74, 011917-1:11&lt;/a&gt;.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Reptiles moleculares</title>
      <link>https://ciencianet.com.ar/post/reptiles-moleculares/</link>
      <pubDate>Wed, 28 Mar 2007 02:39:00 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/reptiles-moleculares/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Luis Pugnaloni.&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP).&lt;/p&gt;
&lt;p&gt;Un estudio teórico muestra cómo moléculas largas en forma de cadena reptan como gusanos al ser puestas bajo un campo eléctrico. Un modelo muy simple de cuentas de collar que se mueven a lo largo de una recta ayuda a analizar la velocidad con que repta una molécula polimérica bajo la acción de un campo eléctrico. El trabajo de G. Terranova, H. O. Mártin y C. M. Aldao, investigadores de la Universidad Nacional de Mar del Plata y del CONICET, presenta resultados analíticos para un modelo muy simple de una cadena de partículas de longitud arbitraria que se mueve en una red unidimensional bajo la acción conjunta de una fuerza externa y de la agitación browniana.&lt;/p&gt;
&lt;p&gt;Los investigadores consideran tres casos: cuando la fuerza es aplicada a todas las partículas (&amp;quot;uniforme&amp;quot;), cuando la fuerza tíra desde la cabeza (&amp;quot;de tiro&amp;quot;) y cuando la fuerza empuja desde la cola del polímero (&amp;quot;de empuje&amp;quot;). El primer caso representaría una molecula bajo un campo eléctrico que posee cargas distribuídas a lo largo de la cadena. Los dos restantes corresponden a moléculas donde la carga está concentrada en un extremo.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2007/03/reptiles_moleculares.jpg&#34; alt=&#34;Reptación: Una molécula de tres unidades funcionales se estira y encoge mientras avanza de izquierda a derecha.:left&#34;&gt; Los resultados muestran que para el caso &amp;quot;uniforme&amp;quot; la velocidad del polímero no depende de la longitud de la cadena, mientras que para los casos &amp;quot;de tiro&amp;quot; &amp;quot;de empuje&amp;quot; las cadenas más cortas se mueven más rápido bajo la acción de la fuerza externa. Este efecto se debe a que cuando la cabeza se mueve un paso necesita que todo el resto de la molécula se acomode para poder dar el siguiente; cuanto más larga la molécula más tiempo lleva el proceso.&lt;/p&gt;
&lt;p&gt;Otro resultado intersante es que para el caso &amp;quot;uniforme&amp;quot; la velocidad de reptación es directamente proporcional a la fuerza aplicada. Para los casos &amp;quot;de tiro&amp;quot; o &amp;quot;de empuje&amp;quot; la velocidad crece con la fuerza aplicada pero llega a un valor máximo límite para fuerzas muy grandes. El estudio proporciona ecuaciones simples para un modelo básico del problema de reptación bajo una fuerza externa que es fundamental en aplicaciones tecnológicas como la electroforesis usada para separar moléculas por tamaños en laboratorios de todo el mundo.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; &lt;a href=&#34;https://doi.org/10.1103/PhysRevE.74.021116&#34;&gt;Physical Review E 74, 021116 (2006)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instituciones:&lt;/strong&gt; &lt;a href=&#34;http://www.intema.gov.ar/&#34;&gt;INTEMA&lt;/a&gt; , &lt;a href=&#34;http://www.mdp.edu.ar/&#34;&gt;Dpto. de Física UNMDP&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Más información:&lt;/strong&gt; &lt;a href=&#34;http://es.wikipedia.org/wiki/Pol%C3%ADmero&#34;&gt;Polímeros&lt;/a&gt; , &lt;a href=&#34;http://prola.aps.org/abstract/PRL/v62/i24/p2877_1&#34;&gt;Modelo del reptón&lt;/a&gt; , &lt;a href=&#34;http://es.wikipedia.org/wiki/Electroforesis&#34;&gt;Electroforesis&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Nanosondas calentadas destruyen células de cáncer de mama en ratones</title>
      <link>https://ciencianet.com.ar/post/nanosondas-calentadas-destruyen-celulas-de-cancer-de-mama-en-ratones/</link>
      <pubDate>Sun, 18 Mar 2007 02:41:23 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/nanosondas-calentadas-destruyen-celulas-de-cancer-de-mama-en-ratones/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro.&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP) y Universidad Tecnológica Nacional.&lt;/p&gt;
&lt;p&gt;En experimentos con ratones de laboratorio que llevan un agresivo cáncer de mama humano, investigadores de la Universidad de California (UC Davis) utilizaron nanosondas calientes para disminuir el crecimiento de tumores sin dañar el tejido sano que los rodean. Los investigadores describen su trabajo en la edición de marzo del &lt;a href=&#34;http://jnm.snmjournals.org/cgi/content/abstract/48/3/437?maxtoshow=&amp;amp;HITS=10&amp;amp;hits=10&amp;amp;RESULTFORMAT=&amp;amp;author1=DeNardo&amp;amp;searchid=1&amp;amp;FIRSTINDEX=0&amp;amp;sortspec=relevance&amp;amp;resourcetype=HWCIT&#34;&gt;Journal of Nuclear Medicine&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2017/12/nanosondas-CN.jpg&#34; alt=&#34; Nanosonda: (ScienceDaily).:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;&amp;quot;Hemos demostrado que el sistema es factible en ratones de laboratorio. El próximo paso será la prueba clínica en pacientes&amp;quot;, dice Sally DeNardo, proesora de medicina interna y radiología en UC Davis y autora principal del estudio.&lt;/p&gt;
&lt;p&gt;Muchos investigadores han estudiado el calor como tratamiento potencial del cáncer, pero la dificultad de confinar calor dentro del tumor y la predicción de una dosis de calor efectiva han limitado su uso. La investigación en UC Davis, realizada en colaboración con científicos de Triton BioSystems en Boston, pareciera haber resuelto estos problemas.&lt;/p&gt;
&lt;p&gt;El sistema experimental utiliza biosondas creadas uniendo nanoesferas magentizadas de óxido de hierro con anticuerpos monoclonales radiomarcados. Las biosondas son camufladas en polímeros y azúcares que las hacen casi invisibles al sistema inmunológico corporal.&lt;/p&gt;
&lt;p&gt;DeNardo y sus colegas inyectaron miles de millones de sondas (más de 10.000 pueden ubicarse en la cabeza de un alfiler) en los flujos sanguíneos de los ratones de laboratorio que portan tumores de cáncer de mama humanos. Una vez en el flujo sanguíneo, las sondas localizan y se adhieren a receptores en la superficie de las células malignas.&lt;/p&gt;
&lt;p&gt;Tres días después, el equipo aplicó un campo magnético alterno (CMA) a la región del tumor, causando que las nanoesferas magnéticas adheridas a las células del tumor cambien su polaridad miles de veces por segundo, generando calor instantáneamente. Tan pronto como el CMA se detiene, las biosondas se enfrían.&lt;/p&gt;
&lt;p&gt;Los ratones del estudio recibieron series de ráfagas de CMA en tratamientos únicos de 20 minutos. La dosis fue calculada utilizando una ecuación que incluye la concentración de biosondas en el tumor, la velocidad de calentamiento de las partículas a diferentes amplitudes, y el intervalo entre las ráfagas de CMA.&lt;/p&gt;
&lt;p&gt;La velocidad de crecimiento del tumor disminuyó en los animales tratados, una respuesta estrechamente correlacionada con la dosis de calor. No se observó ninguna toxicidad relacionada con las biosondas.&lt;/p&gt;
&lt;p&gt;&amp;quot;El uso de calor para matar células cancerígenas no es un concepto nuevo&amp;quot;, dice DeNardo.&amp;quot;Los mayores problemas han sido cómo aplicarlo solo al tumor, cómo predecir la cantidad necesaria y cómo determinar su efectividad. Combinando nanotecnología, la terapia focalizada de CMA y técnicas de imagen molecular cuantitativas, hemos desarrollado una técnica segura que podría combinarse con otras modalidades para el tratamiento del cáncer de mama y otros tipos de cáncer&amp;quot;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fuente:&lt;/strong&gt; &lt;a href=&#34;http://www.sciencedaily.com/releases/2007/03/070307075602.htm&#34;&gt;ScienceDaily&lt;/a&gt;.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Péndulo oscila lejos de la energía oscura</title>
      <link>https://ciencianet.com.ar/post/pendulo-oscila-lejos-de-la-energia-oscura/</link>
      <pubDate>Sat, 17 Mar 2007 02:45:48 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/pendulo-oscila-lejos-de-la-energia-oscura/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro.&lt;/strong&gt; Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP) y Universidad Tecnológica Nacional.&lt;/p&gt;
&lt;p&gt;Físicos en USA encontraron que la ley de gravedad de Newton es válida con un alto nivel de precisión a distancias tan pequeñas como 55 micrómetros. Las mediciones fueron realizadas con una balanza de torsión y muestran que no hay evidencia de que la &amp;quot;energía oscura&amp;quot; debilite la atracción gravitacional a esta escala. El resultado es un revés en la búsqueda de los efectos gravitacionales de la energía oscura, que los cosmólogos creen deberían aparecer a estas distancias.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ciencianet.com.ar/images/2007/03/torsion_0.jpg&#34; alt=&#34;Péndulo de torsión.:left&#34;&gt;&lt;/p&gt;
&lt;p&gt;Un misterio importante que enfrentan los cosmólogos es que la velocidad de expansión del universo parece incrementarse en el tiempo. Los físicos habrían esperado que esta velocidad disminuya debido a que la energía finita de expansión es superada por la atracción gravitacional que mantiene al universo unido. Los cosmólogos han tratado de explicar el aumento en la velocidad en términos de la &amp;quot;energía oscura&amp;quot;, que sostiene la expansión del universo contrarrestando el efecto de la gravedad. Para ser efectiva, la energía oscura debería dar cuenta de aproximadamente el 70% de toda la energía del universo, pero aún no ha sido observado directamente.&lt;/p&gt;
&lt;p&gt;Los cosmólogos han calculado que una nueva fuerza asociada con la energía oscura debería manifestarse a distancias relativamente cortas -del orden de 85 micrómetros. Surge entonces que uno de los mejores lugares para buscar evidencias de la energía oscura a estas distancias no es en el espacio profundo exterior, sino en un simple exprerimento de laboratorio que mida la atracción gravitacional entre dos placas, con el propósito de encontrar alguna desviación de la clásica ley del inverso del cuadrado de la distancia.&lt;/p&gt;
&lt;p&gt;Dan Kapner y colegas de la Universidad de Washington utilizaron una balanza de torsión para medir la fuerza de gravedad a 55 micrómetros de distancia y encontraron que obedece la ley del cuadrado de la distancia más allá de los 85 micrómetros con una certeza del 95%. Aunque esto no invalida la teoría de la energía oscura, permitió a Kapner y sus colegas concluir que la intensidad de la nueva fuerza gravitacional no aparece a dicha escala.&lt;/p&gt;
&lt;p&gt;Aunque otros grupos han medido la fuerza de gravedad a distancias aún menores, Kapner afirma que el experimento de Washinton ofrece la más alta sensibilidad a la escala de longitudes asociada con la energía oscura. Esto es debido a que hace interactuar más masa a las distancias requeridas que otras mediciones. Los investigadores ahora están construyendo una balanza de torsión mejorada que podría aumentar la sensibilidad de las mediciones por un factor de 100.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fuente:&lt;/strong&gt; &lt;a href=&#34;https://physicsworld.com/a/pendulum-swings-away-from-dark-energy/&#34;&gt;physicsworld&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; &lt;a href=&#34;https://doi.org/10.1103/PhysRevLett.98.021101&#34;&gt;D. J. Kapner, T. S. Cook, E. G. Adelberger, J. H. Gundlach, B. R. Heckel, C. D. Hoyle, y H. E. Swanson
Phys. Rev. Lett. 98, 021101&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Electrones forman un cristal líquido</title>
      <link>https://ciencianet.com.ar/post/electrones-forman-un-cristal-liquido/</link>
      <pubDate>Thu, 15 Mar 2007 02:48:39 +0000</pubDate>
      
      <guid>https://ciencianet.com.ar/post/electrones-forman-un-cristal-liquido/</guid>
      <description>
        
          &lt;p&gt;&lt;strong&gt;Manuel Carlevaro&lt;/strong&gt;. Instituto de Física de Líquidos y Sistemas Biológicos (CONICET - UNLP).&lt;/p&gt;
&lt;p&gt;Un experimento realizado sobre el óxido de un metal de transición muestra que en condiciones muy especiales un líquido de electrones presenta resistencias diferentes en direcciones diferentes. Es la primera evidencia directa de la existencia de una fase con anisotropía (nemática) en un líquido de electrones. El trabajo de R. A. Borzi y S. A. Grigera, de la Universidad Nacional de La Plata y el CONICET, en colaboración con J. Farrell, R. S. Perry, S. J. S. Lister, S. I. Lee, D. A. Tennant, Y. Maeno y A. P. Mackenzie, presenta medidas de resistencia eléctrica en cristales de Sr3Ru2O7 de alta pureza.&lt;/p&gt;
&lt;p&gt;Para un rango muy estrecho campos magnéticos aplicados y a temperaturas inferiores a 1 K, en la vecindad de un punto crítico metamagnético, la resistencia eléctrica en una dirección es dos veces mayor que en la dirección perpendicular a ésta. Este tipo de anisotropía ha sido observada con anterioridad pero el efecto podría explicarse si fuera producido por deformaciones espontáneas del cristal formado por los iones del material y no debida a los electrones de conducción.&lt;/p&gt;
&lt;p&gt;Los investigadores muestran que para este material la deformación del cristal de iones es imperceptible y que el efecto obtenido se debe esencialmente a la formación de una nueva fase del fluido de electrones de conducción. Los líquidos anisótropos o nemáticos son comunes en fluidos moleculares. Las moléculas con formas alargadas tienden a alinearse unas con otras de modo que las propiedades del fluido son diferentes en la dirección paralela al eje longitudinal de las moléculas y en la dirección perpendicular a esta. Los electrones en cambio son partículas puntuales y no pueden orientarse.&lt;/p&gt;
&lt;p&gt;Sin embargo, según comenta E. Fradkin de la Universidad de Illinois, los electrones podrían ordenarse en forma de una red regular anisótropa: con forma tiras en una dirección. Si esta red de electrones se desordena parcialmente, en un sentido cuántico, se podría transformar en un fluido de tiras de electrones. Estas tiras de electrones se comportarían como las moléculas alargadas de un líquido nemático clásico.&lt;/p&gt;
&lt;p&gt;Este modelo explicaría los hallazgos de Borzi y colaboradores. Los autores del trabajo aseguran que algunas características del fenómeno sugieren que la formación de un líquido de electrones nemático puede presentarse en otros materiales que se usan para la fabricación de componentes electrónicos como el arseniuro de galio (GaAs). R. Borzi comenta sobre este punto: &amp;quot;Yo diría que intentamos unir intuitivamente, con éxito modesto, lo que ocurre en ambos sistemas: el nuestro y otro más exótico, basado en GaAs&amp;quot;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trabajo original:&lt;/strong&gt; &lt;a href=&#34;http://www.sciencemag.org/cgi/content/abstract/315/5809/214&#34;&gt;Science, vol. 315, pp. 214 (2007)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instituciones:&lt;/strong&gt; &lt;a href=&#34;http://www.inifta.unlp.edu.ar/&#34;&gt;Instituto de Investigaciones Fisicoquímicas Teóricas y Aplicadas&lt;/a&gt; (&lt;a href=&#34;https://ciencianet.com.ar/www.conicet.gov.ar&#34;&gt;CONICET&lt;/a&gt; ), &lt;a href=&#34;https://ciencianet.com.ar/www.fisica.unlp.edu.ar&#34;&gt;Departamento de Física de La Plata&lt;/a&gt; (&lt;a href=&#34;https://ciencianet.com.ar/www.unlp.edu.ar/&#34;&gt;UNLP&lt;/a&gt; ), &lt;a href=&#34;https://ciencianet.com.ar/www.iflysib.unlp.edu.ar&#34;&gt;Instituto de Física de Líquidos y Sistemas Biológicos&lt;/a&gt; (CONICET), &lt;a href=&#34;http://www.st-andrews.ac.uk/physics/&#34;&gt;Escuela de Física y Astronomía de San Andrés&lt;/a&gt; (Escocia), &lt;a href=&#34;http://www.hmi.de/&#34;&gt;Instituto Hahn-Meitner&lt;/a&gt; (Alemania), &lt;a href=&#34;http://www.scphys.kyoto-u.ac.jp/index-e.html&#34;&gt;Departamento de Física de Kyoto&lt;/a&gt; (Japón).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contacto:&lt;/strong&gt; R. A. Borzi (E-mail: &lt;a href=&#34;https://ciencianet.com.ar/r.chufo@gmail.com&#34;&gt;r.chufo@gmail.com&lt;/a&gt; )&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Más información:&lt;/strong&gt; &lt;a href=&#34;http://www.sciencemag.org/cgi/content/summary/sci;315/5809/196&#34;&gt;Comentarios de E. Fradkin y colaboradores&lt;/a&gt; , &lt;a href=&#34;http://es.wikipedia.org/wiki/Cristal_l%C3%ADquido&#34;&gt;cristales líquidos&lt;/a&gt; .&lt;/p&gt;

        
      </description>
    </item>
    
  </channel>
</rss>
